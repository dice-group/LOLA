cpu-bind=MASK - n2gpu1213, task  0  0 [2133123]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||-B------|--------||--------|--------||--------|--------||--------|--------|  set
Total estimated parameters in the Dense GPT-2 model: 125226240 (0.13B)
Total Estimated Parameters in the Sparse(MoE) GPT-2 model: 3752054016 (3.75B)
LAUNCHER: python -u -m torch.distributed.run --nproc_per_node 1 --nnodes 1 --rdzv_id=20366 --rdzv_endpoint n2gpu1213:6008 --rdzv_backend c10d --max_restarts 0 --tee 3
CMD: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/pretrain_gpt.py --override-opt_param-scheduler --adam-beta1 0.9 --adam-beta2 0.95 --tensor-model-parallel-size 1 --moe-expert-parallel-size 1 --num-experts 128 --moe-loss-coeff 0.01 --moe-train-capacity-factor 1.0 --moe-eval-capacity-factor 1.0 --moe-min-capacity 4 --init-method-std 0.01 --lr-decay-tokens 300000000000 --lr-warmup-tokens 375000000 --micro-batch-size 8 --exit-duration-in-mins 30000000 --global-batch-size 8 --num-layers 12 --hidden-size 768 --num-attention-heads 12 --seq-length 2048 --max-position-embeddings 2048 --train-tokens 300000000000 --train-iters 54931640 --lr 2.0e-4 --min-lr 2e-06 --lr-decay-style cosine --split 98,2,0 --log-interval 5 --eval-interval 100 --eval-iters 50 --save-interval 1000000 --weight-decay 0.1 --clip-grad 1.0 --hysteresis 2 --num-workers 0 --fp16 --load /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_8BATCH_1GPU_1Node/checkpoint/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-8-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true --save /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_8BATCH_1GPU_1Node/checkpoint/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-8-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true --tensorboard-queue-size 1 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_8BATCH_1GPU_1Node/tensorboard/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-8-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true_n2gpu1213_2023.07.30-22.21.11 --checkpoint-activations --create-moe-param-group --vocab-file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-vocab.json --merge-file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-merges.txt --data-path /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/meg-gpt2-oscar-en-10k_text_document --data-impl mmap --deepspeed --deepspeed_config /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_8BATCH_1GPU_1Node/ds_config_gpt_gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-8-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true.json --pipeline-model-parallel-size 1 --no-pipeline-parallel --deepspeed-activation-checkpointing
cpu-bind=MASK - n2gpu1213, task  0  0 [2133365]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||-B------|--------||--------|--------||--------|--------||--------|--------|  set
[default0]:[2023-07-30 22:21:13,896] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.10.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:**** Git info for Megatron: git_hash=6e47d55 git_branch=main ****
[default0]:using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
[default0]:using torch.float16 for parameters ...
[default0]:------------------------ arguments ------------------------
[default0]:  accumulate_allreduce_grads_in_fp32 .............. False
[default0]:  adam_beta1 ...................................... 0.9
[default0]:  adam_beta2 ...................................... 0.95
[default0]:  adam_eps ........................................ 1e-08
[default0]:  add_bias_linear ................................. True
[default0]:  add_position_embedding .......................... True
[default0]:  adlr_autoresume ................................. False
[default0]:  adlr_autoresume_interval ........................ 1000
[default0]:  aml_data_download_path .......................... None
[default0]:  apply_layernorm_1p .............................. False
[default0]:  apply_query_key_layer_scaling ................... True
[default0]:  apply_residual_connection_post_layernorm ........ False
[default0]:  async_tensor_model_parallel_allreduce ........... False
[default0]:  attention_dropout ............................... 0.1
[default0]:  attention_softmax_in_fp32 ....................... False
[default0]:  barrier_with_L1_time ............................ True
[default0]:  bert_binary_head ................................ True
[default0]:  bert_embedder_type .............................. megatron
[default0]:  bert_load ....................................... None
[default0]:  bf16 ............................................ False
[default0]:  bias_dropout_fusion ............................. True
[default0]:  bias_gelu_fusion ................................ True
[default0]:  biencoder_projection_dim ........................ 0
[default0]:  biencoder_shared_query_context_model ............ False
[default0]:  block_data_path ................................. None
[default0]:  checkpoint_activations .......................... True
[default0]:  checkpoint_in_cpu ............................... False
[default0]:  checkpoint_num_layers ........................... 1
[default0]:  classes_fraction ................................ 1.0
[default0]:  clip_grad ....................................... 1.0
[default0]:  compression_training ............................ False
[default0]:  consumed_train_samples .......................... 0
[default0]:  consumed_train_tokens ........................... 0
[default0]:  consumed_valid_samples .......................... 0
[default0]:  contigious_checkpointing ........................ False
[default0]:  cpu_optimizer ................................... False
[default0]:  cpu_torch_adam .................................. False
[default0]:  create_moe_param_group .......................... True
[default0]:  curriculum_learning_legacy ...................... False
[default0]:  data_cache_path ................................. None
[default0]:  data_efficiency_curriculum_learning ............. False
[default0]:  data_impl ....................................... mmap
[default0]:  data_parallel_random_init ....................... False
[default0]:  data_parallel_size .............................. 1
[default0]:  data_path ....................................... ['/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/meg-gpt2-oscar-en-10k_text_document']
[default0]:  data_per_class_fraction ......................... 1.0
[default0]:  data_sharding ................................... True
[default0]:  dataloader_type ................................. single
[default0]:  DDP_impl ........................................ local
[default0]:  decoder_num_layers .............................. None
[default0]:  decoder_seq_length .............................. None
[default0]:  deepscale ....................................... False
[default0]:  deepscale_config ................................ None
[default0]:  deepspeed ....................................... True
[default0]:  deepspeed_activation_checkpointing .............. True
[default0]:  deepspeed_config ................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_8BATCH_1GPU_1Node/ds_config_gpt_gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-8-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true.json
[default0]:  deepspeed_mpi ................................... False
[default0]:  dino_bottleneck_size ............................ 256
[default0]:  dino_freeze_last_layer .......................... 1
[default0]:  dino_head_hidden_size ........................... 2048
[default0]:  dino_local_crops_number ......................... 10
[default0]:  dino_local_img_size ............................. 96
[default0]:  dino_norm_last_layer ............................ False
[default0]:  dino_teacher_temp ............................... 0.07
[default0]:  dino_warmup_teacher_temp ........................ 0.04
[default0]:  dino_warmup_teacher_temp_epochs ................. 30
[default0]:  distribute_checkpointed_activations ............. False
[default0]:  distribute_saved_activations .................... False
[default0]:  distributed_backend ............................. nccl
[default0]:  distributed_timeout_minutes ..................... 10
[default0]:  ds_inference .................................... False
[default0]:  ds_pipeline_enabled ............................. False
[default0]:  embedding_path .................................. None
[default0]:  embedding_weights_in_fp32 ....................... False
[default0]:  empty_unused_memory_level ....................... 0
[default0]:  enable_expert_tensor_parallelism ................ False
[default0]:  encoder_num_layers .............................. 12
[default0]:  encoder_seq_length .............................. 2048
[default0]:  end_weight_decay ................................ 0.1
[default0]:  eod_mask_loss ................................... False
[default0]:  eval_interval ................................... 100
[default0]:  eval_iters ...................................... 50
[default0]:  evidence_data_path .............................. None
[default0]:  exit_duration_in_mins ........................... 30000000
[default0]:  exit_interval ................................... None
[default0]:  exit_on_missing_checkpoint ...................... False
[default0]:  exit_signal_handler ............................. False
[default0]:  expert_interval ................................. 2
[default0]:  ffn_hidden_size ................................. 3072
[default0]:  finetune ........................................ False
[default0]:  fp16 ............................................ True
[default0]:  fp16_lm_cross_entropy ........................... False
[default0]:  fp32_residual_connection ........................ False
[default0]:  fp8_amax_compute_algo ........................... most_recent
[default0]:  fp8_amax_history_len ............................ 1
[default0]:  fp8_e4m3 ........................................ False
[default0]:  fp8_hybrid ...................................... False
[default0]:  fp8_interval .................................... 1
[default0]:  fp8_margin ...................................... 0
[default0]:  fp8_wgrad ....................................... True
[default0]:  global_batch_size ............................... 8
[default0]:  gradient_accumulation_fusion .................... True
[default0]:  head_lr_mult .................................... 1.0
[default0]:  hidden_dropout .................................. 0.1
[default0]:  hidden_size ..................................... 768
[default0]:  hidden_size_teacher ............................. None
[default0]:  hysteresis ...................................... 2
[default0]:  ict_head_size ................................... None
[default0]:  ict_load ........................................ None
[default0]:  img_h ........................................... 224
[default0]:  img_w ........................................... 224
[default0]:  indexer_batch_size .............................. 128
[default0]:  indexer_log_interval ............................ 1000
[default0]:  inference ....................................... False
[default0]:  inference_batch_times_seqlen_threshold .......... 512
[default0]:  init_method_std ................................. 0.01
[default0]:  init_method_xavier_uniform ...................... False
[default0]:  initial_loss_scale .............................. 4294967296
[default0]:  iter_per_epoch .................................. 1250
[default0]:  kd .............................................. False
[default0]:  kd_alpha_ce ..................................... 1
[default0]:  kd_beta_ce ...................................... 1
[default0]:  kd_temp ......................................... 1.0
[default0]:  kv_channels ..................................... 64
[default0]:  layernorm_epsilon ............................... 1e-05
[default0]:  lazy_mpu_init ................................... None
[default0]:  load ............................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_8BATCH_1GPU_1Node/checkpoint/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-8-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:  load_teacher .................................... None
[default0]:  local_rank ...................................... None
[default0]:  log_batch_size_to_tensorboard ................... True
[default0]:  log_interval .................................... 5
[default0]:  log_learning_rate_to_tensorboard ................ True
[default0]:  log_loss_scale_to_tensorboard ................... True
[default0]:  log_memory_to_tensorboard ....................... False
[default0]:  log_num_zeros_in_grad ........................... False
[default0]:  log_optimizer_states_to_tensorboard ............. False
[default0]:  log_params_norm ................................. False
[default0]:  log_timers_to_tensorboard ....................... True
[default0]:  log_validation_ppl_to_tensorboard ............... True
[default0]:  log_world_size_to_tensorboard ................... False
[default0]:  loss_scale ...................................... None
[default0]:  loss_scale_window ............................... 1000
[default0]:  lr .............................................. 0.0002
[default0]:  lr_decay_iters .................................. None
[default0]:  lr_decay_samples ................................ None
[default0]:  lr_decay_style .................................. cosine
[default0]:  lr_decay_tokens ................................. 300000000000
[default0]:  lr_warmup_fraction .............................. None
[default0]:  lr_warmup_iters ................................. 0
[default0]:  lr_warmup_samples ............................... 0
[default0]:  lr_warmup_tokens ................................ 375000000
[default0]:  make_vocab_size_divisible_by .................... 128
[default0]:  mask_factor ..................................... 1.0
[default0]:  mask_prob ....................................... 0.15
[default0]:  mask_type ....................................... random
[default0]:  masked_softmax_fusion ........................... True
[default0]:  max_position_embeddings ......................... 2048
[default0]:  max_tokens_to_oom ............................... 12000
[default0]:  memory_centric_tiled_linear ..................... False
[default0]:  merge_file ...................................... /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-merges.txt
[default0]:  micro_batch_size ................................ 8
[default0]:  min_loss_scale .................................. 1.0
[default0]:  min_lr .......................................... 2e-06
[default0]:  mlp_type ........................................ standard
[default0]:  mmap_warmup ..................................... False
[default0]:  moe_eval_capacity_factor ........................ 1.0
[default0]:  moe_expert_parallel_size ........................ 1
[default0]:  moe_loss_coeff .................................. 0.01
[default0]:  moe_min_capacity ................................ 4
[default0]:  moe_token_dropping .............................. True
[default0]:  moe_train_capacity_factor ....................... 1.0
[default0]:  mos ............................................. False
[default0]:  no_load_lr_state ................................ False
[default0]:  no_load_optim ................................... None
[default0]:  no_load_rng ..................................... None
[default0]:  no_persist_layer_norm ........................... False
[default0]:  no_pipeline_parallel ............................ True
[default0]:  no_save_optim ................................... None
[default0]:  no_save_rng ..................................... None
[default0]:  normalization ................................... layernorm
[default0]:  num_attention_heads ............................. 12
[default0]:  num_attention_heads_teacher ..................... None
[default0]:  num_channels .................................... 3
[default0]:  num_classes ..................................... 1000
[default0]:  num_experts ..................................... [128]
[default0]:  num_experts_switch .............................. None
[default0]:  num_experts_teacher ............................. [1]
[default0]:  num_layers ...................................... 12
[default0]:  num_layers_per_virtual_pipeline_stage ........... None
[default0]:  num_layers_teacher .............................. None
[default0]:  num_workers ..................................... 0
[default0]:  onnx_safe ....................................... None
[default0]:  openai_gelu ..................................... False
[default0]:  optimizer ....................................... adam
[default0]:  output_bert_embeddings .......................... False
[default0]:  overlap_p2p_comm ................................ False
[default0]:  override_opt_param_scheduler .................... True
[default0]:  params_dtype .................................... torch.float16
[default0]:  partition_activations ........................... False
[default0]:  patch_dim ....................................... 16
[default0]:  perform_initialization .......................... True
[default0]:  pipeline_model_parallel_size .................... 1
[default0]:  pipeline_model_parallel_split_rank .............. None
[default0]:  profile_backward ................................ False
[default0]:  query_in_block_prob ............................. 0.1
[default0]:  rampup_batch_size ............................... None
[default0]:  random_ltd ...................................... False
[default0]:  rank ............................................ 0
[default0]:  recompute_granularity ........................... None
[default0]:  recompute_method ................................ None
[default0]:  recompute_num_layers ............................ 1
[default0]:  remote_device ................................... none
[default0]:  reset_attention_mask ............................ False
[default0]:  reset_iteration ................................. False
[default0]:  reset_position_ids .............................. False
[default0]:  retriever_report_topk_accuracies ................ []
[default0]:  retriever_score_scaling ......................... False
[default0]:  retriever_seq_length ............................ 256
[default0]:  retro_add_retriever ............................. False
[default0]:  retro_cyclic_train_iters ........................ None
[default0]:  retro_encoder_attention_dropout ................. 0.1
[default0]:  retro_encoder_hidden_dropout .................... 0.1
[default0]:  retro_encoder_layers ............................ 2
[default0]:  retro_num_neighbors ............................. 2
[default0]:  retro_num_retrieved_chunks ...................... 2
[default0]:  retro_return_doc_ids ............................ False
[default0]:  retro_workdir ................................... None
[default0]:  return_data_index ............................... False
[default0]:  rotary_percent .................................. 1.0
[default0]:  sample_rate ..................................... 1.0
[default0]:  save ............................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_8BATCH_1GPU_1Node/checkpoint/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-8-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:  save_interval ................................... 1000000
[default0]:  scatter_gather_tensors_in_pipeline .............. True
[default0]:  scattered_embeddings ............................ False
[default0]:  seed ............................................ 1234
[default0]:  seq_length ...................................... 2048
[default0]:  sequence_parallel ............................... False
[default0]:  sgd_momentum .................................... 0.9
[default0]:  short_seq_prob .................................. 0.1
[default0]:  skip_train ...................................... False
[default0]:  split ........................................... 98,2,0
[default0]:  split_transformers .............................. False
[default0]:  squared_relu .................................... False
[default0]:  standalone_embedding_stage ...................... False
[default0]:  start_weight_decay .............................. 0.1
[default0]:  swiglu .......................................... False
[default0]:  swin_backbone_type .............................. tiny
[default0]:  synchronize_each_layer .......................... False
[default0]:  tensor_model_parallel_size ...................... 1
[default0]:  tensorboard_dir ................................. /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_8BATCH_1GPU_1Node/tensorboard/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-8-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true_n2gpu1213_2023.07.30-22.21.11
[default0]:  tensorboard_log_interval ........................ 1
[default0]:  tensorboard_queue_size .......................... 1
[default0]:  test_data_path .................................. None
[default0]:  tile_factor ..................................... 1
[default0]:  timing_log_level ................................ 0
[default0]:  timing_log_option ............................... minmax
[default0]:  titles_data_path ................................ None
[default0]:  tokenizer_model ................................. None
[default0]:  tokenizer_type .................................. GPT2BPETokenizer
[default0]:  topk ............................................ 1
[default0]:  train_data_exact_num_epochs ..................... None
[default0]:  train_data_path ................................. None
[default0]:  train_desc_path ................................. None
[default0]:  train_doc_idx_path .............................. None
[default0]:  train_idx_path .................................. None
[default0]:  train_iters ..................................... 54931640
[default0]:  train_sample_idx_path ........................... None
[default0]:  train_samples ................................... None
[default0]:  train_shuffle_idx_path .......................... None
[default0]:  train_tokens .................................... 300000000000
[default0]:  transformer_impl ................................ local
[default0]:  transformer_pipeline_model_parallel_size ........ 1
[default0]:  untie_embeddings_and_output_weights ............. False
[default0]:  use_checkpoint_args ............................. False
[default0]:  use_checkpoint_opt_param_scheduler .............. False
[default0]:  use_contiguous_buffers_in_local_ddp ............. True
[default0]:  use_cpu_initialization .......................... None
[default0]:  use_distributed_optimizer ....................... False
[default0]:  use_flash_attn .................................. False
[default0]:  use_one_sent_docs ............................... False
[default0]:  use_pin_memory .................................. False
[default0]:  use_ring_exchange_p2p ........................... False
[default0]:  use_rotary_position_embeddings .................. False
[default0]:  use_tutel ....................................... False
[default0]:  valid_data_path ................................. None
[default0]:  variable_seq_lengths ............................ False
[default0]:  virtual_pipeline_model_parallel_size ............ None
[default0]:  vision_backbone_type ............................ vit
[default0]:  vision_pretraining .............................. False
[default0]:  vision_pretraining_type ......................... classify
[default0]:  vocab_extra_ids ................................. 0
[default0]:  vocab_file ...................................... /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-vocab.json
[default0]:  vocab_size ...................................... None
[default0]:  weight_decay .................................... 0.1
[default0]:  weight_decay_incr_style ......................... constant
[default0]:  world_size ...................................... 1
[default0]:  zero_allgather_bucket_size ...................... 0.0
[default0]:  zero_contigious_gradients ....................... False
[default0]:  zero_reduce_bucket_size ......................... 0.0
[default0]:  zero_reduce_scatter ............................. False
[default0]:  zero_stage ...................................... 1.0
[default0]:-------------------- end of arguments ---------------------
[default0]:setting number of micro-batches to constant 1
[default0]:> building GPT2BPETokenizer tokenizer ...
[default0]: > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
[default0]:> setting tensorboard ...
[default0]:> initializing torch distributed ...
[default0]:[2023-07-30 22:21:34,080] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[default0]:[2023-07-30 22:21:34,080] [INFO] [comm.py:616:init_distributed] cdb=None
[default0]:[2023-07-30 22:21:34,080] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[default0]:> initialized tensor model parallel with size 1
[default0]:> initialized pipeline model parallel with size 1
[default0]:> setting random seeds to 1234 ...
[default0]:> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[default0]:> compiling dataset index builder ...
[default0]:make: Entering directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/data'
[default0]:make: Nothing to be done for 'default'.
[default0]:make: Leaving directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/data'
[default0]:>>> done with dataset index builder. Compilation time: 0.076 seconds
[default0]:> compiling and loading fused kernels ...
[default0]:Loading extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Loading extension module scaled_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_softmax_cuda...
[default0]:n2gpu1213:2133385:2133385 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.81<0>
[default0]:n2gpu1213:2133385:2133385 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1213:2133385:2133385 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.81<0>
[default0]:n2gpu1213:2133385:2133385 [0] NCCL INFO Using network IB
[default0]:NCCL version 2.12.12+cuda11.7
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Connected all rings
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO Connected all trees
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1213:2133385:2133599 [0] NCCL INFO comm 0x153d500090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:>>> done with compiling and loading fused kernels. Compilation time: 1.650 seconds
[default0]:time to initialize megatron (seconds): 5.024
[default0]:[after megatron is initialized] datetime: 2023-07-30 22:21:36 
[default0]:building GPT model ...
[default0]:[2023-07-30 22:21:36,571] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[default0]:[2023-07-30 22:21:36,572] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 1.59 GB         CA 0.0 GB         Max_CA 2 GB 
[default0]:[2023-07-30 22:21:36,572] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.64 GB, percent = 3.3%
[default0]:[2023-07-30 22:21:36,673] [INFO] [utils.py:785:see_memory_usage] After Building Model
[default0]:[2023-07-30 22:21:36,674] [INFO] [utils.py:786:see_memory_usage] MA 0.24 GB         Max_MA 0.24 GB         CA 0.25 GB         Max_CA 0 GB 
[default0]:[2023-07-30 22:21:36,674] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.64 GB, percent = 3.3%
[default0]: > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 125262336
[default0]:param_group keyset: dict_keys(['name', 'params', 'wd_mult', 'lr_mult'])
[default0]:> learning rate decay style: cosine
[default0]:DeepSpeed is enabled.
[default0]:[2023-07-30 22:21:36,676] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Connected all rings
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO Connected all trees
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1213:2133385:2133732 [0] NCCL INFO comm 0x153c880090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:[2023-07-30 22:21:36,997] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[default0]:[2023-07-30 22:21:36,998] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[default0]:[2023-07-30 22:21:36,998] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[default0]:[2023-07-30 22:21:37,001] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[default0]:[2023-07-30 22:21:37,001] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[default0]:[2023-07-30 22:21:37,001] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[default0]:[2023-07-30 22:21:37,001] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[default0]:[2023-07-30 22:21:37,001] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[default0]:[2023-07-30 22:21:37,001] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[default0]:[2023-07-30 22:21:37,001] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
[default0]:Rank: 0 partition count [1, 1] and sizes[(125140992, False), (121344, False)] 
[default0]:[2023-07-30 22:21:37,293] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[default0]:[2023-07-30 22:21:37,294] [INFO] [utils.py:786:see_memory_usage] MA 0.7 GB         Max_MA 0.7 GB         CA 0.7 GB         Max_CA 1 GB 
[default0]:[2023-07-30 22:21:37,294] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.74 GB, percent = 3.3%
[default0]:[2023-07-30 22:21:37,345] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[default0]:[2023-07-30 22:21:37,345] [INFO] [utils.py:786:see_memory_usage] MA 1.64 GB         Max_MA 2.1 GB         CA 2.11 GB         Max_CA 2 GB 
[default0]:[2023-07-30 22:21:37,346] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.74 GB, percent = 3.3%
[default0]:[2023-07-30 22:21:37,346] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[default0]:[2023-07-30 22:21:37,387] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[default0]:[2023-07-30 22:21:37,388] [INFO] [utils.py:786:see_memory_usage] MA 1.64 GB         Max_MA 1.64 GB         CA 2.11 GB         Max_CA 2 GB 
[default0]:[2023-07-30 22:21:37,388] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.74 GB, percent = 3.3%
[default0]:[2023-07-30 22:21:37,391] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[default0]:[2023-07-30 22:21:37,391] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[default0]:[2023-07-30 22:21:37,391] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x153dd494f730>
[default0]:[2023-07-30 22:21:37,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:37,392] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[default0]:[2023-07-30 22:21:37,392] [INFO] [config.py:964:print]   activation_checkpointing_config  {
[default0]:    "partition_activations": false, 
[default0]:    "contiguous_memory_optimization": false, 
[default0]:    "cpu_checkpointing": false, 
[default0]:    "number_checkpoints": null, 
[default0]:    "synchronize_checkpoint_boundary": false, 
[default0]:    "profile": false
[default0]:}
[default0]:[2023-07-30 22:21:37,392] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[default0]:[2023-07-30 22:21:37,392] [INFO] [config.py:964:print]   amp_enabled .................. False
[default0]:[2023-07-30 22:21:37,392] [INFO] [config.py:964:print]   amp_params ................... False
[default0]:[2023-07-30 22:21:37,392] [INFO] [config.py:964:print]   autotuning_config ............ {
[default0]:    "enabled": false, 
[default0]:    "start_step": null, 
[default0]:    "end_step": null, 
[default0]:    "metric_path": null, 
[default0]:    "arg_mappings": null, 
[default0]:    "metric": "throughput", 
[default0]:    "model_info": null, 
[default0]:    "results_dir": "autotuning_results", 
[default0]:    "exps_dir": "autotuning_exps", 
[default0]:    "overwrite": true, 
[default0]:    "fast": true, 
[default0]:    "start_profile_step": 3, 
[default0]:    "end_profile_step": 5, 
[default0]:    "tuner_type": "gridsearch", 
[default0]:    "tuner_early_stopping": 5, 
[default0]:    "tuner_num_trials": 50, 
[default0]:    "model_info_path": null, 
[default0]:    "mp_size": 1, 
[default0]:    "max_train_batch_size": null, 
[default0]:    "min_train_batch_size": 1, 
[default0]:    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[default0]:    "min_train_micro_batch_size_per_gpu": 1, 
[default0]:    "num_tuning_micro_batch_sizes": 3
[default0]:}
[default0]:[2023-07-30 22:21:37,392] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[default0]:[2023-07-30 22:21:37,392] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[default0]:[2023-07-30 22:21:37,392] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[default0]:[2023-07-30 22:21:37,392] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x153dce52dc60>
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   communication_data_type ...... None
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   curriculum_params_legacy ..... {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 7048872, 'difficulty_step': 8}}
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   disable_allgather ............ False
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   dump_state ................... False
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[default0]:[2023-07-30 22:21:37,393] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   flops_profiler_config ........ {
[default0]:    "enabled": false, 
[default0]:    "recompute_fwd_factor": 0.0, 
[default0]:    "profile_step": 1, 
[default0]:    "module_depth": -1, 
[default0]:    "top_modules": 1, 
[default0]:    "detailed": true, 
[default0]:    "output_file": null
[default0]:}
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   fp16_enabled ................. True
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   global_rank .................. 0
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   gradient_clipping ............ 1
[default0]:[2023-07-30 22:21:37,394] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[default0]:[2023-07-30 22:21:37,395] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[default0]:[2023-07-30 22:21:37,395] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[default0]:[2023-07-30 22:21:37,395] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[default0]:[2023-07-30 22:21:37,395] [INFO] [config.py:964:print]   loss_scale ................... 0
[default0]:[2023-07-30 22:21:37,395] [INFO] [config.py:964:print]   memory_breakdown ............. False
[default0]:[2023-07-30 22:21:37,395] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[default0]:[2023-07-30 22:21:37,395] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[default0]:[2023-07-30 22:21:37,395] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   nebula_config ................ {
[default0]:    "enabled": false, 
[default0]:    "persistent_storage_path": null, 
[default0]:    "persistent_time_interval": 100, 
[default0]:    "num_of_version_in_retention": 2, 
[default0]:    "enable_nebula_load": true, 
[default0]:    "load_path": null
[default0]:}
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   optimizer_name ............... None
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   optimizer_params ............. None
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   pld_enabled .................. False
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   pld_params ................... False
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   prescale_gradients ........... False
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   scheduler_name ............... None
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   scheduler_params ............. None
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   sparse_attention ............. None
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   steps_per_print .............. 5
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   train_batch_size ............. 8
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  8
[default0]:[2023-07-30 22:21:37,396] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[default0]:[2023-07-30 22:21:37,397] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[default0]:[2023-07-30 22:21:37,397] [INFO] [config.py:964:print]   world_size ................... 1
[default0]:[2023-07-30 22:21:37,397] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[default0]:[2023-07-30 22:21:37,397] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[default0]:[2023-07-30 22:21:37,397] [INFO] [config.py:964:print]   zero_enabled ................. True
[default0]:[2023-07-30 22:21:37,397] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[default0]:[2023-07-30 22:21:37,397] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[default0]:[2023-07-30 22:21:37,397] [INFO] [config.py:950:print_user_config]   json = {
[default0]:    "train_batch_size": 8, 
[default0]:    "train_micro_batch_size_per_gpu": 8, 
[default0]:    "steps_per_print": 5, 
[default0]:    "zero_optimization": {
[default0]:        "stage": 2
[default0]:    }, 
[default0]:    "gradient_clipping": 1, 
[default0]:    "prescale_gradients": false, 
[default0]:    "fp16": {
[default0]:        "enabled": true, 
[default0]:        "loss_scale": 0, 
[default0]:        "loss_scale_window": 500, 
[default0]:        "hysteresis": 2, 
[default0]:        "min_loss_scale": 1, 
[default0]:        "initial_scale_power": 11
[default0]:    }, 
[default0]:    "bf16": {
[default0]:        "enabled": false
[default0]:    }, 
[default0]:    "curriculum_learning": {
[default0]:        "enabled": false, 
[default0]:        "curriculum_type": "seqlen", 
[default0]:        "min_difficulty": 80, 
[default0]:        "max_difficulty": 2.048000e+03, 
[default0]:        "schedule_type": "fixed_linear", 
[default0]:        "schedule_config": {
[default0]:            "total_curriculum_step": 7.048872e+06, 
[default0]:            "difficulty_step": 8
[default0]:        }
[default0]:    }, 
[default0]:    "wall_clock_breakdown": false
[default0]:}
[default0]:[2023-07-30 22:21:37,398] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_8BATCH_1GPU_1Node/checkpoint/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-8-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:WARNING: could not find the metadata file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_8BATCH_1GPU_1Node/checkpoint/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-8-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true 
[default0]:    will not load any checkpoints and will start from random
[default0]:(min, max) time across ranks (ms):
[default0]:    load-checkpoint ................................: (0.68, 0.68)
[default0]:[after model, optimizer, and learning rate scheduler are built] datetime: 2023-07-30 22:21:37 
[default0]:> building train, validation, and test datasets ...
[default0]: > datasets target sizes (minimum size):
[default0]:    train:      439453120
[default0]:    validation: 219726800
[default0]:    test:       400
[default0]:> building train, validation, and test datasets for GPT ...
[default0]:Single data path provided for train, valid & test
[default0]: > building dataset index ...
[default0]:    reading sizes...
[default0]:    reading pointers...
[default0]:    reading document index...
[default0]:    creating numpy buffer of mmap...
[default0]:    creating memory view of numpy buffer...
[default0]: > finished creating indexed dataset in 0.002816 seconds
[default0]:    number of documents: 10000
[default0]: > dataset split:
[default0]:    train:
[default0]:     document indices in [0, 9800) total of 9800 documents
[default0]:    validation:
[default0]:     document indices in [9800, 10000) total of 200 documents
[default0]:    test:
[default0]:     document indices in [10000, 10000) total of 0 documents
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Connected all rings
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO Connected all trees
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1213:2133385:2133740 [0] NCCL INFO comm 0x153c280090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]: > loading doc-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/3e541422cadca564d1c62edd2b40c688_doc_idx.npy
[default0]: > loading sample-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/3e541422cadca564d1c62edd2b40c688_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/3e541422cadca564d1c62edd2b40c688_shuffle_idx.npy
[default0]:    loaded indexed file in 0.003 seconds
[default0]:    total number of samples: 439466004
[default0]:    total number of epochs: 30909
[default0]: > loading doc-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/e3555f227ec98db13f639e1775e56895_doc_idx.npy
[default0]: > loading sample-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/e3555f227ec98db13f639e1775e56895_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/e3555f227ec98db13f639e1775e56895_shuffle_idx.npy
[default0]:    loaded indexed file in 0.022 seconds
[default0]:    total number of samples: 219726809
[default0]:    total number of epochs: 648115
[default0]:> finished creating GPT datasets ...
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Connected all rings
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO Connected all trees
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1213:2133385:2133750 [0] NCCL INFO comm 0x153b980090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:[after dataloaders are built] datetime: 2023-07-30 22:21:38 
[default0]:done with setup ...
[default0]:(min, max) time across ranks (ms):
[default0]:    model-and-optimizer-setup ......................: (886.33, 886.33)
[default0]:    train/valid/test-data-iterators-setup ..........: (944.67, 944.67)
[default0]:training ...
[default0]:[before the start of training step] datetime: 2023-07-30 22:21:38 
[default0]:[2023-07-30 22:21:43,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[3.4952533333333334e-08, 3.4952533333333334e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:43,774] [INFO] [timer.py:215:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=20.20916300748112, CurrSamplesPerSec=20.272437657649494, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration        5/54931640 | consumed samples:           40 | consumed tokens:        81920 | elapsed time per iteration (ms): 1049.7 | learning rate: 3.495E-08 | global batch size:     8 | lm loss: 1.085323E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 7.621 | TFLOPs: 18.94 |
[default0]:[Rank 0] (after 5 iterations) memory (MB) | allocated: 2442.31103515625 | max allocated: 10655.47314453125 | reserved: 14240.0 | max reserved: 14240.0
[default0]:[2023-07-30 22:21:46,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[7.864320000000001e-08, 7.864320000000001e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:46,058] [INFO] [timer.py:215:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=20.226896817947235, CurrSamplesPerSec=20.09165614813289, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       10/54931640 | consumed samples:           80 | consumed tokens:       163840 | elapsed time per iteration (ms): 456.0 | learning rate: 7.864E-08 | global batch size:     8 | lm loss: 1.085351E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.542 | TFLOPs: 43.59 |
[default0]:[2023-07-30 22:21:48,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=15, skipped=0, lr=[1.2233386666666666e-07, 1.2233386666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:48,424] [INFO] [timer.py:215:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=20.201125787879935, CurrSamplesPerSec=20.074755575019413, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       15/54931640 | consumed samples:          120 | consumed tokens:       245760 | elapsed time per iteration (ms): 473.8 | learning rate: 1.223E-07 | global batch size:     8 | lm loss: 1.084542E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.886 | TFLOPs: 41.96 |
[default0]:[2023-07-30 22:21:50,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[1.6602453333333333e-07, 1.6602453333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:50,717] [INFO] [timer.py:215:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=20.219730075642737, CurrSamplesPerSec=20.264675454717302, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       20/54931640 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 458.7 | learning rate: 1.660E-07 | global batch size:     8 | lm loss: 1.082171E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.442 | TFLOPs: 43.34 |
[default0]:[2023-07-30 22:21:53,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=25, skipped=0, lr=[2.097152e-07, 2.097152e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:53,271] [INFO] [timer.py:215:stop] epoch=0/micro_step=25/global_step=25, RunningAvgSamplesPerSec=20.2351555415542, CurrSamplesPerSec=20.26000141288052, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       25/54931640 | consumed samples:          200 | consumed tokens:       409600 | elapsed time per iteration (ms): 510.8 | learning rate: 2.097E-07 | global batch size:     8 | lm loss: 1.077135E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.662 | TFLOPs: 38.92 |
[default0]:[2023-07-30 22:21:55,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[2.534058666666667e-07, 2.534058666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:55,565] [INFO] [timer.py:215:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=20.240741143287092, CurrSamplesPerSec=20.142928064727396, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       30/54931640 | consumed samples:          240 | consumed tokens:       491520 | elapsed time per iteration (ms): 458.9 | learning rate: 2.534E-07 | global batch size:     8 | lm loss: 1.076043E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.434 | TFLOPs: 43.32 |
[default0]:[2023-07-30 22:21:58,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=35, skipped=0, lr=[2.9709653333333336e-07, 2.9709653333333336e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:58,041] [INFO] [timer.py:215:stop] epoch=0/micro_step=35/global_step=35, RunningAvgSamplesPerSec=20.236196041356838, CurrSamplesPerSec=20.169395737396094, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       35/54931640 | consumed samples:          280 | consumed tokens:       573440 | elapsed time per iteration (ms): 495.2 | learning rate: 2.971E-07 | global batch size:     8 | lm loss: 1.071869E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.155 | TFLOPs: 40.14 |
[default0]:[2023-07-30 22:22:00,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[3.4078720000000003e-07, 3.4078720000000003e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:00,384] [INFO] [timer.py:215:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=20.242107315417805, CurrSamplesPerSec=20.23965410747375, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       40/54931640 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 468.3 | learning rate: 3.408E-07 | global batch size:     8 | lm loss: 1.067187E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.082 | TFLOPs: 42.44 |
[default0]:[2023-07-30 22:22:02,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=45, skipped=0, lr=[3.844778666666667e-07, 3.844778666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:02,755] [INFO] [timer.py:215:stop] epoch=0/micro_step=45/global_step=45, RunningAvgSamplesPerSec=20.24949937650474, CurrSamplesPerSec=20.295354882883146, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       45/54931640 | consumed samples:          360 | consumed tokens:       737280 | elapsed time per iteration (ms): 474.4 | learning rate: 3.845E-07 | global batch size:     8 | lm loss: 1.062865E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.863 | TFLOPs: 41.90 |
[default0]:[2023-07-30 22:22:05,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[4.2816853333333333e-07, 4.2816853333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:05,037] [INFO] [timer.py:215:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=20.22924321600373, CurrSamplesPerSec=19.374582173465928, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       50/54931640 | consumed samples:          400 | consumed tokens:       819200 | elapsed time per iteration (ms): 456.2 | learning rate: 4.282E-07 | global batch size:     8 | lm loss: 1.057595E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.535 | TFLOPs: 43.57 |
[default0]:[2023-07-30 22:22:07,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=55, skipped=0, lr=[4.718592e-07, 4.718592e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:07,435] [INFO] [timer.py:215:stop] epoch=0/micro_step=55/global_step=55, RunningAvgSamplesPerSec=20.227111720891877, CurrSamplesPerSec=20.22895306236748, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       55/54931640 | consumed samples:          440 | consumed tokens:       901120 | elapsed time per iteration (ms): 479.7 | learning rate: 4.719E-07 | global batch size:     8 | lm loss: 1.053532E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.676 | TFLOPs: 41.43 |
[default0]:[2023-07-30 22:22:09,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[5.155498666666666e-07, 5.155498666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:09,638] [INFO] [timer.py:215:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=20.22838367822993, CurrSamplesPerSec=20.20311959991667, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       60/54931640 | consumed samples:          480 | consumed tokens:       983040 | elapsed time per iteration (ms): 440.6 | learning rate: 5.155E-07 | global batch size:     8 | lm loss: 1.049376E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.156 | TFLOPs: 45.11 |
[default0]:[2023-07-30 22:22:12,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=65, skipped=0, lr=[5.592405333333334e-07, 5.592405333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:12,183] [INFO] [timer.py:215:stop] epoch=0/micro_step=65/global_step=65, RunningAvgSamplesPerSec=20.232652436797355, CurrSamplesPerSec=20.297147281569842, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       65/54931640 | consumed samples:          520 | consumed tokens:      1064960 | elapsed time per iteration (ms): 508.8 | learning rate: 5.592E-07 | global batch size:     8 | lm loss: 1.043281E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.724 | TFLOPs: 39.07 |
[default0]:[2023-07-30 22:22:14,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[6.029312000000001e-07, 6.029312000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:14,438] [INFO] [timer.py:215:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=20.23041679855767, CurrSamplesPerSec=19.944230102834442, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       70/54931640 | consumed samples:          560 | consumed tokens:      1146880 | elapsed time per iteration (ms): 451.1 | learning rate: 6.029E-07 | global batch size:     8 | lm loss: 1.039661E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.734 | TFLOPs: 44.07 |
[default0]:[2023-07-30 22:22:16,931] [INFO] [logging.py:96:log_dist] [Rank 0] step=75, skipped=0, lr=[6.466218666666667e-07, 6.466218666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:16,932] [INFO] [timer.py:215:stop] epoch=0/micro_step=75/global_step=75, RunningAvgSamplesPerSec=20.231382916645487, CurrSamplesPerSec=19.90798563729178, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       75/54931640 | consumed samples:          600 | consumed tokens:      1228800 | elapsed time per iteration (ms): 498.9 | learning rate: 6.466E-07 | global batch size:     8 | lm loss: 1.038029E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.036 | TFLOPs: 39.85 |
[default0]:[2023-07-30 22:22:19,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[6.903125333333334e-07, 6.903125333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:19,290] [INFO] [timer.py:215:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=20.230271504951173, CurrSamplesPerSec=20.150681969953926, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       80/54931640 | consumed samples:          640 | consumed tokens:      1310720 | elapsed time per iteration (ms): 471.9 | learning rate: 6.903E-07 | global batch size:     8 | lm loss: 1.036314E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.954 | TFLOPs: 42.13 |
[default0]:[2023-07-30 22:22:21,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=85, skipped=0, lr=[7.340032000000002e-07, 7.340032000000002e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:21,597] [INFO] [timer.py:215:stop] epoch=0/micro_step=85/global_step=85, RunningAvgSamplesPerSec=20.23361734465996, CurrSamplesPerSec=20.259536573746747, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       85/54931640 | consumed samples:          680 | consumed tokens:      1392640 | elapsed time per iteration (ms): 461.4 | learning rate: 7.340E-07 | global batch size:     8 | lm loss: 1.032609E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.339 | TFLOPs: 43.08 |
[default0]:[2023-07-30 22:22:23,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[7.776938666666667e-07, 7.776938666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:23,797] [INFO] [timer.py:215:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=20.235145937197878, CurrSamplesPerSec=20.19861981286088, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       90/54931640 | consumed samples:          720 | consumed tokens:      1474560 | elapsed time per iteration (ms): 440.1 | learning rate: 7.777E-07 | global batch size:     8 | lm loss: 1.028292E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.179 | TFLOPs: 45.17 |
[default0]:[2023-07-30 22:22:26,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=95, skipped=0, lr=[8.213845333333334e-07, 8.213845333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:26,058] [INFO] [timer.py:215:stop] epoch=0/micro_step=95/global_step=95, RunningAvgSamplesPerSec=20.239596085251723, CurrSamplesPerSec=20.253593406228592, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration       95/54931640 | consumed samples:          760 | consumed tokens:      1556480 | elapsed time per iteration (ms): 451.9 | learning rate: 8.214E-07 | global batch size:     8 | lm loss: 1.028134E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.703 | TFLOPs: 43.99 |
[default0]:[2023-07-30 22:22:28,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[8.650752e-07, 8.650752e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:28,157] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=20.239380669477846, CurrSamplesPerSec=20.107295857005973, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      100/54931640 | consumed samples:          800 | consumed tokens:      1638400 | elapsed time per iteration (ms): 419.9 | learning rate: 8.651E-07 | global batch size:     8 | lm loss: 1.027151E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.054 | TFLOPs: 47.35 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 100 | lm loss value: 1.023517E+01 | lm loss PPL: 2.786619E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:22:36,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=105, skipped=0, lr=[9.087658666666667e-07, 9.087658666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:36,022] [INFO] [timer.py:215:stop] epoch=0/micro_step=105/global_step=105, RunningAvgSamplesPerSec=20.240340167790528, CurrSamplesPerSec=20.310403666648305, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      105/54931640 | consumed samples:          840 | consumed tokens:      1720320 | elapsed time per iteration (ms): 1572.8 | learning rate: 9.088E-07 | global batch size:     8 | lm loss: 1.024855E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 5.086 | TFLOPs: 12.64 |
[default0]:[2023-07-30 22:22:38,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[9.524565333333334e-07, 9.524565333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:38,191] [INFO] [timer.py:215:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=20.1888247347323, CurrSamplesPerSec=20.1168071559611, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      110/54931640 | consumed samples:          880 | consumed tokens:      1802240 | elapsed time per iteration (ms): 433.9 | learning rate: 9.525E-07 | global batch size:     8 | lm loss: 1.024178E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.437 | TFLOPs: 45.81 |
[default0]:[2023-07-30 22:22:40,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=115, skipped=0, lr=[9.961472e-07, 9.961472e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:40,257] [INFO] [timer.py:215:stop] epoch=0/micro_step=115/global_step=115, RunningAvgSamplesPerSec=20.192594143153613, CurrSamplesPerSec=20.22389322712621, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      115/54931640 | consumed samples:          920 | consumed tokens:      1884160 | elapsed time per iteration (ms): 412.9 | learning rate: 9.961E-07 | global batch size:     8 | lm loss: 1.022365E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.373 | TFLOPs: 48.14 |
[default0]:[2023-07-30 22:22:42,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[1.0398378666666667e-06, 1.0398378666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:42,324] [INFO] [timer.py:215:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=20.19553780156683, CurrSamplesPerSec=20.252566545489223, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      120/54931640 | consumed samples:          960 | consumed tokens:      1966080 | elapsed time per iteration (ms): 413.4 | learning rate: 1.040E-06 | global batch size:     8 | lm loss: 1.021852E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.351 | TFLOPs: 48.08 |
[default0]:[2023-07-30 22:22:44,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=125, skipped=0, lr=[1.0835285333333334e-06, 1.0835285333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:44,400] [INFO] [timer.py:215:stop] epoch=0/micro_step=125/global_step=125, RunningAvgSamplesPerSec=20.19512459210664, CurrSamplesPerSec=19.980095189154653, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      125/54931640 | consumed samples:         1000 | consumed tokens:      2048000 | elapsed time per iteration (ms): 415.7 | learning rate: 1.084E-06 | global batch size:     8 | lm loss: 1.021711E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.243 | TFLOPs: 47.81 |
[default0]:[2023-07-30 22:22:46,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[1.1272192e-06, 1.1272192e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:46,472] [INFO] [timer.py:215:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=20.197902654509935, CurrSamplesPerSec=20.195240210363874, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      130/54931640 | consumed samples:         1040 | consumed tokens:      2129920 | elapsed time per iteration (ms): 413.9 | learning rate: 1.127E-06 | global batch size:     8 | lm loss: 1.020109E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.329 | TFLOPs: 48.03 |
[default0]:[2023-07-30 22:22:48,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=135, skipped=0, lr=[1.1709098666666667e-06, 1.1709098666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:48,551] [INFO] [timer.py:215:stop] epoch=0/micro_step=135/global_step=135, RunningAvgSamplesPerSec=20.196392789688584, CurrSamplesPerSec=20.171760139855095, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      135/54931640 | consumed samples:         1080 | consumed tokens:      2211840 | elapsed time per iteration (ms): 415.8 | learning rate: 1.171E-06 | global batch size:     8 | lm loss: 1.020553E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.241 | TFLOPs: 47.81 |
[default0]:[2023-07-30 22:22:50,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[1.2146005333333334e-06, 1.2146005333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:50,613] [INFO] [timer.py:215:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=20.198365184497, CurrSamplesPerSec=20.192870300662516, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      140/54931640 | consumed samples:         1120 | consumed tokens:      2293760 | elapsed time per iteration (ms): 412.4 | learning rate: 1.215E-06 | global batch size:     8 | lm loss: 1.018922E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.399 | TFLOPs: 48.20 |
[default0]:[2023-07-30 22:22:52,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=145, skipped=0, lr=[1.2582912000000001e-06, 1.2582912000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:52,675] [INFO] [timer.py:215:stop] epoch=0/micro_step=145/global_step=145, RunningAvgSamplesPerSec=20.20036361451035, CurrSamplesPerSec=20.264699931815723, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      145/54931640 | consumed samples:         1160 | consumed tokens:      2375680 | elapsed time per iteration (ms): 412.9 | learning rate: 1.258E-06 | global batch size:     8 | lm loss: 1.018309E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.374 | TFLOPs: 48.14 |
[default0]:[2023-07-30 22:22:54,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[1.3019818666666667e-06, 1.3019818666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:54,740] [INFO] [timer.py:215:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=20.20204739057291, CurrSamplesPerSec=20.11567352527526, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      150/54931640 | consumed samples:         1200 | consumed tokens:      2457600 | elapsed time per iteration (ms): 412.6 | learning rate: 1.302E-06 | global batch size:     8 | lm loss: 1.016000E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.390 | TFLOPs: 48.18 |
[default0]:[2023-07-30 22:22:56,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=155, skipped=0, lr=[1.3456725333333334e-06, 1.3456725333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:56,801] [INFO] [timer.py:215:stop] epoch=0/micro_step=155/global_step=155, RunningAvgSamplesPerSec=20.203422042233903, CurrSamplesPerSec=20.24361037976944, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      155/54931640 | consumed samples:         1240 | consumed tokens:      2539520 | elapsed time per iteration (ms): 412.3 | learning rate: 1.346E-06 | global batch size:     8 | lm loss: 1.014228E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.406 | TFLOPs: 48.22 |
[default0]:[2023-07-30 22:22:58,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[1.3893632000000001e-06, 1.3893632000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:58,862] [INFO] [timer.py:215:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=20.205068384907385, CurrSamplesPerSec=20.205881272555487, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      160/54931640 | consumed samples:         1280 | consumed tokens:      2621440 | elapsed time per iteration (ms): 413.3 | learning rate: 1.389E-06 | global batch size:     8 | lm loss: 1.013878E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.358 | TFLOPs: 48.10 |
[default0]:[2023-07-30 22:23:00,929] [INFO] [logging.py:96:log_dist] [Rank 0] step=165, skipped=0, lr=[1.4330538666666668e-06, 1.4330538666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:00,930] [INFO] [timer.py:215:stop] epoch=0/micro_step=165/global_step=165, RunningAvgSamplesPerSec=20.20638560693971, CurrSamplesPerSec=20.254290263892727, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      165/54931640 | consumed samples:         1320 | consumed tokens:      2703360 | elapsed time per iteration (ms): 412.3 | learning rate: 1.433E-06 | global batch size:     8 | lm loss: 1.010695E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.405 | TFLOPs: 48.22 |
[default0]:[2023-07-30 22:23:02,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[1.4767445333333336e-06, 1.4767445333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:02,992] [INFO] [timer.py:215:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=20.207770110258284, CurrSamplesPerSec=20.21776385517196, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      170/54931640 | consumed samples:         1360 | consumed tokens:      2785280 | elapsed time per iteration (ms): 412.5 | learning rate: 1.477E-06 | global batch size:     8 | lm loss: 1.010048E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.393 | TFLOPs: 48.19 |
[default0]:[2023-07-30 22:23:05,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=175, skipped=0, lr=[1.5204352000000003e-06, 1.5204352000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:05,062] [INFO] [timer.py:215:stop] epoch=0/micro_step=175/global_step=175, RunningAvgSamplesPerSec=20.20829616589686, CurrSamplesPerSec=20.177449101515545, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      175/54931640 | consumed samples:         1400 | consumed tokens:      2867200 | elapsed time per iteration (ms): 413.8 | learning rate: 1.520E-06 | global batch size:     8 | lm loss: 1.007782E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.334 | TFLOPs: 48.04 |
[default0]:[2023-07-30 22:23:07,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[1.5641258666666666e-06, 1.5641258666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:07,137] [INFO] [timer.py:215:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=20.207948407493, CurrSamplesPerSec=20.050560176302113, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      180/54931640 | consumed samples:         1440 | consumed tokens:      2949120 | elapsed time per iteration (ms): 415.3 | learning rate: 1.564E-06 | global batch size:     8 | lm loss: 1.008109E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.263 | TFLOPs: 47.86 |
[default0]:[2023-07-30 22:23:09,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=185, skipped=0, lr=[1.6078165333333333e-06, 1.6078165333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:09,199] [INFO] [timer.py:215:stop] epoch=0/micro_step=185/global_step=185, RunningAvgSamplesPerSec=20.20968427026738, CurrSamplesPerSec=20.268641515930376, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      185/54931640 | consumed samples:         1480 | consumed tokens:      3031040 | elapsed time per iteration (ms): 412.1 | learning rate: 1.608E-06 | global batch size:     8 | lm loss: 1.004521E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.411 | TFLOPs: 48.23 |
[default0]:[2023-07-30 22:23:11,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[1.6515072e-06, 1.6515072e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:11,261] [INFO] [timer.py:215:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=20.211228415347048, CurrSamplesPerSec=20.208448967005857, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      190/54931640 | consumed samples:         1520 | consumed tokens:      3112960 | elapsed time per iteration (ms): 412.5 | learning rate: 1.652E-06 | global batch size:     8 | lm loss: 1.003419E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.396 | TFLOPs: 48.20 |
[default0]:[2023-07-30 22:23:13,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=195, skipped=0, lr=[1.6951978666666668e-06, 1.6951978666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:13,326] [INFO] [timer.py:215:stop] epoch=0/micro_step=195/global_step=195, RunningAvgSamplesPerSec=20.211431673639105, CurrSamplesPerSec=20.199787132599052, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      195/54931640 | consumed samples:         1560 | consumed tokens:      3194880 | elapsed time per iteration (ms): 413.8 | learning rate: 1.695E-06 | global batch size:     8 | lm loss: 1.003759E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.332 | TFLOPs: 48.04 |
[default0]:[2023-07-30 22:23:15,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[1.7388885333333335e-06, 1.7388885333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:15,393] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=20.212045930303205, CurrSamplesPerSec=20.221114443019545, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      200/54931640 | consumed samples:         1600 | consumed tokens:      3276800 | elapsed time per iteration (ms): 412.7 | learning rate: 1.739E-06 | global batch size:     8 | lm loss: 9.986230E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.383 | TFLOPs: 48.16 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 200 | lm loss value: 9.987690E+00 | lm loss PPL: 2.175698E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:23:23,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=205, skipped=0, lr=[1.7825792e-06, 1.7825792e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:23,405] [INFO] [timer.py:215:stop] epoch=0/micro_step=205/global_step=205, RunningAvgSamplesPerSec=20.21329485249404, CurrSamplesPerSec=20.19042805274451, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      205/54931640 | consumed samples:         1640 | consumed tokens:      3358720 | elapsed time per iteration (ms): 1602.4 | learning rate: 1.783E-06 | global batch size:     8 | lm loss: 9.971092E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.993 | TFLOPs: 12.41 |
[default0]:[2023-07-30 22:23:25,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[1.8262698666666668e-06, 1.8262698666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:25,896] [INFO] [timer.py:215:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=20.215224298917214, CurrSamplesPerSec=20.287943116614567, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      210/54931640 | consumed samples:         1680 | consumed tokens:      3440640 | elapsed time per iteration (ms): 498.2 | learning rate: 1.826E-06 | global batch size:     8 | lm loss: 9.974121E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.058 | TFLOPs: 39.90 |
[default0]:[2023-07-30 22:23:28,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=215, skipped=0, lr=[1.8699605333333335e-06, 1.8699605333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:28,158] [INFO] [timer.py:215:stop] epoch=0/micro_step=215/global_step=215, RunningAvgSamplesPerSec=20.216760296398192, CurrSamplesPerSec=20.142928064727396, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      215/54931640 | consumed samples:         1720 | consumed tokens:      3522560 | elapsed time per iteration (ms): 452.6 | learning rate: 1.870E-06 | global batch size:     8 | lm loss: 9.960967E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.674 | TFLOPs: 43.92 |
[default0]:[2023-07-30 22:23:30,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[1.9136512e-06, 1.9136512e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:30,324] [INFO] [timer.py:215:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=20.217544359167935, CurrSamplesPerSec=20.188022833819467, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      220/54931640 | consumed samples:         1760 | consumed tokens:      3604480 | elapsed time per iteration (ms): 433.8 | learning rate: 1.914E-06 | global batch size:     8 | lm loss: 9.940929E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.443 | TFLOPs: 45.83 |
[default0]:[2023-07-30 22:23:32,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=225, skipped=0, lr=[1.9573418666666665e-06, 1.9573418666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:32,454] [INFO] [timer.py:215:stop] epoch=0/micro_step=225/global_step=225, RunningAvgSamplesPerSec=20.217629308485616, CurrSamplesPerSec=20.232380557290774, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      225/54931640 | consumed samples:         1800 | consumed tokens:      3686400 | elapsed time per iteration (ms): 425.2 | learning rate: 1.957E-06 | global batch size:     8 | lm loss: 9.943339E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.813 | TFLOPs: 46.74 |
[default0]:[2023-07-30 22:23:34,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[2.0010325333333335e-06, 2.0010325333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:34,630] [INFO] [timer.py:215:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=20.21840369425861, CurrSamplesPerSec=20.16425659007665, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      230/54931640 | consumed samples:         1840 | consumed tokens:      3768320 | elapsed time per iteration (ms): 435.6 | learning rate: 2.001E-06 | global batch size:     8 | lm loss: 9.963724E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.365 | TFLOPs: 45.63 |
[default0]:[2023-07-30 22:23:36,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=235, skipped=0, lr=[2.0447232e-06, 2.0447232e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:36,779] [INFO] [timer.py:215:stop] epoch=0/micro_step=235/global_step=235, RunningAvgSamplesPerSec=20.21895069645701, CurrSamplesPerSec=20.261420527451957, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      235/54931640 | consumed samples:         1880 | consumed tokens:      3850240 | elapsed time per iteration (ms): 429.3 | learning rate: 2.045E-06 | global batch size:     8 | lm loss: 9.903264E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.634 | TFLOPs: 46.30 |
[default0]:[2023-07-30 22:23:38,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[2.088413866666667e-06, 2.088413866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:38,882] [INFO] [timer.py:215:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=20.219275756838282, CurrSamplesPerSec=20.246456433027475, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      240/54931640 | consumed samples:         1920 | consumed tokens:      3932160 | elapsed time per iteration (ms): 420.6 | learning rate: 2.088E-06 | global batch size:     8 | lm loss: 9.902560E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.019 | TFLOPs: 47.26 |
[default0]:[2023-07-30 22:23:41,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=245, skipped=0, lr=[2.1321045333333334e-06, 2.1321045333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:41,142] [INFO] [timer.py:215:stop] epoch=0/micro_step=245/global_step=245, RunningAvgSamplesPerSec=20.220515342073377, CurrSamplesPerSec=20.268164037489036, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      245/54931640 | consumed samples:         1960 | consumed tokens:      4014080 | elapsed time per iteration (ms): 451.9 | learning rate: 2.132E-06 | global batch size:     8 | lm loss: 9.872918E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.703 | TFLOPs: 43.99 |
[default0]:[2023-07-30 22:23:43,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[2.1757952000000004e-06, 2.1757952000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:43,257] [INFO] [timer.py:215:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=20.221042948943218, CurrSamplesPerSec=20.22418577514399, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      250/54931640 | consumed samples:         2000 | consumed tokens:      4096000 | elapsed time per iteration (ms): 422.9 | learning rate: 2.176E-06 | global batch size:     8 | lm loss: 9.874544E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.919 | TFLOPs: 47.01 |
[default0]:[2023-07-30 22:23:45,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=255, skipped=0, lr=[2.219485866666667e-06, 2.219485866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:45,479] [INFO] [timer.py:215:stop] epoch=0/micro_step=255/global_step=255, RunningAvgSamplesPerSec=20.221315393189137, CurrSamplesPerSec=20.22188218859251, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      255/54931640 | consumed samples:         2040 | consumed tokens:      4177920 | elapsed time per iteration (ms): 444.5 | learning rate: 2.219E-06 | global batch size:     8 | lm loss: 9.905542E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.998 | TFLOPs: 44.72 |
[default0]:[2023-07-30 22:23:47,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[2.2631765333333334e-06, 2.2631765333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:47,542] [INFO] [timer.py:215:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=20.22083034375685, CurrSamplesPerSec=20.219311079562573, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      260/54931640 | consumed samples:         2080 | consumed tokens:      4259840 | elapsed time per iteration (ms): 412.6 | learning rate: 2.263E-06 | global batch size:     8 | lm loss: 9.888454E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.387 | TFLOPs: 48.17 |
[default0]:[2023-07-30 22:23:49,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=265, skipped=0, lr=[2.3068672e-06, 2.3068672e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:49,853] [INFO] [timer.py:215:stop] epoch=0/micro_step=265/global_step=265, RunningAvgSamplesPerSec=20.221435175555033, CurrSamplesPerSec=20.290642139699703, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      265/54931640 | consumed samples:         2120 | consumed tokens:      4341760 | elapsed time per iteration (ms): 462.4 | learning rate: 2.307E-06 | global batch size:     8 | lm loss: 9.794537E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.302 | TFLOPs: 42.99 |
[default0]:[2023-07-30 22:23:51,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[2.350557866666667e-06, 2.350557866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:51,937] [INFO] [timer.py:215:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=20.221734719078952, CurrSamplesPerSec=20.18031299109718, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      270/54931640 | consumed samples:         2160 | consumed tokens:      4423680 | elapsed time per iteration (ms): 416.5 | learning rate: 2.351E-06 | global batch size:     8 | lm loss: 9.840553E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.210 | TFLOPs: 47.73 |
[default0]:[2023-07-30 22:23:54,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=275, skipped=0, lr=[2.3942485333333334e-06, 2.3942485333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:54,145] [INFO] [timer.py:215:stop] epoch=0/micro_step=275/global_step=275, RunningAvgSamplesPerSec=20.222489721594943, CurrSamplesPerSec=20.250097616114896, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      275/54931640 | consumed samples:         2200 | consumed tokens:      4505600 | elapsed time per iteration (ms): 442.0 | learning rate: 2.394E-06 | global batch size:     8 | lm loss: 9.802496E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.099 | TFLOPs: 44.97 |
[default0]:[2023-07-30 22:23:56,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[2.4379392000000003e-06, 2.4379392000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:56,379] [INFO] [timer.py:215:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=20.22268050591817, CurrSamplesPerSec=20.183299097435466, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      280/54931640 | consumed samples:         2240 | consumed tokens:      4587520 | elapsed time per iteration (ms): 446.5 | learning rate: 2.438E-06 | global batch size:     8 | lm loss: 9.801605E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.919 | TFLOPs: 44.52 |
[default0]:[2023-07-30 22:23:58,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=285, skipped=0, lr=[2.481629866666667e-06, 2.481629866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:58,536] [INFO] [timer.py:215:stop] epoch=0/micro_step=285/global_step=285, RunningAvgSamplesPerSec=20.222529966781114, CurrSamplesPerSec=20.17411296732526, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      285/54931640 | consumed samples:         2280 | consumed tokens:      4669440 | elapsed time per iteration (ms): 431.6 | learning rate: 2.482E-06 | global batch size:     8 | lm loss: 9.793652E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.538 | TFLOPs: 46.06 |
[default0]:[2023-07-30 22:24:00,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[2.5253205333333333e-06, 2.5253205333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:00,734] [INFO] [timer.py:215:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=20.223178018415567, CurrSamplesPerSec=20.229123799751495, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      290/54931640 | consumed samples:         2320 | consumed tokens:      4751360 | elapsed time per iteration (ms): 439.3 | learning rate: 2.525E-06 | global batch size:     8 | lm loss: 9.808792E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.212 | TFLOPs: 45.25 |
[default0]:[2023-07-30 22:24:02,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=295, skipped=0, lr=[2.5690112000000003e-06, 2.5690112000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:02,982] [INFO] [timer.py:215:stop] epoch=0/micro_step=295/global_step=295, RunningAvgSamplesPerSec=20.223886487638506, CurrSamplesPerSec=20.210615586440017, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      295/54931640 | consumed samples:         2360 | consumed tokens:      4833280 | elapsed time per iteration (ms): 449.9 | learning rate: 2.569E-06 | global batch size:     8 | lm loss: 9.746708E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.782 | TFLOPs: 44.18 |
[default0]:[2023-07-30 22:24:05,063] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[2.612701866666667e-06, 2.612701866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:05,064] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=20.22406989206594, CurrSamplesPerSec=20.116903641068316, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      300/54931640 | consumed samples:         2400 | consumed tokens:      4915200 | elapsed time per iteration (ms): 416.1 | learning rate: 2.613E-06 | global batch size:     8 | lm loss: 9.754240E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.227 | TFLOPs: 47.77 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 300 | lm loss value: 9.768401E+00 | lm loss PPL: 1.747281E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:24:12,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=305, skipped=0, lr=[2.6563925333333333e-06, 2.6563925333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:12,740] [INFO] [timer.py:215:stop] epoch=0/micro_step=305/global_step=305, RunningAvgSamplesPerSec=20.224044890814888, CurrSamplesPerSec=20.039340001481104, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      305/54931640 | consumed samples:         2440 | consumed tokens:      4997120 | elapsed time per iteration (ms): 1535.6 | learning rate: 2.656E-06 | global batch size:     8 | lm loss: 9.794696E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 5.210 | TFLOPs: 12.95 |
[default0]:[2023-07-30 22:24:14,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[2.7000832e-06, 2.7000832e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:14,804] [INFO] [timer.py:215:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=20.224708361686076, CurrSamplesPerSec=20.187634165359107, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      310/54931640 | consumed samples:         2480 | consumed tokens:      5079040 | elapsed time per iteration (ms): 412.6 | learning rate: 2.700E-06 | global batch size:     8 | lm loss: 9.759076E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.390 | TFLOPs: 48.18 |
[default0]:[2023-07-30 22:24:16,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=315, skipped=0, lr=[2.743773866666667e-06, 2.743773866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:16,863] [INFO] [timer.py:215:stop] epoch=0/micro_step=315/global_step=315, RunningAvgSamplesPerSec=20.2257402926434, CurrSamplesPerSec=20.18189090467286, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      315/54931640 | consumed samples:         2520 | consumed tokens:      5160960 | elapsed time per iteration (ms): 411.7 | learning rate: 2.744E-06 | global batch size:     8 | lm loss: 9.775990E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.430 | TFLOPs: 48.28 |
[default0]:[2023-07-30 22:24:18,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[2.7874645333333337e-06, 2.7874645333333337e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:18,950] [INFO] [timer.py:215:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=20.224869685152886, CurrSamplesPerSec=20.246993974976693, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      320/54931640 | consumed samples:         2560 | consumed tokens:      5242880 | elapsed time per iteration (ms): 417.3 | learning rate: 2.787E-06 | global batch size:     8 | lm loss: 9.675265E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.171 | TFLOPs: 47.64 |
[default0]:[2023-07-30 22:24:21,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=325, skipped=0, lr=[2.8311552e-06, 2.8311552e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:21,046] [INFO] [timer.py:215:stop] epoch=0/micro_step=325/global_step=325, RunningAvgSamplesPerSec=20.224865476265443, CurrSamplesPerSec=20.132051212053398, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      325/54931640 | consumed samples:         2600 | consumed tokens:      5324800 | elapsed time per iteration (ms): 419.3 | learning rate: 2.831E-06 | global batch size:     8 | lm loss: 9.752688E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.078 | TFLOPs: 47.41 |
[default0]:[2023-07-30 22:24:23,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[2.8748458666666667e-06, 2.8748458666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:23,126] [INFO] [timer.py:215:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=20.225121673475936, CurrSamplesPerSec=20.175095495866003, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      330/54931640 | consumed samples:         2640 | consumed tokens:      5406720 | elapsed time per iteration (ms): 415.9 | learning rate: 2.875E-06 | global batch size:     8 | lm loss: 9.679404E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.236 | TFLOPs: 47.80 |
[default0]:[2023-07-30 22:24:25,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=335, skipped=0, lr=[2.9185365333333332e-06, 2.9185365333333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:25,208] [INFO] [timer.py:215:stop] epoch=0/micro_step=335/global_step=335, RunningAvgSamplesPerSec=20.22579841863172, CurrSamplesPerSec=20.22351536512938, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      335/54931640 | consumed samples:         2680 | consumed tokens:      5488640 | elapsed time per iteration (ms): 416.5 | learning rate: 2.919E-06 | global batch size:     8 | lm loss: 9.668420E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.208 | TFLOPs: 47.73 |
[default0]:[2023-07-30 22:24:27,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[2.9622271999999998e-06, 2.9622271999999998e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:27,279] [INFO] [timer.py:215:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=20.225554032732916, CurrSamplesPerSec=20.07565638127102, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      340/54931640 | consumed samples:         2720 | consumed tokens:      5570560 | elapsed time per iteration (ms): 414.2 | learning rate: 2.962E-06 | global batch size:     8 | lm loss: 9.682721E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.316 | TFLOPs: 48.00 |
[default0]:[2023-07-30 22:24:29,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=345, skipped=0, lr=[3.0059178666666667e-06, 3.0059178666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:29,339] [INFO] [timer.py:215:stop] epoch=0/micro_step=345/global_step=345, RunningAvgSamplesPerSec=20.22589027019439, CurrSamplesPerSec=20.24582119297602, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      345/54931640 | consumed samples:         2760 | consumed tokens:      5652480 | elapsed time per iteration (ms): 412.0 | learning rate: 3.006E-06 | global batch size:     8 | lm loss: 9.663817E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.416 | TFLOPs: 48.24 |
[default0]:[2023-07-30 22:24:31,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[3.0496085333333332e-06, 3.0496085333333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:31,399] [INFO] [timer.py:215:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=20.226471387703953, CurrSamplesPerSec=20.273025574472534, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      350/54931640 | consumed samples:         2800 | consumed tokens:      5734400 | elapsed time per iteration (ms): 412.0 | learning rate: 3.050E-06 | global batch size:     8 | lm loss: 9.665511E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.416 | TFLOPs: 48.24 |
[default0]:[2023-07-30 22:24:33,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=355, skipped=0, lr=[3.0932992e-06, 3.0932992e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:33,464] [INFO] [timer.py:215:stop] epoch=0/micro_step=355/global_step=355, RunningAvgSamplesPerSec=20.22652653309351, CurrSamplesPerSec=20.202377605115252, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      355/54931640 | consumed samples:         2840 | consumed tokens:      5816320 | elapsed time per iteration (ms): 413.1 | learning rate: 3.093E-06 | global batch size:     8 | lm loss: 9.633416E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.367 | TFLOPs: 48.12 |
[default0]:[2023-07-30 22:24:35,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[3.1369898666666667e-06, 3.1369898666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:35,526] [INFO] [timer.py:215:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=20.22707272314846, CurrSamplesPerSec=20.225746174192103, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      360/54931640 | consumed samples:         2880 | consumed tokens:      5898240 | elapsed time per iteration (ms): 412.4 | learning rate: 3.137E-06 | global batch size:     8 | lm loss: 9.606299E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.396 | TFLOPs: 48.20 |
[default0]:[2023-07-30 22:24:37,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=365, skipped=0, lr=[3.1806805333333336e-06, 3.1806805333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:37,595] [INFO] [timer.py:215:stop] epoch=0/micro_step=365/global_step=365, RunningAvgSamplesPerSec=20.226446222239698, CurrSamplesPerSec=20.14754823814799, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      365/54931640 | consumed samples:         2920 | consumed tokens:      5980160 | elapsed time per iteration (ms): 413.5 | learning rate: 3.181E-06 | global batch size:     8 | lm loss: 9.620360E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.346 | TFLOPs: 48.07 |
[default0]:[2023-07-30 22:24:39,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[3.2243712e-06, 3.2243712e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:39,660] [INFO] [timer.py:215:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=20.226515928110373, CurrSamplesPerSec=20.257665200418742, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      370/54931640 | consumed samples:         2960 | consumed tokens:      6062080 | elapsed time per iteration (ms): 412.9 | learning rate: 3.224E-06 | global batch size:     8 | lm loss: 9.593708E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.373 | TFLOPs: 48.14 |
[default0]:[2023-07-30 22:24:41,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=375, skipped=0, lr=[3.268061866666667e-06, 3.268061866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:41,720] [INFO] [timer.py:215:stop] epoch=0/micro_step=375/global_step=375, RunningAvgSamplesPerSec=20.226755574449243, CurrSamplesPerSec=20.202511403428495, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      375/54931640 | consumed samples:         3000 | consumed tokens:      6144000 | elapsed time per iteration (ms): 412.3 | learning rate: 3.268E-06 | global batch size:     8 | lm loss: 9.612012E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.405 | TFLOPs: 48.22 |
[default0]:[2023-07-30 22:24:43,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[3.3117525333333336e-06, 3.3117525333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:43,783] [INFO] [timer.py:215:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=20.227135139613196, CurrSamplesPerSec=20.29560039823721, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      380/54931640 | consumed samples:         3040 | consumed tokens:      6225920 | elapsed time per iteration (ms): 412.4 | learning rate: 3.312E-06 | global batch size:     8 | lm loss: 9.579967E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.401 | TFLOPs: 48.21 |
[default0]:[2023-07-30 22:24:45,846] [INFO] [logging.py:96:log_dist] [Rank 0] step=385, skipped=0, lr=[3.3554432e-06, 3.3554432e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:45,847] [INFO] [timer.py:215:stop] epoch=0/micro_step=385/global_step=385, RunningAvgSamplesPerSec=20.227509902118324, CurrSamplesPerSec=20.311866737854746, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      385/54931640 | consumed samples:         3080 | consumed tokens:      6307840 | elapsed time per iteration (ms): 412.9 | learning rate: 3.355E-06 | global batch size:     8 | lm loss: 9.579479E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.376 | TFLOPs: 48.15 |
[default0]:[2023-07-30 22:24:47,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[3.399133866666667e-06, 3.399133866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:47,916] [INFO] [timer.py:215:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=20.226712464179656, CurrSamplesPerSec=20.251332005552538, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      390/54931640 | consumed samples:         3120 | consumed tokens:      6389760 | elapsed time per iteration (ms): 413.9 | learning rate: 3.399E-06 | global batch size:     8 | lm loss: 9.509238E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.329 | TFLOPs: 48.03 |
[default0]:[2023-07-30 22:24:49,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=395, skipped=0, lr=[3.4428245333333336e-06, 3.4428245333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:49,980] [INFO] [timer.py:215:stop] epoch=0/micro_step=395/global_step=395, RunningAvgSamplesPerSec=20.22674012973035, CurrSamplesPerSec=20.24915664802743, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      395/54931640 | consumed samples:         3160 | consumed tokens:      6471680 | elapsed time per iteration (ms): 412.7 | learning rate: 3.443E-06 | global batch size:     8 | lm loss: 9.512502E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.385 | TFLOPs: 48.17 |
[default0]:[2023-07-30 22:24:52,037] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[3.4865152000000005e-06, 3.4865152000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:52,038] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=20.2276242160966, CurrSamplesPerSec=20.25886381804087, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      400/54931640 | consumed samples:         3200 | consumed tokens:      6553600 | elapsed time per iteration (ms): 411.7 | learning rate: 3.487E-06 | global batch size:     8 | lm loss: 9.525685E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.432 | TFLOPs: 48.28 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 400 | lm loss value: 9.533587E+00 | lm loss PPL: 1.381605E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:25:00,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=405, skipped=0, lr=[3.530205866666667e-06, 3.530205866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:00,178] [INFO] [timer.py:215:stop] epoch=0/micro_step=405/global_step=405, RunningAvgSamplesPerSec=20.22819833597993, CurrSamplesPerSec=20.26691535407151, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      405/54931640 | consumed samples:         3240 | consumed tokens:      6635520 | elapsed time per iteration (ms): 1627.9 | learning rate: 3.530E-06 | global batch size:     8 | lm loss: 9.478799E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.914 | TFLOPs: 12.21 |
[default0]:[2023-07-30 22:25:02,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[3.573896533333333e-06, 3.573896533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:02,343] [INFO] [timer.py:215:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=20.228865423078968, CurrSamplesPerSec=20.258362338309208, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      410/54931640 | consumed samples:         3280 | consumed tokens:      6717440 | elapsed time per iteration (ms): 432.9 | learning rate: 3.574E-06 | global batch size:     8 | lm loss: 9.563397E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.479 | TFLOPs: 45.92 |
[default0]:[2023-07-30 22:25:04,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=415, skipped=0, lr=[3.6175872e-06, 3.6175872e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:04,558] [INFO] [timer.py:215:stop] epoch=0/micro_step=415/global_step=415, RunningAvgSamplesPerSec=20.229484094028166, CurrSamplesPerSec=20.281137406956326, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      415/54931640 | consumed samples:         3320 | consumed tokens:      6799360 | elapsed time per iteration (ms): 443.1 | learning rate: 3.618E-06 | global batch size:     8 | lm loss: 9.492537E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.056 | TFLOPs: 44.87 |
[default0]:[2023-07-30 22:25:06,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[3.6612778666666666e-06, 3.6612778666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:06,679] [INFO] [timer.py:215:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=20.22928946328328, CurrSamplesPerSec=20.163057024770545, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      420/54931640 | consumed samples:         3360 | consumed tokens:      6881280 | elapsed time per iteration (ms): 424.2 | learning rate: 3.661E-06 | global batch size:     8 | lm loss: 9.445360E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.858 | TFLOPs: 46.86 |
[default0]:[2023-07-30 22:25:08,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=425, skipped=0, lr=[3.7049685333333335e-06, 3.7049685333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:08,771] [INFO] [timer.py:215:stop] epoch=0/micro_step=425/global_step=425, RunningAvgSamplesPerSec=20.22930947488252, CurrSamplesPerSec=20.060665483712054, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      425/54931640 | consumed samples:         3400 | consumed tokens:      6963200 | elapsed time per iteration (ms): 418.6 | learning rate: 3.705E-06 | global batch size:     8 | lm loss: 9.442200E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.113 | TFLOPs: 47.49 |
[default0]:[2023-07-30 22:25:10,876] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[3.7486592e-06, 3.7486592e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:10,876] [INFO] [timer.py:215:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=20.229734397544398, CurrSamplesPerSec=20.226441116882537, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      430/54931640 | consumed samples:         3440 | consumed tokens:      7045120 | elapsed time per iteration (ms): 420.8 | learning rate: 3.749E-06 | global batch size:     8 | lm loss: 9.429483E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.011 | TFLOPs: 47.24 |
[default0]:[2023-07-30 22:25:13,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=435, skipped=0, lr=[3.792349866666667e-06, 3.792349866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:13,089] [INFO] [timer.py:215:stop] epoch=0/micro_step=435/global_step=435, RunningAvgSamplesPerSec=20.230436900907904, CurrSamplesPerSec=20.236333986278503, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      435/54931640 | consumed samples:         3480 | consumed tokens:      7127040 | elapsed time per iteration (ms): 442.6 | learning rate: 3.792E-06 | global batch size:     8 | lm loss: 9.428737E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.077 | TFLOPs: 44.92 |
[default0]:[2023-07-30 22:25:15,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[3.8360405333333335e-06, 3.8360405333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:15,165] [INFO] [timer.py:215:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=20.228583057728585, CurrSamplesPerSec=19.438840335219222, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      440/54931640 | consumed samples:         3520 | consumed tokens:      7208960 | elapsed time per iteration (ms): 415.3 | learning rate: 3.836E-06 | global batch size:     8 | lm loss: 9.422472E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.263 | TFLOPs: 47.86 |
[default0]:[2023-07-30 22:25:17,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=445, skipped=0, lr=[3.8797312e-06, 3.8797312e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:17,252] [INFO] [timer.py:215:stop] epoch=0/micro_step=445/global_step=445, RunningAvgSamplesPerSec=20.228248918112993, CurrSamplesPerSec=20.061529042626464, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      445/54931640 | consumed samples:         3560 | consumed tokens:      7290880 | elapsed time per iteration (ms): 417.3 | learning rate: 3.880E-06 | global batch size:     8 | lm loss: 9.367233E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.169 | TFLOPs: 47.63 |
[default0]:[2023-07-30 22:25:19,372] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[3.9234218666666665e-06, 3.9234218666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:19,373] [INFO] [timer.py:215:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=20.22843566838371, CurrSamplesPerSec=20.241961745030817, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      450/54931640 | consumed samples:         3600 | consumed tokens:      7372800 | elapsed time per iteration (ms): 425.1 | learning rate: 3.923E-06 | global batch size:     8 | lm loss: 9.466899E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.819 | TFLOPs: 46.76 |
[default0]:[2023-07-30 22:25:21,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=455, skipped=0, lr=[3.967112533333334e-06, 3.967112533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:21,496] [INFO] [timer.py:215:stop] epoch=0/micro_step=455/global_step=455, RunningAvgSamplesPerSec=20.22886152979426, CurrSamplesPerSec=20.24874118345652, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      455/54931640 | consumed samples:         3640 | consumed tokens:      7454720 | elapsed time per iteration (ms): 423.5 | learning rate: 3.967E-06 | global batch size:     8 | lm loss: 9.341825E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.891 | TFLOPs: 46.94 |
[default0]:[2023-07-30 22:25:23,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[4.0108032e-06, 4.0108032e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:23,567] [INFO] [timer.py:215:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=20.228606244287292, CurrSamplesPerSec=20.13479348810974, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      460/54931640 | consumed samples:         3680 | consumed tokens:      7536640 | elapsed time per iteration (ms): 414.4 | learning rate: 4.011E-06 | global batch size:     8 | lm loss: 9.394095E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.303 | TFLOPs: 47.96 |
[default0]:[2023-07-30 22:25:25,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=465, skipped=0, lr=[4.054493866666667e-06, 4.054493866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:25,697] [INFO] [timer.py:215:stop] epoch=0/micro_step=465/global_step=465, RunningAvgSamplesPerSec=20.228991650555322, CurrSamplesPerSec=20.19127851979868, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      465/54931640 | consumed samples:         3720 | consumed tokens:      7618560 | elapsed time per iteration (ms): 425.9 | learning rate: 4.054E-06 | global batch size:     8 | lm loss: 9.342456E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.784 | TFLOPs: 46.67 |
[default0]:[2023-07-30 22:25:27,760] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[4.098184533333333e-06, 4.098184533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:27,762] [INFO] [timer.py:215:stop] epoch=0/micro_step=470/global_step=470, RunningAvgSamplesPerSec=20.228686773046114, CurrSamplesPerSec=20.137826566296912, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      470/54931640 | consumed samples:         3760 | consumed tokens:      7700480 | elapsed time per iteration (ms): 412.8 | learning rate: 4.098E-06 | global batch size:     8 | lm loss: 9.365424E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.380 | TFLOPs: 48.15 |
[default0]:[2023-07-30 22:25:29,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=475, skipped=0, lr=[4.141875200000001e-06, 4.141875200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:29,902] [INFO] [timer.py:215:stop] epoch=0/micro_step=475/global_step=475, RunningAvgSamplesPerSec=20.22851849690423, CurrSamplesPerSec=20.250830898625843, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      475/54931640 | consumed samples:         3800 | consumed tokens:      7782400 | elapsed time per iteration (ms): 428.9 | learning rate: 4.142E-06 | global batch size:     8 | lm loss: 9.331149E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.653 | TFLOPs: 46.35 |
[default0]:[2023-07-30 22:25:32,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[4.185565866666667e-06, 4.185565866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:32,067] [INFO] [timer.py:215:stop] epoch=0/micro_step=480/global_step=480, RunningAvgSamplesPerSec=20.228759238618128, CurrSamplesPerSec=20.179912483070357, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      480/54931640 | consumed samples:         3840 | consumed tokens:      7864320 | elapsed time per iteration (ms): 432.3 | learning rate: 4.186E-06 | global batch size:     8 | lm loss: 9.318977E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.505 | TFLOPs: 45.98 |
[default0]:[2023-07-30 22:25:34,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=485, skipped=0, lr=[4.229256533333334e-06, 4.229256533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:34,157] [INFO] [timer.py:215:stop] epoch=0/micro_step=485/global_step=485, RunningAvgSamplesPerSec=20.229259669581214, CurrSamplesPerSec=20.345415276630785, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      485/54931640 | consumed samples:         3880 | consumed tokens:      7946240 | elapsed time per iteration (ms): 418.0 | learning rate: 4.229E-06 | global batch size:     8 | lm loss: 9.244772E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.138 | TFLOPs: 47.55 |
[default0]:[2023-07-30 22:25:36,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[4.2729472e-06, 4.2729472e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:36,380] [INFO] [timer.py:215:stop] epoch=0/micro_step=490/global_step=490, RunningAvgSamplesPerSec=20.229929593147155, CurrSamplesPerSec=20.227721399489038, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      490/54931640 | consumed samples:         3920 | consumed tokens:      8028160 | elapsed time per iteration (ms): 444.5 | learning rate: 4.273E-06 | global batch size:     8 | lm loss: 9.279502E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.999 | TFLOPs: 44.72 |
[default0]:[2023-07-30 22:25:38,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=495, skipped=0, lr=[4.316637866666667e-06, 4.316637866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:38,514] [INFO] [timer.py:215:stop] epoch=0/micro_step=495/global_step=495, RunningAvgSamplesPerSec=20.229594022734112, CurrSamplesPerSec=20.189942103736723, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      495/54931640 | consumed samples:         3960 | consumed tokens:      8110080 | elapsed time per iteration (ms): 427.0 | learning rate: 4.317E-06 | global batch size:     8 | lm loss: 9.226134E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.736 | TFLOPs: 46.55 |
[default0]:[2023-07-30 22:25:40,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[4.360328533333333e-06, 4.360328533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:40,571] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=500, RunningAvgSamplesPerSec=20.230115367794685, CurrSamplesPerSec=20.228123807497102, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      500/54931640 | consumed samples:         4000 | consumed tokens:      8192000 | elapsed time per iteration (ms): 411.2 | learning rate: 4.360E-06 | global batch size:     8 | lm loss: 9.250919E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.457 | TFLOPs: 48.35 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 500 | lm loss value: 9.255599E+00 | lm loss PPL: 1.046298E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:25:48,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=505, skipped=0, lr=[4.4040192e-06, 4.4040192e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:48,206] [INFO] [timer.py:215:stop] epoch=0/micro_step=505/global_step=505, RunningAvgSamplesPerSec=20.22949321529312, CurrSamplesPerSec=20.210031283766774, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      505/54931640 | consumed samples:         4040 | consumed tokens:      8273920 | elapsed time per iteration (ms): 1526.9 | learning rate: 4.404E-06 | global batch size:     8 | lm loss: 9.161015E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 5.239 | TFLOPs: 13.02 |
[default0]:[2023-07-30 22:25:50,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[4.447709866666666e-06, 4.447709866666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:50,270] [INFO] [timer.py:215:stop] epoch=0/micro_step=510/global_step=510, RunningAvgSamplesPerSec=20.229422933218732, CurrSamplesPerSec=20.19836447965231, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      510/54931640 | consumed samples:         4080 | consumed tokens:      8355840 | elapsed time per iteration (ms): 412.8 | learning rate: 4.448E-06 | global batch size:     8 | lm loss: 9.246819E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.380 | TFLOPs: 48.16 |
[default0]:[2023-07-30 22:25:52,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=515, skipped=0, lr=[4.491400533333334e-06, 4.491400533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:52,336] [INFO] [timer.py:215:stop] epoch=0/micro_step=515/global_step=515, RunningAvgSamplesPerSec=20.229768786108487, CurrSamplesPerSec=20.2738952639669, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      515/54931640 | consumed samples:         4120 | consumed tokens:      8437760 | elapsed time per iteration (ms): 413.6 | learning rate: 4.491E-06 | global batch size:     8 | lm loss: 9.176492E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.342 | TFLOPs: 48.06 |
[default0]:[2023-07-30 22:25:54,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[4.5350912e-06, 4.5350912e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:54,421] [INFO] [timer.py:215:stop] epoch=0/micro_step=520/global_step=520, RunningAvgSamplesPerSec=20.22986535893053, CurrSamplesPerSec=20.207548374275063, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      520/54931640 | consumed samples:         4160 | consumed tokens:      8519680 | elapsed time per iteration (ms): 416.6 | learning rate: 4.535E-06 | global batch size:     8 | lm loss: 9.144815E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.202 | TFLOPs: 47.71 |
[default0]:[2023-07-30 22:25:56,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=525, skipped=0, lr=[4.578781866666667e-06, 4.578781866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:56,494] [INFO] [timer.py:215:stop] epoch=0/micro_step=525/global_step=525, RunningAvgSamplesPerSec=20.22968824233201, CurrSamplesPerSec=20.136980596035908, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      525/54931640 | consumed samples:         4200 | consumed tokens:      8601600 | elapsed time per iteration (ms): 414.6 | learning rate: 4.579E-06 | global batch size:     8 | lm loss: 9.149986E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.294 | TFLOPs: 47.94 |
[default0]:[2023-07-30 22:25:58,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[4.622472533333333e-06, 4.622472533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:58,571] [INFO] [timer.py:215:stop] epoch=0/micro_step=530/global_step=530, RunningAvgSamplesPerSec=20.229724637174446, CurrSamplesPerSec=20.195896591407013, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      530/54931640 | consumed samples:         4240 | consumed tokens:      8683520 | elapsed time per iteration (ms): 415.1 | learning rate: 4.622E-06 | global batch size:     8 | lm loss: 9.108040E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.271 | TFLOPs: 47.88 |
[default0]:[2023-07-30 22:26:00,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=535, skipped=0, lr=[4.666163200000001e-06, 4.666163200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:00,663] [INFO] [timer.py:215:stop] epoch=0/micro_step=535/global_step=535, RunningAvgSamplesPerSec=20.22715710808109, CurrSamplesPerSec=18.84504510980914, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      535/54931640 | consumed samples:         4280 | consumed tokens:      8765440 | elapsed time per iteration (ms): 418.7 | learning rate: 4.666E-06 | global batch size:     8 | lm loss: 9.147506E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.108 | TFLOPs: 47.48 |
[default0]:[2023-07-30 22:26:02,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[4.709853866666667e-06, 4.709853866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:02,774] [INFO] [timer.py:215:stop] epoch=0/micro_step=540/global_step=540, RunningAvgSamplesPerSec=20.22698140402624, CurrSamplesPerSec=20.112936454512706, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      540/54931640 | consumed samples:         4320 | consumed tokens:      8847360 | elapsed time per iteration (ms): 422.0 | learning rate: 4.710E-06 | global batch size:     8 | lm loss: 9.088327E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.955 | TFLOPs: 47.10 |
[default0]:[2023-07-30 22:26:04,866] [INFO] [logging.py:96:log_dist] [Rank 0] step=545, skipped=0, lr=[4.753544533333334e-06, 4.753544533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:04,866] [INFO] [timer.py:215:stop] epoch=0/micro_step=545/global_step=545, RunningAvgSamplesPerSec=20.22718775467707, CurrSamplesPerSec=20.25821556845014, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      545/54931640 | consumed samples:         4360 | consumed tokens:      8929280 | elapsed time per iteration (ms): 418.5 | learning rate: 4.754E-06 | global batch size:     8 | lm loss: 9.105347E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.117 | TFLOPs: 47.50 |
[default0]:[2023-07-30 22:26:06,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[4.7972352e-06, 4.7972352e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:06,935] [INFO] [timer.py:215:stop] epoch=0/micro_step=550/global_step=550, RunningAvgSamplesPerSec=20.22737541350025, CurrSamplesPerSec=20.196467920748812, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      550/54931640 | consumed samples:         4400 | consumed tokens:      9011200 | elapsed time per iteration (ms): 413.7 | learning rate: 4.797E-06 | global batch size:     8 | lm loss: 9.084272E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.337 | TFLOPs: 48.05 |
[default0]:[2023-07-30 22:26:08,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=555, skipped=0, lr=[4.840925866666667e-06, 4.840925866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:08,998] [INFO] [timer.py:215:stop] epoch=0/micro_step=555/global_step=555, RunningAvgSamplesPerSec=20.227420458422113, CurrSamplesPerSec=20.18621322368243, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      555/54931640 | consumed samples:         4440 | consumed tokens:      9093120 | elapsed time per iteration (ms): 412.8 | learning rate: 4.841E-06 | global batch size:     8 | lm loss: 9.038979E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.378 | TFLOPs: 48.15 |
[default0]:[2023-07-30 22:26:11,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[4.884616533333334e-06, 4.884616533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:11,056] [INFO] [timer.py:215:stop] epoch=0/micro_step=560/global_step=560, RunningAvgSamplesPerSec=20.22796982674075, CurrSamplesPerSec=20.24528371329707, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      560/54931640 | consumed samples:         4480 | consumed tokens:      9175040 | elapsed time per iteration (ms): 411.4 | learning rate: 4.885E-06 | global batch size:     8 | lm loss: 9.093192E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.446 | TFLOPs: 48.32 |
[default0]:[2023-07-30 22:26:13,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=565, skipped=0, lr=[4.928307200000001e-06, 4.928307200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:13,115] [INFO] [timer.py:215:stop] epoch=0/micro_step=565/global_step=565, RunningAvgSamplesPerSec=20.228098920504763, CurrSamplesPerSec=20.178565436367737, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      565/54931640 | consumed samples:         4520 | consumed tokens:      9256960 | elapsed time per iteration (ms): 411.7 | learning rate: 4.928E-06 | global batch size:     8 | lm loss: 9.075622E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.431 | TFLOPs: 48.28 |
[default0]:[2023-07-30 22:26:15,177] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[4.971997866666667e-06, 4.971997866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:15,178] [INFO] [timer.py:215:stop] epoch=0/micro_step=570/global_step=570, RunningAvgSamplesPerSec=20.228409092356404, CurrSamplesPerSec=20.267013284465378, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      570/54931640 | consumed samples:         4560 | consumed tokens:      9338880 | elapsed time per iteration (ms): 412.6 | learning rate: 4.972E-06 | global batch size:     8 | lm loss: 8.984744E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.390 | TFLOPs: 48.18 |
[default0]:[2023-07-30 22:26:17,245] [INFO] [logging.py:96:log_dist] [Rank 0] step=575, skipped=0, lr=[5.015688533333334e-06, 5.015688533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:17,246] [INFO] [timer.py:215:stop] epoch=0/micro_step=575/global_step=575, RunningAvgSamplesPerSec=20.228220022603704, CurrSamplesPerSec=20.237786405260767, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      575/54931640 | consumed samples:         4600 | consumed tokens:      9420800 | elapsed time per iteration (ms): 413.6 | learning rate: 5.016E-06 | global batch size:     8 | lm loss: 8.977033E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.343 | TFLOPs: 48.06 |
[default0]:[2023-07-30 22:26:19,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[5.0593792e-06, 5.0593792e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:19,334] [INFO] [timer.py:215:stop] epoch=0/micro_step=580/global_step=580, RunningAvgSamplesPerSec=20.227619017328553, CurrSamplesPerSec=20.233991027063535, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      580/54931640 | consumed samples:         4640 | consumed tokens:      9502720 | elapsed time per iteration (ms): 417.7 | learning rate: 5.059E-06 | global batch size:     8 | lm loss: 8.955922E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.153 | TFLOPs: 47.59 |
[default0]:[2023-07-30 22:26:21,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=585, skipped=0, lr=[5.103069866666667e-06, 5.103069866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:21,394] [INFO] [timer.py:215:stop] epoch=0/micro_step=585/global_step=585, RunningAvgSamplesPerSec=20.227896884977515, CurrSamplesPerSec=20.240191288276353, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      585/54931640 | consumed samples:         4680 | consumed tokens:      9584640 | elapsed time per iteration (ms): 412.5 | learning rate: 5.103E-06 | global batch size:     8 | lm loss: 8.920236E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.395 | TFLOPs: 48.19 |
[default0]:[2023-07-30 22:26:23,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[5.146760533333333e-06, 5.146760533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:23,459] [INFO] [timer.py:215:stop] epoch=0/micro_step=590/global_step=590, RunningAvgSamplesPerSec=20.227969864426846, CurrSamplesPerSec=20.218470432208377, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      590/54931640 | consumed samples:         4720 | consumed tokens:      9666560 | elapsed time per iteration (ms): 412.7 | learning rate: 5.147E-06 | global batch size:     8 | lm loss: 8.905588E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.387 | TFLOPs: 48.17 |
[default0]:[2023-07-30 22:26:25,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=595, skipped=0, lr=[5.1904512000000005e-06, 5.1904512000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:25,522] [INFO] [timer.py:215:stop] epoch=0/micro_step=595/global_step=595, RunningAvgSamplesPerSec=20.228008197609125, CurrSamplesPerSec=20.113635723332873, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      595/54931640 | consumed samples:         4760 | consumed tokens:      9748480 | elapsed time per iteration (ms): 412.6 | learning rate: 5.190E-06 | global batch size:     8 | lm loss: 8.943362E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.388 | TFLOPs: 48.17 |
[default0]:[2023-07-30 22:26:27,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[5.234141866666667e-06, 5.234141866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:27,616] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=600, RunningAvgSamplesPerSec=20.22588513444485, CurrSamplesPerSec=18.69997553441149, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      600/54931640 | consumed samples:         4800 | consumed tokens:      9830400 | elapsed time per iteration (ms): 418.9 | learning rate: 5.234E-06 | global batch size:     8 | lm loss: 8.909998E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.099 | TFLOPs: 47.46 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 600 | lm loss value: 8.935916E+00 | lm loss PPL: 7.600094E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:26:35,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=605, skipped=0, lr=[5.2778325333333336e-06, 5.2778325333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:35,392] [INFO] [timer.py:215:stop] epoch=0/micro_step=605/global_step=605, RunningAvgSamplesPerSec=20.225925955781676, CurrSamplesPerSec=20.28231428417723, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      605/54931640 | consumed samples:         4840 | consumed tokens:      9912320 | elapsed time per iteration (ms): 1554.8 | learning rate: 5.278E-06 | global batch size:     8 | lm loss: 8.893250E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 5.145 | TFLOPs: 12.79 |
[default0]:[2023-07-30 22:26:37,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[5.3215232e-06, 5.3215232e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:37,516] [INFO] [timer.py:215:stop] epoch=0/micro_step=610/global_step=610, RunningAvgSamplesPerSec=20.22617220593562, CurrSamplesPerSec=20.164268707626075, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      610/54931640 | consumed samples:         4880 | consumed tokens:      9994240 | elapsed time per iteration (ms): 425.2 | learning rate: 5.322E-06 | global batch size:     8 | lm loss: 8.836277E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.814 | TFLOPs: 46.75 |
[default0]:[2023-07-30 22:26:39,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=615, skipped=0, lr=[5.365213866666667e-06, 5.365213866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:39,585] [INFO] [timer.py:215:stop] epoch=0/micro_step=615/global_step=615, RunningAvgSamplesPerSec=20.225681696369165, CurrSamplesPerSec=20.23012389088149, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      615/54931640 | consumed samples:         4920 | consumed tokens:     10076160 | elapsed time per iteration (ms): 413.5 | learning rate: 5.365E-06 | global batch size:     8 | lm loss: 8.895032E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.346 | TFLOPs: 48.07 |
[default0]:[2023-07-30 22:26:41,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[5.408904533333334e-06, 5.408904533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:41,706] [INFO] [timer.py:215:stop] epoch=0/micro_step=620/global_step=620, RunningAvgSamplesPerSec=20.225825123716525, CurrSamplesPerSec=20.206063788530976, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      620/54931640 | consumed samples:         4960 | consumed tokens:     10158080 | elapsed time per iteration (ms): 424.2 | learning rate: 5.409E-06 | global batch size:     8 | lm loss: 8.868340E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.859 | TFLOPs: 46.86 |
[default0]:[2023-07-30 22:26:43,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=625, skipped=0, lr=[5.4525952000000005e-06, 5.4525952000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:43,781] [INFO] [timer.py:215:stop] epoch=0/micro_step=625/global_step=625, RunningAvgSamplesPerSec=20.225489724202493, CurrSamplesPerSec=19.925801484469133, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      625/54931640 | consumed samples:         5000 | consumed tokens:     10240000 | elapsed time per iteration (ms): 415.2 | learning rate: 5.453E-06 | global batch size:     8 | lm loss: 8.769933E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.267 | TFLOPs: 47.87 |
[default0]:[2023-07-30 22:26:45,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[5.496285866666666e-06, 5.496285866666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:45,846] [INFO] [timer.py:215:stop] epoch=0/micro_step=630/global_step=630, RunningAvgSamplesPerSec=20.22585005534278, CurrSamplesPerSec=20.171832899490813, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      630/54931640 | consumed samples:         5040 | consumed tokens:     10321920 | elapsed time per iteration (ms): 413.4 | learning rate: 5.496E-06 | global batch size:     8 | lm loss: 8.812601E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.352 | TFLOPs: 48.09 |
[default0]:[2023-07-30 22:26:47,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=635, skipped=0, lr=[5.5399765333333335e-06, 5.5399765333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:47,911] [INFO] [timer.py:215:stop] epoch=0/micro_step=635/global_step=635, RunningAvgSamplesPerSec=20.225663202601304, CurrSamplesPerSec=20.066459948629472, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      635/54931640 | consumed samples:         5080 | consumed tokens:     10403840 | elapsed time per iteration (ms): 412.4 | learning rate: 5.540E-06 | global batch size:     8 | lm loss: 8.705524E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.399 | TFLOPs: 48.20 |
[default0]:[2023-07-30 22:26:49,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[5.5836672e-06, 5.5836672e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:49,973] [INFO] [timer.py:215:stop] epoch=0/micro_step=640/global_step=640, RunningAvgSamplesPerSec=20.225663986608325, CurrSamplesPerSec=20.22827014165153, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      640/54931640 | consumed samples:         5120 | consumed tokens:     10485760 | elapsed time per iteration (ms): 412.3 | learning rate: 5.584E-06 | global batch size:     8 | lm loss: 8.821041E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.403 | TFLOPs: 48.21 |
[default0]:[2023-07-30 22:26:52,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=645, skipped=0, lr=[5.6273578666666665e-06, 5.6273578666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:52,175] [INFO] [timer.py:215:stop] epoch=0/micro_step=645/global_step=645, RunningAvgSamplesPerSec=20.225882538542404, CurrSamplesPerSec=20.27929880969289, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      645/54931640 | consumed samples:         5160 | consumed tokens:     10567680 | elapsed time per iteration (ms): 440.5 | learning rate: 5.627E-06 | global batch size:     8 | lm loss: 8.716903E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.163 | TFLOPs: 45.13 |
[default0]:[2023-07-30 22:26:54,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=0, lr=[5.671048533333333e-06, 5.671048533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:54,256] [INFO] [timer.py:215:stop] epoch=0/micro_step=650/global_step=650, RunningAvgSamplesPerSec=20.22516965033689, CurrSamplesPerSec=20.166789474506654, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      650/54931640 | consumed samples:         5200 | consumed tokens:     10649600 | elapsed time per iteration (ms): 416.2 | learning rate: 5.671E-06 | global batch size:     8 | lm loss: 8.695701E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.219 | TFLOPs: 47.76 |
[default0]:[2023-07-30 22:26:56,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=655, skipped=0, lr=[5.7147392e-06, 5.7147392e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:56,322] [INFO] [timer.py:215:stop] epoch=0/micro_step=655/global_step=655, RunningAvgSamplesPerSec=20.225438384040885, CurrSamplesPerSec=20.20780393973028, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      655/54931640 | consumed samples:         5240 | consumed tokens:     10731520 | elapsed time per iteration (ms): 413.1 | learning rate: 5.715E-06 | global batch size:     8 | lm loss: 8.741798E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.365 | TFLOPs: 48.12 |
[default0]:[2023-07-30 22:26:58,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[5.758429866666667e-06, 5.758429866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:58,389] [INFO] [timer.py:215:stop] epoch=0/micro_step=660/global_step=660, RunningAvgSamplesPerSec=20.22510613764456, CurrSamplesPerSec=20.158284411143445, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      660/54931640 | consumed samples:         5280 | consumed tokens:     10813440 | elapsed time per iteration (ms): 413.5 | learning rate: 5.758E-06 | global batch size:     8 | lm loss: 8.672836E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.347 | TFLOPs: 48.07 |
[default0]:[2023-07-30 22:27:00,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=665, skipped=0, lr=[5.8021205333333335e-06, 5.8021205333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:00,502] [INFO] [timer.py:215:stop] epoch=0/micro_step=665/global_step=665, RunningAvgSamplesPerSec=20.22530892412642, CurrSamplesPerSec=20.28475428764879, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      665/54931640 | consumed samples:         5320 | consumed tokens:     10895360 | elapsed time per iteration (ms): 422.4 | learning rate: 5.802E-06 | global batch size:     8 | lm loss: 8.637691E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.939 | TFLOPs: 47.06 |
[default0]:[2023-07-30 22:27:02,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[5.8458112e-06, 5.8458112e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:02,593] [INFO] [timer.py:215:stop] epoch=0/micro_step=670/global_step=670, RunningAvgSamplesPerSec=20.225577099865152, CurrSamplesPerSec=20.165007905682295, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      670/54931640 | consumed samples:         5360 | consumed tokens:     10977280 | elapsed time per iteration (ms): 418.4 | learning rate: 5.846E-06 | global batch size:     8 | lm loss: 8.678261E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.120 | TFLOPs: 47.51 |
[default0]:[2023-07-30 22:27:04,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=675, skipped=0, lr=[5.8895018666666665e-06, 5.8895018666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:04,744] [INFO] [timer.py:215:stop] epoch=0/micro_step=675/global_step=675, RunningAvgSamplesPerSec=20.225378459572095, CurrSamplesPerSec=20.25028093176406, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      675/54931640 | consumed samples:         5400 | consumed tokens:     11059200 | elapsed time per iteration (ms): 430.2 | learning rate: 5.890E-06 | global batch size:     8 | lm loss: 8.623372E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.596 | TFLOPs: 46.21 |
[default0]:[2023-07-30 22:27:06,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[5.933192533333334e-06, 5.933192533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:06,840] [INFO] [timer.py:215:stop] epoch=0/micro_step=680/global_step=680, RunningAvgSamplesPerSec=20.225517755882457, CurrSamplesPerSec=20.170147436024596, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      680/54931640 | consumed samples:         5440 | consumed tokens:     11141120 | elapsed time per iteration (ms): 419.0 | learning rate: 5.933E-06 | global batch size:     8 | lm loss: 8.682207E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.093 | TFLOPs: 47.44 |
[default0]:[2023-07-30 22:27:08,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=685, skipped=0, lr=[5.9768832e-06, 5.9768832e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:08,977] [INFO] [timer.py:215:stop] epoch=0/micro_step=685/global_step=685, RunningAvgSamplesPerSec=20.22506122103223, CurrSamplesPerSec=20.169395737396094, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      685/54931640 | consumed samples:         5480 | consumed tokens:     11223040 | elapsed time per iteration (ms): 427.5 | learning rate: 5.977E-06 | global batch size:     8 | lm loss: 8.578991E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.715 | TFLOPs: 46.50 |
[default0]:[2023-07-30 22:27:11,081] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[6.020573866666667e-06, 6.020573866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:11,082] [INFO] [timer.py:215:stop] epoch=0/micro_step=690/global_step=690, RunningAvgSamplesPerSec=20.225221791305618, CurrSamplesPerSec=20.216253407841425, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      690/54931640 | consumed samples:         5520 | consumed tokens:     11304960 | elapsed time per iteration (ms): 420.8 | learning rate: 6.021E-06 | global batch size:     8 | lm loss: 8.560959E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.010 | TFLOPs: 47.23 |
[default0]:[2023-07-30 22:27:13,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=695, skipped=0, lr=[6.064264533333333e-06, 6.064264533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:13,143] [INFO] [timer.py:215:stop] epoch=0/micro_step=695/global_step=695, RunningAvgSamplesPerSec=20.22544471470245, CurrSamplesPerSec=20.28318477082122, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      695/54931640 | consumed samples:         5560 | consumed tokens:     11386880 | elapsed time per iteration (ms): 412.2 | learning rate: 6.064E-06 | global batch size:     8 | lm loss: 8.564443E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.407 | TFLOPs: 48.22 |
[default0]:[2023-07-30 22:27:15,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[6.1079552e-06, 6.1079552e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:15,230] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=700, RunningAvgSamplesPerSec=20.225621918874445, CurrSamplesPerSec=20.23793287925129, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      700/54931640 | consumed samples:         5600 | consumed tokens:     11468800 | elapsed time per iteration (ms): 417.6 | learning rate: 6.108E-06 | global batch size:     8 | lm loss: 8.533223E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.157 | TFLOPs: 47.60 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 700 | lm loss value: 8.606908E+00 | lm loss PPL: 5.469311E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:27:22,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=705, skipped=0, lr=[6.151645866666667e-06, 6.151645866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:22,862] [INFO] [timer.py:215:stop] epoch=0/micro_step=705/global_step=705, RunningAvgSamplesPerSec=20.22583437694468, CurrSamplesPerSec=20.285355182601513, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      705/54931640 | consumed samples:         5640 | consumed tokens:     11550720 | elapsed time per iteration (ms): 1526.1 | learning rate: 6.152E-06 | global batch size:     8 | lm loss: 8.519010E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 5.242 | TFLOPs: 13.03 |
[default0]:[2023-07-30 22:27:24,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[6.195336533333334e-06, 6.195336533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:24,928] [INFO] [timer.py:215:stop] epoch=0/micro_step=710/global_step=710, RunningAvgSamplesPerSec=20.22576112091566, CurrSamplesPerSec=20.211759943715727, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      710/54931640 | consumed samples:         5680 | consumed tokens:     11632640 | elapsed time per iteration (ms): 413.4 | learning rate: 6.195E-06 | global batch size:     8 | lm loss: 8.488313E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.351 | TFLOPs: 48.08 |
[default0]:[2023-07-30 22:27:27,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=715, skipped=0, lr=[6.2390272e-06, 6.2390272e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:27,023] [INFO] [timer.py:215:stop] epoch=0/micro_step=715/global_step=715, RunningAvgSamplesPerSec=20.22598393801911, CurrSamplesPerSec=20.237163914454243, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      715/54931640 | consumed samples:         5720 | consumed tokens:     11714560 | elapsed time per iteration (ms): 418.9 | learning rate: 6.239E-06 | global batch size:     8 | lm loss: 8.484419E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.098 | TFLOPs: 47.45 |
[default0]:[2023-07-30 22:27:29,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[6.282717866666667e-06, 6.282717866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:29,176] [INFO] [timer.py:215:stop] epoch=0/micro_step=720/global_step=720, RunningAvgSamplesPerSec=20.22612615721914, CurrSamplesPerSec=20.185873198927013, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      720/54931640 | consumed samples:         5760 | consumed tokens:     11796480 | elapsed time per iteration (ms): 430.5 | learning rate: 6.283E-06 | global batch size:     8 | lm loss: 8.429023E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.582 | TFLOPs: 46.17 |
[default0]:[2023-07-30 22:27:31,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=725, skipped=0, lr=[6.326408533333334e-06, 6.326408533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:31,272] [INFO] [timer.py:215:stop] epoch=0/micro_step=725/global_step=725, RunningAvgSamplesPerSec=20.22610422113034, CurrSamplesPerSec=20.109561357824404, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      725/54931640 | consumed samples:         5800 | consumed tokens:     11878400 | elapsed time per iteration (ms): 419.1 | learning rate: 6.326E-06 | global batch size:     8 | lm loss: 8.478324E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.087 | TFLOPs: 47.43 |
[default0]:[2023-07-30 22:27:33,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[6.370099200000001e-06, 6.370099200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:33,339] [INFO] [timer.py:215:stop] epoch=0/micro_step=730/global_step=730, RunningAvgSamplesPerSec=20.22623744897878, CurrSamplesPerSec=20.216850251186038, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      730/54931640 | consumed samples:         5840 | consumed tokens:     11960320 | elapsed time per iteration (ms): 413.4 | learning rate: 6.370E-06 | global batch size:     8 | lm loss: 8.418149E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.351 | TFLOPs: 48.08 |
[default0]:[2023-07-30 22:27:35,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=735, skipped=0, lr=[6.413789866666667e-06, 6.413789866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:35,415] [INFO] [timer.py:215:stop] epoch=0/micro_step=735/global_step=735, RunningAvgSamplesPerSec=20.226262623777274, CurrSamplesPerSec=20.280218066652967, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      735/54931640 | consumed samples:         5880 | consumed tokens:     12042240 | elapsed time per iteration (ms): 415.4 | learning rate: 6.414E-06 | global batch size:     8 | lm loss: 8.380669E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.257 | TFLOPs: 47.85 |
[default0]:[2023-07-30 22:27:37,481] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[6.457480533333334e-06, 6.457480533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:37,482] [INFO] [timer.py:215:stop] epoch=0/micro_step=740/global_step=740, RunningAvgSamplesPerSec=20.226349376953628, CurrSamplesPerSec=20.22991654679035, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      740/54931640 | consumed samples:         5920 | consumed tokens:     12124160 | elapsed time per iteration (ms): 414.6 | learning rate: 6.457E-06 | global batch size:     8 | lm loss: 8.425439E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.294 | TFLOPs: 47.94 |
[default0]:[2023-07-30 22:27:39,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=745, skipped=0, lr=[6.501171200000001e-06, 6.501171200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:39,552] [INFO] [timer.py:215:stop] epoch=0/micro_step=745/global_step=745, RunningAvgSamplesPerSec=20.226407083253175, CurrSamplesPerSec=20.24913220846308, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      745/54931640 | consumed samples:         5960 | consumed tokens:     12206080 | elapsed time per iteration (ms): 412.6 | learning rate: 6.501E-06 | global batch size:     8 | lm loss: 8.356584E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.387 | TFLOPs: 48.17 |
[default0]:[2023-07-30 22:27:41,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[6.544861866666668e-06, 6.544861866666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:41,625] [INFO] [timer.py:215:stop] epoch=0/micro_step=750/global_step=750, RunningAvgSamplesPerSec=20.226581819414378, CurrSamplesPerSec=20.242523471891342, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      750/54931640 | consumed samples:         6000 | consumed tokens:     12288000 | elapsed time per iteration (ms): 414.5 | learning rate: 6.545E-06 | global batch size:     8 | lm loss: 8.324998E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.299 | TFLOPs: 47.95 |
[default0]:[2023-07-30 22:27:43,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=755, skipped=0, lr=[6.588552533333334e-06, 6.588552533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:43,682] [INFO] [timer.py:215:stop] epoch=0/micro_step=755/global_step=755, RunningAvgSamplesPerSec=20.226905264358606, CurrSamplesPerSec=20.28577215104163, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      755/54931640 | consumed samples:         6040 | consumed tokens:     12369920 | elapsed time per iteration (ms): 411.4 | learning rate: 6.589E-06 | global batch size:     8 | lm loss: 8.333765E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.444 | TFLOPs: 48.31 |
[default0]:[2023-07-30 22:27:45,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[6.632243200000001e-06, 6.632243200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:45,750] [INFO] [timer.py:215:stop] epoch=0/micro_step=760/global_step=760, RunningAvgSamplesPerSec=20.226933827142872, CurrSamplesPerSec=20.26205674679639, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      760/54931640 | consumed samples:         6080 | consumed tokens:     12451840 | elapsed time per iteration (ms): 413.8 | learning rate: 6.632E-06 | global batch size:     8 | lm loss: 8.297530E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.334 | TFLOPs: 48.04 |
[default0]:[2023-07-30 22:27:47,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=765, skipped=0, lr=[6.675933866666667e-06, 6.675933866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:47,819] [INFO] [timer.py:215:stop] epoch=0/micro_step=765/global_step=765, RunningAvgSamplesPerSec=20.226972482806968, CurrSamplesPerSec=20.225148805487493, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      765/54931640 | consumed samples:         6120 | consumed tokens:     12533760 | elapsed time per iteration (ms): 413.6 | learning rate: 6.676E-06 | global batch size:     8 | lm loss: 8.316183E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.341 | TFLOPs: 48.06 |
[default0]:[2023-07-30 22:27:49,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[6.7196245333333345e-06, 6.7196245333333345e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:49,909] [INFO] [timer.py:215:stop] epoch=0/micro_step=770/global_step=770, RunningAvgSamplesPerSec=20.227285110380095, CurrSamplesPerSec=20.28643443026832, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      770/54931640 | consumed samples:         6160 | consumed tokens:     12615680 | elapsed time per iteration (ms): 417.9 | learning rate: 6.720E-06 | global batch size:     8 | lm loss: 8.325215E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.142 | TFLOPs: 47.56 |
[default0]:[2023-07-30 22:27:52,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=775, skipped=0, lr=[6.763315200000001e-06, 6.763315200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:52,017] [INFO] [timer.py:215:stop] epoch=0/micro_step=775/global_step=775, RunningAvgSamplesPerSec=20.227292443778282, CurrSamplesPerSec=20.191740231918594, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      775/54931640 | consumed samples:         6200 | consumed tokens:     12697600 | elapsed time per iteration (ms): 421.5 | learning rate: 6.763E-06 | global batch size:     8 | lm loss: 8.278875E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.979 | TFLOPs: 47.16 |
[default0]:[2023-07-30 22:27:54,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=0, lr=[6.807005866666667e-06, 6.807005866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:54,163] [INFO] [timer.py:215:stop] epoch=0/micro_step=780/global_step=780, RunningAvgSamplesPerSec=20.227434234723816, CurrSamplesPerSec=20.184841051085264, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      780/54931640 | consumed samples:         6240 | consumed tokens:     12779520 | elapsed time per iteration (ms): 429.7 | learning rate: 6.807E-06 | global batch size:     8 | lm loss: 8.301543E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.620 | TFLOPs: 46.27 |
[default0]:[2023-07-30 22:27:56,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=785, skipped=0, lr=[6.850696533333333e-06, 6.850696533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:56,255] [INFO] [timer.py:215:stop] epoch=0/micro_step=785/global_step=785, RunningAvgSamplesPerSec=20.227508622934064, CurrSamplesPerSec=20.18276493222073, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      785/54931640 | consumed samples:         6280 | consumed tokens:     12861440 | elapsed time per iteration (ms): 418.0 | learning rate: 6.851E-06 | global batch size:     8 | lm loss: 8.251862E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.139 | TFLOPs: 47.56 |
[default0]:[2023-07-30 22:27:58,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=0, lr=[6.8943872e-06, 6.8943872e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:58,327] [INFO] [timer.py:215:stop] epoch=0/micro_step=790/global_step=790, RunningAvgSamplesPerSec=20.227337823803772, CurrSamplesPerSec=20.20502957493149, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      790/54931640 | consumed samples:         6320 | consumed tokens:     12943360 | elapsed time per iteration (ms): 414.3 | learning rate: 6.894E-06 | global batch size:     8 | lm loss: 8.224274E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.310 | TFLOPs: 47.98 |
[default0]:[2023-07-30 22:28:00,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=795, skipped=0, lr=[6.938077866666666e-06, 6.938077866666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:00,404] [INFO] [timer.py:215:stop] epoch=0/micro_step=795/global_step=795, RunningAvgSamplesPerSec=20.22747066567761, CurrSamplesPerSec=20.26402683315397, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      795/54931640 | consumed samples:         6360 | consumed tokens:     13025280 | elapsed time per iteration (ms): 415.5 | learning rate: 6.938E-06 | global batch size:     8 | lm loss: 8.210614E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.254 | TFLOPs: 47.84 |
[default0]:[2023-07-30 22:28:02,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=0, lr=[6.981768533333334e-06, 6.981768533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:02,619] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=800, RunningAvgSamplesPerSec=20.227869958825217, CurrSamplesPerSec=20.250354258953035, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      800/54931640 | consumed samples:         6400 | consumed tokens:     13107200 | elapsed time per iteration (ms): 442.9 | learning rate: 6.982E-06 | global batch size:     8 | lm loss: 8.132164E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.061 | TFLOPs: 44.88 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 800 | lm loss value: 8.239635E+00 | lm loss PPL: 3.788156E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:28:10,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=805, skipped=0, lr=[7.0254592e-06, 7.0254592e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:10,365] [INFO] [timer.py:215:stop] epoch=0/micro_step=805/global_step=805, RunningAvgSamplesPerSec=20.228000983203554, CurrSamplesPerSec=20.299222439807597, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      805/54931640 | consumed samples:         6440 | consumed tokens:     13189120 | elapsed time per iteration (ms): 1549.2 | learning rate: 7.025E-06 | global batch size:     8 | lm loss: 8.263889E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 5.164 | TFLOPs: 12.83 |
[default0]:[2023-07-30 22:28:12,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=0, lr=[7.069149866666667e-06, 7.069149866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:12,439] [INFO] [timer.py:215:stop] epoch=0/micro_step=810/global_step=810, RunningAvgSamplesPerSec=20.22807845580908, CurrSamplesPerSec=20.206684367511944, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      810/54931640 | consumed samples:         6480 | consumed tokens:     13271040 | elapsed time per iteration (ms): 414.7 | learning rate: 7.069E-06 | global batch size:     8 | lm loss: 8.101662E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.289 | TFLOPs: 47.93 |
[default0]:[2023-07-30 22:28:14,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=815, skipped=0, lr=[7.112840533333333e-06, 7.112840533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:14,499] [INFO] [timer.py:215:stop] epoch=0/micro_step=815/global_step=815, RunningAvgSamplesPerSec=20.22823399289864, CurrSamplesPerSec=20.254498107918373, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      815/54931640 | consumed samples:         6520 | consumed tokens:     13352960 | elapsed time per iteration (ms): 412.2 | learning rate: 7.113E-06 | global batch size:     8 | lm loss: 8.111409E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.406 | TFLOPs: 48.22 |
[default0]:[2023-07-30 22:28:16,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=0, lr=[7.1565312e-06, 7.1565312e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:16,564] [INFO] [timer.py:215:stop] epoch=0/micro_step=820/global_step=820, RunningAvgSamplesPerSec=20.22822009620585, CurrSamplesPerSec=20.113816576631013, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      820/54931640 | consumed samples:         6560 | consumed tokens:     13434880 | elapsed time per iteration (ms): 413.1 | learning rate: 7.157E-06 | global batch size:     8 | lm loss: 8.108152E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.366 | TFLOPs: 48.12 |
[default0]:[2023-07-30 22:28:18,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=825, skipped=0, lr=[7.200221866666667e-06, 7.200221866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:18,649] [INFO] [timer.py:215:stop] epoch=0/micro_step=825/global_step=825, RunningAvgSamplesPerSec=20.227851296152355, CurrSamplesPerSec=20.073062278096334, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      825/54931640 | consumed samples:         6600 | consumed tokens:     13516800 | elapsed time per iteration (ms): 417.0 | learning rate: 7.200E-06 | global batch size:     8 | lm loss: 8.122357E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.186 | TFLOPs: 47.67 |
[default0]:[2023-07-30 22:28:20,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=0, lr=[7.2439125333333336e-06, 7.2439125333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:20,716] [INFO] [timer.py:215:stop] epoch=0/micro_step=830/global_step=830, RunningAvgSamplesPerSec=20.22805925678129, CurrSamplesPerSec=20.294520175108595, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      830/54931640 | consumed samples:         6640 | consumed tokens:     13598720 | elapsed time per iteration (ms): 413.1 | learning rate: 7.244E-06 | global batch size:     8 | lm loss: 8.052557E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.367 | TFLOPs: 48.12 |
[default0]:[2023-07-30 22:28:22,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=835, skipped=0, lr=[7.2876032e-06, 7.2876032e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:22,772] [INFO] [timer.py:215:stop] epoch=0/micro_step=835/global_step=835, RunningAvgSamplesPerSec=20.228208773569097, CurrSamplesPerSec=20.229587244322445, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      835/54931640 | consumed samples:         6680 | consumed tokens:     13680640 | elapsed time per iteration (ms): 411.6 | learning rate: 7.288E-06 | global batch size:     8 | lm loss: 8.033189E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.438 | TFLOPs: 48.30 |
[default0]:[2023-07-30 22:28:24,832] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=0, lr=[7.331293866666667e-06, 7.331293866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:24,833] [INFO] [timer.py:215:stop] epoch=0/micro_step=840/global_step=840, RunningAvgSamplesPerSec=20.228361426947266, CurrSamplesPerSec=20.27431176141465, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      840/54931640 | consumed samples:         6720 | consumed tokens:     13762560 | elapsed time per iteration (ms): 411.8 | learning rate: 7.331E-06 | global batch size:     8 | lm loss: 8.127558E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.426 | TFLOPs: 48.27 |
[default0]:[2023-07-30 22:28:26,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=845, skipped=0, lr=[7.374984533333334e-06, 7.374984533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:26,899] [INFO] [timer.py:215:stop] epoch=0/micro_step=845/global_step=845, RunningAvgSamplesPerSec=20.22848307927619, CurrSamplesPerSec=20.25744506157933, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      845/54931640 | consumed samples:         6760 | consumed tokens:     13844480 | elapsed time per iteration (ms): 413.3 | learning rate: 7.375E-06 | global batch size:     8 | lm loss: 7.961227E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.357 | TFLOPs: 48.10 |
[default0]:[2023-07-30 22:28:28,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=0, lr=[7.4186752000000005e-06, 7.4186752000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:28,968] [INFO] [timer.py:215:stop] epoch=0/micro_step=850/global_step=850, RunningAvgSamplesPerSec=20.22812058632884, CurrSamplesPerSec=20.137113529512728, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      850/54931640 | consumed samples:         6800 | consumed tokens:     13926400 | elapsed time per iteration (ms): 413.8 | learning rate: 7.419E-06 | global batch size:     8 | lm loss: 8.079235E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.332 | TFLOPs: 48.04 |
[default0]:[2023-07-30 22:28:31,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=855, skipped=0, lr=[7.462365866666667e-06, 7.462365866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:31,031] [INFO] [timer.py:215:stop] epoch=0/micro_step=855/global_step=855, RunningAvgSamplesPerSec=20.22825573116016, CurrSamplesPerSec=20.234186252942767, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      855/54931640 | consumed samples:         6840 | consumed tokens:     14008320 | elapsed time per iteration (ms): 412.6 | learning rate: 7.462E-06 | global batch size:     8 | lm loss: 8.088918E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.388 | TFLOPs: 48.18 |
[default0]:[2023-07-30 22:28:33,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[7.5060565333333335e-06, 7.5060565333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:33,088] [INFO] [timer.py:215:stop] epoch=0/micro_step=860/global_step=860, RunningAvgSamplesPerSec=20.22850596353968, CurrSamplesPerSec=20.25854580362372, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      860/54931640 | consumed samples:         6880 | consumed tokens:     14090240 | elapsed time per iteration (ms): 411.6 | learning rate: 7.506E-06 | global batch size:     8 | lm loss: 8.043033E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.436 | TFLOPs: 48.29 |
[default0]:[2023-07-30 22:28:35,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=865, skipped=0, lr=[7.5497472e-06, 7.5497472e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:35,152] [INFO] [timer.py:215:stop] epoch=0/micro_step=865/global_step=865, RunningAvgSamplesPerSec=20.228604940568772, CurrSamplesPerSec=20.256699071091923, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      865/54931640 | consumed samples:         6920 | consumed tokens:     14172160 | elapsed time per iteration (ms): 413.1 | learning rate: 7.550E-06 | global batch size:     8 | lm loss: 8.004800E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.366 | TFLOPs: 48.12 |
[default0]:[2023-07-30 22:28:37,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[7.593437866666667e-06, 7.593437866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:37,320] [INFO] [timer.py:215:stop] epoch=0/micro_step=870/global_step=870, RunningAvgSamplesPerSec=20.22280106366185, CurrSamplesPerSec=20.22689224559377, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      870/54931640 | consumed samples:         6960 | consumed tokens:     14254080 | elapsed time per iteration (ms): 433.2 | learning rate: 7.593E-06 | global batch size:     8 | lm loss: 7.951138E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.467 | TFLOPs: 45.89 |
[default0]:[2023-07-30 22:28:39,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=875, skipped=0, lr=[7.637128533333333e-06, 7.637128533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:39,414] [INFO] [timer.py:215:stop] epoch=0/micro_step=875/global_step=875, RunningAvgSamplesPerSec=20.221109473711376, CurrSamplesPerSec=19.992035224232747, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      875/54931640 | consumed samples:         7000 | consumed tokens:     14336000 | elapsed time per iteration (ms): 418.8 | learning rate: 7.637E-06 | global batch size:     8 | lm loss: 7.986472E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.101 | TFLOPs: 47.46 |
[default0]:[2023-07-30 22:28:41,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[7.680819200000001e-06, 7.680819200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:41,477] [INFO] [timer.py:215:stop] epoch=0/micro_step=880/global_step=880, RunningAvgSamplesPerSec=20.221270849414065, CurrSamplesPerSec=20.19855901865426, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      880/54931640 | consumed samples:         7040 | consumed tokens:     14417920 | elapsed time per iteration (ms): 412.6 | learning rate: 7.681E-06 | global batch size:     8 | lm loss: 7.870486E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.387 | TFLOPs: 48.17 |
[default0]:[2023-07-30 22:28:43,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=885, skipped=0, lr=[7.724509866666668e-06, 7.724509866666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:43,536] [INFO] [timer.py:215:stop] epoch=0/micro_step=885/global_step=885, RunningAvgSamplesPerSec=20.22146760785129, CurrSamplesPerSec=20.245979999251812, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      885/54931640 | consumed samples:         7080 | consumed tokens:     14499840 | elapsed time per iteration (ms): 411.7 | learning rate: 7.725E-06 | global batch size:     8 | lm loss: 7.925298E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.433 | TFLOPs: 48.29 |
[default0]:[2023-07-30 22:28:45,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[7.768200533333334e-06, 7.768200533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:45,607] [INFO] [timer.py:215:stop] epoch=0/micro_step=890/global_step=890, RunningAvgSamplesPerSec=20.221492105514162, CurrSamplesPerSec=20.195179436499515, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      890/54931640 | consumed samples:         7120 | consumed tokens:     14581760 | elapsed time per iteration (ms): 414.1 | learning rate: 7.768E-06 | global batch size:     8 | lm loss: 7.863795E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.320 | TFLOPs: 48.01 |
[default0]:[2023-07-30 22:28:47,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=895, skipped=0, lr=[7.8118912e-06, 7.8118912e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:47,672] [INFO] [timer.py:215:stop] epoch=0/micro_step=895/global_step=895, RunningAvgSamplesPerSec=20.22175694951982, CurrSamplesPerSec=20.301015521721638, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      895/54931640 | consumed samples:         7160 | consumed tokens:     14663680 | elapsed time per iteration (ms): 413.0 | learning rate: 7.812E-06 | global batch size:     8 | lm loss: 7.928176E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.369 | TFLOPs: 48.13 |
[default0]:[2023-07-30 22:28:49,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[7.855581866666667e-06, 7.855581866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:49,746] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=900, RunningAvgSamplesPerSec=20.221290042001478, CurrSamplesPerSec=20.201173500042746, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      900/54931640 | consumed samples:         7200 | consumed tokens:     14745600 | elapsed time per iteration (ms): 415.0 | learning rate: 7.856E-06 | global batch size:     8 | lm loss: 7.885512E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.278 | TFLOPs: 47.90 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 900 | lm loss value: 7.910901E+00 | lm loss PPL: 2.726845E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:28:57,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=905, skipped=0, lr=[7.899272533333334e-06, 7.899272533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:57,492] [INFO] [timer.py:215:stop] epoch=0/micro_step=905/global_step=905, RunningAvgSamplesPerSec=20.221392713520412, CurrSamplesPerSec=20.22725804040336, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      905/54931640 | consumed samples:         7240 | consumed tokens:     14827520 | elapsed time per iteration (ms): 1548.8 | learning rate: 7.899E-06 | global batch size:     8 | lm loss: 7.826654E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 5.165 | TFLOPs: 12.83 |
[default0]:[2023-07-30 22:28:59,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[7.9429632e-06, 7.9429632e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:59,557] [INFO] [timer.py:215:stop] epoch=0/micro_step=910/global_step=910, RunningAvgSamplesPerSec=20.221294845373347, CurrSamplesPerSec=19.995788030928296, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      910/54931640 | consumed samples:         7280 | consumed tokens:     14909440 | elapsed time per iteration (ms): 413.3 | learning rate: 7.943E-06 | global batch size:     8 | lm loss: 7.899742E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.357 | TFLOPs: 48.10 |
[default0]:[2023-07-30 22:29:01,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=915, skipped=0, lr=[7.986653866666667e-06, 7.986653866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:01,759] [INFO] [timer.py:215:stop] epoch=0/micro_step=915/global_step=915, RunningAvgSamplesPerSec=20.221779981994064, CurrSamplesPerSec=20.31371124558969, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      915/54931640 | consumed samples:         7320 | consumed tokens:     14991360 | elapsed time per iteration (ms): 440.6 | learning rate: 7.987E-06 | global batch size:     8 | lm loss: 7.886288E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.156 | TFLOPs: 45.11 |
[default0]:[2023-07-30 22:29:03,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[8.030344533333333e-06, 8.030344533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:03,860] [INFO] [timer.py:215:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=20.222061422579465, CurrSamplesPerSec=20.25190647571028, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      920/54931640 | consumed samples:         7360 | consumed tokens:     15073280 | elapsed time per iteration (ms): 419.7 | learning rate: 8.030E-06 | global batch size:     8 | lm loss: 7.849635E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.062 | TFLOPs: 47.37 |
[default0]:[2023-07-30 22:29:05,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=925, skipped=0, lr=[8.074035200000002e-06, 8.074035200000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:05,959] [INFO] [timer.py:215:stop] epoch=0/micro_step=925/global_step=925, RunningAvgSamplesPerSec=20.221920597867122, CurrSamplesPerSec=20.09632504431987, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      925/54931640 | consumed samples:         7400 | consumed tokens:     15155200 | elapsed time per iteration (ms): 419.8 | learning rate: 8.074E-06 | global batch size:     8 | lm loss: 7.744520E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.056 | TFLOPs: 47.35 |
[default0]:[2023-07-30 22:29:08,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=0, lr=[8.117725866666668e-06, 8.117725866666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:08,031] [INFO] [timer.py:215:stop] epoch=0/micro_step=930/global_step=930, RunningAvgSamplesPerSec=20.22200353302677, CurrSamplesPerSec=20.213428015664995, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      930/54931640 | consumed samples:         7440 | consumed tokens:     15237120 | elapsed time per iteration (ms): 414.7 | learning rate: 8.118E-06 | global batch size:     8 | lm loss: 7.774886E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.293 | TFLOPs: 47.94 |
[default0]:[2023-07-30 22:29:10,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=935, skipped=0, lr=[8.161416533333335e-06, 8.161416533333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:10,148] [INFO] [timer.py:215:stop] epoch=0/micro_step=935/global_step=935, RunningAvgSamplesPerSec=20.221820640361326, CurrSamplesPerSec=20.17974257537687, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      935/54931640 | consumed samples:         7480 | consumed tokens:     15319040 | elapsed time per iteration (ms): 423.4 | learning rate: 8.161E-06 | global batch size:     8 | lm loss: 7.693834E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.894 | TFLOPs: 46.95 |
[default0]:[2023-07-30 22:29:12,230] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=0, lr=[8.205107200000001e-06, 8.205107200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:12,230] [INFO] [timer.py:215:stop] epoch=0/micro_step=940/global_step=940, RunningAvgSamplesPerSec=20.222214007734614, CurrSamplesPerSec=20.289783281733747, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      940/54931640 | consumed samples:         7520 | consumed tokens:     15400960 | elapsed time per iteration (ms): 416.2 | learning rate: 8.205E-06 | global batch size:     8 | lm loss: 7.777198E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.221 | TFLOPs: 47.76 |
[default0]:[2023-07-30 22:29:14,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=945, skipped=0, lr=[8.248797866666666e-06, 8.248797866666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:14,319] [INFO] [timer.py:215:stop] epoch=0/micro_step=945/global_step=945, RunningAvgSamplesPerSec=20.222367854051107, CurrSamplesPerSec=20.203229079173553, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      945/54931640 | consumed samples:         7560 | consumed tokens:     15482880 | elapsed time per iteration (ms): 417.7 | learning rate: 8.249E-06 | global batch size:     8 | lm loss: 7.659581E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.153 | TFLOPs: 47.59 |
[default0]:[2023-07-30 22:29:16,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=0, lr=[8.292488533333332e-06, 8.292488533333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:16,380] [INFO] [timer.py:215:stop] epoch=0/micro_step=950/global_step=950, RunningAvgSamplesPerSec=20.22252794781219, CurrSamplesPerSec=20.169735207257983, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      950/54931640 | consumed samples:         7600 | consumed tokens:     15564800 | elapsed time per iteration (ms): 412.4 | learning rate: 8.292E-06 | global batch size:     8 | lm loss: 7.756956E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.401 | TFLOPs: 48.21 |
[default0]:[2023-07-30 22:29:18,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=955, skipped=0, lr=[8.3361792e-06, 8.3361792e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:18,458] [INFO] [timer.py:215:stop] epoch=0/micro_step=955/global_step=955, RunningAvgSamplesPerSec=20.22246857069733, CurrSamplesPerSec=20.038502285461075, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      955/54931640 | consumed samples:         7640 | consumed tokens:     15646720 | elapsed time per iteration (ms): 415.5 | learning rate: 8.336E-06 | global batch size:     8 | lm loss: 7.653751E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.252 | TFLOPs: 47.84 |
[default0]:[2023-07-30 22:29:20,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=0, lr=[8.379869866666667e-06, 8.379869866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:20,536] [INFO] [timer.py:215:stop] epoch=0/micro_step=960/global_step=960, RunningAvgSamplesPerSec=20.222555441842267, CurrSamplesPerSec=20.287611922340453, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      960/54931640 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 415.6 | learning rate: 8.380E-06 | global batch size:     8 | lm loss: 7.650691E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.250 | TFLOPs: 47.83 |
[default0]:[2023-07-30 22:29:22,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=965, skipped=0, lr=[8.423560533333334e-06, 8.423560533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:22,595] [INFO] [timer.py:215:stop] epoch=0/micro_step=965/global_step=965, RunningAvgSamplesPerSec=20.22284931537483, CurrSamplesPerSec=20.24740936785136, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      965/54931640 | consumed samples:         7720 | consumed tokens:     15810560 | elapsed time per iteration (ms): 411.7 | learning rate: 8.424E-06 | global batch size:     8 | lm loss: 7.588142E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.430 | TFLOPs: 48.28 |
[default0]:[2023-07-30 22:29:24,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=0, lr=[8.4672512e-06, 8.4672512e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:24,665] [INFO] [timer.py:215:stop] epoch=0/micro_step=970/global_step=970, RunningAvgSamplesPerSec=20.22288652467089, CurrSamplesPerSec=20.17690311314651, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      970/54931640 | consumed samples:         7760 | consumed tokens:     15892480 | elapsed time per iteration (ms): 414.0 | learning rate: 8.467E-06 | global batch size:     8 | lm loss: 7.618043E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.325 | TFLOPs: 48.02 |
[default0]:[2023-07-30 22:29:26,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=975, skipped=0, lr=[8.510941866666667e-06, 8.510941866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:26,730] [INFO] [timer.py:215:stop] epoch=0/micro_step=975/global_step=975, RunningAvgSamplesPerSec=20.222940750822584, CurrSamplesPerSec=20.24373251154434, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      975/54931640 | consumed samples:         7800 | consumed tokens:     15974400 | elapsed time per iteration (ms): 413.0 | learning rate: 8.511E-06 | global batch size:     8 | lm loss: 7.687720E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.370 | TFLOPs: 48.13 |
[default0]:[2023-07-30 22:29:28,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=0, lr=[8.554632533333333e-06, 8.554632533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:28,811] [INFO] [timer.py:215:stop] epoch=0/micro_step=980/global_step=980, RunningAvgSamplesPerSec=20.22292222829387, CurrSamplesPerSec=20.22367382166703, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      980/54931640 | consumed samples:         7840 | consumed tokens:     16056320 | elapsed time per iteration (ms): 416.2 | learning rate: 8.555E-06 | global batch size:     8 | lm loss: 7.636818E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.220 | TFLOPs: 47.76 |
[default0]:[2023-07-30 22:29:30,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=985, skipped=0, lr=[8.5983232e-06, 8.5983232e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:30,876] [INFO] [timer.py:215:stop] epoch=0/micro_step=985/global_step=985, RunningAvgSamplesPerSec=20.222932647348184, CurrSamplesPerSec=20.264834556914018, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      985/54931640 | consumed samples:         7880 | consumed tokens:     16138240 | elapsed time per iteration (ms): 414.1 | learning rate: 8.598E-06 | global batch size:     8 | lm loss: 7.649901E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.319 | TFLOPs: 48.00 |
[default0]:[2023-07-30 22:29:32,943] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=0, lr=[8.642013866666666e-06, 8.642013866666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:32,945] [INFO] [timer.py:215:stop] epoch=0/micro_step=990/global_step=990, RunningAvgSamplesPerSec=20.2231788937931, CurrSamplesPerSec=20.20219515572601, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      990/54931640 | consumed samples:         7920 | consumed tokens:     16220160 | elapsed time per iteration (ms): 412.7 | learning rate: 8.642E-06 | global batch size:     8 | lm loss: 7.706475E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.386 | TFLOPs: 48.17 |
[default0]:[2023-07-30 22:29:35,076] [INFO] [logging.py:96:log_dist] [Rank 0] step=995, skipped=0, lr=[8.685704533333333e-06, 8.685704533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:35,079] [INFO] [timer.py:215:stop] epoch=0/micro_step=995/global_step=995, RunningAvgSamplesPerSec=20.223264348738866, CurrSamplesPerSec=20.161167090265636, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration      995/54931640 | consumed samples:         7960 | consumed tokens:     16302080 | elapsed time per iteration (ms): 427.0 | learning rate: 8.686E-06 | global batch size:     8 | lm loss: 7.509843E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.735 | TFLOPs: 46.55 |
[default0]:[2023-07-30 22:29:37,180] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[8.729395200000001e-06, 8.729395200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:37,185] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=20.223286099744943, CurrSamplesPerSec=20.059754029953538, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1000/54931640 | consumed samples:         8000 | consumed tokens:     16384000 | elapsed time per iteration (ms): 421.6 | learning rate: 8.729E-06 | global batch size:     8 | lm loss: 7.548872E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.976 | TFLOPs: 47.15 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1000 | lm loss value: 7.672999E+00 | lm loss PPL: 2.149518E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:29:44,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=1005, skipped=0, lr=[8.773085866666668e-06, 8.773085866666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:44,784] [INFO] [timer.py:215:stop] epoch=0/micro_step=1005/global_step=1005, RunningAvgSamplesPerSec=20.223465139336575, CurrSamplesPerSec=20.267515192588444, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1005/54931640 | consumed samples:         8040 | consumed tokens:     16465920 | elapsed time per iteration (ms): 1519.3 | learning rate: 8.773E-06 | global batch size:     8 | lm loss: 7.585951E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 5.266 | TFLOPs: 13.08 |
[default0]:[2023-07-30 22:29:46,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=0, lr=[8.816776533333334e-06, 8.816776533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:46,852] [INFO] [timer.py:215:stop] epoch=0/micro_step=1010/global_step=1010, RunningAvgSamplesPerSec=20.22341364662765, CurrSamplesPerSec=20.162911632613575, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1010/54931640 | consumed samples:         8080 | consumed tokens:     16547840 | elapsed time per iteration (ms): 413.5 | learning rate: 8.817E-06 | global batch size:     8 | lm loss: 7.598312E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.347 | TFLOPs: 48.07 |
[default0]:[2023-07-30 22:29:48,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=1015, skipped=0, lr=[8.8604672e-06, 8.8604672e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:48,941] [INFO] [timer.py:215:stop] epoch=0/micro_step=1015/global_step=1015, RunningAvgSamplesPerSec=20.223159343319217, CurrSamplesPerSec=20.232270761868985, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1015/54931640 | consumed samples:         8120 | consumed tokens:     16629760 | elapsed time per iteration (ms): 417.6 | learning rate: 8.860E-06 | global batch size:     8 | lm loss: 7.517346E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.159 | TFLOPs: 47.61 |
[default0]:[2023-07-30 22:29:51,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=0, lr=[8.904157866666667e-06, 8.904157866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:51,162] [INFO] [timer.py:215:stop] epoch=0/micro_step=1020/global_step=1020, RunningAvgSamplesPerSec=20.223125388710493, CurrSamplesPerSec=20.05067998979373, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1020/54931640 | consumed samples:         8160 | consumed tokens:     16711680 | elapsed time per iteration (ms): 444.5 | learning rate: 8.904E-06 | global batch size:     8 | lm loss: 7.640097E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.996 | TFLOPs: 44.72 |
[default0]:[2023-07-30 22:29:53,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=1025, skipped=0, lr=[8.947848533333334e-06, 8.947848533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:53,360] [INFO] [timer.py:215:stop] epoch=0/micro_step=1025/global_step=1025, RunningAvgSamplesPerSec=20.22312989205015, CurrSamplesPerSec=19.890708620342156, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1025/54931640 | consumed samples:         8200 | consumed tokens:     16793600 | elapsed time per iteration (ms): 439.5 | learning rate: 8.948E-06 | global batch size:     8 | lm loss: 7.519144E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.202 | TFLOPs: 45.23 |
[default0]:[2023-07-30 22:29:55,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=0, lr=[8.9915392e-06, 8.9915392e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:55,447] [INFO] [timer.py:215:stop] epoch=0/micro_step=1030/global_step=1030, RunningAvgSamplesPerSec=20.223249573024564, CurrSamplesPerSec=20.270123066588376, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1030/54931640 | consumed samples:         8240 | consumed tokens:     16875520 | elapsed time per iteration (ms): 417.3 | learning rate: 8.992E-06 | global batch size:     8 | lm loss: 7.483575E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.172 | TFLOPs: 47.64 |
[default0]:[2023-07-30 22:29:57,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=1035, skipped=0, lr=[9.035229866666667e-06, 9.035229866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:57,528] [INFO] [timer.py:215:stop] epoch=0/micro_step=1035/global_step=1035, RunningAvgSamplesPerSec=20.22320120869762, CurrSamplesPerSec=20.179973165082853, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1035/54931640 | consumed samples:         8280 | consumed tokens:     16957440 | elapsed time per iteration (ms): 416.4 | learning rate: 9.035E-06 | global batch size:     8 | lm loss: 7.473519E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.214 | TFLOPs: 47.74 |
[default0]:[2023-07-30 22:29:59,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=0, lr=[9.078920533333333e-06, 9.078920533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:59,597] [INFO] [timer.py:215:stop] epoch=0/micro_step=1040/global_step=1040, RunningAvgSamplesPerSec=20.222953612439234, CurrSamplesPerSec=20.24590670373803, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1040/54931640 | consumed samples:         8320 | consumed tokens:     17039360 | elapsed time per iteration (ms): 413.8 | learning rate: 9.079E-06 | global batch size:     8 | lm loss: 7.543778E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.332 | TFLOPs: 48.04 |
[default0]:[2023-07-30 22:30:01,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=1045, skipped=0, lr=[9.122611200000001e-06, 9.122611200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:01,673] [INFO] [timer.py:215:stop] epoch=0/micro_step=1045/global_step=1045, RunningAvgSamplesPerSec=20.223087104323476, CurrSamplesPerSec=20.294962070671573, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1045/54931640 | consumed samples:         8360 | consumed tokens:     17121280 | elapsed time per iteration (ms): 415.3 | learning rate: 9.123E-06 | global batch size:     8 | lm loss: 7.464951E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.262 | TFLOPs: 47.86 |
[default0]:[2023-07-30 22:30:03,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=0, lr=[9.166301866666668e-06, 9.166301866666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:03,735] [INFO] [timer.py:215:stop] epoch=0/micro_step=1050/global_step=1050, RunningAvgSamplesPerSec=20.2233471531589, CurrSamplesPerSec=20.265789222560848, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1050/54931640 | consumed samples:         8400 | consumed tokens:     17203200 | elapsed time per iteration (ms): 412.4 | learning rate: 9.166E-06 | global batch size:     8 | lm loss: 7.425797E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.399 | TFLOPs: 48.20 |
[default0]:[2023-07-30 22:30:05,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=1055, skipped=0, lr=[9.209992533333334e-06, 9.209992533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:05,801] [INFO] [timer.py:215:stop] epoch=0/micro_step=1055/global_step=1055, RunningAvgSamplesPerSec=20.223463368682594, CurrSamplesPerSec=20.285723095041323, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1055/54931640 | consumed samples:         8440 | consumed tokens:     17285120 | elapsed time per iteration (ms): 412.9 | learning rate: 9.210E-06 | global batch size:     8 | lm loss: 7.505474E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.375 | TFLOPs: 48.14 |
[default0]:[2023-07-30 22:30:07,867] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=0, lr=[9.253683200000001e-06, 9.253683200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:07,869] [INFO] [timer.py:215:stop] epoch=0/micro_step=1060/global_step=1060, RunningAvgSamplesPerSec=20.223520365101425, CurrSamplesPerSec=20.13582052382224, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1060/54931640 | consumed samples:         8480 | consumed tokens:     17367040 | elapsed time per iteration (ms): 413.6 | learning rate: 9.254E-06 | global batch size:     8 | lm loss: 7.493221E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.344 | TFLOPs: 48.07 |
[default0]:[2023-07-30 22:30:09,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=1065, skipped=0, lr=[9.297373866666667e-06, 9.297373866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:09,939] [INFO] [timer.py:215:stop] epoch=0/micro_step=1065/global_step=1065, RunningAvgSamplesPerSec=20.223362174108303, CurrSamplesPerSec=20.234003228570618, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1065/54931640 | consumed samples:         8520 | consumed tokens:     17448960 | elapsed time per iteration (ms): 414.1 | learning rate: 9.297E-06 | global batch size:     8 | lm loss: 7.389516E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.320 | TFLOPs: 48.01 |
[default0]:[2023-07-30 22:30:12,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=0, lr=[9.341064533333334e-06, 9.341064533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:12,010] [INFO] [timer.py:215:stop] epoch=0/micro_step=1070/global_step=1070, RunningAvgSamplesPerSec=20.223281576652223, CurrSamplesPerSec=20.201307282407075, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1070/54931640 | consumed samples:         8560 | consumed tokens:     17530880 | elapsed time per iteration (ms): 414.2 | learning rate: 9.341E-06 | global batch size:     8 | lm loss: 7.480013E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.315 | TFLOPs: 47.99 |
[default0]:[2023-07-30 22:30:14,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=1075, skipped=0, lr=[9.3847552e-06, 9.3847552e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:14,078] [INFO] [timer.py:215:stop] epoch=0/micro_step=1075/global_step=1075, RunningAvgSamplesPerSec=20.22349707616271, CurrSamplesPerSec=20.27732575285114, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1075/54931640 | consumed samples:         8600 | consumed tokens:     17612800 | elapsed time per iteration (ms): 413.6 | learning rate: 9.385E-06 | global batch size:     8 | lm loss: 7.359998E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.344 | TFLOPs: 48.06 |
[default0]:[2023-07-30 22:30:16,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=0, lr=[9.428445866666667e-06, 9.428445866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:16,140] [INFO] [timer.py:215:stop] epoch=0/micro_step=1080/global_step=1080, RunningAvgSamplesPerSec=20.223516699347034, CurrSamplesPerSec=20.216825889543347, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1080/54931640 | consumed samples:         8640 | consumed tokens:     17694720 | elapsed time per iteration (ms): 412.2 | learning rate: 9.428E-06 | global batch size:     8 | lm loss: 7.535919E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.407 | TFLOPs: 48.22 |
[default0]:[2023-07-30 22:30:18,202] [INFO] [logging.py:96:log_dist] [Rank 0] step=1085, skipped=0, lr=[9.472136533333333e-06, 9.472136533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:18,203] [INFO] [timer.py:215:stop] epoch=0/micro_step=1085/global_step=1085, RunningAvgSamplesPerSec=20.22344242350923, CurrSamplesPerSec=20.08854074681185, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1085/54931640 | consumed samples:         8680 | consumed tokens:     17776640 | elapsed time per iteration (ms): 412.9 | learning rate: 9.472E-06 | global batch size:     8 | lm loss: 7.364519E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.377 | TFLOPs: 48.15 |
[default0]:[2023-07-30 22:30:20,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=0, lr=[9.515827200000002e-06, 9.515827200000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:20,274] [INFO] [timer.py:215:stop] epoch=0/micro_step=1090/global_step=1090, RunningAvgSamplesPerSec=20.223208228221704, CurrSamplesPerSec=20.24323178063217, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1090/54931640 | consumed samples:         8720 | consumed tokens:     17858560 | elapsed time per iteration (ms): 413.9 | learning rate: 9.516E-06 | global batch size:     8 | lm loss: 7.326588E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.329 | TFLOPs: 48.03 |
[default0]:[2023-07-30 22:30:22,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=1095, skipped=0, lr=[9.559517866666668e-06, 9.559517866666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:22,340] [INFO] [timer.py:215:stop] epoch=0/micro_step=1095/global_step=1095, RunningAvgSamplesPerSec=20.22321636867735, CurrSamplesPerSec=20.196455764468805, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1095/54931640 | consumed samples:         8760 | consumed tokens:     17940480 | elapsed time per iteration (ms): 413.4 | learning rate: 9.560E-06 | global batch size:     8 | lm loss: 7.453773E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.353 | TFLOPs: 48.09 |
[default0]:[2023-07-30 22:30:24,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=0, lr=[9.603208533333335e-06, 9.603208533333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:24,402] [INFO] [timer.py:215:stop] epoch=0/micro_step=1100/global_step=1100, RunningAvgSamplesPerSec=20.22342005261989, CurrSamplesPerSec=20.225697408077156, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1100/54931640 | consumed samples:         8800 | consumed tokens:     18022400 | elapsed time per iteration (ms): 412.4 | learning rate: 9.603E-06 | global batch size:     8 | lm loss: 7.431147E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.400 | TFLOPs: 48.21 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1100 | lm loss value: 7.439597E+00 | lm loss PPL: 1.702064E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:30:31,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=1105, skipped=0, lr=[9.646899200000001e-06, 9.646899200000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:31,996] [INFO] [timer.py:215:stop] epoch=0/micro_step=1105/global_step=1105, RunningAvgSamplesPerSec=20.223443469844383, CurrSamplesPerSec=20.084777091414068, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1105/54931640 | consumed samples:         8840 | consumed tokens:     18104320 | elapsed time per iteration (ms): 1518.9 | learning rate: 9.647E-06 | global batch size:     8 | lm loss: 7.349934E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 5.267 | TFLOPs: 13.09 |
[default0]:[2023-07-30 22:30:34,107] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=0, lr=[9.690589866666666e-06, 9.690589866666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:34,107] [INFO] [timer.py:215:stop] epoch=0/micro_step=1110/global_step=1110, RunningAvgSamplesPerSec=20.22368639576827, CurrSamplesPerSec=20.217288770794546, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1110/54931640 | consumed samples:         8880 | consumed tokens:     18186240 | elapsed time per iteration (ms): 422.2 | learning rate: 9.691E-06 | global batch size:     8 | lm loss: 7.450424E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.950 | TFLOPs: 47.09 |
[default0]:[2023-07-30 22:30:36,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=1115, skipped=0, lr=[9.734280533333333e-06, 9.734280533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:36,235] [INFO] [timer.py:215:stop] epoch=0/micro_step=1115/global_step=1115, RunningAvgSamplesPerSec=20.223813958496994, CurrSamplesPerSec=20.279715529188355, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1115/54931640 | consumed samples:         8920 | consumed tokens:     18268160 | elapsed time per iteration (ms): 425.6 | learning rate: 9.734E-06 | global batch size:     8 | lm loss: 7.453535E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.795 | TFLOPs: 46.70 |
[default0]:[2023-07-30 22:30:38,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=0, lr=[9.7779712e-06, 9.7779712e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:38,319] [INFO] [timer.py:215:stop] epoch=0/micro_step=1120/global_step=1120, RunningAvgSamplesPerSec=20.223872413712648, CurrSamplesPerSec=20.150911895995392, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1120/54931640 | consumed samples:         8960 | consumed tokens:     18350080 | elapsed time per iteration (ms): 417.5 | learning rate: 9.778E-06 | global batch size:     8 | lm loss: 7.378527E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.161 | TFLOPs: 47.61 |
[default0]:[2023-07-30 22:30:40,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=1125, skipped=0, lr=[9.821661866666667e-06, 9.821661866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:40,447] [INFO] [timer.py:215:stop] epoch=0/micro_step=1125/global_step=1125, RunningAvgSamplesPerSec=20.22383743642749, CurrSamplesPerSec=20.15525727445612, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1125/54931640 | consumed samples:         9000 | consumed tokens:     18432000 | elapsed time per iteration (ms): 424.7 | learning rate: 9.822E-06 | global batch size:     8 | lm loss: 7.387065E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.838 | TFLOPs: 46.81 |
[default0]:[2023-07-30 22:30:42,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=0, lr=[9.865352533333334e-06, 9.865352533333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:42,559] [INFO] [timer.py:215:stop] epoch=0/micro_step=1130/global_step=1130, RunningAvgSamplesPerSec=20.223593393641924, CurrSamplesPerSec=20.22913599538921, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1130/54931640 | consumed samples:         9040 | consumed tokens:     18513920 | elapsed time per iteration (ms): 423.7 | learning rate: 9.865E-06 | global batch size:     8 | lm loss: 7.452812E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.881 | TFLOPs: 46.91 |
[default0]:[2023-07-30 22:30:44,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=1135, skipped=0, lr=[9.9090432e-06, 9.9090432e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:44,705] [INFO] [timer.py:215:stop] epoch=0/micro_step=1135/global_step=1135, RunningAvgSamplesPerSec=20.223744471125322, CurrSamplesPerSec=20.227562879518125, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1135/54931640 | consumed samples:         9080 | consumed tokens:     18595840 | elapsed time per iteration (ms): 428.0 | learning rate: 9.909E-06 | global batch size:     8 | lm loss: 7.329802E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.691 | TFLOPs: 46.44 |
[default0]:[2023-07-30 22:30:46,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=0, lr=[9.952733866666667e-06, 9.952733866666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:46,821] [INFO] [timer.py:215:stop] epoch=0/micro_step=1140/global_step=1140, RunningAvgSamplesPerSec=20.22387776019212, CurrSamplesPerSec=20.162717779665925, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1140/54931640 | consumed samples:         9120 | consumed tokens:     18677760 | elapsed time per iteration (ms): 423.2 | learning rate: 9.953E-06 | global batch size:     8 | lm loss: 7.288135E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.904 | TFLOPs: 46.97 |
[default0]:[2023-07-30 22:30:48,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=1145, skipped=0, lr=[9.996424533333333e-06, 9.996424533333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:48,905] [INFO] [timer.py:215:stop] epoch=0/micro_step=1145/global_step=1145, RunningAvgSamplesPerSec=20.224052158822936, CurrSamplesPerSec=20.27433626179744, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1145/54931640 | consumed samples:         9160 | consumed tokens:     18759680 | elapsed time per iteration (ms): 416.9 | learning rate: 9.996E-06 | global batch size:     8 | lm loss: 7.262934E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.188 | TFLOPs: 47.68 |
[default0]:[2023-07-30 22:30:51,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=0, lr=[1.00401152e-05, 1.00401152e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:51,056] [INFO] [timer.py:215:stop] epoch=0/micro_step=1150/global_step=1150, RunningAvgSamplesPerSec=20.224229957510794, CurrSamplesPerSec=20.25305551380738, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1150/54931640 | consumed samples:         9200 | consumed tokens:     18841600 | elapsed time per iteration (ms): 430.0 | learning rate: 1.004E-05 | global batch size:     8 | lm loss: 7.279087E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.603 | TFLOPs: 46.22 |
[default0]:[2023-07-30 22:30:53,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=1155, skipped=0, lr=[1.0083805866666666e-05, 1.0083805866666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:53,251] [INFO] [timer.py:215:stop] epoch=0/micro_step=1155/global_step=1155, RunningAvgSamplesPerSec=20.224193630246837, CurrSamplesPerSec=20.225794940542208, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1155/54931640 | consumed samples:         9240 | consumed tokens:     18923520 | elapsed time per iteration (ms): 438.9 | learning rate: 1.008E-05 | global batch size:     8 | lm loss: 7.283933E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.226 | TFLOPs: 45.29 |
[default0]:[2023-07-30 22:30:55,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=0, lr=[1.0127496533333333e-05, 1.0127496533333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:55,338] [INFO] [timer.py:215:stop] epoch=0/micro_step=1160/global_step=1160, RunningAvgSamplesPerSec=20.224291156398728, CurrSamplesPerSec=20.263610758163555, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1160/54931640 | consumed samples:         9280 | consumed tokens:     19005440 | elapsed time per iteration (ms): 417.5 | learning rate: 1.013E-05 | global batch size:     8 | lm loss: 7.232948E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.161 | TFLOPs: 47.61 |
[default0]:[2023-07-30 22:30:57,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=1165, skipped=0, lr=[1.0171187200000001e-05, 1.0171187200000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:57,461] [INFO] [timer.py:215:stop] epoch=0/micro_step=1165/global_step=1165, RunningAvgSamplesPerSec=20.2244391307461, CurrSamplesPerSec=20.223100952077882, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1165/54931640 | consumed samples:         9320 | consumed tokens:     19087360 | elapsed time per iteration (ms): 424.5 | learning rate: 1.017E-05 | global batch size:     8 | lm loss: 7.249908E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.847 | TFLOPs: 46.83 |
[default0]:[2023-07-30 22:30:59,568] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=0, lr=[1.0214877866666668e-05, 1.0214877866666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:59,569] [INFO] [timer.py:215:stop] epoch=0/micro_step=1170/global_step=1170, RunningAvgSamplesPerSec=20.224232206648942, CurrSamplesPerSec=20.184901762742328, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1170/54931640 | consumed samples:         9360 | consumed tokens:     19169280 | elapsed time per iteration (ms): 421.9 | learning rate: 1.021E-05 | global batch size:     8 | lm loss: 7.246864E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.961 | TFLOPs: 47.11 |
[default0]:[2023-07-30 22:31:01,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=1175, skipped=0, lr=[1.0258568533333334e-05, 1.0258568533333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:01,641] [INFO] [timer.py:215:stop] epoch=0/micro_step=1175/global_step=1175, RunningAvgSamplesPerSec=20.22432101599495, CurrSamplesPerSec=20.187464127613644, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1175/54931640 | consumed samples:         9400 | consumed tokens:     19251200 | elapsed time per iteration (ms): 414.0 | learning rate: 1.026E-05 | global batch size:     8 | lm loss: 7.313651E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.322 | TFLOPs: 48.01 |
[default0]:[2023-07-30 22:31:03,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=0, lr=[1.03022592e-05, 1.03022592e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:03,712] [INFO] [timer.py:215:stop] epoch=0/micro_step=1180/global_step=1180, RunningAvgSamplesPerSec=20.22453391127722, CurrSamplesPerSec=20.245418080541384, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1180/54931640 | consumed samples:         9440 | consumed tokens:     19333120 | elapsed time per iteration (ms): 414.4 | learning rate: 1.030E-05 | global batch size:     8 | lm loss: 7.235303E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.307 | TFLOPs: 47.97 |
[default0]:[2023-07-30 22:31:05,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=1185, skipped=0, lr=[1.0345949866666667e-05, 1.0345949866666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:05,786] [INFO] [timer.py:215:stop] epoch=0/micro_step=1185/global_step=1185, RunningAvgSamplesPerSec=20.224651559305297, CurrSamplesPerSec=20.241680893291267, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1185/54931640 | consumed samples:         9480 | consumed tokens:     19415040 | elapsed time per iteration (ms): 414.7 | learning rate: 1.035E-05 | global batch size:     8 | lm loss: 7.355215E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.291 | TFLOPs: 47.93 |
[default0]:[2023-07-30 22:31:07,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=0, lr=[1.0389640533333334e-05, 1.0389640533333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:07,874] [INFO] [timer.py:215:stop] epoch=0/micro_step=1190/global_step=1190, RunningAvgSamplesPerSec=20.224836507310425, CurrSamplesPerSec=20.24508827321648, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1190/54931640 | consumed samples:         9520 | consumed tokens:     19496960 | elapsed time per iteration (ms): 417.7 | learning rate: 1.039E-05 | global batch size:     8 | lm loss: 7.345853E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.154 | TFLOPs: 47.59 |
[default0]:[2023-07-30 22:31:09,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=1195, skipped=0, lr=[1.04333312e-05, 1.04333312e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:09,958] [INFO] [timer.py:215:stop] epoch=0/micro_step=1195/global_step=1195, RunningAvgSamplesPerSec=20.22492765609033, CurrSamplesPerSec=20.200577581946018, MemAllocated=2.39GB, MaxMemAllocated=10.41GB
[default0]: iteration     1195/54931640 | consumed samples:         9560 | consumed tokens:     19578880 | elapsed time per iteration (ms): 416.9 | learning rate: 1.043E-05 | global batch size:     8 | lm loss: 7.244269E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.190 | TFLOPs: 47.68 |
slurmstepd: error: *** JOB 4504981 ON n2gpu1213 CANCELLED AT 2023-07-30T22:31:10 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 4504981.0 ON n2gpu1213 CANCELLED AT 2023-07-30T22:31:10 DUE TO TIME LIMIT ***
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2133385 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2133385 closing signal SIGTERM
