cpu-bind=MASK - n2gpu1201, task  0  0 [919875]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
LAUNCHER: python -u -m torch.distributed.run --nproc_per_node 4 --nnodes 12 --rdzv_id=2767 --rdzv_endpoint n2gpu1201:6005 --rdzv_backend c10d --max_restarts 0 --tee 3
CMD: pretrain_gpt.py --tensor-model-parallel-size 1 --pipeline-model-parallel-size 1 --distributed-backend nccl --num-layers 50 --hidden-size 4096 --num-attention-heads 32 --seq-length 1024 --max-position-embeddings 1024 --micro-batch-size 1 --global-batch-size 1152 --train-samples 1000000 --optimizer adam --adam-beta1 0.9 --adam-beta2 0.95 --adam-eps 1e-8 --lr 1e-4 --min-lr 1e-6 --lr-decay-style cosine --clip-grad 1.0 --weight-decay 1e-1 --fp16 --partition-activations --seed 42 --vocab-file data/gpt2-vocab.json --merge-file data/gpt2-merges.txt --exit-interval 1000 --log-interval 2 --save-interval 50 --eval-interval 50 --eval-iters 10 --checkpoint-activations --save checkpoints/gpt2-10b-dist --load checkpoints/gpt2-10b-dist --data-path data/meg-gpt2-oscar-en-10k_text_document --tensorboard-dir output_dir/tensorboard-dist-1 --tensorboard-queue-size 5 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --deepspeed --no-pipeline-parallel --zero-stage 2 --deepspeed_config ./ds_config.json --deepspeed-activation-checkpointing
cpu-bind=MASK - n2gpu1224, task  9  0 [114146]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1201, task  0  0 [919997]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1210, task  5  0 [132225]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1206, task  3  0 [131634]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1230, task 11  0 [115845]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1227, task 10  0 [180416]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1207, task  4  0 [132812]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1219, task  6  0 [255830]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1205, task  2  0 [131543]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1221, task  7  0 [148914]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1202, task  1  0 [1117332]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1222, task  8  0 [136892]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 1.12.0
[default3]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.9.0, unknown, unknown
[default3]:torch cuda version ............... 11.7
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 11.7
[default3]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.9.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 1.12.0
[default2]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.9.0, unknown, unknown
[default2]:torch cuda version ............... 11.7
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 11.7
[default2]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.9.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 1.12.0
[default2]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.9.0, unknown, unknown
[default2]:torch cuda version ............... 11.7
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 11.7
[default2]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 1.12.0
[default3]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.9.0, unknown, unknown
[default3]:torch cuda version ............... 11.7
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 11.7
[default3]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 1.12.0
[default2]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.9.0, unknown, unknown
[default2]:torch cuda version ............... 11.7
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 11.7
[default2]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.9.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 1.12.0
[default3]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.9.0, unknown, unknown
[default3]:torch cuda version ............... 11.7
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 11.7
[default3]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 1.12.0
[default2]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.9.0, unknown, unknown
[default2]:torch cuda version ............... 11.7
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 11.7
[default2]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 1.12.0
[default3]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.9.0, unknown, unknown
[default3]:torch cuda version ............... 11.7
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 11.7
[default3]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.9.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 1.12.0
[default3]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.9.0, unknown, unknown
[default3]:torch cuda version ............... 11.7
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 11.7
[default3]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 1.12.0
[default2]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.9.0, unknown, unknown
[default2]:torch cuda version ............... 11.7
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 11.7
[default2]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.9.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default1]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default2]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default2]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default1]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default1]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:using world size: 48, data-parallel-size: 48, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
[default0]:using torch.float16 for parameters ...
[default0]:------------------------ arguments ------------------------
[default0]:  accumulate_allreduce_grads_in_fp32 .............. False
[default0]:  adam_beta1 ...................................... 0.9
[default0]:  adam_beta2 ...................................... 0.95
[default0]:  adam_eps ........................................ 1e-08
[default0]:  adlr_autoresume ................................. False
[default0]:  adlr_autoresume_interval ........................ 1000
[default0]:  aml_data_download_path .......................... None
[default0]:  apply_query_key_layer_scaling ................... True
[default0]:  apply_residual_connection_post_layernorm ........ False
[default0]:  attention_dropout ............................... 0.1
[default0]:  attention_softmax_in_fp32 ....................... False
[default0]:  bert_binary_head ................................ True
[default0]:  bert_load ....................................... None
[default0]:  bf16 ............................................ False
[default0]:  bias_dropout_fusion ............................. True
[default0]:  bias_gelu_fusion ................................ True
[default0]:  biencoder_projection_dim ........................ 0
[default0]:  biencoder_shared_query_context_model ............ False
[default0]:  block_data_path ................................. None
[default0]:  checkpoint_activations .......................... True
[default0]:  checkpoint_in_cpu ............................... False
[default0]:  checkpoint_num_layers ........................... 1
[default0]:  clip_grad ....................................... 1.0
[default0]:  compression_training ............................ False
[default0]:  consumed_train_samples .......................... 0
[default0]:  consumed_train_tokens ........................... 0
[default0]:  consumed_valid_samples .......................... 0
[default0]:  contigious_checkpointing ........................ False
[default0]:  cpu_optimizer ................................... False
[default0]:  cpu_torch_adam .................................. False
[default0]:  create_moe_param_group .......................... False
[default0]:  curriculum_learning ............................. False
[default0]:  custom_token_counting ........................... False
[default0]:  data_impl ....................................... infer
[default0]:  data_parallel_size .............................. 48
[default0]:  data_path ....................................... ['data/meg-gpt2-oscar-en-10k_text_document']
[default0]:  dataloader_type ................................. single
[default0]:  DDP_impl ........................................ local
[default0]:  decoder_seq_length .............................. None
[default0]:  deepscale ....................................... False
[default0]:  deepscale_config ................................ None
[default0]:  deepspeed ....................................... True
[default0]:  deepspeed_activation_checkpointing .............. True
[default0]:  deepspeed_config ................................ ./ds_config.json
[default0]:  deepspeed_mpi ................................... False
[default0]:  distribute_checkpointed_activations ............. False
[default0]:  distributed_backend ............................. nccl
[default0]:  ds_inference .................................... False
[default0]:  ds_pipeline_enabled ............................. False
[default0]:  embedding_path .................................. None
[default0]:  enable_expert_tensor_parallelism ................ False
[default0]:  encoder_seq_length .............................. 1024
[default0]:  eod_mask_loss ................................... False
[default0]:  eval_interval ................................... 50
[default0]:  eval_iters ...................................... 10
[default0]:  evidence_data_path .............................. None
[default0]:  exit_duration_in_mins ........................... None
[default0]:  exit_interval ................................... 1000
[default0]:  expert_interval ................................. 2
[default0]:  ffn_hidden_size ................................. 16384
[default0]:  finetune ........................................ False
[default0]:  fp16 ............................................ True
[default0]:  fp16_lm_cross_entropy ........................... False
[default0]:  fp32_residual_connection ........................ False
[default0]:  global_batch_size ............................... 1152
[default0]:  hidden_dropout .................................. 0.1
[default0]:  hidden_size ..................................... 4096
[default0]:  hidden_size_teacher ............................. None
[default0]:  hysteresis ...................................... 2
[default0]:  ict_head_size ................................... None
[default0]:  ict_load ........................................ None
[default0]:  img_dim ......................................... 224
[default0]:  indexer_batch_size .............................. 128
[default0]:  indexer_log_interval ............................ 1000
[default0]:  inference ....................................... False
[default0]:  init_method_std ................................. 0.02
[default0]:  init_method_xavier_uniform ...................... False
[default0]:  initial_loss_scale .............................. 4294967296
[default0]:  kd .............................................. False
[default0]:  kd_alpha_ce ..................................... 1
[default0]:  kd_beta_ce ...................................... 1
[default0]:  kd_temp ......................................... 1.0
[default0]:  kv_channels ..................................... 128
[default0]:  layernorm_epsilon ............................... 1e-05
[default0]:  lazy_mpu_init ................................... None
[default0]:  load ............................................ checkpoints/gpt2-10b-dist
[default0]:  load_teacher .................................... None
[default0]:  local_rank ...................................... None
[default0]:  log_batch_size_to_tensorboard ................... True
[default0]:  log_interval .................................... 2
[default0]:  log_learning_rate_to_tensorboard ................ True
[default0]:  log_loss_scale_to_tensorboard ................... True
[default0]:  log_num_zeros_in_grad ........................... False
[default0]:  log_optimizer_states_to_tensorboard ............. False
[default0]:  log_params_norm ................................. False
[default0]:  log_timers_to_tensorboard ....................... True
[default0]:  log_validation_ppl_to_tensorboard ............... True
[default0]:  loss_scale ...................................... None
[default0]:  loss_scale_window ............................... 1000
[default0]:  lr .............................................. 0.0001
[default0]:  lr_decay_iters .................................. None
[default0]:  lr_decay_samples ................................ None
[default0]:  lr_decay_style .................................. cosine
[default0]:  lr_decay_tokens ................................. None
[default0]:  lr_warmup_fraction .............................. None
[default0]:  lr_warmup_iters ................................. 0
[default0]:  lr_warmup_samples ............................... 0
[default0]:  lr_warmup_tokens ................................ None
[default0]:  make_vocab_size_divisible_by .................... 128
[default0]:  mask_prob ....................................... 0.15
[default0]:  masked_softmax_fusion ........................... True
[default0]:  max_position_embeddings ......................... 1024
[default0]:  memory_centric_tiled_linear ..................... False
[default0]:  merge_file ...................................... data/gpt2-merges.txt
[default0]:  micro_batch_size ................................ 1
[default0]:  min_loss_scale .................................. 1.0
[default0]:  min_lr .......................................... 1e-06
[default0]:  mlp_type ........................................ standard
[default0]:  mmap_warmup ..................................... False
[default0]:  moe_eval_capacity_factor ........................ 1.0
[default0]:  moe_expert_parallel_size ........................ 1
[default0]:  moe_loss_coeff .................................. 0.1
[default0]:  moe_min_capacity ................................ 4
[default0]:  moe_token_dropping .............................. True
[default0]:  moe_train_capacity_factor ....................... 1.0
[default0]:  mos ............................................. False
[default0]:  no_load_lr_state ................................ False
[default0]:  no_load_optim ................................... None
[default0]:  no_load_rng ..................................... None
[default0]:  no_pipeline_parallel ............................ True
[default0]:  no_save_optim ................................... None
[default0]:  no_save_rng ..................................... None
[default0]:  num_attention_heads ............................. 32
[default0]:  num_attention_heads_teacher ..................... None
[default0]:  num_channels .................................... 3
[default0]:  num_classes ..................................... 1000
[default0]:  num_experts ..................................... [1]
[default0]:  num_experts_teacher ............................. [1]
[default0]:  num_layers ...................................... 50
[default0]:  num_layers_per_virtual_pipeline_stage ........... None
[default0]:  num_layers_teacher .............................. None
[default0]:  num_workers ..................................... 2
[default0]:  onnx_safe ....................................... None
[default0]:  openai_gelu ..................................... False
[default0]:  optimizer ....................................... adam
[default0]:  override_lr_scheduler ........................... False
[default0]:  params_dtype .................................... torch.float16
[default0]:  partition_activations ........................... True
[default0]:  patch_dim ....................................... 16
[default0]:  pipeline_model_parallel_size .................... 1
[default0]:  profile_backward ................................ False
[default0]:  query_in_block_prob ............................. 0.1
[default0]:  rampup_batch_size ............................... None
[default0]:  rank ............................................ 0
[default0]:  remote_device ................................... none
[default0]:  reset_attention_mask ............................ False
[default0]:  reset_iteration ................................. False
[default0]:  reset_position_ids .............................. False
[default0]:  retriever_report_topk_accuracies ................ []
[default0]:  retriever_score_scaling ......................... False
[default0]:  retriever_seq_length ............................ 256
[default0]:  sample_rate ..................................... 1.0
[default0]:  save ............................................ checkpoints/gpt2-10b-dist
[default0]:  save_interval ................................... 50
[default0]:  scatter_gather_tensors_in_pipeline .............. True
[default0]:  scattered_embeddings ............................ False
[default0]:  seed ............................................ 42
[default0]:  seq_length ...................................... 1024
[default0]:  sgd_momentum .................................... 0.9
[default0]:  short_seq_prob .................................. 0.1
[default0]:  split ........................................... 969, 30, 1
[default0]:  split_transformers .............................. False
[default0]:  synchronize_each_layer .......................... False
[default0]:  tensor_model_parallel_size ...................... 1
[default0]:  tensorboard_dir ................................. output_dir/tensorboard-dist-1
[default0]:  tensorboard_log_interval ........................ 1
[default0]:  tensorboard_queue_size .......................... 5
[default0]:  tile_factor ..................................... 1
[default0]:  titles_data_path ................................ None
[default0]:  tokenizer_type .................................. GPT2BPETokenizer
[default0]:  topk ............................................ 1
[default0]:  train_iters ..................................... None
[default0]:  train_samples ................................... 1000000
[default0]:  train_tokens .................................... None
[default0]:  use_checkpoint_lr_scheduler ..................... False
[default0]:  use_contiguous_buffers_in_ddp ................... False
[default0]:  use_cpu_initialization .......................... None
[default0]:  use_one_sent_docs ............................... False
[default0]:  use_pin_memory .................................. False
[default0]:  use_tutel ....................................... False
[default0]:  virtual_pipeline_model_parallel_size ............ None
[default0]:  vocab_extra_ids ................................. 0
[default0]:  vocab_file ...................................... data/gpt2-vocab.json
[default0]:  weight_decay .................................... 0.1
[default0]:  world_size ...................................... 48
[default0]:  zero_allgather_bucket_size ...................... 0.0
[default0]:  zero_contigious_gradients ....................... False
[default0]:  zero_reduce_bucket_size ......................... 0.0
[default0]:  zero_reduce_scatter ............................. False
[default0]:  zero_stage ...................................... 2
[default0]:-------------------- end of arguments ---------------------
[default0]:setting number of micro-batches to constant 24
[default0]:> building GPT2BPETokenizer tokenizer ...
[default1]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default2]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default1]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default2]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 1.12.0
[default2]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.9.0, unknown, unknown
[default2]:torch cuda version ............... 11.7
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 11.7
[default2]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 1.12.0
[default3]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.9.0, unknown, unknown
[default3]:torch cuda version ............... 11.7
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 11.7
[default3]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.9.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 1.12.0
[default2]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.9.0, unknown, unknown
[default2]:torch cuda version ............... 11.7
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 11.7
[default2]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.9.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 1.12.0
[default3]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.9.0, unknown, unknown
[default3]:torch cuda version ............... 11.7
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 11.7
[default3]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 1.12.0
[default2]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.9.0, unknown, unknown
[default2]:torch cuda version ............... 11.7
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 11.7
[default2]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 1.12.0
[default3]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.9.0, unknown, unknown
[default3]:torch cuda version ............... 11.7
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 11.7
[default3]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.9.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default2]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default1]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.9.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 1.12.0
[default3]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.9.0, unknown, unknown
[default3]:torch cuda version ............... 11.7
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 11.7
[default3]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 1.12.0
[default2]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.9.0, unknown, unknown
[default2]:torch cuda version ............... 11.7
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 11.7
[default2]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default0]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default1]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default2]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default2]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default1]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]: > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
[default0]:> initializing torch distributed ...
[default0]:[2023-04-24 00:19:59,091] [INFO] [comm.py:586:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[default3]:WARNING: TensorBoard writing requested but is not available (are you using PyTorch 1.1.0 or later?), no TensorBoard logs will be written.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 1.12.0
[default2]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.9.0, unknown, unknown
[default2]:torch cuda version ............... 11.7
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 11.7
[default2]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.9.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 1.12.0
[default3]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.9.0, unknown, unknown
[default3]:torch cuda version ............... 11.7
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 11.7
[default3]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default2]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default2]:--------------------------------------------------
[default2]:DeepSpeed C++/CUDA extension op report
[default2]:--------------------------------------------------
[default2]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default2]:      runtime if needed. Op compatibility means that your system
[default2]:      meet the required dependencies to JIT install the op.
[default2]:--------------------------------------------------
[default2]:JIT compiled ops requires ninja
[default2]:ninja .................. [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:op name ................ installed .. compatible
[default2]:--------------------------------------------------
[default3]:--------------------------------------------------
[default3]:DeepSpeed C++/CUDA extension op report
[default3]:--------------------------------------------------
[default3]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default3]:      runtime if needed. Op compatibility means that your system
[default3]:      meet the required dependencies to JIT install the op.
[default3]:--------------------------------------------------
[default3]:JIT compiled ops requires ninja
[default3]:ninja .................. [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:op name ................ installed .. compatible
[default3]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default3]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default3]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default3]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default3]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default3]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.9.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 1.12.0
[default3]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.9.0, unknown, unknown
[default3]:torch cuda version ............... 11.7
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 11.7
[default3]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 1.12.0
[default2]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.9.0, unknown, unknown
[default2]:torch cuda version ............... 11.7
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 11.7
[default2]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default3]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default3]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default3]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default3]:--------------------------------------------------
[default3]:DeepSpeed general environment info:
[default3]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default3]:torch version .................... 1.12.0
[default3]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default3]:deepspeed info ................... 0.9.0, unknown, unknown
[default3]:torch cuda version ............... 11.7
[default3]:torch hip version ................ None
[default3]:nvcc version ..................... 11.7
[default3]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.9.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.9.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default2]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default2]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default2]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default2]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default2]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default2]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default2]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default2]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default2]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:utils .................. [93m[NO][0m ....... [92m[OKAY][0m
[default2]:--------------------------------------------------
[default2]:DeepSpeed general environment info:
[default2]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default2]:torch version .................... 1.12.0
[default2]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default2]:deepspeed info ................... 0.9.0, unknown, unknown
[default2]:torch cuda version ............... 11.7
[default2]:torch hip version ................ None
[default2]:nvcc version ..................... 11.7
[default2]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default2]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default3]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default1]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default2]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:**** Git info for Megatron: git_hash=d461af5 git_branch=develop ****
[default0]:> initializing tensor model parallel with size 1
[default0]:> initializing pipeline model parallel with size 1
[default0]:> setting random seeds to 42 ...
[default0]:[2023-04-24 00:20:12,376] [INFO] [checkpointing.py:226:model_parallel_cuda_manual_seed] > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 2760 and data parallel seed: 42
[default0]:> compiling dataset index builder ...
[default0]:make: Entering directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/data'
[default0]:make: Nothing to be done for 'default'.
[default0]:make: Leaving directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/data'
[default0]:>>> done with dataset index builder. Compilation time: 1.804 seconds
[default0]:> compiling and loading fused kernels ...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:[1/3] c++ -MMD -MF scaled_upper_triang_masked_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/TH -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/THC -isystem /opt/software/pc2/EB-SW/software/CUDA/11.7.0/include -isystem /opt/software/pc2/EB-SW/software/Python/3.10.4-GCCcore-11.3.0/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -O3 -c /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/scaled_upper_triang_masked_softmax.cpp -o scaled_upper_triang_masked_softmax.o 
[default0]:[3/3] c++ scaled_upper_triang_masked_softmax.o scaled_upper_triang_masked_softmax_cuda.cuda.o -shared -L/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/software/pc2/EB-SW/software/CUDA/11.7.0/lib64 -lcudart -o scaled_upper_triang_masked_softmax_cuda.so
[default0]:Loading extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:[1/3] c++ -MMD -MF scaled_masked_softmax.o.d -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/TH -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/THC -isystem /opt/software/pc2/EB-SW/software/CUDA/11.7.0/include -isystem /opt/software/pc2/EB-SW/software/Python/3.10.4-GCCcore-11.3.0/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -O3 -c /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/scaled_masked_softmax.cpp -o scaled_masked_softmax.o 
[default0]:[3/3] c++ scaled_masked_softmax.o scaled_masked_softmax_cuda.cuda.o -shared -L/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/software/pc2/EB-SW/software/CUDA/11.7.0/lib64 -lcudart -o scaled_masked_softmax_cuda.so
[default0]:Loading extension module scaled_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module fused_mix_prec_layer_norm_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:[1/3] c++ -MMD -MF layer_norm_cuda.o.d -DTORCH_EXTENSION_NAME=fused_mix_prec_layer_norm_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/TH -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/THC -isystem /opt/software/pc2/EB-SW/software/CUDA/11.7.0/include -isystem /opt/software/pc2/EB-SW/software/Python/3.10.4-GCCcore-11.3.0/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++14 -O3 -c /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda.cpp -o layer_norm_cuda.o 
[default0]:[2/3] /opt/software/pc2/EB-SW/software/CUDA/11.7.0/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_mix_prec_layer_norm_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1016\" -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/TH -isystem /opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/THC -isystem /opt/software/pc2/EB-SW/software/CUDA/11.7.0/include -isystem /opt/software/pc2/EB-SW/software/Python/3.10.4-GCCcore-11.3.0/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 --use_fast_math -maxrregcount=50 -gencode arch=compute_80,code=sm_80 -std=c++14 -c /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu -o layer_norm_cuda_kernel.cuda.o 
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In function ‘void cuda_layer_norm(at::Tensor*, at::Tensor*, at::Tensor*, at::Tensor*, int, int, c10::IntList, at::Tensor*, at::Tensor*, double)’:
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:223: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                               ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:246: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                      ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:271: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                               ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:295: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                       ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:358: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                      ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:413: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                                             ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:118: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                      ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:141: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                             ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:166: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                      ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:190: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                              ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:257: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                 ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:316: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                            ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:130: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                  ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:153: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                         ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:178: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                  ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:202: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                          ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:273: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                 ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:336: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:150: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                      ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:173: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                             ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:198: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                      ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:226: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                  ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:293: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                     ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:352: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:165: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                     ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:188: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                            ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:213: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                     ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:245: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                     ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:316: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                            ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:725:379: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  725 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                           ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In function ‘void cuda_layer_norm_gradient(at::Tensor*, at::Tensor*, at::Tensor*, at::Tensor*, int, int, c10::IntList, at::Tensor*, at::Tensor*, double, at::Tensor*, at::Tensor*, at::Tensor*)’:
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:223: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                               ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:246: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                      ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:271: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                               ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:332: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                            ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:388: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                    ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:437: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:488: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:549: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:119: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                       ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:142: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                              ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:167: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                       ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:232: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                        ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:292: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                    ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:341: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                     ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:396: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                            ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:461: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:131: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                   ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:154: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                          ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:179: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                   ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:248: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                        ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:312: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                        ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:361: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                         ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:420: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                                                    ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:489: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:151: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                       ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:174: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                              ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:199: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                       ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:264: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                        ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:324: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                    ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:377: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                         ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:432: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                                                                ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:497: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:166: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                      ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:189: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                             ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:214: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                      ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:283: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                           ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:347: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                           ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:404: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                                    ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:463: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:532: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  833 |     DISPATCH_FLOAT_HALF_AND_BFLOAT_INOUT_TYPES(
[default0]:      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of ‘void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = float; U = float; V = float]’:
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:203:   required from here
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:138: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                          ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:210: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                                                                                                  ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:247: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                                                                                                                                       ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:783:137: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  783 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
[default0]:      |                                                                                                                                         ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:783:174: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  783 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
[default0]:      |                                                                                                                                                                              ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:801:129: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  801 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
[default0]:      |                                                                                                                                 ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of ‘void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = float; U = float; V = c10::Half]’:
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:95:   required from here
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:138: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                          ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:210: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                                                                                                  ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:247: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                                                                                                                                       ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:783:137: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  783 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
[default0]:      |                                                                                                                                         ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:783:174: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  783 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
[default0]:      |                                                                                                                                                                              ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:801:129: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  801 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
[default0]:      |                                                                                                                                 ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of ‘void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = float; U = float; V = c10::BFloat16]’:
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:103:   required from here
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:138: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                          ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:210: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                                                                                                  ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:247: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                                                                                                                                       ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:783:137: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  783 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
[default0]:      |                                                                                                                                         ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:783:174: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  783 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
[default0]:      |                                                                                                                                                                              ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:801:129: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  801 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
[default0]:      |                                                                                                                                 ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of ‘void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = c10::Half; U = float; V = c10::Half]’:
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:127:   required from here
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:138: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                          ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:210: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                                                                                                  ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:247: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                                                                                                                                       ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:783:137: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  783 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
[default0]:      |                                                                                                                                         ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:783:174: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  783 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
[default0]:      |                                                                                                                                                                              ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:801:129: warning: ‘T* at::Tensor::data() const [with T = c10::Half]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  801 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
[default0]:      |                                                                                                                                 ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu: In instantiation of ‘void HostLayerNormGradient(const V*, const U*, const U*, at::Tensor*, int, int, const V*, const V*, double, T*, V*, V*) [with T = c10::BFloat16; U = float; V = c10::BFloat16]’:
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:833:138:   required from here
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:138: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                          ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:210: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                                                                                                  ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:770:247: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  770 |       cuComputePartGradGammaBeta<<<blocks2, threads2, nshared2, stream>>>(
[default0]:      |                                                                                                                                                                                                                                                       ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:783:137: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  783 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
[default0]:      |                                                                                                                                         ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:783:174: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  783 |       cuComputeGradGammaBeta<<<blocks3, threads3, nshared3, stream>>>(
[default0]:      |                                                                                                                                                                              ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/layer_norm_cuda_kernel.cu:801:129: warning: ‘T* at::Tensor::data() const [with T = c10::BFloat16]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]
[default0]:  801 |     cuComputeGradInput<<<blocks1, threads1, nshared, stream>>>(
[default0]:      |                                                                                                                                 ^ 
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/include/ATen/core/TensorBody.h:235:1: note: declared here
[default0]:  235 |   T * data() const {
[default0]:      | ^ ~~
[default0]:[3/3] c++ layer_norm_cuda.o layer_norm_cuda_kernel.cuda.o -shared -L/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/software/pc2/EB-SW/software/CUDA/11.7.0/lib64 -lcudart -o fused_mix_prec_layer_norm_cuda.so
[default0]:Loading extension module fused_mix_prec_layer_norm_cuda...
[default0]:n2gpu1201:920020:920020 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.69<0>
[default0]:n2gpu1201:920020:920020 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1201:920020:920020 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.69<0>
[default0]:n2gpu1201:920020:920020 [0] NCCL INFO Using network IB
[default0]:NCCL version 2.12.12+cuda11.7
[default0]:n2gpu1221:148943:148943 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.89<0>
[default3]:n2gpu1221:148946:148946 [3] NCCL INFO Bootstrap : Using ib1:10.10.103.89<0>
[default1]:n2gpu1221:148944:148944 [1] NCCL INFO Bootstrap : Using ib1:10.10.103.89<0>
[default2]:n2gpu1221:148945:148945 [2] NCCL INFO Bootstrap : Using ib1:10.10.103.89<0>
[default3]:n2gpu1207:132840:132840 [3] NCCL INFO Bootstrap : Using ib1:10.10.103.75<0>
[default1]:n2gpu1207:132838:132838 [1] NCCL INFO Bootstrap : Using ib1:10.10.103.75<0>
[default0]:n2gpu1207:132837:132837 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.75<0>
[default2]:n2gpu1207:132839:132839 [2] NCCL INFO Bootstrap : Using ib1:10.10.103.75<0>
[default2]:n2gpu1219:255859:255859 [2] NCCL INFO Bootstrap : Using ib1:10.10.103.87<0>
[default1]:n2gpu1219:255858:255858 [1] NCCL INFO Bootstrap : Using ib1:10.10.103.87<0>
[default3]:n2gpu1227:180445:180445 [3] NCCL INFO Bootstrap : Using ib1:10.10.103.95<0>
[default0]:n2gpu1227:180442:180442 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.95<0>
[default2]:n2gpu1227:180444:180444 [2] NCCL INFO Bootstrap : Using ib1:10.10.103.95<0>
[default1]:n2gpu1222:136919:136919 [1] NCCL INFO Bootstrap : Using ib1:10.10.103.90<0>
[default0]:n2gpu1210:132252:132252 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.78<0>
[default1]:n2gpu1210:132253:132253 [1] NCCL INFO Bootstrap : Using ib1:10.10.103.78<0>
[default2]:n2gpu1210:132254:132254 [2] NCCL INFO Bootstrap : Using ib1:10.10.103.78<0>
[default3]:n2gpu1210:132255:132255 [3] NCCL INFO Bootstrap : Using ib1:10.10.103.78<0>
[default3]:n2gpu1224:114176:114176 [3] NCCL INFO Bootstrap : Using ib1:10.10.103.92<0>
[default2]:n2gpu1202:1117360:1117360 [2] NCCL INFO Bootstrap : Using ib1:10.10.103.70<0>
[default2]:n2gpu1202:1117360:1117360 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1202:1117359:1117359 [1] NCCL INFO Bootstrap : Using ib1:10.10.103.70<0>
[default1]:n2gpu1202:1117359:1117359 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default3]:n2gpu1206:131663:131663 [3] NCCL INFO Bootstrap : Using ib1:10.10.103.74<0>
[default3]:n2gpu1206:131663:131663 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default3]:n2gpu1206:131663:131663 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.74<0>
[default3]:n2gpu1206:131663:131663 [3] NCCL INFO Using network IB
[default1]:n2gpu1206:131661:131661 [1] NCCL INFO Bootstrap : Using ib1:10.10.103.74<0>
[default1]:n2gpu1206:131661:131661 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1206:131660:131660 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.74<0>
[default0]:n2gpu1206:131660:131660 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default2]:n2gpu1206:131662:131662 [2] NCCL INFO Bootstrap : Using ib1:10.10.103.74<0>
[default2]:n2gpu1206:131662:131662 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default2]:n2gpu1206:131662:131662 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.74<0>
[default2]:n2gpu1206:131662:131662 [2] NCCL INFO Using network IB
[default3]:n2gpu1205:131573:131573 [3] NCCL INFO Bootstrap : Using ib1:10.10.103.73<0>
[default3]:n2gpu1205:131573:131573 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1205:131570:131570 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.73<0>
[default0]:n2gpu1205:131570:131570 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default2]:n2gpu1205:131572:131572 [2] NCCL INFO Bootstrap : Using ib1:10.10.103.73<0>
[default2]:n2gpu1205:131572:131572 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default2]:n2gpu1205:131572:131572 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.73<0>
[default2]:n2gpu1205:131572:131572 [2] NCCL INFO Using network IB
[default0]:n2gpu1221:148943:148943 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default3]:n2gpu1221:148946:148946 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default3]:n2gpu1221:148946:148946 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.89<0>
[default3]:n2gpu1221:148946:148946 [3] NCCL INFO Using network IB
[default1]:n2gpu1221:148944:148944 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1221:148944:148944 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.89<0>
[default1]:n2gpu1221:148944:148944 [1] NCCL INFO Using network IB
[default2]:n2gpu1221:148945:148945 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1230:115872:115872 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.98<0>
[default0]:n2gpu1230:115872:115872 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default3]:n2gpu1230:115875:115875 [3] NCCL INFO Bootstrap : Using ib1:10.10.103.98<0>
[default3]:n2gpu1230:115875:115875 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default3]:n2gpu1230:115875:115875 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.98<0>
[default3]:n2gpu1230:115875:115875 [3] NCCL INFO Using network IB
[default2]:n2gpu1230:115874:115874 [2] NCCL INFO Bootstrap : Using ib1:10.10.103.98<0>
[default2]:n2gpu1230:115874:115874 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default2]:n2gpu1230:115874:115874 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.98<0>
[default2]:n2gpu1230:115874:115874 [2] NCCL INFO Using network IB
[default3]:n2gpu1207:132840:132840 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1207:132838:132838 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1207:132838:132838 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.75<0>
[default1]:n2gpu1207:132838:132838 [1] NCCL INFO Using network IB
[default0]:n2gpu1207:132837:132837 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default2]:n2gpu1207:132839:132839 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1222:136919:136919 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1222:136919:136919 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.90<0>
[default1]:n2gpu1222:136919:136919 [1] NCCL INFO Using network IB
[default0]:n2gpu1210:132252:132252 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1210:132252:132252 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.78<0>
[default0]:n2gpu1210:132252:132252 [0] NCCL INFO Using network IB
[default1]:n2gpu1210:132253:132253 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1210:132253:132253 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.78<0>
[default1]:n2gpu1210:132253:132253 [1] NCCL INFO Using network IB
[default2]:n2gpu1210:132254:132254 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default3]:n2gpu1210:132255:132255 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default3]:n2gpu1210:132255:132255 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.78<0>
[default3]:n2gpu1227:180445:180445 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1227:180442:180442 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default2]:n2gpu1227:180444:180444 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default2]:n2gpu1227:180444:180444 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.95<0>
[default2]:n2gpu1227:180444:180444 [2] NCCL INFO Using network IB
[default3]:n2gpu1210:132255:132255 [3] NCCL INFO Using network IB
[default2]:n2gpu1202:1117360:1117360 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.70<0>
[default2]:n2gpu1202:1117360:1117360 [2] NCCL INFO Using network IB
[default1]:n2gpu1202:1117359:1117359 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.70<0>
[default1]:n2gpu1202:1117359:1117359 [1] NCCL INFO Using network IB
[default2]:n2gpu1201:920022:920022 [2] NCCL INFO Bootstrap : Using ib1:10.10.103.69<0>
[default2]:n2gpu1201:920022:920022 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1206:131661:131661 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.74<0>
[default1]:n2gpu1206:131661:131661 [1] NCCL INFO Using network IB
[default0]:n2gpu1206:131660:131660 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.74<0>
[default0]:n2gpu1206:131660:131660 [0] NCCL INFO Using network IB
[default0]:n2gpu1230:115872:115872 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.98<0>
[default0]:n2gpu1230:115872:115872 [0] NCCL INFO Using network IB
[default1]:n2gpu1201:920021:920021 [1] NCCL INFO Bootstrap : Using ib1:10.10.103.69<0>
[default1]:n2gpu1201:920021:920021 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1227:180443:180443 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.95<0>
[default1]:n2gpu1227:180443:180443 [1] NCCL INFO Using network IB
[default3]:n2gpu1207:132840:132840 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.75<0>
[default3]:n2gpu1207:132840:132840 [3] NCCL INFO Using network IB
[default0]:n2gpu1207:132837:132837 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.75<0>
[default0]:n2gpu1207:132837:132837 [0] NCCL INFO Using network IB
[default2]:n2gpu1207:132839:132839 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.75<0>
[default2]:n2gpu1207:132839:132839 [2] NCCL INFO Using network IB
[default3]:n2gpu1205:131573:131573 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.73<0>
[default3]:n2gpu1205:131573:131573 [3] NCCL INFO Using network IB
[default0]:n2gpu1205:131570:131570 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.73<0>
[default0]:n2gpu1205:131570:131570 [0] NCCL INFO Using network IB
[default3]:n2gpu1227:180445:180445 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.95<0>
[default3]:n2gpu1227:180445:180445 [3] NCCL INFO Using network IB
[default0]:n2gpu1227:180442:180442 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.95<0>
[default0]:n2gpu1227:180442:180442 [0] NCCL INFO Using network IB
[default2]:n2gpu1210:132254:132254 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.78<0>
[default2]:n2gpu1210:132254:132254 [2] NCCL INFO Using network IB
[default2]:n2gpu1201:920022:920022 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.69<0>
[default2]:n2gpu1201:920022:920022 [2] NCCL INFO Using network IB
[default2]:n2gpu1222:136920:136920 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.90<0>
[default2]:n2gpu1222:136920:136920 [2] NCCL INFO Using network IB
[default1]:n2gpu1201:920021:920021 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.69<0>
[default1]:n2gpu1201:920021:920021 [1] NCCL INFO Using network IB
[default3]:n2gpu1222:136921:136921 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.90<0>
[default3]:n2gpu1222:136921:136921 [3] NCCL INFO Using network IB
[default0]:n2gpu1221:148943:148943 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.89<0>
[default0]:n2gpu1221:148943:148943 [0] NCCL INFO Using network IB
[default2]:n2gpu1221:148945:148945 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.89<0>
[default2]:n2gpu1221:148945:148945 [2] NCCL INFO Using network IB
[default0]:n2gpu1219:255857:255857 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default2]:n2gpu1224:114175:114175 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default3]:n2gpu1201:920023:920023 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.69<0>
[default3]:n2gpu1201:920023:920023 [3] NCCL INFO Using network IB
[default2]:n2gpu1219:255859:255859 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default2]:n2gpu1219:255859:255859 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.87<0>
[default2]:n2gpu1219:255859:255859 [2] NCCL INFO Using network IB
[default1]:n2gpu1219:255858:255858 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1219:255858:255858 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.87<0>
[default1]:n2gpu1219:255858:255858 [1] NCCL INFO Using network IB
[default3]:n2gpu1219:255860:255860 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default3]:n2gpu1224:114176:114176 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default3]:n2gpu1224:114176:114176 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.92<0>
[default3]:n2gpu1224:114176:114176 [3] NCCL INFO Using network IB
[default0]:n2gpu1222:136918:136918 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.90<0>
[default0]:n2gpu1222:136918:136918 [0] NCCL INFO Using network IB
[default0]:n2gpu1224:114173:114173 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.92<0>
[default0]:n2gpu1224:114173:114173 [0] NCCL INFO Using network IB
[default1]:n2gpu1230:115873:115873 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.98<0>
[default1]:n2gpu1230:115873:115873 [1] NCCL INFO Using network IB
[default0]:n2gpu1219:255857:255857 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.87<0>
[default0]:n2gpu1219:255857:255857 [0] NCCL INFO Using network IB
[default2]:n2gpu1224:114175:114175 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.92<0>
[default2]:n2gpu1224:114175:114175 [2] NCCL INFO Using network IB
[default3]:n2gpu1219:255860:255860 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.87<0>
[default3]:n2gpu1219:255860:255860 [3] NCCL INFO Using network IB
[default1]:n2gpu1205:131571:131571 [1] NCCL INFO Bootstrap : Using ib1:10.10.103.73<0>
[default1]:n2gpu1205:131571:131571 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1205:131571:131571 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.73<0>
[default1]:n2gpu1205:131571:131571 [1] NCCL INFO Using network IB
[default0]:n2gpu1202:1117358:1117358 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.70<0>
[default0]:n2gpu1202:1117358:1117358 [0] NCCL INFO Using network IB
[default3]:n2gpu1202:1117361:1117361 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.70<0>
[default3]:n2gpu1202:1117361:1117361 [3] NCCL INFO Using network IB
[default1]:n2gpu1224:114174:114174 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.92<0>
[default1]:n2gpu1224:114174:114174 [1] NCCL INFO Using network IB
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Trees [0] 5/-1/-1->4->9 [1] 5/-1/-1->4->9 [2] 5/0/-1->4->12 [3] 5/0/-1->4->12
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/8/-1->5->4 [3] 6/8/-1->5->4
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 00/0 : 3[c4000] -> 4[3000] [receive] via NET/IB/0
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Trees [0] 38/-1/-1->37->36 [1] 38/-1/-1->37->36 [2] 38/40/-1->37->36 [3] 38/40/-1->37->36
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Trees [0] 10/4/-1->9->8 [1] 10/4/-1->9->8 [2] 10/-1/-1->9->8 [3] 10/-1/-1->9->8
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Trees [0] 9/12/-1->8->17 [1] 9/12/-1->8->17 [2] 9/-1/-1->8->5 [3] 9/-1/-1->8->5
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/-1/-1->10->9 [3] 11/-1/-1->10->9
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO Trees [0] 39/-1/-1->38->37 [1] 39/-1/-1->38->37 [2] 39/-1/-1->38->37 [3] 39/-1/-1->38->37
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Trees [0] 41/44/-1->40->32 [1] 41/44/-1->40->32 [2] 41/-1/-1->40->37 [3] 41/-1/-1->40->37
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] -1/-1/-1->11->10 [3] -1/-1/-1->11->10
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO Trees [0] -1/-1/-1->39->38 [1] -1/-1/-1->39->38 [2] -1/-1/-1->39->38 [3] -1/-1/-1->39->38
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 01/0 : 3[c4000] -> 4[3000] [receive] via NET/IB/0
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Trees [0] 13/-1/-1->12->8 [1] 13/-1/-1->12->8 [2] 13/4/-1->12->44 [3] 13/4/-1->12->44
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Trees [0] 42/36/-1->41->40 [1] 42/36/-1->41->40 [2] 42/-1/-1->41->40 [3] 42/-1/-1->41->40
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO Trees [0] 43/-1/-1->42->41 [1] 43/-1/-1->42->41 [2] 43/-1/-1->42->41 [3] 43/-1/-1->42->41
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 00/0 : 7[c4000] -> 8[3000] [receive] via NET/IB/0
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 00/0 : 11[c4000] -> 12[3000] [receive] via NET/IB/0
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/28/-1->13->12 [3] 14/28/-1->13->12
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO Trees [0] -1/-1/-1->43->42 [1] -1/-1/-1->43->42 [2] -1/-1/-1->43->42 [3] -1/-1/-1->43->42
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 00/0 : 39[c4000] -> 40[3000] [receive] via NET/IB/0
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 01/0 : 7[c4000] -> 8[3000] [receive] via NET/IB/0
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO Trees [0] 46/-1/-1->45->44 [1] 46/-1/-1->45->44 [2] 46/-1/-1->45->44 [3] 46/-1/-1->45->44
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Trees [0] 45/-1/-1->44->40 [1] 45/-1/-1->44->40 [2] 45/12/-1->44->-1 [3] 45/12/-1->44->-1
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO Trees [0] 47/-1/-1->46->45 [1] 47/-1/-1->46->45 [2] 47/-1/-1->46->45 [3] 47/-1/-1->46->45
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 02/0 : 3[c4000] -> 4[3000] [receive] via NET/IB/0
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 03/0 : 3[c4000] -> 4[3000] [receive] via NET/IB/0
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 00 : 4[3000] -> 5[44000] via P2P/IPC/read
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Channel 00 : 5[44000] -> 6[84000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO Trees [0] -1/-1/-1->47->46 [1] -1/-1/-1->47->46 [2] -1/-1/-1->47->46 [3] -1/-1/-1->47->46
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 01/0 : 11[c4000] -> 12[3000] [receive] via NET/IB/0
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] -1/-1/-1->15->14 [3] -1/-1/-1->15->14
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Trees [0] 17/24/-1->16->33 [1] 17/24/-1->16->33 [2] 17/-1/-1->16->20 [3] 17/-1/-1->16->20
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 01/0 : 39[c4000] -> 40[3000] [receive] via NET/IB/0
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 02/0 : 7[c4000] -> 8[3000] [receive] via NET/IB/0
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 01 : 4[3000] -> 5[44000] via P2P/IPC/read
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO Channel 00 : 6[84000] -> 7[c4000] via P2P/IPC/read
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Channel 01 : 5[44000] -> 6[84000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 00/0 : 43[c4000] -> 44[3000] [receive] via NET/IB/0
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 01/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 03/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Trees [0] 1/32/-1->0->-1 [1] 1/32/-1->0->-1 [2] 1/-1/-1->0->4 [3] 1/-1/-1->0->4
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 02/0 : 11[c4000] -> 12[3000] [receive] via NET/IB/0
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO Trees [0] 19/-1/-1->18->17 [1] 19/-1/-1->18->17 [2] 19/-1/-1->18->17 [3] 19/-1/-1->18->17
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 00/0 : 15[c4000] -> 16[3000] [receive] via NET/IB/0
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO Trees [0] 35/-1/-1->34->33 [1] 35/-1/-1->34->33 [2] 35/-1/-1->34->33 [3] 35/-1/-1->34->33
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Channel 02 : 5[44000] -> 6[84000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 01/0 : 43[c4000] -> 44[3000] [receive] via NET/IB/0
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 00/0 : 47[c4000] -> 0[3000] [receive] via NET/IB/0
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Channel 00 : 37[44000] -> 38[84000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO Trees [0] -1/-1/-1->35->34 [1] -1/-1/-1->35->34 [2] -1/-1/-1->35->34 [3] -1/-1/-1->35->34
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO Trees [0] -1/-1/-1->19->18 [1] -1/-1/-1->19->18 [2] -1/-1/-1->19->18 [3] -1/-1/-1->19->18
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 02/0 : 39[c4000] -> 40[3000] [receive] via NET/IB/0
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Channel 00 : 9[44000] -> 10[84000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 03/0 : 7[c4000] -> 8[3000] [receive] via NET/IB/0
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 00 : 8[3000] -> 9[44000] via P2P/IPC/read
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO Channel 00 : 10[84000] -> 11[c4000] via P2P/IPC/read
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO Channel 01 : 6[84000] -> 7[c4000] via P2P/IPC/read
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Channel 03 : 5[44000] -> 6[84000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO Channel 00/0 : 7[c4000] -> 8[3000] [send] via NET/IB/0
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 03/0 : 11[c4000] -> 12[3000] [receive] via NET/IB/0
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 00 : 12[3000] -> 13[44000] via P2P/IPC/read
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO Channel 00/0 : 39[c4000] -> 40[3000] [send] via NET/IB/0
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Trees [0] 22/-1/-1->21->20 [1] 22/-1/-1->21->20 [2] 22/24/-1->21->20 [3] 22/24/-1->21->20
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO Channel 00 : 38[84000] -> 39[c4000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Trees [0] 37/-1/-1->36->41 [1] 37/-1/-1->36->41 [2] 37/32/-1->36->29 [3] 37/32/-1->36->29
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 01/0 : 15[c4000] -> 16[3000] [receive] via NET/IB/0
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO Channel 00/0 : 3[c4000] -> 4[3000] [send] via NET/IB/0
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO Channel 01/0 : 3[c4000] -> 4[3000] [send] via NET/IB/0
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO Channel 00 : 42[84000] -> 43[c4000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO Channel 00/0 : 11[c4000] -> 12[3000] [send] via NET/IB/0
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 01 : 8[3000] -> 9[44000] via P2P/IPC/read
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO Channel 01 : 10[84000] -> 11[c4000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO Channel 01/0 : 7[c4000] -> 8[3000] [send] via NET/IB/0
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 02 : 4[3000] -> 5[44000] via P2P/IPC/read
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/-1/-1->22->21 [2] 23/-1/-1->22->21 [3] 23/-1/-1->22->21
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Channel 00 : 13[44000] -> 14[84000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 00/0 : 35[c4000] -> 36[3000] [receive] via NET/IB/0
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 01/0 : 47[c4000] -> 0[3000] [receive] via NET/IB/0
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 02/0 : 43[c4000] -> 44[3000] [receive] via NET/IB/0
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 00/0 : 19[c4000] -> 20[3000] [receive] via NET/IB/0
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] -1/-1/-1->23->22 [2] -1/-1/-1->23->22 [3] -1/-1/-1->23->22
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Channel 00 : 41[44000] -> 42[84000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 03/0 : 39[c4000] -> 40[3000] [receive] via NET/IB/0
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 00 : 40[3000] -> 41[44000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 02/0 : 15[c4000] -> 16[3000] [receive] via NET/IB/0
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Channel 01 : 9[44000] -> 10[84000] via P2P/IPC/read
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO Channel 02 : 10[84000] -> 11[c4000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Trees [0] 25/28/-1->24->16 [1] 25/28/-1->24->16 [2] 25/-1/-1->24->21 [3] 25/-1/-1->24->21
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO Channel 02 : 6[84000] -> 7[c4000] via P2P/IPC/read
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO Channel 03 : 6[84000] -> 7[c4000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO Channel 02/0 : 7[c4000] -> 8[3000] [send] via NET/IB/0
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 03 : 4[3000] -> 5[44000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 01 : 12[3000] -> 13[44000] via P2P/IPC/read
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Channel 01 : 13[44000] -> 14[84000] via P2P/IPC/read
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO Channel 01/0 : 39[c4000] -> 40[3000] [send] via NET/IB/0
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 01/0 : 19[c4000] -> 20[3000] [receive] via NET/IB/0
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO Channel 00 : 45[44000] -> 46[84000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 03/0 : 43[c4000] -> 44[3000] [receive] via NET/IB/0
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 00 : 44[3000] -> 45[44000] via P2P/IPC/read
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO Channel 00 : 46[84000] -> 47[c4000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Channel 01 : 37[44000] -> 38[84000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Channel 02 : 37[44000] -> 38[84000] via P2P/IPC/read
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO Channel 01 : 38[84000] -> 39[c4000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 01/0 : 35[c4000] -> 36[3000] [receive] via NET/IB/0
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO Channel 02/0 : 3[c4000] -> 4[3000] [send] via NET/IB/0
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO Channel 01 : 42[84000] -> 43[c4000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO Channel 01/0 : 11[c4000] -> 12[3000] [send] via NET/IB/0
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Channel 02 : 9[44000] -> 10[84000] via P2P/IPC/read
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Channel 03 : 9[44000] -> 10[84000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 02 : 8[3000] -> 9[44000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO Channel 03/0 : 7[c4000] -> 8[3000] [send] via NET/IB/0
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 02 : 12[3000] -> 13[44000] via P2P/IPC/read
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO Channel 00 : 14[84000] -> 15[c4000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 00/0 : 23[c4000] -> 24[3000] [receive] via NET/IB/0
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Trees [0] 26/20/-1->25->24 [1] 26/20/-1->25->24 [2] 26/-1/-1->25->24 [3] 26/-1/-1->25->24
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO Trees [0] 27/-1/-1->26->25 [1] 27/-1/-1->26->25 [2] 27/-1/-1->26->25 [3] 27/-1/-1->26->25
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO Channel 02/0 : 39[c4000] -> 40[3000] [send] via NET/IB/0
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO Channel 00 : 1[44000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 02/0 : 47[c4000] -> 0[3000] [receive] via NET/IB/0
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 02/0 : 35[c4000] -> 36[3000] [receive] via NET/IB/0
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO Channel 03/0 : 3[c4000] -> 4[3000] [send] via NET/IB/0
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Channel 01 : 41[44000] -> 42[84000] via P2P/IPC/read
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO Channel 00/0 : 43[c4000] -> 44[3000] [send] via NET/IB/0
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 01 : 40[3000] -> 41[44000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 03/0 : 15[c4000] -> 16[3000] [receive] via NET/IB/0
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 00 : 16[3000] -> 17[44000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO Channel 02/0 : 11[c4000] -> 12[3000] [send] via NET/IB/0
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 03 : 8[3000] -> 9[44000] via P2P/IPC/read
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO Channel 03 : 10[84000] -> 11[c4000] via P2P/IPC/read
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO Connected all rings
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 03 : 12[3000] -> 13[44000] via P2P/IPC/read
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO Trees [0] -1/-1/-1->27->26 [1] -1/-1/-1->27->26 [2] -1/-1/-1->27->26 [3] -1/-1/-1->27->26
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Trees [0] 29/-1/-1->28->24 [1] 29/-1/-1->28->24 [2] 29/20/-1->28->13 [3] 29/20/-1->28->13
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO Channel 00/0 : 15[c4000] -> 16[3000] [send] via NET/IB/0
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO Channel 03/0 : 39[c4000] -> 40[3000] [send] via NET/IB/0
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 03/0 : 47[c4000] -> 0[3000] [receive] via NET/IB/0
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 00 : 0[3000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 02/0 : 19[c4000] -> 20[3000] [receive] via NET/IB/0
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO Channel 01 : 45[44000] -> 46[84000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 01 : 44[3000] -> 45[44000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO Channel 00/0 : 47[c4000] -> 0[3000] [send] via NET/IB/0
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO Channel 01 : 46[84000] -> 47[c4000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Channel 03 : 37[44000] -> 38[84000] via P2P/IPC/read
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO Channel 02 : 38[84000] -> 39[c4000] via P2P/IPC/read
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Channel 00 : 17[44000] -> 18[84000] via P2P/IPC/read
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO Channel 00 : 18[84000] -> 19[c4000] via P2P/IPC/read
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Channel 02 : 41[44000] -> 42[84000] via P2P/IPC/read
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO Channel 01/0 : 43[c4000] -> 44[3000] [send] via NET/IB/0
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO Channel 02/0 : 43[c4000] -> 44[3000] [send] via NET/IB/0
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 02 : 40[3000] -> 41[44000] via P2P/IPC/read
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO Channel 02 : 42[84000] -> 43[c4000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO Channel 03/0 : 11[c4000] -> 12[3000] [send] via NET/IB/0
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Connected all rings
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Trees [0] 30/-1/-1->29->28 [1] 30/-1/-1->29->28 [2] 30/36/-1->29->28 [3] 30/36/-1->29->28
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO Channel 01 : 14[84000] -> 15[c4000] via P2P/IPC/read
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Channel 02 : 13[44000] -> 14[84000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 01/0 : 23[c4000] -> 24[3000] [receive] via NET/IB/0
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO Channel 00 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO Channel 01 : 1[44000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO Channel 03 : 38[84000] -> 39[c4000] via P2P/IPC/read
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO Channel 02 : 45[44000] -> 46[84000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO Channel 01/0 : 47[c4000] -> 0[3000] [send] via NET/IB/0
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO Channel 02/0 : 47[c4000] -> 0[3000] [send] via NET/IB/0
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO Channel 03/0 : 43[c4000] -> 44[3000] [send] via NET/IB/0
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO Channel 03 : 42[84000] -> 43[c4000] via P2P/IPC/read
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Channel 01 : 17[44000] -> 18[84000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 01 : 16[3000] -> 17[44000] via P2P/IPC/read
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO Channel 00 : 34[84000] -> 35[c4000] via P2P/IPC/read
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Channel 02/0 : 5[44000] -> 8[3000] [send] via NET/IB/1
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Connected all rings
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO Connected all rings
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 00/0 : 27[c4000] -> 28[3000] [receive] via NET/IB/0
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO Trees [0] -1/-1/-1->31->30 [1] -1/-1/-1->31->30 [2] -1/-1/-1->31->30 [3] -1/-1/-1->31->30
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO Channel 02 : 14[84000] -> 15[c4000] via P2P/IPC/read
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO Channel 01/0 : 15[c4000] -> 16[3000] [send] via NET/IB/0
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO Channel 02/0 : 15[c4000] -> 16[3000] [send] via NET/IB/0
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO Trees [0] 31/-1/-1->30->29 [1] 31/-1/-1->30->29 [2] 31/-1/-1->30->29 [3] 31/-1/-1->30->29
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO Channel 02 : 1[44000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 01 : 0[3000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 03/0 : 19[c4000] -> 20[3000] [receive] via NET/IB/0
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 00 : 20[3000] -> 21[44000] via P2P/IPC/read
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO Connected all rings
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 03/0 : 35[c4000] -> 36[3000] [receive] via NET/IB/0
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 00 : 36[3000] -> 37[44000] via P2P/IPC/read
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO Channel 03 : 45[44000] -> 46[84000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 02 : 44[3000] -> 45[44000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO Channel 03/0 : 47[c4000] -> 0[3000] [send] via NET/IB/0
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO Channel 02 : 46[84000] -> 47[c4000] via P2P/IPC/read
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO Channel 01 : 18[84000] -> 19[c4000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO Channel 00/0 : 35[c4000] -> 36[3000] [send] via NET/IB/0
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Trees [0] 33/40/-1->32->0 [1] 33/40/-1->32->0 [2] 33/-1/-1->32->36 [3] 33/-1/-1->32->36
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Channel 03 : 41[44000] -> 42[84000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 03 : 40[3000] -> 41[44000] via P2P/IPC/read
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO Channel 03/0 : 15[c4000] -> 16[3000] [send] via NET/IB/0
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Channel 03 : 13[44000] -> 14[84000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 02/0 : 23[c4000] -> 24[3000] [receive] via NET/IB/0
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Channel 03/0 : 5[44000] -> 8[3000] [send] via NET/IB/1
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO Channel 01 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 02 : 0[3000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 01 : 20[3000] -> 21[44000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 03 : 44[3000] -> 45[44000] via P2P/IPC/read
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO Channel 03 : 46[84000] -> 47[c4000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 01 : 36[3000] -> 37[44000] via P2P/IPC/read
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Channel 02 : 17[44000] -> 18[84000] via P2P/IPC/read
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO Channel 02 : 18[84000] -> 19[c4000] via P2P/IPC/read
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO Channel 00/0 : 19[c4000] -> 20[3000] [send] via NET/IB/0
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 02 : 16[3000] -> 17[44000] via P2P/IPC/read
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO Channel 00 : 22[84000] -> 23[c4000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO Channel 01/0 : 35[c4000] -> 36[3000] [send] via NET/IB/0
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO Channel 01 : 34[84000] -> 35[c4000] via P2P/IPC/read
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Trees [0] 34/16/-1->33->32 [1] 34/16/-1->33->32 [2] 34/-1/-1->33->32 [3] 34/-1/-1->33->32
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO Connected all rings
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO Channel 03 : 14[84000] -> 15[c4000] via P2P/IPC/read
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 01/0 : 27[c4000] -> 28[3000] [receive] via NET/IB/0
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO Channel 03 : 1[44000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 03 : 0[3000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 03/0 : 23[c4000] -> 24[3000] [receive] via NET/IB/0
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 00 : 24[3000] -> 25[44000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Channel 00 : 21[44000] -> 22[84000] via P2P/IPC/read
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO Channel 03 : 18[84000] -> 19[c4000] via P2P/IPC/read
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO Channel 01/0 : 19[c4000] -> 20[3000] [send] via NET/IB/0
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 03 : 16[3000] -> 17[44000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 00/0 : 31[c4000] -> 32[3000] [receive] via NET/IB/0
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Connected all rings
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO Channel 02 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Channel 00/0 : 4[3000] -> 9[44000] [receive] via NET/IB/1
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO Channel 00/0 : 23[c4000] -> 24[3000] [send] via NET/IB/0
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 02 : 20[3000] -> 21[44000] via P2P/IPC/read
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO Connected all rings
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Channel 00 : 25[44000] -> 26[84000] via P2P/IPC/read
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO Channel 00 : 26[84000] -> 27[c4000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 02 : 36[3000] -> 37[44000] via P2P/IPC/read
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Channel 03 : 17[44000] -> 18[84000] via P2P/IPC/read
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO Channel 02/0 : 19[c4000] -> 20[3000] [send] via NET/IB/0
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO Channel 01 : 22[84000] -> 23[c4000] via P2P/IPC/read
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO Connected all rings
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Connected all rings
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO Channel 02/0 : 35[c4000] -> 36[3000] [send] via NET/IB/0
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO Channel 03/0 : 35[c4000] -> 36[3000] [send] via NET/IB/0
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO Channel 02 : 34[84000] -> 35[c4000] via P2P/IPC/read
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Channel 01/0 : 4[3000] -> 9[44000] [receive] via NET/IB/1
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO Channel 03 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 02/0 : 27[c4000] -> 28[3000] [receive] via NET/IB/0
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO Channel 01/0 : 23[c4000] -> 24[3000] [send] via NET/IB/0
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 01 : 24[3000] -> 25[44000] via P2P/IPC/read
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO Channel 00/0 : 27[c4000] -> 28[3000] [send] via NET/IB/0
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 03 : 36[3000] -> 37[44000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Channel 01 : 21[44000] -> 22[84000] via P2P/IPC/read
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO Connected all rings
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Connected all rings
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO Channel 03/0 : 19[c4000] -> 20[3000] [send] via NET/IB/0
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Channel 02/0 : 13[44000] -> 28[3000] [send] via NET/IB/1
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO Channel 03 : 34[84000] -> 35[c4000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 01/0 : 31[c4000] -> 32[3000] [receive] via NET/IB/0
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Channel 00/0 : 36[3000] -> 41[44000] [receive] via NET/IB/1
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO Connected all rings
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO Channel 00 : 11[c4000] -> 10[84000] via P2P/IPC/read
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 03/0 : 27[c4000] -> 28[3000] [receive] via NET/IB/0
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 00 : 28[3000] -> 29[44000] via P2P/IPC/read
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO Channel 02/0 : 23[c4000] -> 24[3000] [send] via NET/IB/0
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Channel 01 : 25[44000] -> 26[84000] via P2P/IPC/read
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO Channel 01 : 26[84000] -> 27[c4000] via P2P/IPC/read
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO Channel 01/0 : 27[c4000] -> 28[3000] [send] via NET/IB/0
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Channel 02 : 21[44000] -> 22[84000] via P2P/IPC/read
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO Connected all rings
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO Channel 02 : 22[84000] -> 23[c4000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 02/0 : 31[c4000] -> 32[3000] [receive] via NET/IB/0
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO Connected all rings
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO Connected all rings
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO Channel 00 : 10[84000] -> 9[44000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO Connected all rings
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO Channel 00 : 7[c4000] -> 6[84000] via P2P/IPC/read
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO Connected all rings
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO Channel 00 : 39[c4000] -> 38[84000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Channel 00 : 29[44000] -> 30[84000] via P2P/IPC/read
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO Channel 00 : 30[84000] -> 31[c4000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 02 : 24[3000] -> 25[44000] via P2P/IPC/read
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO Channel 02 : 26[84000] -> 27[c4000] via P2P/IPC/read
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO Channel 02/0 : 27[c4000] -> 28[3000] [send] via NET/IB/0
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Connected all rings
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO Channel 03/0 : 23[c4000] -> 24[3000] [send] via NET/IB/0
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 03 : 20[3000] -> 21[44000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Channel 03 : 21[44000] -> 22[84000] via P2P/IPC/read
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO Channel 03 : 22[84000] -> 23[c4000] via P2P/IPC/read
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Channel 03/0 : 13[44000] -> 28[3000] [send] via NET/IB/1
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Channel 01/0 : 36[3000] -> 41[44000] [receive] via NET/IB/1
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO Channel 00 : 6[84000] -> 5[44000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO Channel 01 : 11[c4000] -> 10[84000] via P2P/IPC/read
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 01 : 28[3000] -> 29[44000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Channel 01 : 29[44000] -> 30[84000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 03 : 24[3000] -> 25[44000] via P2P/IPC/read
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Channel 02 : 25[44000] -> 26[84000] via P2P/IPC/read
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO Channel 03 : 26[84000] -> 27[c4000] via P2P/IPC/read
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO Channel 03/0 : 27[c4000] -> 28[3000] [send] via NET/IB/0
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Channel 00/0 : 8[3000] -> 17[44000] [receive] via NET/IB/1
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Channel 00 : 33[44000] -> 34[84000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 03/0 : 31[c4000] -> 32[3000] [receive] via NET/IB/0
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 00 : 32[3000] -> 33[44000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Connected all rings
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Connected all rings
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO Connected all rings
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO Channel 00 : 43[c4000] -> 42[84000] via P2P/IPC/read
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO Channel 01 : 6[84000] -> 5[44000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO Channel 01 : 7[c4000] -> 6[84000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Connected all rings
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO Channel 01 : 10[84000] -> 9[44000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Channel 02/0 : 37[44000] -> 40[3000] [send] via NET/IB/1
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO Channel 00/0 : 31[c4000] -> 32[3000] [send] via NET/IB/0
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO Channel 01 : 30[84000] -> 31[c4000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO Connected all rings
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO Channel 00 : 47[c4000] -> 46[84000] via P2P/IPC/read
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Channel 03 : 25[44000] -> 26[84000] via P2P/IPC/read
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Channel 01 : 33[44000] -> 34[84000] via P2P/IPC/read
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO Channel 00 : 42[84000] -> 41[44000] via P2P/IPC/read
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO Channel 01 : 43[c4000] -> 42[84000] via P2P/IPC/read
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO Channel 01 : 39[c4000] -> 38[84000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 02/0 : 5[44000] -> 8[3000] [receive] via NET/IB/1
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO Channel 00 : 38[84000] -> 37[44000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Channel 03/0 : 37[44000] -> 40[3000] [send] via NET/IB/1
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Connected all rings
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 02 : 28[3000] -> 29[44000] via P2P/IPC/read
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 03 : 28[3000] -> 29[44000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Channel 02 : 29[44000] -> 30[84000] via P2P/IPC/read
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO Channel 01/0 : 31[c4000] -> 32[3000] [send] via NET/IB/0
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO Channel 02 : 30[84000] -> 31[c4000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Connected all rings
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Connected all rings
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Channel 01/0 : 8[3000] -> 17[44000] [receive] via NET/IB/1
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO Connected all rings
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 00/0 : 8[3000] -> 12[3000] [receive] via NET/IB/1
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO Connected all rings
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO Channel 00 : 15[c4000] -> 14[84000] via P2P/IPC/read
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO Channel 02 : 6[84000] -> 5[44000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO Channel 02 : 7[c4000] -> 6[84000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 01 : 32[3000] -> 33[44000] via P2P/IPC/read
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO Channel 01 : 42[84000] -> 41[44000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 02/0 : 37[44000] -> 40[3000] [receive] via NET/IB/1
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO Channel 02 : 43[c4000] -> 42[84000] via P2P/IPC/read
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO Channel 02/0 : 31[c4000] -> 32[3000] [send] via NET/IB/0
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO Channel 02 : 11[c4000] -> 10[84000] via P2P/IPC/read
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO Channel 02 : 10[84000] -> 9[44000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 02/0 : 0[3000] -> 4[3000] [send] via NET/IB/1
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Connected all rings
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO Connected all rings
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO Connected all rings
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO Channel 00 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 00/0 : 40[3000] -> 44[3000] [receive] via NET/IB/1
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO Channel 01 : 47[c4000] -> 46[84000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Connected all rings
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 02/0 : 16[3000] -> 20[3000] [send] via NET/IB/1
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO Connected all rings
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO Channel 00 : 35[c4000] -> 34[84000] via P2P/IPC/read
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO Channel 03 : 6[84000] -> 5[44000] via P2P/IPC/read
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Channel 02 : 33[44000] -> 34[84000] via P2P/IPC/read
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO Channel 03 : 43[c4000] -> 42[84000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Channel 03 : 29[44000] -> 30[84000] via P2P/IPC/read
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO Channel 03/0 : 31[c4000] -> 32[3000] [send] via NET/IB/0
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO Channel 03 : 30[84000] -> 31[c4000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 03/0 : 5[44000] -> 8[3000] [receive] via NET/IB/1
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO Channel 03 : 11[c4000] -> 10[84000] via P2P/IPC/read
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO Channel 02 : 39[c4000] -> 38[84000] via P2P/IPC/read
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO Channel 01 : 38[84000] -> 37[44000] via P2P/IPC/read
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO Channel 01 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Channel 02/0 : 21[44000] -> 24[3000] [send] via NET/IB/1
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Channel 03/0 : 21[44000] -> 24[3000] [send] via NET/IB/1
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO Channel 00 : 45[44000] -> 44[3000] via P2P/IPC/read
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO Channel 00 : 46[84000] -> 45[44000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO Channel 03 : 7[c4000] -> 6[84000] via P2P/IPC/read
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Connected all rings
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 02 : 32[3000] -> 33[44000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 03 : 32[3000] -> 33[44000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 01/0 : 8[3000] -> 12[3000] [receive] via NET/IB/1
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO Channel 00 : 14[84000] -> 13[44000] via P2P/IPC/read
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO Channel 01 : 15[c4000] -> 14[84000] via P2P/IPC/read
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO Channel 02 : 42[84000] -> 41[44000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 03/0 : 37[44000] -> 40[3000] [receive] via NET/IB/1
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 03/0 : 0[3000] -> 4[3000] [send] via NET/IB/1
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Channel 00/0 : 20[3000] -> 25[44000] [receive] via NET/IB/1
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO Channel 03 : 10[84000] -> 9[44000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 01/0 : 40[3000] -> 44[3000] [receive] via NET/IB/1
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO Channel 02 : 47[c4000] -> 46[84000] via P2P/IPC/read
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO Connected all rings
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO Channel 00 : 19[c4000] -> 18[84000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 03/0 : 16[3000] -> 20[3000] [send] via NET/IB/1
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO Channel 01 : 35[c4000] -> 34[84000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO Channel 02 : 35[c4000] -> 34[84000] via P2P/IPC/read
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO Channel 01 : 14[84000] -> 13[44000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO Connected all trees
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO Channel 03 : 39[c4000] -> 38[84000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO Connected all trees
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO Channel 00 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Connected all rings
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO Connected all rings
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO Channel 02 : 38[84000] -> 37[44000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Connected all rings
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 02/0 : 32[3000] -> 36[3000] [receive] via NET/IB/1
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO Channel 01 : 45[44000] -> 44[3000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO Channel 03 : 47[c4000] -> 46[84000] via P2P/IPC/read
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO Channel 01 : 46[84000] -> 45[44000] via P2P/IPC/read
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO Channel 00 : 18[84000] -> 17[44000] via P2P/IPC/read
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Channel 03 : 33[44000] -> 34[84000] via P2P/IPC/read
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 02/0 : 0[3000] -> 4[3000] [receive] via NET/IB/1
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO Channel 02 : 14[84000] -> 13[44000] via P2P/IPC/read
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO Channel 02 : 15[c4000] -> 14[84000] via P2P/IPC/read
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO Channel 03 : 42[84000] -> 41[44000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Channel 02/0 : 29[44000] -> 36[3000] [send] via NET/IB/1
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Channel 01/0 : 20[3000] -> 25[44000] [receive] via NET/IB/1
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO Channel 00 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 03/0 : 32[3000] -> 36[3000] [receive] via NET/IB/1
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO Connected all rings
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO Channel 00 : 23[c4000] -> 22[84000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Connected all rings
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO Channel 02 : 45[44000] -> 44[3000] via P2P/IPC/read
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO Channel 02 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO Connected all trees
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO Channel 01 : 18[84000] -> 17[44000] via P2P/IPC/read
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO Channel 01 : 19[c4000] -> 18[84000] via P2P/IPC/read
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Channel 02/0 : 8[3000] -> 5[44000] [receive] via NET/IB/1
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 03/0 : 0[3000] -> 4[3000] [receive] via NET/IB/1
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO Connected all rings
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Connected all rings
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO Channel 03 : 15[c4000] -> 14[84000] via P2P/IPC/read
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO Channel 01 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO Channel 01 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO Channel 03 : 38[84000] -> 37[44000] via P2P/IPC/read
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO Channel 02 : 46[84000] -> 45[44000] via P2P/IPC/read
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO Channel 03 : 46[84000] -> 45[44000] via P2P/IPC/read
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO Channel 03 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO Channel 02 : 19[c4000] -> 18[84000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO Channel 03 : 35[c4000] -> 34[84000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Channel 03/0 : 29[44000] -> 36[3000] [send] via NET/IB/1
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 00/0 : 8[3000] -> 12[3000] [send] via NET/IB/1
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO Channel 03 : 45[44000] -> 44[3000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Connected all rings
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO Connected all rings
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO Channel 00 : 27[c4000] -> 26[84000] via P2P/IPC/read
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO Channel 01 : 23[c4000] -> 22[84000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 02/0 : 16[3000] -> 20[3000] [receive] via NET/IB/1
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO Channel 02 : 18[84000] -> 17[44000] via P2P/IPC/read
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Channel 03/0 : 8[3000] -> 5[44000] [receive] via NET/IB/1
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Channel 00/0 : 16[3000] -> 33[44000] [receive] via NET/IB/1
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 00/0 : 40[3000] -> 44[3000] [send] via NET/IB/1
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO Channel 03 : 14[84000] -> 13[44000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 01/0 : 8[3000] -> 12[3000] [send] via NET/IB/1
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO Connected all trees
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Connected all rings
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO Channel 02 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO Channel 02 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 02/0 : 21[44000] -> 24[3000] [receive] via NET/IB/1
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO Channel 01 : 27[c4000] -> 26[84000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Channel 02/0 : 40[3000] -> 37[44000] [receive] via NET/IB/1
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 03/0 : 16[3000] -> 20[3000] [receive] via NET/IB/1
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 01/0 : 40[3000] -> 44[3000] [send] via NET/IB/1
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO Channel 03 : 19[c4000] -> 18[84000] via P2P/IPC/read
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO Channel 00 : 22[84000] -> 21[44000] via P2P/IPC/read
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO Connected all trees
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO Channel 03 : 2[84000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO Channel 03 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO Connected all trees
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO Connected all trees
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO Channel 00 : 26[84000] -> 25[44000] via P2P/IPC/read
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO Channel 02 : 23[c4000] -> 22[84000] via P2P/IPC/read
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO Channel 03 : 18[84000] -> 17[44000] via P2P/IPC/read
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Channel 01/0 : 16[3000] -> 33[44000] [receive] via NET/IB/1
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 00/0 : 24[3000] -> 28[3000] [receive] via NET/IB/1
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO Connected all rings
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO Channel 00 : 31[c4000] -> 30[84000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Channel 03/0 : 40[3000] -> 37[44000] [receive] via NET/IB/1
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 03/0 : 21[44000] -> 24[3000] [receive] via NET/IB/1
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO Channel 02 : 27[c4000] -> 26[84000] via P2P/IPC/read
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO Connected all trees
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 00/0 : 4[3000] -> 9[44000] [send] via NET/IB/1
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 01/0 : 4[3000] -> 9[44000] [send] via NET/IB/1
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 00/0 : 32[3000] -> 40[3000] [receive] via NET/IB/1
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO Channel 01 : 22[84000] -> 21[44000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Connected all rings
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO Connected all trees
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 00/0 : 8[3000] -> 17[44000] [send] via NET/IB/1
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 00/0 : 32[3000] -> 0[3000] [receive] via NET/IB/1
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO Channel 01 : 26[84000] -> 25[44000] via P2P/IPC/read
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO Channel 03 : 23[c4000] -> 22[84000] via P2P/IPC/read
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO Connected all trees
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 00/0 : 16[3000] -> 24[3000] [send] via NET/IB/1
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO Channel 02 : 22[84000] -> 21[44000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 02/0 : 32[3000] -> 36[3000] [send] via NET/IB/1
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 01/0 : 24[3000] -> 28[3000] [receive] via NET/IB/1
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO Channel 01 : 31[c4000] -> 30[84000] via P2P/IPC/read
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO Channel 00 : 30[84000] -> 29[44000] via P2P/IPC/read
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO Channel 01 : 30[84000] -> 29[44000] via P2P/IPC/read
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO Channel 02 : 26[84000] -> 25[44000] via P2P/IPC/read
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO Channel 03 : 27[c4000] -> 26[84000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 01/0 : 16[3000] -> 24[3000] [send] via NET/IB/1
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO Channel 00 : 34[84000] -> 33[44000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 03/0 : 32[3000] -> 36[3000] [send] via NET/IB/1
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 01/0 : 8[3000] -> 17[44000] [send] via NET/IB/1
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 01/0 : 32[3000] -> 0[3000] [receive] via NET/IB/1
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 02/0 : 44[3000] -> 12[3000] [send] via NET/IB/1
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 03/0 : 44[3000] -> 12[3000] [send] via NET/IB/1
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 01/0 : 32[3000] -> 40[3000] [receive] via NET/IB/1
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 02/0 : 4[3000] -> 12[3000] [receive] via NET/IB/1
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 03/0 : 4[3000] -> 12[3000] [receive] via NET/IB/1
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO Channel 03 : 22[84000] -> 21[44000] via P2P/IPC/read
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO Channel 02 : 31[c4000] -> 30[84000] via P2P/IPC/read
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Channel 00/0 : 9[44000] -> 4[3000] [send] via NET/IB/1
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 00/0 : 24[3000] -> 28[3000] [send] via NET/IB/1
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 00/0 : 20[3000] -> 25[44000] [send] via NET/IB/1
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Channel 02/0 : 24[3000] -> 21[44000] [receive] via NET/IB/1
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 02/0 : 4[3000] -> 12[3000] [send] via NET/IB/1
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO Channel 01 : 34[84000] -> 33[44000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 01/0 : 24[3000] -> 28[3000] [send] via NET/IB/1
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO Channel 03 : 26[84000] -> 25[44000] via P2P/IPC/read
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO Channel 02 : 30[84000] -> 29[44000] via P2P/IPC/read
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO Connected all trees
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 01/0 : 20[3000] -> 25[44000] [send] via NET/IB/1
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 00/0 : 36[3000] -> 41[44000] [send] via NET/IB/1
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 03/0 : 4[3000] -> 12[3000] [send] via NET/IB/1
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 00/0 : 32[3000] -> 40[3000] [send] via NET/IB/1
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Channel 01/0 : 9[44000] -> 4[3000] [send] via NET/IB/1
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO Channel 03 : 31[c4000] -> 30[84000] via P2P/IPC/read
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO Channel 03 : 30[84000] -> 29[44000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 01/0 : 36[3000] -> 41[44000] [send] via NET/IB/1
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Channel 03/0 : 24[3000] -> 21[44000] [receive] via NET/IB/1
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Channel 00/0 : 17[44000] -> 8[3000] [send] via NET/IB/1
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Channel 01/0 : 17[44000] -> 8[3000] [send] via NET/IB/1
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO Channel 02 : 34[84000] -> 33[44000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 01/0 : 32[3000] -> 40[3000] [send] via NET/IB/1
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 00/0 : 17[44000] -> 8[3000] [receive] via NET/IB/1
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO Connected all trees
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO Channel 03 : 34[84000] -> 33[44000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 01/0 : 17[44000] -> 8[3000] [receive] via NET/IB/1
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 02/0 : 20[3000] -> 28[3000] [send] via NET/IB/1
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO Connected all trees
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO Connected all trees
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 02/0 : 29[44000] -> 36[3000] [receive] via NET/IB/1
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 00/0 : 16[3000] -> 24[3000] [receive] via NET/IB/1
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 03/0 : 20[3000] -> 28[3000] [send] via NET/IB/1
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 02/0 : 12[3000] -> 4[3000] [receive] via NET/IB/1
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Channel 00/0 : 41[44000] -> 36[3000] [send] via NET/IB/1
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 02/0 : 20[3000] -> 28[3000] [receive] via NET/IB/1
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Channel 00/0 : 25[44000] -> 20[3000] [send] via NET/IB/1
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 03/0 : 29[44000] -> 36[3000] [receive] via NET/IB/1
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 03/0 : 12[3000] -> 4[3000] [receive] via NET/IB/1
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Channel 01/0 : 41[44000] -> 36[3000] [send] via NET/IB/1
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 02/0 : 44[3000] -> 12[3000] [receive] via NET/IB/1
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 03/0 : 20[3000] -> 28[3000] [receive] via NET/IB/1
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 00/0 : 32[3000] -> 0[3000] [send] via NET/IB/1
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 01/0 : 16[3000] -> 24[3000] [receive] via NET/IB/1
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Channel 01/0 : 25[44000] -> 20[3000] [send] via NET/IB/1
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 00/0 : 40[3000] -> 32[3000] [send] via NET/IB/1
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 01/0 : 32[3000] -> 0[3000] [send] via NET/IB/1
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 01/0 : 40[3000] -> 32[3000] [send] via NET/IB/1
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 03/0 : 44[3000] -> 12[3000] [receive] via NET/IB/1
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Channel 02/0 : 36[3000] -> 29[44000] [receive] via NET/IB/1
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 00/0 : 12[3000] -> 8[3000] [receive] via NET/IB/1
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 02/0 : 28[3000] -> 20[3000] [receive] via NET/IB/1
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 00/0 : 16[3000] -> 33[44000] [send] via NET/IB/1
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 01/0 : 12[3000] -> 8[3000] [receive] via NET/IB/1
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 00/0 : 24[3000] -> 16[3000] [send] via NET/IB/1
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 02/0 : 36[3000] -> 29[44000] [send] via NET/IB/1
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 01/0 : 16[3000] -> 33[44000] [send] via NET/IB/1
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 00/0 : 0[3000] -> 32[3000] [send] via NET/IB/1
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 01/0 : 0[3000] -> 32[3000] [send] via NET/IB/1
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 02/0 : 13[44000] -> 28[3000] [receive] via NET/IB/1
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Channel 03/0 : 36[3000] -> 29[44000] [receive] via NET/IB/1
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 03/0 : 28[3000] -> 20[3000] [receive] via NET/IB/1
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 01/0 : 24[3000] -> 16[3000] [send] via NET/IB/1
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 02/0 : 12[3000] -> 44[3000] [send] via NET/IB/1
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 03/0 : 12[3000] -> 44[3000] [send] via NET/IB/1
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 00/0 : 0[3000] -> 32[3000] [receive] via NET/IB/1
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 02/0 : 12[3000] -> 44[3000] [receive] via NET/IB/1
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Channel 00 : 17[44000] -> 16[3000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 01/0 : 0[3000] -> 32[3000] [receive] via NET/IB/1
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 03/0 : 13[44000] -> 28[3000] [receive] via NET/IB/1
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 03/0 : 36[3000] -> 29[44000] [send] via NET/IB/1
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Channel 01 : 17[44000] -> 16[3000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 03/0 : 12[3000] -> 44[3000] [receive] via NET/IB/1
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 00/0 : 33[44000] -> 16[3000] [receive] via NET/IB/1
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 02/0 : 4[3000] -> 0[3000] [receive] via NET/IB/1
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Channel 02 : 17[44000] -> 16[3000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 01/0 : 33[44000] -> 16[3000] [receive] via NET/IB/1
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Channel 00/0 : 33[44000] -> 16[3000] [send] via NET/IB/1
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Channel 03/0 : 4[3000] -> 0[3000] [receive] via NET/IB/1
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 00/0 : 41[44000] -> 36[3000] [receive] via NET/IB/1
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Channel 03 : 17[44000] -> 16[3000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 02/0 : 12[3000] -> 4[3000] [send] via NET/IB/1
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 01/0 : 41[44000] -> 36[3000] [receive] via NET/IB/1
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Channel 01/0 : 33[44000] -> 16[3000] [send] via NET/IB/1
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 00/0 : 40[3000] -> 32[3000] [receive] via NET/IB/1
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 03/0 : 12[3000] -> 4[3000] [send] via NET/IB/1
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Channel 02/0 : 28[3000] -> 13[44000] [receive] via NET/IB/1
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 00/0 : 44[3000] -> 40[3000] [send] via NET/IB/1
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO Connected all trees
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 02/0 : 28[3000] -> 13[44000] [send] via NET/IB/1
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Channel 01/0 : 44[3000] -> 40[3000] [send] via NET/IB/1
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 01/0 : 40[3000] -> 32[3000] [receive] via NET/IB/1
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 03/0 : 28[3000] -> 13[44000] [send] via NET/IB/1
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Channel 03/0 : 28[3000] -> 13[44000] [receive] via NET/IB/1
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 00/0 : 12[3000] -> 8[3000] [send] via NET/IB/1
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 00/0 : 9[44000] -> 4[3000] [receive] via NET/IB/1
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Channel 00 : 29[44000] -> 28[3000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Channel 01/0 : 12[3000] -> 8[3000] [send] via NET/IB/1
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 02/0 : 36[3000] -> 32[3000] [send] via NET/IB/1
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 00/0 : 24[3000] -> 16[3000] [receive] via NET/IB/1
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 02/0 : 28[3000] -> 20[3000] [send] via NET/IB/1
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 00/0 : 44[3000] -> 40[3000] [receive] via NET/IB/1
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 01/0 : 9[44000] -> 4[3000] [receive] via NET/IB/1
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 02/0 : 36[3000] -> 32[3000] [receive] via NET/IB/1
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 03/0 : 28[3000] -> 20[3000] [send] via NET/IB/1
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Channel 01 : 29[44000] -> 28[3000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Channel 03/0 : 36[3000] -> 32[3000] [send] via NET/IB/1
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 01/0 : 24[3000] -> 16[3000] [receive] via NET/IB/1
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 01/0 : 44[3000] -> 40[3000] [receive] via NET/IB/1
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Channel 03/0 : 36[3000] -> 32[3000] [receive] via NET/IB/1
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Channel 02 : 29[44000] -> 28[3000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 02/0 : 8[3000] -> 5[44000] [send] via NET/IB/1
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Channel 00 : 41[44000] -> 40[3000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Channel 03 : 29[44000] -> 28[3000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Channel 03/0 : 8[3000] -> 5[44000] [send] via NET/IB/1
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Channel 01 : 41[44000] -> 40[3000] via P2P/IPC/read
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 02/0 : 4[3000] -> 0[3000] [send] via NET/IB/1
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Channel 00 : 33[44000] -> 32[3000] via P2P/IPC/read
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 00/0 : 28[3000] -> 24[3000] [send] via NET/IB/1
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Channel 02 : 41[44000] -> 40[3000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 00/0 : 28[3000] -> 24[3000] [receive] via NET/IB/1
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Channel 03/0 : 4[3000] -> 0[3000] [send] via NET/IB/1
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Channel 01 : 33[44000] -> 32[3000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 02/0 : 20[3000] -> 16[3000] [receive] via NET/IB/1
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Channel 01/0 : 28[3000] -> 24[3000] [send] via NET/IB/1
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO Connected all trees
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 00/0 : 25[44000] -> 20[3000] [receive] via NET/IB/1
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 01/0 : 28[3000] -> 24[3000] [receive] via NET/IB/1
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 02/0 : 40[3000] -> 37[44000] [send] via NET/IB/1
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 01/0 : 25[44000] -> 20[3000] [receive] via NET/IB/1
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Channel 03 : 41[44000] -> 40[3000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Channel 03/0 : 20[3000] -> 16[3000] [receive] via NET/IB/1
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Channel 02 : 33[44000] -> 32[3000] via P2P/IPC/read
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Channel 00 : 13[44000] -> 12[3000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Channel 03/0 : 40[3000] -> 37[44000] [send] via NET/IB/1
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Channel 01 : 13[44000] -> 12[3000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO Connected all trees
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Channel 03 : 33[44000] -> 32[3000] via P2P/IPC/read
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Channel 02 : 13[44000] -> 12[3000] via P2P/IPC/read
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO Connected all trees
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO Connected all trees
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 02/0 : 24[3000] -> 21[44000] [send] via NET/IB/1
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Channel 00 : 9[44000] -> 8[3000] via P2P/IPC/read
[default1]:n2gpu1230:115873:116089 [1] NCCL INFO comm 0x154cd40090d0 rank 45 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1230:115872:116086 [0] NCCL INFO comm 0x14c2e40090d0 rank 44 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default3]:n2gpu1230:115875:116084 [3] NCCL INFO comm 0x14fbb40090d0 rank 47 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1230:115874:116080 [2] NCCL INFO comm 0x1535940090d0 rank 46 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Channel 03/0 : 24[3000] -> 21[44000] [send] via NET/IB/1
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO Connected all trees
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO Connected all trees
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 02/0 : 20[3000] -> 16[3000] [send] via NET/IB/1
[default2]:n2gpu1222:136920:137138 [2] NCCL INFO comm 0x14cd0c0090d0 rank 34 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1222:136921:137140 [3] NCCL INFO comm 0x147f640090d0 rank 35 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO Connected all trees
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1222:136919:137134 [1] NCCL INFO comm 0x1512340090d0 rank 33 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Channel 03 : 13[44000] -> 12[3000] via P2P/IPC/read
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Channel 01 : 9[44000] -> 8[3000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Channel 03/0 : 20[3000] -> 16[3000] [send] via NET/IB/1
[default0]:n2gpu1222:136918:137143 [0] NCCL INFO comm 0x14d4f40090d0 rank 32 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Channel 02 : 9[44000] -> 8[3000] via P2P/IPC/read
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Channel 00 : 5[44000] -> 4[3000] via P2P/IPC/read
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO Connected all trees
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO Connected all trees
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO Connected all trees
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO Connected all trees
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO Connected all trees
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO Connected all trees
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1221:148943:149169 [0] NCCL INFO comm 0x14f75c0090d0 rank 28 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO Connected all trees
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1221:148944:149163 [1] NCCL INFO comm 0x14cc840090d0 rank 29 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default3]:n2gpu1221:148946:149165 [3] NCCL INFO comm 0x14c7a40090d0 rank 31 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1221:148945:149171 [2] NCCL INFO comm 0x1534080090d0 rank 30 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1206:131660:131882 [0] NCCL INFO comm 0x147b240090d0 rank 12 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default2]:n2gpu1206:131662:131877 [2] NCCL INFO comm 0x148a580090d0 rank 14 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1206:131663:131875 [3] NCCL INFO comm 0x14db0c0090d0 rank 15 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1206:131661:131880 [1] NCCL INFO comm 0x15385c0090d0 rank 13 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Channel 03 : 9[44000] -> 8[3000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO Connected all trees
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1201:920022:920860 [2] NCCL INFO comm 0x14b0180090d0 rank 2 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1201:920021:920858 [1] NCCL INFO comm 0x149abc0090d0 rank 1 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO Connected all trees
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Channel 01 : 5[44000] -> 4[3000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920854 [0] NCCL INFO comm 0x14b5480090d0 rank 0 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1201:920020:920020 [0] NCCL INFO Launch mode Parallel
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO Connected all trees
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO Connected all trees
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO Connected all trees
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default3]:n2gpu1201:920023:920863 [3] NCCL INFO comm 0x14dfd40090d0 rank 3 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Channel 00 : 25[44000] -> 24[3000] via P2P/IPC/read
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Channel 02 : 5[44000] -> 4[3000] via P2P/IPC/read
[default3]:n2gpu1227:180445:180655 [3] NCCL INFO comm 0x1461200090d0 rank 43 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1227:180444:180652 [2] NCCL INFO comm 0x1546280090d0 rank 42 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1227:180442:180658 [0] NCCL INFO comm 0x150e000090d0 rank 40 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1205:131570:131790 [0] NCCL INFO comm 0x1527900090d0 rank 8 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default3]:n2gpu1205:131573:131788 [3] NCCL INFO comm 0x1522940090d0 rank 11 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1205:131571:131793 [1] NCCL INFO comm 0x1468880090d0 rank 9 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1205:131572:131785 [2] NCCL INFO comm 0x14a84c0090d0 rank 10 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Channel 01 : 25[44000] -> 24[3000] via P2P/IPC/read
[default1]:n2gpu1227:180443:180660 [1] NCCL INFO comm 0x15093c0090d0 rank 41 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Channel 00 : 37[44000] -> 36[3000] via P2P/IPC/read
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Channel 03 : 5[44000] -> 4[3000] via P2P/IPC/read
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Channel 02 : 25[44000] -> 24[3000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Channel 01 : 37[44000] -> 36[3000] via P2P/IPC/read
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO Connected all trees
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO Connected all trees
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO Connected all trees
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Channel 02 : 37[44000] -> 36[3000] via P2P/IPC/read
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Channel 03 : 25[44000] -> 24[3000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Channel 00 : 21[44000] -> 20[3000] via P2P/IPC/read
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO Connected all trees
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO Connected all trees
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1202:1117358:1117584 [0] NCCL INFO comm 0x14ca9c0090d0 rank 4 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default2]:n2gpu1202:1117360:1117581 [2] NCCL INFO comm 0x145f340090d0 rank 6 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1202:1117359:1117578 [1] NCCL INFO comm 0x14dd200090d0 rank 5 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default3]:n2gpu1202:1117361:1117587 [3] NCCL INFO comm 0x152e3c0090d0 rank 7 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO Connected all trees
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO Connected all trees
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO Connected all trees
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Channel 01 : 21[44000] -> 20[3000] via P2P/IPC/read
[default1]:n2gpu1207:132838:133051 [1] NCCL INFO comm 0x14a2000090d0 rank 17 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1207:132839:133058 [2] NCCL INFO comm 0x152c600090d0 rank 18 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1207:132840:133056 [3] NCCL INFO comm 0x1524dc0090d0 rank 19 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1207:132837:133053 [0] NCCL INFO comm 0x1496740090d0 rank 16 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Channel 03 : 37[44000] -> 36[3000] via P2P/IPC/read
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO Connected all trees
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO Connected all trees
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1219:255857:256087 [0] NCCL INFO comm 0x1542b80090d0 rank 24 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default3]:n2gpu1219:255860:256089 [3] NCCL INFO comm 0x154a8c0090d0 rank 27 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1219:255859:256082 [2] NCCL INFO comm 0x14a04c0090d0 rank 26 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1219:255858:256084 [1] NCCL INFO comm 0x1527800090d0 rank 25 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Channel 02 : 21[44000] -> 20[3000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO Connected all trees
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default3]:n2gpu1224:114176:114381 [3] NCCL INFO comm 0x1455500090d0 rank 39 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1224:114175:114385 [2] NCCL INFO comm 0x148cf80090d0 rank 38 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1224:114174:114390 [1] NCCL INFO comm 0x14b3b00090d0 rank 37 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1224:114173:114387 [0] NCCL INFO comm 0x154d680090d0 rank 36 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Channel 03 : 21[44000] -> 20[3000] via P2P/IPC/read
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO Connected all trees
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO Connected all trees
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO Connected all trees
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default3]:n2gpu1210:132255:132463 [3] NCCL INFO comm 0x1549340090d0 rank 23 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1210:132252:132465 [0] NCCL INFO comm 0x1493440090d0 rank 20 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1210:132253:132467 [1] NCCL INFO comm 0x1544c80090d0 rank 21 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1210:132254:132470 [2] NCCL INFO comm 0x1488940090d0 rank 22 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:>>> done with compiling and loading fused kernels. Compilation time: 232.147 seconds
[default0]:time to initialize megatron (seconds): 262.418
[default0]:[after megatron is initialized] datetime: 2023-04-24 00:24:06 
[default0]:building GPT model ...
[default0]:[2023-04-24 00:24:07,032] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[default0]:[2023-04-24 00:24:07,032] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[default0]:[2023-04-24 00:24:07,033] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.36 GB, percent = 3.2%
[default0]:[2023-04-24 00:24:07,815] [INFO] [utils.py:785:see_memory_usage] After Building Model
[default0]:[2023-04-24 00:24:07,816] [INFO] [utils.py:786:see_memory_usage] MA 19.14 GB         Max_MA 19.14 GB         CA 19.14 GB         Max_CA 19 GB 
[default0]:[2023-04-24 00:24:07,834] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.46 GB, percent = 3.3%
[default0]: > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 10279239680
[default0]:setting training iterations to 868
[default0]:> learning rate decay style: cosine
[default0]:DeepSpeed is enabled.
[default0]:[2023-04-24 00:24:07,839] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.9.0, git-hash=unknown, git-branch=unknown
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO Trees [0] 23/-1/-1->22->21 [1] 23/-1/-1->22->21 [2] 23/-1/-1->22->21 [3] 23/-1/-1->22->21
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO Trees [0] -1/-1/-1->23->22 [1] -1/-1/-1->23->22 [2] -1/-1/-1->23->22 [3] -1/-1/-1->23->22
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Trees [0] 25/28/-1->24->16 [1] 25/28/-1->24->16 [2] 25/-1/-1->24->21 [3] 25/-1/-1->24->21
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Trees [0] 26/20/-1->25->24 [1] 26/20/-1->25->24 [2] 26/-1/-1->25->24 [3] 26/-1/-1->25->24
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Trees [0] 29/-1/-1->28->24 [1] 29/-1/-1->28->24 [2] 29/20/-1->28->13 [3] 29/20/-1->28->13
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Trees [0] 30/-1/-1->29->28 [1] 30/-1/-1->29->28 [2] 30/36/-1->29->28 [3] 30/36/-1->29->28
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO Trees [0] 31/-1/-1->30->29 [1] 31/-1/-1->30->29 [2] 31/-1/-1->30->29 [3] 31/-1/-1->30->29
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 00/0 : 23[c4000] -> 24[3000] [receive] via NET/IB/0
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/8/-1->5->4 [3] 6/8/-1->5->4
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO Trees [0] -1/-1/-1->31->30 [1] -1/-1/-1->31->30 [2] -1/-1/-1->31->30 [3] -1/-1/-1->31->30
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Trees [0] 33/40/-1->32->0 [1] 33/40/-1->32->0 [2] 33/-1/-1->32->36 [3] 33/-1/-1->32->36
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 01/0 : 23[c4000] -> 24[3000] [receive] via NET/IB/0
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 00/0 : 27[c4000] -> 28[3000] [receive] via NET/IB/0
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO Trees [0] 35/-1/-1->34->33 [1] 35/-1/-1->34->33 [2] 35/-1/-1->34->33 [3] 35/-1/-1->34->33
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Trees [0] 34/16/-1->33->32 [1] 34/16/-1->33->32 [2] 34/-1/-1->33->32 [3] 34/-1/-1->33->32
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 00/0 : 31[c4000] -> 32[3000] [receive] via NET/IB/0
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO Channel 00 : 22[84000] -> 23[c4000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Trees [0] 9/12/-1->8->17 [1] 9/12/-1->8->17 [2] 9/-1/-1->8->5 [3] 9/-1/-1->8->5
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO Trees [0] -1/-1/-1->35->34 [1] -1/-1/-1->35->34 [2] -1/-1/-1->35->34 [3] -1/-1/-1->35->34
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 02/0 : 23[c4000] -> 24[3000] [receive] via NET/IB/0
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 01/0 : 27[c4000] -> 28[3000] [receive] via NET/IB/0
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Trees [0] 38/-1/-1->37->36 [1] 38/-1/-1->37->36 [2] 38/40/-1->37->36 [3] 38/40/-1->37->36
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO Trees [0] -1/-1/-1->39->38 [1] -1/-1/-1->39->38 [2] -1/-1/-1->39->38 [3] -1/-1/-1->39->38
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->9 [2] 11/-1/-1->10->9 [3] 11/-1/-1->10->9
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 00/0 : 7[c4000] -> 8[3000] [receive] via NET/IB/0
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Channel 00 : 29[44000] -> 30[84000] via P2P/IPC/read
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Channel 00 : 25[44000] -> 26[84000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Trees [0] 41/44/-1->40->32 [1] 41/44/-1->40->32 [2] 41/-1/-1->40->37 [3] 41/-1/-1->40->37
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Trees [0] 42/36/-1->41->40 [1] 42/36/-1->41->40 [2] 42/-1/-1->41->40 [3] 42/-1/-1->41->40
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 01/0 : 31[c4000] -> 32[3000] [receive] via NET/IB/0
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 02/0 : 27[c4000] -> 28[3000] [receive] via NET/IB/0
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO Channel 01 : 22[84000] -> 23[c4000] via P2P/IPC/read
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO Channel 00/0 : 23[c4000] -> 24[3000] [send] via NET/IB/0
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Trees [0] 13/-1/-1->12->8 [1] 13/-1/-1->12->8 [2] 13/4/-1->12->44 [3] 13/4/-1->12->44
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 03/0 : 23[c4000] -> 24[3000] [receive] via NET/IB/0
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 00 : 24[3000] -> 25[44000] via P2P/IPC/read
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Channel 01 : 25[44000] -> 26[84000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO Trees [0] -1/-1/-1->11->10 [1] -1/-1/-1->11->10 [2] -1/-1/-1->11->10 [3] -1/-1/-1->11->10
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO Trees [0] -1/-1/-1->43->42 [1] -1/-1/-1->43->42 [2] -1/-1/-1->43->42 [3] -1/-1/-1->43->42
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO Trees [0] 43/-1/-1->42->41 [1] 43/-1/-1->42->41 [2] 43/-1/-1->42->41 [3] 43/-1/-1->42->41
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Trees [0] 45/-1/-1->44->40 [1] 45/-1/-1->44->40 [2] 45/12/-1->44->-1 [3] 45/12/-1->44->-1
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO Trees [0] 47/-1/-1->46->45 [1] 47/-1/-1->46->45 [2] 47/-1/-1->46->45 [3] 47/-1/-1->46->45
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO Trees [0] 46/-1/-1->45->44 [1] 46/-1/-1->45->44 [2] 46/-1/-1->45->44 [3] 46/-1/-1->45->44
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO Channel 00 : 26[84000] -> 27[c4000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 00/0 : 35[c4000] -> 36[3000] [receive] via NET/IB/0
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO Channel 01/0 : 23[c4000] -> 24[3000] [send] via NET/IB/0
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] -1/-1/-1->15->14 [2] -1/-1/-1->15->14 [3] -1/-1/-1->15->14
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/28/-1->13->12 [3] 14/28/-1->13->12
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Trees [0] 17/24/-1->16->33 [1] 17/24/-1->16->33 [2] 17/-1/-1->16->20 [3] 17/-1/-1->16->20
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 01/0 : 7[c4000] -> 8[3000] [receive] via NET/IB/0
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO Channel 00/0 : 27[c4000] -> 28[3000] [send] via NET/IB/0
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO Channel 00 : 30[84000] -> 31[c4000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 02/0 : 31[c4000] -> 32[3000] [receive] via NET/IB/0
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 00/0 : 39[c4000] -> 40[3000] [receive] via NET/IB/0
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 03/0 : 27[c4000] -> 28[3000] [receive] via NET/IB/0
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 00 : 28[3000] -> 29[44000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO Trees [0] -1/-1/-1->47->46 [1] -1/-1/-1->47->46 [2] -1/-1/-1->47->46 [3] -1/-1/-1->47->46
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO Channel 02 : 22[84000] -> 23[c4000] via P2P/IPC/read
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO Channel 01 : 26[84000] -> 27[c4000] via P2P/IPC/read
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Trees [0] 18/8/-1->17->16 [1] 18/8/-1->17->16 [2] 18/-1/-1->17->16 [3] 18/-1/-1->17->16
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO Channel 02/0 : 23[c4000] -> 24[3000] [send] via NET/IB/0
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 00/0 : 11[c4000] -> 12[3000] [receive] via NET/IB/0
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Channel 01 : 29[44000] -> 30[84000] via P2P/IPC/read
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO Trees [0] 19/-1/-1->18->17 [1] 19/-1/-1->18->17 [2] 19/-1/-1->18->17 [3] 19/-1/-1->18->17
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 01 : 24[3000] -> 25[44000] via P2P/IPC/read
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO Channel 01/0 : 27[c4000] -> 28[3000] [send] via NET/IB/0
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Channel 00 : 33[44000] -> 34[84000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 03/0 : 31[c4000] -> 32[3000] [receive] via NET/IB/0
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 00 : 32[3000] -> 33[44000] via P2P/IPC/read
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO Channel 03 : 22[84000] -> 23[c4000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 01/0 : 39[c4000] -> 40[3000] [receive] via NET/IB/0
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 00/0 : 43[c4000] -> 44[3000] [receive] via NET/IB/0
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Channel 00 : 5[44000] -> 6[84000] via P2P/IPC/read
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 01/0 : 35[c4000] -> 36[3000] [receive] via NET/IB/0
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO Channel 03/0 : 23[c4000] -> 24[3000] [send] via NET/IB/0
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 01/0 : 11[c4000] -> 12[3000] [receive] via NET/IB/0
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 00/0 : 15[c4000] -> 16[3000] [receive] via NET/IB/0
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Channel 02 : 25[44000] -> 26[84000] via P2P/IPC/read
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO Channel 02/0 : 27[c4000] -> 28[3000] [send] via NET/IB/0
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 00/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 01/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 02/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 03/04 :    0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Trees [0] 1/32/-1->0->-1 [1] 1/32/-1->0->-1 [2] 1/-1/-1->0->4 [3] 1/-1/-1->0->4
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO Channel 01 : 30[84000] -> 31[c4000] via P2P/IPC/read
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO Channel 00/0 : 31[c4000] -> 32[3000] [send] via NET/IB/0
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO Channel 00 : 6[84000] -> 7[c4000] via P2P/IPC/read
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO Trees [0] -1/-1/-1->19->18 [1] -1/-1/-1->19->18 [2] -1/-1/-1->19->18 [3] -1/-1/-1->19->18
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 02/0 : 7[c4000] -> 8[3000] [receive] via NET/IB/0
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO Channel 00 : 34[84000] -> 35[c4000] via P2P/IPC/read
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 01 : 28[3000] -> 29[44000] via P2P/IPC/read
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO Channel 02 : 26[84000] -> 27[c4000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Trees [0] 22/-1/-1->21->20 [1] 22/-1/-1->21->20 [2] 22/24/-1->21->20 [3] 22/24/-1->21->20
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Trees [0] 21/-1/-1->20->25 [1] 21/-1/-1->20->25 [2] 21/16/-1->20->28 [3] 21/16/-1->20->28
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Channel 01 : 5[44000] -> 6[84000] via P2P/IPC/read
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 02 : 24[3000] -> 25[44000] via P2P/IPC/read
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Channel 03 : 25[44000] -> 26[84000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 00/0 : 47[c4000] -> 0[3000] [receive] via NET/IB/0
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO Channel 02 : 30[84000] -> 31[c4000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Channel 02 : 29[44000] -> 30[84000] via P2P/IPC/read
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO Channel 01 : 6[84000] -> 7[c4000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Channel 00 : 37[44000] -> 38[84000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 03/0 : 7[c4000] -> 8[3000] [receive] via NET/IB/0
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 00 : 8[3000] -> 9[44000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 02/0 : 39[c4000] -> 40[3000] [receive] via NET/IB/0
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO Channel 00/0 : 35[c4000] -> 36[3000] [send] via NET/IB/0
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Channel 01 : 33[44000] -> 34[84000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 01 : 32[3000] -> 33[44000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 01/0 : 43[c4000] -> 44[3000] [receive] via NET/IB/0
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 02 : 28[3000] -> 29[44000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 00/0 : 19[c4000] -> 20[3000] [receive] via NET/IB/0
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Trees [0] 5/-1/-1->4->9 [1] 5/-1/-1->4->9 [2] 5/0/-1->4->12 [3] 5/0/-1->4->12
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO Channel 00/0 : 7[c4000] -> 8[3000] [send] via NET/IB/0
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 03 : 24[3000] -> 25[44000] via P2P/IPC/read
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO Channel 03/0 : 27[c4000] -> 28[3000] [send] via NET/IB/0
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 02/0 : 35[c4000] -> 36[3000] [receive] via NET/IB/0
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 01/0 : 47[c4000] -> 0[3000] [receive] via NET/IB/0
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 02/0 : 11[c4000] -> 12[3000] [receive] via NET/IB/0
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO Channel 01/0 : 31[c4000] -> 32[3000] [send] via NET/IB/0
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO Channel 02/0 : 31[c4000] -> 32[3000] [send] via NET/IB/0
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 01/0 : 15[c4000] -> 16[3000] [receive] via NET/IB/0
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO Channel 00 : 10[84000] -> 11[c4000] via P2P/IPC/read
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Channel 00 : 9[44000] -> 10[84000] via P2P/IPC/read
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO Channel 01 : 34[84000] -> 35[c4000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO Channel 01/0 : 35[c4000] -> 36[3000] [send] via NET/IB/0
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Channel 00 : 41[44000] -> 42[84000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 02/0 : 43[c4000] -> 44[3000] [receive] via NET/IB/0
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO Channel 03 : 26[84000] -> 27[c4000] via P2P/IPC/read
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 00/0 : 3[c4000] -> 4[3000] [receive] via NET/IB/0
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Channel 02 : 5[44000] -> 6[84000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO Channel 01/0 : 7[c4000] -> 8[3000] [send] via NET/IB/0
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO Channel 02/0 : 7[c4000] -> 8[3000] [send] via NET/IB/0
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 03/0 : 35[c4000] -> 36[3000] [receive] via NET/IB/0
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 00 : 36[3000] -> 37[44000] via P2P/IPC/read
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO Channel 02 : 6[84000] -> 7[c4000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Channel 01 : 37[44000] -> 38[84000] via P2P/IPC/read
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO Channel 00/0 : 39[c4000] -> 40[3000] [send] via NET/IB/0
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO Channel 03 : 30[84000] -> 31[c4000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Channel 03 : 29[44000] -> 30[84000] via P2P/IPC/read
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO Channel 03/0 : 31[c4000] -> 32[3000] [send] via NET/IB/0
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Channel 00 : 13[44000] -> 14[84000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 02/0 : 15[c4000] -> 16[3000] [receive] via NET/IB/0
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 03 : 28[3000] -> 29[44000] via P2P/IPC/read
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO Channel 01 : 10[84000] -> 11[c4000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 01 : 8[3000] -> 9[44000] via P2P/IPC/read
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO Channel 02 : 34[84000] -> 35[c4000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO Channel 02/0 : 35[c4000] -> 36[3000] [send] via NET/IB/0
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Channel 02 : 33[44000] -> 34[84000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 02 : 32[3000] -> 33[44000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 03/0 : 43[c4000] -> 44[3000] [receive] via NET/IB/0
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 00 : 44[3000] -> 45[44000] via P2P/IPC/read
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO Channel 00 : 46[84000] -> 47[c4000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 03/0 : 39[c4000] -> 40[3000] [receive] via NET/IB/0
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 00 : 40[3000] -> 41[44000] via P2P/IPC/read
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO Channel 00 : 42[84000] -> 43[c4000] via P2P/IPC/read
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Channel 01 : 41[44000] -> 42[84000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 01/0 : 19[c4000] -> 20[3000] [receive] via NET/IB/0
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO Channel 00 : 38[84000] -> 39[c4000] via P2P/IPC/read
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Channel 00 : 17[44000] -> 18[84000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 03/0 : 15[c4000] -> 16[3000] [receive] via NET/IB/0
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 00 : 16[3000] -> 17[44000] via P2P/IPC/read
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO Channel 01/0 : 39[c4000] -> 40[3000] [send] via NET/IB/0
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO Channel 02/0 : 39[c4000] -> 40[3000] [send] via NET/IB/0
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO Channel 00/0 : 15[c4000] -> 16[3000] [send] via NET/IB/0
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO Channel 00 : 14[84000] -> 15[c4000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 03/0 : 11[c4000] -> 12[3000] [receive] via NET/IB/0
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 00 : 12[3000] -> 13[44000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 02/0 : 47[c4000] -> 0[3000] [receive] via NET/IB/0
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO Channel 00/0 : 11[c4000] -> 12[3000] [send] via NET/IB/0
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO Channel 01/0 : 11[c4000] -> 12[3000] [send] via NET/IB/0
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Channel 01 : 9[44000] -> 10[84000] via P2P/IPC/read
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO Channel 03 : 34[84000] -> 35[c4000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO Channel 03/0 : 35[c4000] -> 36[3000] [send] via NET/IB/0
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Channel 03 : 33[44000] -> 34[84000] via P2P/IPC/read
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO Channel 00/0 : 43[c4000] -> 44[3000] [send] via NET/IB/0
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO Channel 01/0 : 43[c4000] -> 44[3000] [send] via NET/IB/0
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 01 : 40[3000] -> 41[44000] via P2P/IPC/read
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO Channel 01 : 42[84000] -> 43[c4000] via P2P/IPC/read
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO Connected all rings
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO Channel 01 : 38[84000] -> 39[c4000] via P2P/IPC/read
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO Channel 01 : 46[84000] -> 47[c4000] via P2P/IPC/read
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO Channel 00 : 45[44000] -> 46[84000] via P2P/IPC/read
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Connected all rings
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 01/0 : 3[c4000] -> 4[3000] [receive] via NET/IB/0
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Channel 03 : 5[44000] -> 6[84000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO Channel 03/0 : 7[c4000] -> 8[3000] [send] via NET/IB/0
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 01 : 36[3000] -> 37[44000] via P2P/IPC/read
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO Connected all rings
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO Channel 01/0 : 15[c4000] -> 16[3000] [send] via NET/IB/0
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Channel 01 : 13[44000] -> 14[84000] via P2P/IPC/read
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO Channel 01 : 14[84000] -> 15[c4000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Channel 02 : 37[44000] -> 38[84000] via P2P/IPC/read
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO Channel 03/0 : 39[c4000] -> 40[3000] [send] via NET/IB/0
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO Channel 03 : 6[84000] -> 7[c4000] via P2P/IPC/read
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO Channel 00 : 18[84000] -> 19[c4000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 03 : 32[3000] -> 33[44000] via P2P/IPC/read
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO Channel 02 : 10[84000] -> 11[c4000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO Channel 02/0 : 11[c4000] -> 12[3000] [send] via NET/IB/0
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 02 : 8[3000] -> 9[44000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 03 : 8[3000] -> 9[44000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 01 : 44[3000] -> 45[44000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO Channel 00/0 : 47[c4000] -> 0[3000] [send] via NET/IB/0
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO Channel 02/0 : 43[c4000] -> 44[3000] [send] via NET/IB/0
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO Channel 02 : 42[84000] -> 43[c4000] via P2P/IPC/read
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Channel 02 : 41[44000] -> 42[84000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 02/0 : 19[c4000] -> 20[3000] [receive] via NET/IB/0
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 02/0 : 3[c4000] -> 4[3000] [receive] via NET/IB/0
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO Channel 00 : 1[44000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO Channel 00 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 02 : 36[3000] -> 37[44000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Connected all rings
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO Channel 02/0 : 15[c4000] -> 16[3000] [send] via NET/IB/0
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO Channel 02 : 14[84000] -> 15[c4000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 01 : 12[3000] -> 13[44000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 03/0 : 47[c4000] -> 0[3000] [receive] via NET/IB/0
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 00 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Channel 01 : 17[44000] -> 18[84000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 01 : 16[3000] -> 17[44000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Channel 03 : 37[44000] -> 38[84000] via P2P/IPC/read
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO Connected all rings
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO Channel 03 : 10[84000] -> 11[c4000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO Channel 03/0 : 11[c4000] -> 12[3000] [send] via NET/IB/0
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Channel 02 : 9[44000] -> 10[84000] via P2P/IPC/read
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Channel 03 : 9[44000] -> 10[84000] via P2P/IPC/read
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO Connected all rings
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO Channel 02 : 38[84000] -> 39[c4000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 02 : 44[3000] -> 45[44000] via P2P/IPC/read
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO Channel 02 : 46[84000] -> 47[c4000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO Channel 01/0 : 47[c4000] -> 0[3000] [send] via NET/IB/0
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO Channel 01 : 45[44000] -> 46[84000] via P2P/IPC/read
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO Channel 03/0 : 43[c4000] -> 44[3000] [send] via NET/IB/0
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 02 : 40[3000] -> 41[44000] via P2P/IPC/read
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Channel 03 : 41[44000] -> 42[84000] via P2P/IPC/read
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Channel 00/0 : 20[3000] -> 25[44000] [receive] via NET/IB/1
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Channel 02/0 : 29[44000] -> 36[3000] [send] via NET/IB/1
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO Channel 03/0 : 15[c4000] -> 16[3000] [send] via NET/IB/0
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Channel 02 : 13[44000] -> 14[84000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 01 : 0[3000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO Channel 00/0 : 19[c4000] -> 20[3000] [send] via NET/IB/0
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO Channel 01 : 18[84000] -> 19[c4000] via P2P/IPC/read
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Connected all rings
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO Channel 03 : 42[84000] -> 43[c4000] via P2P/IPC/read
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO Channel 03 : 38[84000] -> 39[c4000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Channel 00 : 21[44000] -> 22[84000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO Channel 02/0 : 47[c4000] -> 0[3000] [send] via NET/IB/0
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO Channel 02 : 45[44000] -> 46[84000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 03/0 : 19[c4000] -> 20[3000] [receive] via NET/IB/0
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 00 : 20[3000] -> 21[44000] via P2P/IPC/read
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 03/0 : 3[c4000] -> 4[3000] [receive] via NET/IB/0
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 00 : 4[3000] -> 5[44000] via P2P/IPC/read
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO Channel 01 : 1[44000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Channel 03/0 : 29[44000] -> 36[3000] [send] via NET/IB/1
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO Connected all rings
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO Channel 00 : 23[c4000] -> 22[84000] via P2P/IPC/read
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Channel 01/0 : 20[3000] -> 25[44000] [receive] via NET/IB/1
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO Connected all rings
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO Channel 00 : 27[c4000] -> 26[84000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 03 : 36[3000] -> 37[44000] via P2P/IPC/read
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Channel 03 : 13[44000] -> 14[84000] via P2P/IPC/read
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO Channel 03 : 14[84000] -> 15[c4000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 02 : 12[3000] -> 13[44000] via P2P/IPC/read
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Channel 02 : 17[44000] -> 18[84000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 02 : 16[3000] -> 17[44000] via P2P/IPC/read
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO Channel 01/0 : 19[c4000] -> 20[3000] [send] via NET/IB/0
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO Connected all rings
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Connected all rings
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 03 : 40[3000] -> 41[44000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 03 : 44[3000] -> 45[44000] via P2P/IPC/read
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO Channel 03 : 46[84000] -> 47[c4000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO Channel 03/0 : 47[c4000] -> 0[3000] [send] via NET/IB/0
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 01 : 20[3000] -> 21[44000] via P2P/IPC/read
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO Channel 00/0 : 3[c4000] -> 4[3000] [send] via NET/IB/0
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO Channel 02 : 1[44000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO Channel 01 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO Connected all rings
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO Channel 00 : 31[c4000] -> 30[84000] via P2P/IPC/read
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO Channel 01 : 23[c4000] -> 22[84000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 02 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Channel 00/0 : 16[3000] -> 33[44000] [receive] via NET/IB/1
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Channel 03 : 17[44000] -> 18[84000] via P2P/IPC/read
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO Channel 02/0 : 19[c4000] -> 20[3000] [send] via NET/IB/0
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO Channel 02 : 18[84000] -> 19[c4000] via P2P/IPC/read
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO Connected all rings
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO Connected all rings
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Channel 01 : 21[44000] -> 22[84000] via P2P/IPC/read
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO Channel 03 : 45[44000] -> 46[84000] via P2P/IPC/read
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 01 : 4[3000] -> 5[44000] via P2P/IPC/read
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO Channel 01/0 : 3[c4000] -> 4[3000] [send] via NET/IB/0
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 02 : 20[3000] -> 21[44000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Connected all rings
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO Channel 01 : 27[c4000] -> 26[84000] via P2P/IPC/read
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO Connected all rings
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 03 : 12[3000] -> 13[44000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Connected all rings
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 03 : 16[3000] -> 17[44000] via P2P/IPC/read
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO Channel 03/0 : 19[c4000] -> 20[3000] [send] via NET/IB/0
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO Channel 03 : 18[84000] -> 19[c4000] via P2P/IPC/read
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Channel 00/0 : 4[3000] -> 9[44000] [receive] via NET/IB/1
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Connected all rings
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO Channel 02/0 : 3[c4000] -> 4[3000] [send] via NET/IB/0
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO Channel 03/0 : 3[c4000] -> 4[3000] [send] via NET/IB/0
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO Channel 03 : 1[44000] -> 2[84000] via P2P/IPC/read
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO Channel 02 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO Channel 02 : 23[c4000] -> 22[84000] via P2P/IPC/read
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO Connected all rings
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO Connected all rings
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO Channel 00 : 30[84000] -> 29[44000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 03 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Channel 02/0 : 37[44000] -> 40[3000] [send] via NET/IB/1
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Connected all rings
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Channel 01/0 : 16[3000] -> 33[44000] [receive] via NET/IB/1
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Connected all rings
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 02 : 4[3000] -> 5[44000] via P2P/IPC/read
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO Connected all rings
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO Channel 00 : 7[c4000] -> 6[84000] via P2P/IPC/read
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO Channel 03 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 03 : 20[3000] -> 21[44000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Channel 02 : 21[44000] -> 22[84000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 02/0 : 21[44000] -> 24[3000] [receive] via NET/IB/1
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO Channel 02 : 27[c4000] -> 26[84000] via P2P/IPC/read
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO Channel 01 : 31[c4000] -> 30[84000] via P2P/IPC/read
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO Channel 03 : 23[c4000] -> 22[84000] via P2P/IPC/read
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Connected all rings
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 02/0 : 32[3000] -> 36[3000] [send] via NET/IB/1
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO Connected all rings
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO Channel 00 : 26[84000] -> 25[44000] via P2P/IPC/read
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO Channel 01 : 26[84000] -> 25[44000] via P2P/IPC/read
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Channel 01/0 : 4[3000] -> 9[44000] [receive] via NET/IB/1
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO Connected all rings
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO Channel 01 : 7[c4000] -> 6[84000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 03/0 : 21[44000] -> 24[3000] [receive] via NET/IB/1
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Channel 03/0 : 37[44000] -> 40[3000] [send] via NET/IB/1
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 00/0 : 24[3000] -> 28[3000] [receive] via NET/IB/1
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO Channel 00 : 34[84000] -> 33[44000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO Connected all rings
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO Channel 00 : 35[c4000] -> 34[84000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 03/0 : 32[3000] -> 36[3000] [send] via NET/IB/1
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Connected all rings
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Channel 00/0 : 36[3000] -> 41[44000] [receive] via NET/IB/1
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO Connected all rings
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Channel 03 : 21[44000] -> 22[84000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Connected all rings
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 03 : 4[3000] -> 5[44000] via P2P/IPC/read
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO Channel 02 : 31[c4000] -> 30[84000] via P2P/IPC/read
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Channel 02/0 : 13[44000] -> 28[3000] [send] via NET/IB/1
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO Channel 03 : 27[c4000] -> 26[84000] via P2P/IPC/read
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 01/0 : 24[3000] -> 28[3000] [receive] via NET/IB/1
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO Channel 00 : 6[84000] -> 5[44000] via P2P/IPC/read
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO Connected all rings
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO Channel 00 : 39[c4000] -> 38[84000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Connected all rings
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO Connected all rings
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO Channel 02 : 7[c4000] -> 6[84000] via P2P/IPC/read
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO Channel 01 : 30[84000] -> 29[44000] via P2P/IPC/read
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Channel 03/0 : 13[44000] -> 28[3000] [send] via NET/IB/1
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Connected all rings
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO Channel 01 : 34[84000] -> 33[44000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO Channel 01 : 35[c4000] -> 34[84000] via P2P/IPC/read
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Channel 00/0 : 8[3000] -> 17[44000] [receive] via NET/IB/1
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO Channel 02 : 26[84000] -> 25[44000] via P2P/IPC/read
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO Connected all rings
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO Channel 00 : 43[c4000] -> 42[84000] via P2P/IPC/read
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Channel 01/0 : 36[3000] -> 41[44000] [receive] via NET/IB/1
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO Channel 00 : 38[84000] -> 37[44000] via P2P/IPC/read
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Connected all rings
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Channel 02/0 : 5[44000] -> 8[3000] [send] via NET/IB/1
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO Channel 03 : 31[c4000] -> 30[84000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Channel 02/0 : 21[44000] -> 24[3000] [send] via NET/IB/1
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 02/0 : 32[3000] -> 36[3000] [receive] via NET/IB/1
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO Channel 01 : 6[84000] -> 5[44000] via P2P/IPC/read
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO Channel 02 : 34[84000] -> 33[44000] via P2P/IPC/read
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO Channel 03 : 26[84000] -> 25[44000] via P2P/IPC/read
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO Channel 01 : 39[c4000] -> 38[84000] via P2P/IPC/read
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO Channel 00 : 10[84000] -> 9[44000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO Connected all rings
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO Channel 00 : 11[c4000] -> 10[84000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO Channel 01 : 11[c4000] -> 10[84000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 02/0 : 5[44000] -> 8[3000] [receive] via NET/IB/1
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO Channel 01 : 43[c4000] -> 42[84000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Channel 03/0 : 21[44000] -> 24[3000] [send] via NET/IB/1
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO Connected all rings
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO Channel 00 : 15[c4000] -> 14[84000] via P2P/IPC/read
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO Channel 02 : 30[84000] -> 29[44000] via P2P/IPC/read
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO Channel 02 : 6[84000] -> 5[44000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO Channel 02 : 35[c4000] -> 34[84000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO Channel 03 : 35[c4000] -> 34[84000] via P2P/IPC/read
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO Channel 02 : 39[c4000] -> 38[84000] via P2P/IPC/read
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Channel 01/0 : 8[3000] -> 17[44000] [receive] via NET/IB/1
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO Connected all rings
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO Channel 00 : 19[c4000] -> 18[84000] via P2P/IPC/read
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO Channel 00 : 42[84000] -> 41[44000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 03/0 : 5[44000] -> 8[3000] [receive] via NET/IB/1
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Channel 03/0 : 5[44000] -> 8[3000] [send] via NET/IB/1
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO Channel 03 : 7[c4000] -> 6[84000] via P2P/IPC/read
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO Channel 03 : 30[84000] -> 29[44000] via P2P/IPC/read
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO Channel 00 : 14[84000] -> 13[44000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Connected all rings
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO Channel 00 : 46[84000] -> 45[44000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO Connected all rings
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO Channel 00 : 47[c4000] -> 46[84000] via P2P/IPC/read
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO Channel 00 : 45[44000] -> 44[3000] via P2P/IPC/read
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO Connected all trees
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 03/0 : 32[3000] -> 36[3000] [receive] via NET/IB/1
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO Channel 03 : 6[84000] -> 5[44000] via P2P/IPC/read
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO Channel 03 : 34[84000] -> 33[44000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Connected all rings
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO Channel 01 : 19[c4000] -> 18[84000] via P2P/IPC/read
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO Channel 01 : 10[84000] -> 9[44000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO Channel 02 : 11[c4000] -> 10[84000] via P2P/IPC/read
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO Channel 02 : 43[c4000] -> 42[84000] via P2P/IPC/read
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO Channel 01 : 42[84000] -> 41[44000] via P2P/IPC/read
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO Channel 01 : 38[84000] -> 37[44000] via P2P/IPC/read
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO Channel 01 : 15[c4000] -> 14[84000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Connected all rings
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO Channel 01 : 46[84000] -> 45[44000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Connected all rings
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO Channel 03 : 39[c4000] -> 38[84000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 02/0 : 16[3000] -> 20[3000] [send] via NET/IB/1
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO Channel 00 : 18[84000] -> 17[44000] via P2P/IPC/read
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO Channel 03 : 11[c4000] -> 10[84000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Connected all rings
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 00/0 : 24[3000] -> 28[3000] [send] via NET/IB/1
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO Connected all rings
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO Channel 00 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO Channel 01 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Connected all rings
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Channel 02/0 : 24[3000] -> 21[44000] [receive] via NET/IB/1
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO Channel 01 : 14[84000] -> 13[44000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 00/0 : 8[3000] -> 12[3000] [receive] via NET/IB/1
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 00/0 : 40[3000] -> 44[3000] [receive] via NET/IB/1
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO Channel 01 : 47[c4000] -> 46[84000] via P2P/IPC/read
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO Channel 01 : 45[44000] -> 44[3000] via P2P/IPC/read
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO Connected all trees
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO Connected all trees
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO Channel 02 : 10[84000] -> 9[44000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 03/0 : 16[3000] -> 20[3000] [send] via NET/IB/1
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO Channel 02 : 19[c4000] -> 18[84000] via P2P/IPC/read
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO Channel 01 : 18[84000] -> 17[44000] via P2P/IPC/read
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO Channel 02 : 38[84000] -> 37[44000] via P2P/IPC/read
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO Channel 03 : 38[84000] -> 37[44000] via P2P/IPC/read
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO Channel 03 : 43[c4000] -> 42[84000] via P2P/IPC/read
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO Channel 02 : 42[84000] -> 41[44000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 01/0 : 24[3000] -> 28[3000] [send] via NET/IB/1
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Connected all rings
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO Connected all trees
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO Channel 02 : 15[c4000] -> 14[84000] via P2P/IPC/read
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO Channel 02 : 14[84000] -> 13[44000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 01/0 : 8[3000] -> 12[3000] [receive] via NET/IB/1
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO Channel 02 : 46[84000] -> 45[44000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 02/0 : 0[3000] -> 4[3000] [send] via NET/IB/1
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 03/0 : 0[3000] -> 4[3000] [send] via NET/IB/1
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO Channel 03 : 19[c4000] -> 18[84000] via P2P/IPC/read
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO Channel 00 : 22[84000] -> 21[44000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 02/0 : 37[44000] -> 40[3000] [receive] via NET/IB/1
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 02/0 : 16[3000] -> 20[3000] [receive] via NET/IB/1
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Channel 03/0 : 24[3000] -> 21[44000] [receive] via NET/IB/1
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Channel 02/0 : 8[3000] -> 5[44000] [receive] via NET/IB/1
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO Channel 02 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO Channel 00 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO Channel 00 : 2[84000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 01/0 : 40[3000] -> 44[3000] [receive] via NET/IB/1
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO Channel 03 : 46[84000] -> 45[44000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO Channel 02 : 47[c4000] -> 46[84000] via P2P/IPC/read
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO Channel 02 : 45[44000] -> 44[3000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 00/0 : 32[3000] -> 40[3000] [send] via NET/IB/1
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 01/0 : 32[3000] -> 40[3000] [send] via NET/IB/1
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO Channel 02 : 18[84000] -> 17[44000] via P2P/IPC/read
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO Channel 03 : 10[84000] -> 9[44000] via P2P/IPC/read
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 00/0 : 8[3000] -> 12[3000] [send] via NET/IB/1
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO Channel 01 : 22[84000] -> 21[44000] via P2P/IPC/read
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO Channel 03 : 42[84000] -> 41[44000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 03/0 : 16[3000] -> 20[3000] [receive] via NET/IB/1
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 02/0 : 0[3000] -> 4[3000] [receive] via NET/IB/1
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Channel 03/0 : 8[3000] -> 5[44000] [receive] via NET/IB/1
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO Channel 01 : 2[84000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO Channel 03 : 15[c4000] -> 14[84000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO Channel 03 : 47[c4000] -> 46[84000] via P2P/IPC/read
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO Channel 03 : 45[44000] -> 44[3000] via P2P/IPC/read
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 00/0 : 36[3000] -> 41[44000] [send] via NET/IB/1
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 01/0 : 36[3000] -> 41[44000] [send] via NET/IB/1
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO Connected all trees
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 01/0 : 8[3000] -> 12[3000] [send] via NET/IB/1
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO Connected all trees
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 03/0 : 37[44000] -> 40[3000] [receive] via NET/IB/1
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 03/0 : 0[3000] -> 4[3000] [receive] via NET/IB/1
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO Channel 03 : 14[84000] -> 13[44000] via P2P/IPC/read
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO Channel 03 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO Channel 01 : 1[44000] -> 0[3000] via P2P/IPC/read
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO Channel 02 : 2[84000] -> 1[44000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO Connected all trees
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO Channel 03 : 18[84000] -> 17[44000] via P2P/IPC/read
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO Channel 02 : 22[84000] -> 21[44000] via P2P/IPC/read
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO Connected all trees
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO Connected all trees
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO Channel 03 : 2[84000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 00/0 : 16[3000] -> 24[3000] [receive] via NET/IB/1
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO Connected all trees
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 00/0 : 16[3000] -> 24[3000] [send] via NET/IB/1
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO Connected all trees
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO Channel 03 : 22[84000] -> 21[44000] via P2P/IPC/read
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO Channel 02 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 00/0 : 20[3000] -> 25[44000] [send] via NET/IB/1
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 01/0 : 16[3000] -> 24[3000] [receive] via NET/IB/1
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 02/0 : 29[44000] -> 36[3000] [receive] via NET/IB/1
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 01/0 : 16[3000] -> 24[3000] [send] via NET/IB/1
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 00/0 : 8[3000] -> 17[44000] [send] via NET/IB/1
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 02/0 : 20[3000] -> 28[3000] [receive] via NET/IB/1
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 02/0 : 4[3000] -> 12[3000] [receive] via NET/IB/1
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO Connected all trees
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 01/0 : 20[3000] -> 25[44000] [send] via NET/IB/1
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Channel 02/0 : 40[3000] -> 37[44000] [receive] via NET/IB/1
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 01/0 : 8[3000] -> 17[44000] [send] via NET/IB/1
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 03/0 : 20[3000] -> 28[3000] [receive] via NET/IB/1
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Channel 00/0 : 41[44000] -> 36[3000] [send] via NET/IB/1
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Channel 01/0 : 41[44000] -> 36[3000] [send] via NET/IB/1
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO Channel 03 : 1[44000] -> 0[3000] via P2P/IPC/read
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO Connected all trees
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Channel 03/0 : 40[3000] -> 37[44000] [receive] via NET/IB/1
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 00/0 : 32[3000] -> 0[3000] [receive] via NET/IB/1
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 00/0 : 40[3000] -> 44[3000] [send] via NET/IB/1
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 03/0 : 4[3000] -> 12[3000] [receive] via NET/IB/1
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 02/0 : 20[3000] -> 28[3000] [send] via NET/IB/1
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 03/0 : 29[44000] -> 36[3000] [receive] via NET/IB/1
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 00/0 : 16[3000] -> 33[44000] [send] via NET/IB/1
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 01/0 : 40[3000] -> 44[3000] [send] via NET/IB/1
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO Connected all trees
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 03/0 : 20[3000] -> 28[3000] [send] via NET/IB/1
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Channel 00/0 : 25[44000] -> 20[3000] [send] via NET/IB/1
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 01/0 : 32[3000] -> 0[3000] [receive] via NET/IB/1
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Channel 00/0 : 17[44000] -> 8[3000] [send] via NET/IB/1
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 01/0 : 16[3000] -> 33[44000] [send] via NET/IB/1
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 00/0 : 4[3000] -> 9[44000] [send] via NET/IB/1
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 01/0 : 4[3000] -> 9[44000] [send] via NET/IB/1
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Channel 01/0 : 25[44000] -> 20[3000] [send] via NET/IB/1
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 00/0 : 24[3000] -> 16[3000] [send] via NET/IB/1
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 00/0 : 17[44000] -> 8[3000] [receive] via NET/IB/1
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Channel 02/0 : 36[3000] -> 29[44000] [receive] via NET/IB/1
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 01/0 : 24[3000] -> 16[3000] [send] via NET/IB/1
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Channel 01/0 : 17[44000] -> 8[3000] [send] via NET/IB/1
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 00/0 : 32[3000] -> 40[3000] [receive] via NET/IB/1
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 02/0 : 36[3000] -> 29[44000] [send] via NET/IB/1
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 01/0 : 17[44000] -> 8[3000] [receive] via NET/IB/1
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 02/0 : 4[3000] -> 12[3000] [send] via NET/IB/1
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 02/0 : 28[3000] -> 20[3000] [receive] via NET/IB/1
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 03/0 : 36[3000] -> 29[44000] [send] via NET/IB/1
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 02/0 : 44[3000] -> 12[3000] [send] via NET/IB/1
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 02/0 : 13[44000] -> 28[3000] [receive] via NET/IB/1
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 00/0 : 33[44000] -> 16[3000] [receive] via NET/IB/1
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Channel 00/0 : 33[44000] -> 16[3000] [send] via NET/IB/1
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Channel 00/0 : 9[44000] -> 4[3000] [send] via NET/IB/1
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 01/0 : 32[3000] -> 40[3000] [receive] via NET/IB/1
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 03/0 : 4[3000] -> 12[3000] [send] via NET/IB/1
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Channel 03/0 : 36[3000] -> 29[44000] [receive] via NET/IB/1
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Channel 01/0 : 33[44000] -> 16[3000] [send] via NET/IB/1
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Channel 01/0 : 9[44000] -> 4[3000] [send] via NET/IB/1
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 01/0 : 33[44000] -> 16[3000] [receive] via NET/IB/1
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 03/0 : 28[3000] -> 20[3000] [receive] via NET/IB/1
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 03/0 : 44[3000] -> 12[3000] [send] via NET/IB/1
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 03/0 : 13[44000] -> 28[3000] [receive] via NET/IB/1
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 00/0 : 12[3000] -> 8[3000] [receive] via NET/IB/1
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 02/0 : 44[3000] -> 12[3000] [receive] via NET/IB/1
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 02/0 : 12[3000] -> 4[3000] [receive] via NET/IB/1
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 00/0 : 41[44000] -> 36[3000] [receive] via NET/IB/1
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 00/0 : 32[3000] -> 0[3000] [send] via NET/IB/1
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 00/0 : 40[3000] -> 32[3000] [send] via NET/IB/1
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 01/0 : 41[44000] -> 36[3000] [receive] via NET/IB/1
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 01/0 : 32[3000] -> 0[3000] [send] via NET/IB/1
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 01/0 : 12[3000] -> 8[3000] [receive] via NET/IB/1
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 03/0 : 44[3000] -> 12[3000] [receive] via NET/IB/1
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 01/0 : 40[3000] -> 32[3000] [send] via NET/IB/1
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 03/0 : 12[3000] -> 4[3000] [receive] via NET/IB/1
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 02/0 : 28[3000] -> 13[44000] [send] via NET/IB/1
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Channel 02/0 : 28[3000] -> 13[44000] [receive] via NET/IB/1
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 00/0 : 24[3000] -> 16[3000] [receive] via NET/IB/1
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 03/0 : 28[3000] -> 13[44000] [send] via NET/IB/1
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 01/0 : 24[3000] -> 16[3000] [receive] via NET/IB/1
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Channel 00 : 17[44000] -> 16[3000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 00/0 : 0[3000] -> 32[3000] [receive] via NET/IB/1
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Channel 03/0 : 28[3000] -> 13[44000] [receive] via NET/IB/1
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Channel 00 : 29[44000] -> 28[3000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 02/0 : 12[3000] -> 44[3000] [receive] via NET/IB/1
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 00/0 : 0[3000] -> 32[3000] [send] via NET/IB/1
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 01/0 : 0[3000] -> 32[3000] [send] via NET/IB/1
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Channel 00 : 33[44000] -> 32[3000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 01/0 : 0[3000] -> 32[3000] [receive] via NET/IB/1
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 02/0 : 36[3000] -> 32[3000] [send] via NET/IB/1
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Channel 03/0 : 36[3000] -> 32[3000] [send] via NET/IB/1
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Channel 01 : 17[44000] -> 16[3000] via P2P/IPC/read
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 02/0 : 28[3000] -> 20[3000] [send] via NET/IB/1
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 02/0 : 12[3000] -> 44[3000] [send] via NET/IB/1
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 00/0 : 28[3000] -> 24[3000] [receive] via NET/IB/1
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Channel 01 : 29[44000] -> 28[3000] via P2P/IPC/read
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 03/0 : 12[3000] -> 44[3000] [receive] via NET/IB/1
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Channel 02 : 17[44000] -> 16[3000] via P2P/IPC/read
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Channel 01 : 33[44000] -> 32[3000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 03/0 : 12[3000] -> 44[3000] [send] via NET/IB/1
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 03/0 : 28[3000] -> 20[3000] [send] via NET/IB/1
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Channel 00 : 41[44000] -> 40[3000] via P2P/IPC/read
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Channel 02 : 29[44000] -> 28[3000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 02/0 : 20[3000] -> 16[3000] [receive] via NET/IB/1
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Channel 03 : 17[44000] -> 16[3000] via P2P/IPC/read
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Channel 02 : 33[44000] -> 32[3000] via P2P/IPC/read
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 00/0 : 40[3000] -> 32[3000] [receive] via NET/IB/1
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 01/0 : 28[3000] -> 24[3000] [receive] via NET/IB/1
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Channel 03 : 29[44000] -> 28[3000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 02/0 : 4[3000] -> 0[3000] [receive] via NET/IB/1
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Channel 01 : 41[44000] -> 40[3000] via P2P/IPC/read
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Channel 03 : 33[44000] -> 32[3000] via P2P/IPC/read
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 00/0 : 28[3000] -> 24[3000] [send] via NET/IB/1
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Channel 03/0 : 20[3000] -> 16[3000] [receive] via NET/IB/1
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO Connected all trees
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 01/0 : 40[3000] -> 32[3000] [receive] via NET/IB/1
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Channel 02 : 41[44000] -> 40[3000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 02/0 : 12[3000] -> 4[3000] [send] via NET/IB/1
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 00/0 : 44[3000] -> 40[3000] [send] via NET/IB/1
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Channel 01/0 : 28[3000] -> 24[3000] [send] via NET/IB/1
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Channel 03/0 : 4[3000] -> 0[3000] [receive] via NET/IB/1
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO Connected all trees
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO Connected all trees
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 03/0 : 12[3000] -> 4[3000] [send] via NET/IB/1
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Channel 01/0 : 44[3000] -> 40[3000] [send] via NET/IB/1
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Channel 03 : 41[44000] -> 40[3000] via P2P/IPC/read
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Channel 00 : 13[44000] -> 12[3000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 00/0 : 25[44000] -> 20[3000] [receive] via NET/IB/1
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 00/0 : 44[3000] -> 40[3000] [receive] via NET/IB/1
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO Connected all trees
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 01/0 : 25[44000] -> 20[3000] [receive] via NET/IB/1
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Channel 01 : 13[44000] -> 12[3000] via P2P/IPC/read
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 00/0 : 12[3000] -> 8[3000] [send] via NET/IB/1
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 02/0 : 24[3000] -> 21[44000] [send] via NET/IB/1
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 02/0 : 36[3000] -> 32[3000] [receive] via NET/IB/1
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Channel 01/0 : 12[3000] -> 8[3000] [send] via NET/IB/1
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Channel 03/0 : 24[3000] -> 21[44000] [send] via NET/IB/1
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 00/0 : 9[44000] -> 4[3000] [receive] via NET/IB/1
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Channel 03/0 : 36[3000] -> 32[3000] [receive] via NET/IB/1
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 01/0 : 44[3000] -> 40[3000] [receive] via NET/IB/1
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Channel 02 : 13[44000] -> 12[3000] via P2P/IPC/read
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Channel 03 : 13[44000] -> 12[3000] via P2P/IPC/read
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 01/0 : 9[44000] -> 4[3000] [receive] via NET/IB/1
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 02/0 : 20[3000] -> 16[3000] [send] via NET/IB/1
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO Connected all trees
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO Connected all trees
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO Connected all trees
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 02/0 : 8[3000] -> 5[44000] [send] via NET/IB/1
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Channel 03/0 : 20[3000] -> 16[3000] [send] via NET/IB/1
[default1]:n2gpu1221:148944:149193 [1] NCCL INFO comm 0x14c7a00090d0 rank 29 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1221:148943:149192 [0] NCCL INFO comm 0x14f26c0090d0 rank 28 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Channel 03/0 : 8[3000] -> 5[44000] [send] via NET/IB/1
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 02/0 : 40[3000] -> 37[44000] [send] via NET/IB/1
[default2]:n2gpu1221:148945:149191 [2] NCCL INFO comm 0x152f240090d0 rank 30 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1221:148946:149190 [3] NCCL INFO comm 0x14c2c80090d0 rank 31 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Channel 03/0 : 40[3000] -> 37[44000] [send] via NET/IB/1
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 02/0 : 4[3000] -> 0[3000] [send] via NET/IB/1
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Channel 00 : 25[44000] -> 24[3000] via P2P/IPC/read
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Channel 03/0 : 4[3000] -> 0[3000] [send] via NET/IB/1
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Channel 00 : 21[44000] -> 20[3000] via P2P/IPC/read
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO Connected all trees
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO Connected all trees
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Channel 01 : 25[44000] -> 24[3000] via P2P/IPC/read
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO Connected all trees
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO Connected all trees
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Channel 01 : 21[44000] -> 20[3000] via P2P/IPC/read
[default3]:n2gpu1206:131663:131899 [3] NCCL INFO comm 0x14d62c0090d0 rank 15 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1206:131661:131900 [1] NCCL INFO comm 0x1533740090d0 rank 13 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1206:131662:131898 [2] NCCL INFO comm 0x1485780090d0 rank 14 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1206:131660:131901 [0] NCCL INFO comm 0x1476380090d0 rank 12 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Channel 02 : 25[44000] -> 24[3000] via P2P/IPC/read
[default3]:n2gpu1230:115875:116113 [3] NCCL INFO comm 0x14f6d80090d0 rank 47 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1230:115874:116115 [2] NCCL INFO comm 0x1530b00090d0 rank 46 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1230:115873:116116 [1] NCCL INFO comm 0x1547f00090d0 rank 45 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1230:115872:116114 [0] NCCL INFO comm 0x14bdf80090d0 rank 44 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO Connected all trees
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO Connected all trees
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1222:136920:137161 [2] NCCL INFO comm 0x14c82c0090d0 rank 34 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1222:136918:137162 [0] NCCL INFO comm 0x14d0080090d0 rank 32 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Channel 00 : 9[44000] -> 8[3000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Channel 02 : 21[44000] -> 20[3000] via P2P/IPC/read
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO Connected all trees
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO Connected all trees
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Channel 03 : 25[44000] -> 24[3000] via P2P/IPC/read
[default3]:n2gpu1222:136921:137160 [3] NCCL INFO comm 0x147a880090d0 rank 35 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1222:136919:137163 [1] NCCL INFO comm 0x150d440090d0 rank 33 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO Connected all trees
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO Connected all trees
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1207:132837:133077 [0] NCCL INFO comm 0x1491840090d0 rank 16 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default3]:n2gpu1207:132840:133078 [3] NCCL INFO comm 0x151ffc0090d0 rank 19 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1207:132839:133076 [2] NCCL INFO comm 0x15277c0090d0 rank 18 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1207:132838:133075 [1] NCCL INFO comm 0x149d100090d0 rank 17 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Channel 03 : 21[44000] -> 20[3000] via P2P/IPC/read
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Channel 01 : 9[44000] -> 8[3000] via P2P/IPC/read
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO Connected all trees
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Channel 00 : 5[44000] -> 4[3000] via P2P/IPC/read
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO Connected all trees
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO Connected all trees
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Channel 02 : 9[44000] -> 8[3000] via P2P/IPC/read
[default3]:n2gpu1219:255860:256115 [3] NCCL INFO comm 0x1545ac0090d0 rank 27 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1219:255858:256114 [1] NCCL INFO comm 0x1522900090d0 rank 25 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1219:255857:256113 [0] NCCL INFO comm 0x153dc80090d0 rank 24 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO Connected all trees
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO Connected all trees
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1227:180443:180678 [1] NCCL INFO comm 0x15044c0090d0 rank 41 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1219:255859:256116 [2] NCCL INFO comm 0x149b680090d0 rank 26 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Channel 00 : 37[44000] -> 36[3000] via P2P/IPC/read
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Channel 01 : 5[44000] -> 4[3000] via P2P/IPC/read
[default1]:n2gpu1210:132253:132487 [1] NCCL INFO comm 0x153fe00090d0 rank 21 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO Connected all trees
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1227:180444:180680 [2] NCCL INFO comm 0x1541480090d0 rank 42 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1227:180442:180677 [0] NCCL INFO comm 0x1509140090d0 rank 40 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default3]:n2gpu1227:180445:180679 [3] NCCL INFO comm 0x145c440090d0 rank 43 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Channel 03 : 9[44000] -> 8[3000] via P2P/IPC/read
[default3]:n2gpu1210:132255:132489 [3] NCCL INFO comm 0x1544540090d0 rank 23 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO Connected all trees
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1210:132254:132486 [2] NCCL INFO comm 0x1483b00090d0 rank 22 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Channel 01 : 37[44000] -> 36[3000] via P2P/IPC/read
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO Connected all trees
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Channel 02 : 5[44000] -> 4[3000] via P2P/IPC/read
[default0]:n2gpu1201:920020:920945 [0] NCCL INFO comm 0x14b05c0090d0 rank 0 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO Connected all trees
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO Connected all trees
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO Connected all trees
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1201:920021:920947 [1] NCCL INFO comm 0x1495d80090d0 rank 1 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1201:920022:920948 [2] NCCL INFO comm 0x14ab380090d0 rank 2 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1201:920023:920946 [3] NCCL INFO comm 0x14daf80090d0 rank 3 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1201:920020:920020 [0] NCCL INFO Launch mode Parallel
[default1]:n2gpu1205:131571:131810 [1] NCCL INFO comm 0x1463980090d0 rank 9 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default3]:n2gpu1205:131573:131809 [3] NCCL INFO comm 0x151db40090d0 rank 11 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1205:131572:131812 [2] NCCL INFO comm 0x14a3680090d0 rank 10 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Channel 02 : 37[44000] -> 36[3000] via P2P/IPC/read
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Channel 03 : 37[44000] -> 36[3000] via P2P/IPC/read
[default0]:n2gpu1210:132252:132488 [0] NCCL INFO comm 0x148e540090d0 rank 20 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1205:131570:131811 [0] NCCL INFO comm 0x1522a40090d0 rank 8 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Channel 03 : 5[44000] -> 4[3000] via P2P/IPC/read
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO Connected all trees
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO Connected all trees
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO Connected all trees
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO Connected all trees
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO Connected all trees
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1224:114173:114406 [0] NCCL INFO comm 0x1548780090d0 rank 36 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1224:114174:114407 [1] NCCL INFO comm 0x14aec80090d0 rank 37 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default3]:n2gpu1224:114176:114409 [3] NCCL INFO comm 0x1450700090d0 rank 39 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1224:114175:114408 [2] NCCL INFO comm 0x1488140090d0 rank 38 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO Connected all trees
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO threadThresholds 8/8/64 | 384/8/64 | 8/8/512
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO 4 coll channels, 4 p2p channels, 2 p2p channels per peer
[default3]:n2gpu1202:1117361:1117607 [3] NCCL INFO comm 0x15295c0090d0 rank 7 nranks 48 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1202:1117359:1117605 [1] NCCL INFO comm 0x14d83c0090d0 rank 5 nranks 48 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1202:1117358:1117608 [0] NCCL INFO comm 0x14c5ac0090d0 rank 4 nranks 48 cudaDev 0 busId 3000 - Init COMPLETE
[default2]:n2gpu1202:1117360:1117606 [2] NCCL INFO comm 0x145a500090d0 rank 6 nranks 48 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:[2023-04-24 00:25:13,319] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[default0]:[2023-04-24 00:25:13,353] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[default0]:[2023-04-24 00:25:13,353] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[default0]:[2023-04-24 00:25:13,484] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[default0]:[2023-04-24 00:25:13,484] [INFO] [utils.py:51:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[default0]:[2023-04-24 00:25:13,485] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[default0]:[2023-04-24 00:25:13,485] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500000000
[default0]:[2023-04-24 00:25:13,485] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500000000
[default0]:[2023-04-24 00:25:13,485] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[default0]:[2023-04-24 00:25:13,485] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Emitting ninja build file /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117/utils/build.ninja...
[default0]:Building extension module utils...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.20836615562438965 seconds
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Emitting ninja build file /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117/utils/build.ninja...
[default3]:Building extension module utils...
[default3]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.24960637092590332 seconds
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.2584831714630127 seconds
[default3]:ninja: no work to do.
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.19646048545837402 seconds
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.2559468746185303 seconds
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Emitting ninja build file /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117/utils/build.ninja...
[default3]:Building extension module utils...
[default3]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default3]:ninja: no work to do.
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.1951463222503662 seconds
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.2168734073638916 seconds
[default3]:Emitting ninja build file /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117/utils/build.ninja...
[default3]:Building extension module utils...
[default3]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default3]:ninja: no work to do.
[default3]:Loading extension module utils...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.23331999778747559 seconds
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.2267920970916748 seconds
[default3]:Time to load utils op: 0.2515394687652588 seconds
[default3]:Loading extension module utils...
[default2]:Emitting ninja build file /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117/utils/build.ninja...
[default2]:Building extension module utils...
[default2]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default2]:ninja: no work to do.
[default2]:Loading extension module utils...
[default1]:Loading extension module utils...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 1.5937635898590088 seconds
[default1]:Time to load utils op: 1.582495927810669 seconds
[default2]:Time to load utils op: 1.6220595836639404 seconds
[default3]:Time to load utils op: 1.685729742050171 seconds
[default2]:Loading extension module utils...
[default0]:Loading extension module utils...
[default3]:Loading extension module utils...
[default1]:Loading extension module utils...
[default3]:Time to load utils op: 3.178257703781128 seconds
[default1]:Loading extension module utils...
[default2]:Loading extension module utils...
[default3]:Loading extension module utils...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 3.05932354927063 seconds
[default2]:Loading extension module utils...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 3.062838554382324 seconds
[default0]:Loading extension module utils...
[default0]:Loading extension module utils...
[default2]:Time to load utils op: 3.1873223781585693 seconds
[default3]:Loading extension module utils...
[default0]:Time to load utils op: 3.196756601333618 seconds
[default1]:Time to load utils op: 3.196115255355835 seconds
[default2]:Time to load utils op: 3.066753387451172 seconds
[default0]:Time to load utils op: 3.0789668560028076 seconds
[default1]:Time to load utils op: 3.368056535720825 seconds
[default2]:Time to load utils op: 3.353590965270996 seconds
[default3]:Time to load utils op: 3.3482937812805176 seconds
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 3.123873710632324 seconds
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 3.1127302646636963 seconds
[default0]:Time to load utils op: 3.114975690841675 seconds
[default3]:Time to load utils op: 3.078294038772583 seconds
[default2]:Loading extension module utils...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 3.0636045932769775 seconds
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 3.083354949951172 seconds
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 3.0530028343200684 seconds
[default2]:Time to load utils op: 3.0551278591156006 seconds
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 3.2634451389312744 seconds
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 3.2843151092529297 seconds
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 3.263381242752075 seconds
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 3.2605738639831543 seconds
[default2]:Loading extension module utils...
[default1]:Loading extension module utils...
[default0]:Loading extension module utils...
[default3]:Loading extension module utils...
[default2]:Loading extension module utils...
[default1]:Loading extension module utils...
[default3]:Loading extension module utils...
[default0]:Loading extension module utils...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 3.4312331676483154 seconds
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 3.3867785930633545 seconds
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 3.418843984603882 seconds
[default2]:Time to load utils op: 3.071233034133911 seconds
[default1]:Time to load utils op: 3.0507168769836426 seconds
[default0]:Time to load utils op: 3.0635509490966797 seconds
[default3]:Time to load utils op: 3.0287716388702393 seconds
[default3]:Time to load utils op: 3.3456735610961914 seconds
[default2]:Time to load utils op: 3.360257148742676 seconds
[default0]:Time to load utils op: 3.3747177124023438 seconds
[default1]:Time to load utils op: 3.406581401824951 seconds
[default3]:Rank: 23 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default0]:Rank: 0 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default3]:Rank: 11 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default0]:Rank: 8 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default1]:Rank: 9 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default2]:Rank: 34 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default0]:Rank: 32 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default3]:Rank: 35 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default2]:Rank: 10 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default1]:Rank: 33 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default3]:Rank: 43 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default0]:Rank: 40 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default2]:Rank: 42 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default1]:Rank: 41 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default2]:Rank: 6 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default3]:Rank: 19 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default0]:Rank: 12 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default2]:Rank: 18 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default1]:Rank: 13 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default3]:Rank: 47 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default1]:Rank: 5 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default2]:Rank: 14 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default2]:Rank: 46 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default3]:Rank: 7 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default3]:Rank: 39 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default0]:Rank: 16 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default3]:Rank: 27 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default0]:Rank: 4 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default1]:Rank: 17 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default0]:Rank: 44 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default1]:Rank: 25 partition count [48, 48] and sizes[(214095190, False), (55638, False)] [default1]:
[default2]:Rank: 26 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default0]:Rank: 36 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default1]:Rank: 45 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default0]:Rank: 24 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default0]:Rank: 20 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default1]:Rank: 21 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default1]:Rank: 37 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default2]:Rank: 38 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default2]:Rank: 22 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default3]:Rank: 31 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default0]:Rank: 28 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default2]:Rank: 30 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default1]:Rank: 29 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default3]:Rank: 3 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default2]:Rank: 2 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default1]:Rank: 1 partition count [48, 48] and sizes[(214095190, False), (55638, False)] 
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.032770633697509766 seconds
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.03573727607727051 seconds
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.018512725830078125 seconds
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.011983871459960938 seconds
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.029701709747314453 seconds
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.04765486717224121 seconds
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.03799319267272949 seconds
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.030980348587036133 seconds
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.03840351104736328 seconds
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.006922721862792969 seconds
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.04247760772705078 seconds
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.08784317970275879 seconds
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.10600900650024414 seconds
[default0]:[2023-04-24 00:26:18,429] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[default0]:[2023-04-24 00:26:18,430] [INFO] [utils.py:786:see_memory_usage] MA 19.95 GB         Max_MA 19.95 GB         CA 19.96 GB         Max_CA 20 GB 
[default0]:[2023-04-24 00:26:18,430] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.72 GB, percent = 3.3%
[default0]:[2023-04-24 00:26:18,463] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[default0]:[2023-04-24 00:26:18,464] [INFO] [utils.py:786:see_memory_usage] MA 21.54 GB         Max_MA 22.34 GB         CA 22.36 GB         Max_CA 22 GB 
[default0]:[2023-04-24 00:26:18,464] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.72 GB, percent = 3.3%
[default0]:[2023-04-24 00:26:18,464] [INFO] [stage_1_and_2.py:489:__init__] optimizer state initialized
[default0]:[2023-04-24 00:26:18,486] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[default0]:[2023-04-24 00:26:18,487] [INFO] [utils.py:786:see_memory_usage] MA 21.54 GB         Max_MA 21.54 GB         CA 22.36 GB         Max_CA 22 GB 
[default0]:[2023-04-24 00:26:18,487] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 16.72 GB, percent = 3.3%
[default0]:[2023-04-24 00:26:18,488] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[default0]:[2023-04-24 00:26:18,488] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[default0]:[2023-04-24 00:26:18,488] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.learning_rates.AnnealingLR object at 0x14b5b79fdab0>
[default0]:[2023-04-24 00:26:18,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-04-24 00:26:18,489] [INFO] [config.py:953:print] DeepSpeedEngine configuration:
[default0]:[2023-04-24 00:26:18,489] [INFO] [config.py:957:print]   activation_checkpointing_config  {
[default0]:    "partition_activations": false, 
[default0]:    "contiguous_memory_optimization": false, 
[default0]:    "cpu_checkpointing": false, 
[default0]:    "number_checkpoints": null, 
[default0]:    "synchronize_checkpoint_boundary": false, 
[default0]:    "profile": false
[default0]:}
[default0]:[2023-04-24 00:26:18,489] [INFO] [config.py:957:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[default0]:[2023-04-24 00:26:18,489] [INFO] [config.py:957:print]   amp_enabled .................. False
[default0]:[2023-04-24 00:26:18,489] [INFO] [config.py:957:print]   amp_params ................... False
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   autotuning_config ............ {
[default0]:    "enabled": false, 
[default0]:    "start_step": null, 
[default0]:    "end_step": null, 
[default0]:    "metric_path": null, 
[default0]:    "arg_mappings": null, 
[default0]:    "metric": "throughput", 
[default0]:    "model_info": null, 
[default0]:    "results_dir": "autotuning_results", 
[default0]:    "exps_dir": "autotuning_exps", 
[default0]:    "overwrite": true, 
[default0]:    "fast": true, 
[default0]:    "start_profile_step": 3, 
[default0]:    "end_profile_step": 5, 
[default0]:    "tuner_type": "gridsearch", 
[default0]:    "tuner_early_stopping": 5, 
[default0]:    "tuner_num_trials": 50, 
[default0]:    "model_info_path": null, 
[default0]:    "mp_size": 1, 
[default0]:    "max_train_batch_size": null, 
[default0]:    "min_train_batch_size": 1, 
[default0]:    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[default0]:    "min_train_micro_batch_size_per_gpu": 1, 
[default0]:    "num_tuning_micro_batch_sizes": 3
[default0]:}
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   bfloat16_enabled ............. False
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   checkpoint_parallel_write_pipeline  False
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   checkpoint_tag_validation_enabled  True
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   checkpoint_tag_validation_fail  False
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14b5b70d4d00>
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   communication_data_type ...... None
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   curriculum_enabled_legacy .... False
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   curriculum_params_legacy ..... False
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   data_efficiency_enabled ...... False
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   dataloader_drop_last ......... False
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   disable_allgather ............ False
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   dump_state ................... False
[default0]:[2023-04-24 00:26:18,490] [INFO] [config.py:957:print]   dynamic_loss_scale_args ...... {'init_scale': 4096, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   eigenvalue_enabled ........... False
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   eigenvalue_gas_boundary_resolution  1
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   eigenvalue_layer_name ........ bert.encoder.layer
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   eigenvalue_layer_num ......... 0
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   eigenvalue_max_iter .......... 100
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   eigenvalue_stability ......... 1e-06
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   eigenvalue_tol ............... 0.01
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   eigenvalue_verbose ........... False
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   elasticity_enabled ........... False
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   flops_profiler_config ........ {
[default0]:    "enabled": false, 
[default0]:    "profile_step": 1, 
[default0]:    "module_depth": -1, 
[default0]:    "top_modules": 1, 
[default0]:    "detailed": true, 
[default0]:    "output_file": null
[default0]:}
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   fp16_auto_cast ............... False
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   fp16_enabled ................. True
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   fp16_master_weights_and_gradients  False
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   global_rank .................. 0
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   grad_accum_dtype ............. None
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   gradient_accumulation_steps .. 24
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   gradient_clipping ............ 1
[default0]:[2023-04-24 00:26:18,491] [INFO] [config.py:957:print]   gradient_predivide_factor .... 1.0
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.03754901885986328 seconds
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.019269466400146484 seconds
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.0366058349609375 seconds
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.03383922576904297 seconds
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.0262753963470459 seconds
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.02940964698791504 seconds
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.06126046180725098 seconds
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.02633523941040039 seconds
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.033597469329833984 seconds
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.029541969299316406 seconds
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.03781533241271973 seconds
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.03624391555786133 seconds
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.03327155113220215 seconds
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.03753042221069336 seconds
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.03512692451477051 seconds
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.046144723892211914 seconds
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.01878666877746582 seconds
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.011163473129272461 seconds
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.028728961944580078 seconds
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.03832530975341797 seconds
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.03282308578491211 seconds
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.02562713623046875 seconds
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.0651402473449707 seconds
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.06347274780273438 seconds
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.12530970573425293 seconds
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.05692768096923828 seconds
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.06365203857421875 seconds
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.029398441314697266 seconds
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.032559871673583984 seconds
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.0322873592376709 seconds
[default2]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default2]:No modifications detected for re-loaded extension module utils, skipping build step...
[default2]:Loading extension module utils...
[default2]:Time to load utils op: 0.050417423248291016 seconds
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.06721186637878418 seconds
[default3]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default3]:No modifications detected for re-loaded extension module utils, skipping build step...
[default3]:Loading extension module utils...
[default3]:Time to load utils op: 0.007532835006713867 seconds
[default0]:[2023-04-24 00:26:18,684] [INFO] [config.py:957:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[default0]:[2023-04-24 00:26:18,684] [INFO] [config.py:957:print]   initial_dynamic_scale ........ 4096
[default0]:[2023-04-24 00:26:18,684] [INFO] [config.py:957:print]   load_universal_checkpoint .... False
[default0]:[2023-04-24 00:26:18,684] [INFO] [config.py:957:print]   loss_scale ................... 0
[default0]:[2023-04-24 00:26:18,684] [INFO] [config.py:957:print]   memory_breakdown ............. False
[default0]:[2023-04-24 00:26:18,684] [INFO] [config.py:957:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[default0]:[2023-04-24 00:26:18,684] [INFO] [config.py:957:print]   nebula_config ................ {
[default0]:    "enabled": false, 
[default0]:    "persistent_storage_path": null, 
[default0]:    "persistent_time_interval": 100, 
[default0]:    "num_of_version_in_retention": 2, 
[default0]:    "enable_nebula_load": true, 
[default0]:    "load_path": null
[default0]:}
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   optimizer_legacy_fusion ...... False
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   optimizer_name ............... None
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   optimizer_params ............. None
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   pld_enabled .................. False
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   pld_params ................... False
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   prescale_gradients ........... False
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   scheduler_name ............... None
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   scheduler_params ............. None
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   sparse_attention ............. None
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   sparse_gradients_enabled ..... False
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   steps_per_print .............. 2000
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   train_batch_size ............. 1152
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   train_micro_batch_size_per_gpu  1
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   use_node_local_storage ....... False
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   wall_clock_breakdown ......... False
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   world_size ................... 48
[default0]:[2023-04-24 00:26:18,685] [INFO] [config.py:957:print]   zero_allow_untested_optimizer  False
[default0]:[2023-04-24 00:26:18,686] [INFO] [config.py:957:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False memory_efficient_linear=True
[default0]:[2023-04-24 00:26:18,686] [INFO] [config.py:957:print]   zero_enabled ................. True
[default0]:[2023-04-24 00:26:18,686] [INFO] [config.py:957:print]   zero_force_ds_cpu_optimizer .. True
[default0]:[2023-04-24 00:26:18,686] [INFO] [config.py:957:print]   zero_optimization_stage ...... 2
[default0]:[2023-04-24 00:26:18,686] [INFO] [config.py:943:print_user_config]   json = {
[default0]:    "train_micro_batch_size_per_gpu": 1, 
[default0]:    "train_batch_size": 1.152000e+03, 
[default0]:    "gradient_clipping": 1, 
[default0]:    "zero_optimization": {
[default0]:        "stage": 2, 
[default0]:        "contiguous_gradients": true, 
[default0]:        "overlap_comm": true, 
[default0]:        "reduce_scatter": true, 
[default0]:        "reduce_bucket_size": 5.000000e+08, 
[default0]:        "allgather_bucket_size": 5.000000e+08
[default0]:    }, 
[default0]:    "fp16": {
[default0]:        "enabled": true, 
[default0]:        "loss_scale": 0, 
[default0]:        "loss_scale_window": 500, 
[default0]:        "hysteresis": 2, 
[default0]:        "min_loss_scale": 1, 
[default0]:        "initial_scale_power": 12
[default0]:    }, 
[default0]:    "steps_per_print": 2.000000e+03, 
[default0]:    "wall_clock_breakdown": false
[default0]:}
[default0]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default0]:No modifications detected for re-loaded extension module utils, skipping build step...
[default0]:Loading extension module utils...
[default0]:Time to load utils op: 0.02229619026184082 seconds
[default1]:Using /pc2/users/n/nikit/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...
[default1]:No modifications detected for re-loaded extension module utils, skipping build step...
[default1]:Loading extension module utils...
[default1]:Time to load utils op: 0.3392782211303711 seconds
[default2]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2023-04-24 00:26:18,849] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-24 00:26:18,848] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2023-04-24 00:26:18,851] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2023-04-24 00:26:18,854] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2023-04-24 00:26:18,874] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-24 00:26:18,857] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-24 00:26:18,864] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2023-04-24 00:26:18,843] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2023-04-24 00:26:18,876] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2023-04-24 00:26:18,854] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-24 00:26:18,887] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2023-04-24 00:26:18,862] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-24 00:26:18,853] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:WARNING: could not find the metadata file checkpoints/gpt2-10b-dist 
[default0]:    will not load any checkpoints and will start from random
[default1]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2023-04-24 00:26:18,899] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2023-04-24 00:26:18,866] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2023-04-24 00:26:18,850] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2023-04-24 00:26:18,888] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-24 00:26:18,886] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2023-04-24 00:26:18,875] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-24 00:26:18,854] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2023-04-24 00:26:18,876] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2023-04-24 00:26:18,855] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2023-04-24 00:26:18,841] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2023-04-24 00:26:18,883] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2023-04-24 00:26:18,862] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2023-04-24 00:26:18,891] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2023-04-24 00:26:18,869] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2023-04-24 00:26:18,886] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-24 00:26:18,851] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2023-04-24 00:26:18,874] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default2]:[2023-04-24 00:26:18,917] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2023-04-24 00:26:18,888] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[2023-04-24 00:26:18,866] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2023-04-24 00:26:18,881] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2023-04-24 00:26:18,905] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:[2023-04-24 00:26:18,833] [WARNING] [engine.py:2535:load_checkpoint] Unable to find latest file at checkpoints/gpt2-10b-dist/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default3]:time (ms) | load-checkpoint: 347.96
[default0]:[after model, optimizer, and learning rate scheduler are built] datetime: 2023-04-24 00:26:19 
[default0]:> building train, validation, and test datasets ...
[default0]: > datasets target sizes (minimum size):
[default0]:    train:      1000000
[default0]:    validation: 207360
[default0]:    test:       11520
[default0]:> building train, validation, and test datasets for GPT ...
[default0]: > building dataset index ...
[default0]:    reading sizes...
[default0]:    reading pointers...
[default0]:    reading document index...
[default0]:    creating numpy buffer of mmap...
[default0]:    creating memory view of numpy buffer...
[default0]: > finished creating indexed dataset in 0.145948 seconds
[default0]:    number of documents: 10000
[default0]: > dataset split:
[default0]:    train:
[default0]:     document indices in [0, 9690) total of 9690 documents
[default0]:    validation:
[default0]:     document indices in [9690, 9990) total of 300 documents
[default0]:    test:
[default0]:     document indices in [9990, 10000) total of 10 documents
[default0]: > WARNING: could not find index map files, building the indices on rank 0 ...
[default0]: > last epoch number of samples (14141) is smaller than 80% of number of samples per epoch (28167), setting separate_last_epoch to True
[default0]: > elasped time to build and save doc-idx mapping (seconds): 0.072824
[default0]:    using:
[default0]:     number of documents:       9690
[default0]:     number of epochs:          36
[default0]:     sequence length:           1024
[default0]:     total number of samples:   1014026
[default0]: > elasped time to build and save sample-idx mapping (seconds): 0.080662
[default0]: > building shuffle index with split [0, 985859) and [985859, 1014026) ...
[default3]:NCCL version 2.12.12+cuda11.7
[default2]:NCCL version 2.12.12+cuda11.7
[default0]: > elasped time to build and save shuffle-idx mapping (seconds): 0.106184
[default1]:NCCL version 2.12.12+cuda11.7
[default3]:NCCL version 2.12.12+cuda11.7
[default2]:NCCL version 2.12.12+cuda11.7
[default0]:NCCL version 2.12.12+cuda11.7[default1]:NCCL version 2.12.12+cuda11.7
[default2]:NCCL version 2.12.12+cuda11.7
[default3]:NCCL version 2.12.12+cuda11.7
[default1]:NCCL version 2.12.12+cuda11.7
[default0]:NCCL version 2.12.12+cuda11.7
[default2]:NCCL version 2.12.12+cuda11.7
[default0]:NCCL version 2.12.12+cuda11.7
[default1]:NCCL version 2.12.12+cuda11.7
[default3]:NCCL version 2.12.12+cuda11.7
[default1]:NCCL version 2.12.12+cuda11.7
[default0]:NCCL version 2.12.12+cuda11.7
[default2]:NCCL version 2.12.12+cuda11.7
[default3]:NCCL version 2.12.12+cuda11.7
[default1]:NCCL version 2.12.12+cuda11.7
[default2]:NCCL version 2.12.12+cuda11.7
[default0]:NCCL version 2.12.12+cuda11.7
[default2]:NCCL version 2.12.12+cuda11.7
[default2]:NCCL version 2.12.12+cuda11.7
[default3]:NCCL version 2.12.12+cuda11.7
[default0]:NCCL version 2.12.12+cuda11.7
[default1]:NCCL version 2.12.12+cuda11.7
[default0]:NCCL version 2.12.12+cuda11.7
[default0]:NCCL version 2.12.12+cuda11.7
[default3]:NCCL version 2.12.12+cuda11.7
[default1]:NCCL version 2.12.12+cuda11.7
[default3]:NCCL version 2.12.12+cuda11.7
[default0]:NCCL version 2.12.12+cuda11.7
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 13/32 :    0
[default3]:NCCL version 2.12.12+cuda11.7
[default1]:NCCL version 2.12.12+cuda11.7
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:NCCL version 2.12.12+cuda11.7
[default3]:NCCL version 2.12.12+cuda11.7
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:NCCL version 2.12.12+cuda11.7
[default3]:NCCL version 2.12.12+cuda11.7
[default1]:NCCL version 2.12.12+cuda11.7
[default2]:NCCL version 2.12.12+cuda11.7
[default0]:NCCL version 2.12.12+cuda11.7
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Connected all rings
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO Connected all trees
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 10/32 :    0
[default1]:NCCL version 2.12.12+cuda11.7
[default2]:NCCL version 2.12.12+cuda11.7
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:NCCL version 2.12.12+cuda11.7
[default0]:NCCL version 2.12.12+cuda11.7
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Connected all rings
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO Connected all trees
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1222:136919:137196 [1] NCCL INFO comm 0x1511f00090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Connected all rings
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO Connected all trees
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Connected all rings
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO Connected all trees
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Connected all rings
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO Connected all trees
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Connected all rings
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO Connected all trees
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Connected all rings
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO Connected all trees
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Connected all rings
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO Connected all trees
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Connected all rings
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO Connected all trees
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1207:132840:133103 [3] NCCL INFO comm 0x1524a00090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Connected all rings
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO Connected all trees
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Connected all rings
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO Connected all trees
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Connected all rings
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO Connected all trees
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:NCCL version 2.12.12+cuda11.7
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Connected all rings
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO Connected all trees
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1205:131572:131849 [2] NCCL INFO comm 0x14a80c0090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Connected all rings
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO Connected all trees
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Connected all rings
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO Connected all trees
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Connected all rings
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO Connected all trees
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Connected all rings
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO Connected all trees
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1222:136920:137198 [2] NCCL INFO comm 0x14ccd40090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Connected all rings
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO Connected all trees
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1222:136921:137202 [3] NCCL INFO comm 0x147f300090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1222:136918:137200 [0] NCCL INFO comm 0x14d4b80090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Connected all rings
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO Connected all trees
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Connected all rings
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO Connected all trees
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Connected all rings
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO Connected all trees
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1201:920020:921229 [0] NCCL INFO comm 0x14b5080090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default0]: > loading doc-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_train_indexmap_1000000ns_1024sl_42s_doc_idx.npy
[default1]:n2gpu1201:920021:921235 [1] NCCL INFO comm 0x149a7c0090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1206:131661:131930 [1] NCCL INFO comm 0x1538180090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1206:131662:131926 [2] NCCL INFO comm 0x148a200090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Connected all rings
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO Connected all trees
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1227:180444:180721 [2] NCCL INFO comm 0x1545f00090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Connected all rings
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO Connected all trees
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1227:180445:180719 [3] NCCL INFO comm 0x1460ec0090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Connected all rings
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO Connected all trees
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Connected all rings
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO Connected all trees
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Connected all rings
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO Connected all trees
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Connected all rings
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO Connected all trees
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Connected all rings
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO Connected all trees
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1227:180442:180717 [0] NCCL INFO comm 0x150dc40090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1227:180443:180715 [1] NCCL INFO comm 0x1508f80090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Connected all rings
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO Connected all trees
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Connected all rings
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO Connected all trees
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1207:132839:133107 [2] NCCL INFO comm 0x152c200090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1207:132837:133109 [0] NCCL INFO comm 0x1496340090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1207:132838:133105 [1] NCCL INFO comm 0x14a1bc0090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Connected all rings
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO Connected all trees
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Connected all rings
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO Connected all trees
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Connected all rings
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO Connected all trees
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Connected all rings
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO Connected all trees
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Connected all rings
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO Connected all trees
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1205:131570:131851 [0] NCCL INFO comm 0x1527540090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1205:131571:131845 [1] NCCL INFO comm 0x1468440090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default3]:n2gpu1205:131573:131847 [3] NCCL INFO comm 0x1522580090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Connected all rings
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO Connected all trees
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1206:131660:131932 [0] NCCL INFO comm 0x147ae80090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Connected all rings
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO Connected all trees
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1230:115875:116152 [3] NCCL INFO comm 0x14fb800090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1230:115872:116146 [0] NCCL INFO comm 0x14c2a00090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default2]:n2gpu1230:115874:116148 [2] NCCL INFO comm 0x1535540090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Connected all rings
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO Connected all trees
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1230:115873:116150 [1] NCCL INFO comm 0x154c940090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1202:1117359:1117643 [1] NCCL INFO comm 0x14dce40090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1202:1117358:1117639 [0] NCCL INFO comm 0x14ca5c0090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default2]:n2gpu1202:1117360:1117645 [2] NCCL INFO comm 0x145ef40090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1202:1117361:1117641 [3] NCCL INFO comm 0x152e000090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1201:920022:921233 [2] NCCL INFO comm 0x14afe00090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1201:920023:921231 [3] NCCL INFO comm 0x14dfa00090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default0]: > loading sample-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_train_indexmap_1000000ns_1024sl_42s_sample_idx.npy
[default0]: > loading shuffle-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_train_indexmap_1000000ns_1024sl_42s_shuffle_idx.npy
[default0]:    loaded indexed file in 0.053 seconds
[default0]:    total number of samples: 1014027
[default0]:    total number of epochs: 36
[default0]: > WARNING: could not find index map files, building the indices on rank 0 ...
[default0]: > last epoch number of samples (576) is smaller than 80% of number of samples per epoch (914), setting separate_last_epoch to True
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Connected all rings
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO Connected all trees
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1219:255860:256155 [3] NCCL INFO comm 0x154a500090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1224:114175:114450 [2] NCCL INFO comm 0x148cb80090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Connected all rings
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO Connected all trees
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1206:131663:131928 [3] NCCL INFO comm 0x14dad00090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1219:255857:256149 [0] NCCL INFO comm 0x1542780090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1224:114173:114448 [0] NCCL INFO comm 0x154d280090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default3]:n2gpu1224:114176:114446 [3] NCCL INFO comm 0x1455140090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Connected all rings
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO Connected all trees
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1219:255858:256151 [1] NCCL INFO comm 0x15273c0090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Connected all rings
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO Connected all trees
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1219:255859:256153 [2] NCCL INFO comm 0x14a00c0090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1224:114174:114444 [1] NCCL INFO comm 0x14b36c0090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Connected all rings
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO Connected all trees
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1210:132254:132524 [2] NCCL INFO comm 0x1488540090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Connected all rings
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO Connected all trees
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Connected all rings
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO Connected all trees
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Connected all rings
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO Connected all trees
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1221:148945:149223 [2] NCCL INFO comm 0x1533c80090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Connected all rings
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO Connected all trees
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]: > elasped time to build and save doc-idx mapping (seconds): 0.051304
[default0]:    using:
[default0]:     number of documents:       300
[default0]:     number of epochs:          227
[default0]:     sequence length:           1024
[default0]:     total number of samples:   207699
[default0]: > elasped time to build and save sample-idx mapping (seconds): 0.056019
[default0]: > building shuffle index with split [0, 206784) and [206784, 207699) ...
[default0]: > elasped time to build and save shuffle-idx mapping (seconds): 0.006612
[default3]:n2gpu1210:132255:132528 [3] NCCL INFO comm 0x1548f80090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Connected all rings
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO Connected all trees
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1221:148946:149227 [3] NCCL INFO comm 0x14c7700090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1221:148944:149229 [1] NCCL INFO comm 0x14cc480090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1221:148943:149225 [0] NCCL INFO comm 0x14f71c0090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1210:132253:132526 [1] NCCL INFO comm 0x1544840090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1210:132252:132522 [0] NCCL INFO comm 0x1493040090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default0]: > loading doc-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_valid_indexmap_207360ns_1024sl_42s_doc_idx.npy
[default0]: > loading sample-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_valid_indexmap_207360ns_1024sl_42s_sample_idx.npy
[default0]: > loading shuffle-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_valid_indexmap_207360ns_1024sl_42s_shuffle_idx.npy
[default0]:    loaded indexed file in 0.033 seconds
[default0]:    total number of samples: 207700
[default0]:    total number of epochs: 227
[default0]: > loading doc-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_test_indexmap_11520ns_1024sl_42s_doc_idx.npy
[default0]: > loading sample-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_test_indexmap_11520ns_1024sl_42s_sample_idx.npy
[default0]: > loading shuffle-idx mapping from data/meg-gpt2-oscar-en-10k_text_document_test_indexmap_11520ns_1024sl_42s_shuffle_idx.npy
[default3]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default3]:  warnings.warn(_create_warning_msg(
[default1]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default1]:  warnings.warn(_create_warning_msg(
[default1]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default1]:  warnings.warn(_create_warning_msg(
[default3]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default3]:  warnings.warn(_create_warning_msg(
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default1]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default1]:  warnings.warn(_create_warning_msg(
[default3]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default3]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default3]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default3]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default1]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default1]:  warnings.warn(_create_warning_msg(
[default0]:    loaded indexed file in 0.845 seconds
[default0]:    total number of samples: 11536
[default0]:    total number of epochs: 363
[default0]:> finished creating GPT datasets ...
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default3]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default3]:  warnings.warn(_create_warning_msg(
[default1]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default1]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default3]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default3]:  warnings.warn(_create_warning_msg(
[default1]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default1]:  warnings.warn(_create_warning_msg(
[default1]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default1]:  warnings.warn(_create_warning_msg(
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default3]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default3]:  warnings.warn(_create_warning_msg(
[default1]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default1]:  warnings.warn(_create_warning_msg(
[default3]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default3]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default1]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default1]:  warnings.warn(_create_warning_msg(
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default3]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default3]:  warnings.warn(_create_warning_msg(
[default3]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default3]:  warnings.warn(_create_warning_msg(
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default1]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default1]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default3]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default3]:  warnings.warn(_create_warning_msg(
[default1]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default1]:  warnings.warn(_create_warning_msg(
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default3]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default3]:  warnings.warn(_create_warning_msg(
[default1]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default1]:  warnings.warn(_create_warning_msg(
[default0]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default0]:  warnings.warn(_create_warning_msg(
[default2]:/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 2 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
[default2]:  warnings.warn(_create_warning_msg(
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Connected all rings
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO Connected all trees
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Connected all rings
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO Connected all trees
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Connected all rings
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO Connected all trees
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Connected all rings
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO Connected all trees
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Connected all rings
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO Connected all trees
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Connected all rings
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO Connected all trees
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Connected all rings
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO Connected all trees
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Connected all rings
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO Connected all trees
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Connected all rings
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO Connected all trees
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Connected all rings
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO Connected all trees
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Connected all rings
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO Connected all trees
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Connected all rings
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO Connected all trees
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Connected all rings
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO Connected all trees
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Connected all rings
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO Connected all trees
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Connected all rings
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO Connected all trees
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Connected all rings
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO Connected all trees
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Connected all rings
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO Connected all trees
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Connected all rings
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO Connected all trees
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1224:114175:114460 [2] NCCL INFO comm 0x148cb00090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Connected all rings
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO Connected all trees
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Connected all rings
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO Connected all trees
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Connected all rings
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO Connected all trees
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Connected all rings
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO Connected all trees
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Connected all rings
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO Connected all trees
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1224:114176:114464 [3] NCCL INFO comm 0x14550c0090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Connected all rings
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO Connected all trees
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Connected all rings
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO Connected all trees
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1224:114174:114462 [1] NCCL INFO comm 0x14b3640090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Connected all rings
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO Connected all trees
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Connected all rings
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO Connected all trees
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Connected all rings
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO Connected all trees
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Connected all rings
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO Connected all trees
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1221:148946:149241 [3] NCCL INFO comm 0x14c7680090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Connected all rings
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO Connected all trees
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1221:148945:149243 [2] NCCL INFO comm 0x1533c00090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Connected all rings
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO Connected all trees
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Connected all rings
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO Connected all trees
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 00/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 01/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 02/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 03/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 04/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 05/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 06/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 07/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 08/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 09/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 10/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 11/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 12/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 13/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 14/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 15/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 16/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 17/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 18/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 19/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 20/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 21/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 22/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 23/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 24/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 25/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 26/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 27/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 28/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 29/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 30/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Channel 31/32 :    0
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Connected all rings
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO Connected all trees
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Connected all rings
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO Connected all trees
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Connected all rings
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO Connected all trees
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default3]:n2gpu1222:136921:137214 [3] NCCL INFO comm 0x147f280090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1222:136920:137216 [2] NCCL INFO comm 0x14cccc0090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Connected all rings
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO Connected all trees
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1222:136919:137212 [1] NCCL INFO comm 0x1511e80090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Connected all rings
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO Connected all trees
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1230:115872:116164 [0] NCCL INFO comm 0x14c2980090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Connected all rings
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO Connected all trees
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1230:115873:116162 [1] NCCL INFO comm 0x154c8c0090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1230:115874:116168 [2] NCCL INFO comm 0x15354c0090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1230:115875:116166 [3] NCCL INFO comm 0x14fb780090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Connected all rings
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO Connected all trees
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1205:131571:131861 [1] NCCL INFO comm 0x14683c0090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 00/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 01/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 02/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 03/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 04/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 05/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 06/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 07/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 08/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 09/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 10/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 11/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 12/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 13/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 14/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 15/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 16/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 17/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 18/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 19/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 20/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 21/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 22/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 23/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 24/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 25/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 26/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 27/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 28/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 29/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 30/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Channel 31/32 :    0
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Connected all rings
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO Connected all trees
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1205:131572:131865 [2] NCCL INFO comm 0x14a8040090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1205:131573:131863 [3] NCCL INFO comm 0x1522500090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1201:920021:921245 [1] NCCL INFO comm 0x149a740090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default3]:n2gpu1201:920023:921247 [3] NCCL INFO comm 0x14df980090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1201:920022:921249 [2] NCCL INFO comm 0x14afd80090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Connected all rings
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO Connected all trees
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1206:131661:131947 [1] NCCL INFO comm 0x1538100090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Connected all rings
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO Connected all trees
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1206:131662:131943 [2] NCCL INFO comm 0x148a180090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1219:255860:256165 [3] NCCL INFO comm 0x154a480090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default1]:n2gpu1202:1117359:1117655 [1] NCCL INFO comm 0x14dcdc0090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1202:1117360:1117657 [2] NCCL INFO comm 0x145eec0090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Connected all rings
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO Connected all trees
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1202:1117361:1117659 [3] NCCL INFO comm 0x152df80090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1202:1117358:1117661 [0] NCCL INFO comm 0x14ca580090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Connected all rings
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO Connected all trees
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1224:114173:114466 [0] NCCL INFO comm 0x154d240090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Connected all rings
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO Connected all trees
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default3]:n2gpu1206:131663:131945 [3] NCCL INFO comm 0x14dac80090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1227:180442:180733 [0] NCCL INFO comm 0x150dc00090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default3]:n2gpu1227:180445:180735 [3] NCCL INFO comm 0x1460e40090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1227:180444:180731 [2] NCCL INFO comm 0x1545e80090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default3]:n2gpu1210:132255:132542 [3] NCCL INFO comm 0x1548f00090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default0]:n2gpu1219:255857:256171 [0] NCCL INFO comm 0x1542740090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Connected all rings
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO Connected all trees
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default2]:n2gpu1219:255859:256169 [2] NCCL INFO comm 0x14a0040090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1219:255858:256167 [1] NCCL INFO comm 0x1527340090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Connected all rings
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO Connected all trees
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1210:132252:132544 [0] NCCL INFO comm 0x1493000090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1210:132253:132540 [1] NCCL INFO comm 0x15447c0090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default2]:n2gpu1210:132254:132538 [2] NCCL INFO comm 0x14884c0090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1227:180443:180737 [1] NCCL INFO comm 0x1508f00090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1207:132837:133125 [0] NCCL INFO comm 0x1496300090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default3]:n2gpu1207:132840:133121 [3] NCCL INFO comm 0x1524980090d0 rank 0 nranks 1 cudaDev 3 busId c4000 - Init COMPLETE
[default2]:n2gpu1207:132839:133123 [2] NCCL INFO comm 0x152c180090d0 rank 0 nranks 1 cudaDev 2 busId 84000 - Init COMPLETE
[default1]:n2gpu1207:132838:133119 [1] NCCL INFO comm 0x14a1b40090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1221:148944:149239 [1] NCCL INFO comm 0x14cc400090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default0]:n2gpu1221:148943:149245 [0] NCCL INFO comm 0x14f7180090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Connected all rings
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO Connected all trees
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1222:136918:137218 [0] NCCL INFO comm 0x14d4b40090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1205:131570:131867 [0] NCCL INFO comm 0x1527500090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1201:920020:921251 [0] NCCL INFO comm 0x14b5000090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1206:131660:131949 [0] NCCL INFO comm 0x147ae40090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default3]:time (ms) | model-and-optimizer-setup: 132653.84 | train/valid/test-data-iterators-setup: 26299.28
[default0]:[after dataloaders are built] datetime: 2023-04-24 00:26:46 
[default0]:done with setup ...
[default0]:training ...
[default0]:[before the start of training step] datetime: 2023-04-24 00:26:46 
[default0]:[2023-04-24 00:26:46,133] [INFO] [checkpointing.py:529:forward] Activation Checkpointing Information
[default0]:[2023-04-24 00:26:46,133] [INFO] [checkpointing.py:530:forward] ----Partition Activations True, CPU CHECKPOINTING False
[default0]:[2023-04-24 00:26:46,133] [INFO] [checkpointing.py:531:forward] ----contiguous Memory Checkpointing False with 50 total layers
[default0]:[2023-04-24 00:26:46,133] [INFO] [checkpointing.py:533:forward] ----Synchronization False
[default0]:[2023-04-24 00:26:46,133] [INFO] [checkpointing.py:534:forward] ----Profiling time in checkpointing False
[default0]:[Rank 0] (after 2 iterations) memory (MB) | allocated: 22057.85888671875 | max allocated: 26142.87255859375 | reserved: 28290.0 | max reserved: 28290.0
[default3]: iteration        2/     868 | consumed samples:         2304 | consumed tokens:      2359296 | elapsed time per iteration (ms): 1753560.5 | learning rate: 1.000E-04 | global batch size:  1152 | lm loss: 1.738353E+01 | loss scale: 4096.0 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.657 | TFLOPs: 1.19 |
[default3]:time (ms) | forward-compute: 21962.26 | backward-compute: 1713342.34 | backward-embedding-all-reduce: 0.01 | optimizer: 18196.72 | batch-generator: 30.41
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 3411267 ON n2gpu1201 CANCELLED AT 2023-04-24T01:33:50 ***
