cpu-bind=MASK - n2gpu1212, task  0  0 [1379885]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
Total estimated parameters in the Dense GPT-2 model: 1315723264 (1.32B)
Total Estimated Parameters in the Sparse(MoE) GPT-2 model: 52871059456 (52.87B)
LAUNCHER: python -u -m torch.distributed.run --nproc_per_node 1 --nnodes 1 --rdzv_id=13399 --rdzv_endpoint n2gpu1212:6053 --rdzv_backend c10d --max_restarts 0 --tee 3
CMD: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/pretrain_gpt.py --override-opt_param-scheduler --adam-beta1 0.9 --adam-beta2 0.95 --tensor-model-parallel-size 1 --moe-expert-parallel-size 1 --num-experts 128 --moe-loss-coeff 0.01 --moe-train-capacity-factor 1.0 --moe-eval-capacity-factor 1.0 --moe-min-capacity 4 --init-method-std 0.01 --lr-decay-tokens 300000000000 --lr-warmup-tokens 375000000 --micro-batch-size 1 --exit-duration-in-mins 30000000 --global-batch-size 1 --num-layers 24 --hidden-size 2048 --num-attention-heads 16 --seq-length 2048 --max-position-embeddings 2048 --train-tokens 300000000000 --train-iters 439453125 --lr 2.0e-4 --min-lr 2e-06 --lr-decay-style cosine --split 98,2,0 --log-interval 5 --eval-interval 100 --eval-iters 50 --save-interval 1000000 --weight-decay 0.1 --clip-grad 1.0 --hysteresis 2 --num-workers 0 --fp16 --load /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_1.3B_MoE128_1BATCH_1GPU_1Node/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-1-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true --save /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_1.3B_MoE128_1BATCH_1GPU_1Node/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-1-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true --tensorboard-queue-size 1 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_1.3B_MoE128_1BATCH_1GPU_1Node/tensorboard/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-1-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true_n2gpu1212_2023.07.30-22.23.24 --checkpoint-activations --create-moe-param-group --vocab-file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-vocab.json --merge-file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-merges.txt --data-path /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/meg-gpt2-oscar-en-10k_text_document --data-impl mmap --deepspeed --deepspeed_config /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_1.3B_MoE128_1BATCH_1GPU_1Node/ds_config_gpt_gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-1-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true.json --pipeline-model-parallel-size 1 --no-pipeline-parallel --deepspeed-activation-checkpointing
cpu-bind=MASK - n2gpu1212, task  0  0 [1380122]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
[default0]:[2023-07-30 22:23:28,237] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.10.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:**** Git info for Megatron: git_hash=6e47d55 git_branch=main ****
[default0]:using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
[default0]:using torch.float16 for parameters ...
[default0]:------------------------ arguments ------------------------
[default0]:  accumulate_allreduce_grads_in_fp32 .............. False
[default0]:  adam_beta1 ...................................... 0.9
[default0]:  adam_beta2 ...................................... 0.95
[default0]:  adam_eps ........................................ 1e-08
[default0]:  add_bias_linear ................................. True
[default0]:  add_position_embedding .......................... True
[default0]:  adlr_autoresume ................................. False
[default0]:  adlr_autoresume_interval ........................ 1000
[default0]:  aml_data_download_path .......................... None
[default0]:  apply_layernorm_1p .............................. False
[default0]:  apply_query_key_layer_scaling ................... True
[default0]:  apply_residual_connection_post_layernorm ........ False
[default0]:  async_tensor_model_parallel_allreduce ........... False
[default0]:  attention_dropout ............................... 0.1
[default0]:  attention_softmax_in_fp32 ....................... False
[default0]:  barrier_with_L1_time ............................ True
[default0]:  bert_binary_head ................................ True
[default0]:  bert_embedder_type .............................. megatron
[default0]:  bert_load ....................................... None
[default0]:  bf16 ............................................ False
[default0]:  bias_dropout_fusion ............................. True
[default0]:  bias_gelu_fusion ................................ True
[default0]:  biencoder_projection_dim ........................ 0
[default0]:  biencoder_shared_query_context_model ............ False
[default0]:  block_data_path ................................. None
[default0]:  checkpoint_activations .......................... True
[default0]:  checkpoint_in_cpu ............................... False
[default0]:  checkpoint_num_layers ........................... 1
[default0]:  classes_fraction ................................ 1.0
[default0]:  clip_grad ....................................... 1.0
[default0]:  compression_training ............................ False
[default0]:  consumed_train_samples .......................... 0
[default0]:  consumed_train_tokens ........................... 0
[default0]:  consumed_valid_samples .......................... 0
[default0]:  contigious_checkpointing ........................ False
[default0]:  cpu_optimizer ................................... False
[default0]:  cpu_torch_adam .................................. False
[default0]:  create_moe_param_group .......................... True
[default0]:  curriculum_learning_legacy ...................... False
[default0]:  data_cache_path ................................. None
[default0]:  data_efficiency_curriculum_learning ............. False
[default0]:  data_impl ....................................... mmap
[default0]:  data_parallel_random_init ....................... False
[default0]:  data_parallel_size .............................. 1
[default0]:  data_path ....................................... ['/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/meg-gpt2-oscar-en-10k_text_document']
[default0]:  data_per_class_fraction ......................... 1.0
[default0]:  data_sharding ................................... True
[default0]:  dataloader_type ................................. single
[default0]:  DDP_impl ........................................ local
[default0]:  decoder_num_layers .............................. None
[default0]:  decoder_seq_length .............................. None
[default0]:  deepscale ....................................... False
[default0]:  deepscale_config ................................ None
[default0]:  deepspeed ....................................... True
[default0]:  deepspeed_activation_checkpointing .............. True
[default0]:  deepspeed_config ................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_1.3B_MoE128_1BATCH_1GPU_1Node/ds_config_gpt_gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-1-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true.json
[default0]:  deepspeed_mpi ................................... False
[default0]:  dino_bottleneck_size ............................ 256
[default0]:  dino_freeze_last_layer .......................... 1
[default0]:  dino_head_hidden_size ........................... 2048
[default0]:  dino_local_crops_number ......................... 10
[default0]:  dino_local_img_size ............................. 96
[default0]:  dino_norm_last_layer ............................ False
[default0]:  dino_teacher_temp ............................... 0.07
[default0]:  dino_warmup_teacher_temp ........................ 0.04
[default0]:  dino_warmup_teacher_temp_epochs ................. 30
[default0]:  distribute_checkpointed_activations ............. False
[default0]:  distribute_saved_activations .................... False
[default0]:  distributed_backend ............................. nccl
[default0]:  distributed_timeout_minutes ..................... 10
[default0]:  ds_inference .................................... False
[default0]:  ds_pipeline_enabled ............................. False
[default0]:  embedding_path .................................. None
[default0]:  embedding_weights_in_fp32 ....................... False
[default0]:  empty_unused_memory_level ....................... 0
[default0]:  enable_expert_tensor_parallelism ................ False
[default0]:  encoder_num_layers .............................. 24
[default0]:  encoder_seq_length .............................. 2048
[default0]:  end_weight_decay ................................ 0.1
[default0]:  eod_mask_loss ................................... False
[default0]:  eval_interval ................................... 100
[default0]:  eval_iters ...................................... 50
[default0]:  evidence_data_path .............................. None
[default0]:  exit_duration_in_mins ........................... 30000000
[default0]:  exit_interval ................................... None
[default0]:  exit_on_missing_checkpoint ...................... False
[default0]:  exit_signal_handler ............................. False
[default0]:  expert_interval ................................. 2
[default0]:  ffn_hidden_size ................................. 8192
[default0]:  finetune ........................................ False
[default0]:  fp16 ............................................ True
[default0]:  fp16_lm_cross_entropy ........................... False
[default0]:  fp32_residual_connection ........................ False
[default0]:  fp8_amax_compute_algo ........................... most_recent
[default0]:  fp8_amax_history_len ............................ 1
[default0]:  fp8_e4m3 ........................................ False
[default0]:  fp8_hybrid ...................................... False
[default0]:  fp8_interval .................................... 1
[default0]:  fp8_margin ...................................... 0
[default0]:  fp8_wgrad ....................................... True
[default0]:  global_batch_size ............................... 1
[default0]:  gradient_accumulation_fusion .................... True
[default0]:  head_lr_mult .................................... 1.0
[default0]:  hidden_dropout .................................. 0.1
[default0]:  hidden_size ..................................... 2048
[default0]:  hidden_size_teacher ............................. None
[default0]:  hysteresis ...................................... 2
[default0]:  ict_head_size ................................... None
[default0]:  ict_load ........................................ None
[default0]:  img_h ........................................... 224
[default0]:  img_w ........................................... 224
[default0]:  indexer_batch_size .............................. 128
[default0]:  indexer_log_interval ............................ 1000
[default0]:  inference ....................................... False
[default0]:  inference_batch_times_seqlen_threshold .......... 512
[default0]:  init_method_std ................................. 0.01
[default0]:  init_method_xavier_uniform ...................... False
[default0]:  initial_loss_scale .............................. 4294967296
[default0]:  iter_per_epoch .................................. 1250
[default0]:  kd .............................................. False
[default0]:  kd_alpha_ce ..................................... 1
[default0]:  kd_beta_ce ...................................... 1
[default0]:  kd_temp ......................................... 1.0
[default0]:  kv_channels ..................................... 128
[default0]:  layernorm_epsilon ............................... 1e-05
[default0]:  lazy_mpu_init ................................... None
[default0]:  load ............................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_1.3B_MoE128_1BATCH_1GPU_1Node/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-1-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:  load_teacher .................................... None
[default0]:  local_rank ...................................... None
[default0]:  log_batch_size_to_tensorboard ................... True
[default0]:  log_interval .................................... 5
[default0]:  log_learning_rate_to_tensorboard ................ True
[default0]:  log_loss_scale_to_tensorboard ................... True
[default0]:  log_memory_to_tensorboard ....................... False
[default0]:  log_num_zeros_in_grad ........................... False
[default0]:  log_optimizer_states_to_tensorboard ............. False
[default0]:  log_params_norm ................................. False
[default0]:  log_timers_to_tensorboard ....................... True
[default0]:  log_validation_ppl_to_tensorboard ............... True
[default0]:  log_world_size_to_tensorboard ................... False
[default0]:  loss_scale ...................................... None
[default0]:  loss_scale_window ............................... 1000
[default0]:  lr .............................................. 0.0002
[default0]:  lr_decay_iters .................................. None
[default0]:  lr_decay_samples ................................ None
[default0]:  lr_decay_style .................................. cosine
[default0]:  lr_decay_tokens ................................. 300000000000
[default0]:  lr_warmup_fraction .............................. None
[default0]:  lr_warmup_iters ................................. 0
[default0]:  lr_warmup_samples ............................... 0
[default0]:  lr_warmup_tokens ................................ 375000000
[default0]:  make_vocab_size_divisible_by .................... 128
[default0]:  mask_factor ..................................... 1.0
[default0]:  mask_prob ....................................... 0.15
[default0]:  mask_type ....................................... random
[default0]:  masked_softmax_fusion ........................... True
[default0]:  max_position_embeddings ......................... 2048
[default0]:  max_tokens_to_oom ............................... 12000
[default0]:  memory_centric_tiled_linear ..................... False
[default0]:  merge_file ...................................... /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-merges.txt
[default0]:  micro_batch_size ................................ 1
[default0]:  min_loss_scale .................................. 1.0
[default0]:  min_lr .......................................... 2e-06
[default0]:  mlp_type ........................................ standard
[default0]:  mmap_warmup ..................................... False
[default0]:  moe_eval_capacity_factor ........................ 1.0
[default0]:  moe_expert_parallel_size ........................ 1
[default0]:  moe_loss_coeff .................................. 0.01
[default0]:  moe_min_capacity ................................ 4
[default0]:  moe_token_dropping .............................. True
[default0]:  moe_train_capacity_factor ....................... 1.0
[default0]:  mos ............................................. False
[default0]:  no_load_lr_state ................................ False
[default0]:  no_load_optim ................................... None
[default0]:  no_load_rng ..................................... None
[default0]:  no_persist_layer_norm ........................... False
[default0]:  no_pipeline_parallel ............................ True
[default0]:  no_save_optim ................................... None
[default0]:  no_save_rng ..................................... None
[default0]:  normalization ................................... layernorm
[default0]:  num_attention_heads ............................. 16
[default0]:  num_attention_heads_teacher ..................... None
[default0]:  num_channels .................................... 3
[default0]:  num_classes ..................................... 1000
[default0]:  num_experts ..................................... [128]
[default0]:  num_experts_switch .............................. None
[default0]:  num_experts_teacher ............................. [1]
[default0]:  num_layers ...................................... 24
[default0]:  num_layers_per_virtual_pipeline_stage ........... None
[default0]:  num_layers_teacher .............................. None
[default0]:  num_workers ..................................... 0
[default0]:  onnx_safe ....................................... None
[default0]:  openai_gelu ..................................... False
[default0]:  optimizer ....................................... adam
[default0]:  output_bert_embeddings .......................... False
[default0]:  overlap_p2p_comm ................................ False
[default0]:  override_opt_param_scheduler .................... True
[default0]:  params_dtype .................................... torch.float16
[default0]:  partition_activations ........................... False
[default0]:  patch_dim ....................................... 16
[default0]:  perform_initialization .......................... True
[default0]:  pipeline_model_parallel_size .................... 1
[default0]:  pipeline_model_parallel_split_rank .............. None
[default0]:  profile_backward ................................ False
[default0]:  query_in_block_prob ............................. 0.1
[default0]:  rampup_batch_size ............................... None
[default0]:  random_ltd ...................................... False
[default0]:  rank ............................................ 0
[default0]:  recompute_granularity ........................... None
[default0]:  recompute_method ................................ None
[default0]:  recompute_num_layers ............................ 1
[default0]:  remote_device ................................... none
[default0]:  reset_attention_mask ............................ False
[default0]:  reset_iteration ................................. False
[default0]:  reset_position_ids .............................. False
[default0]:  retriever_report_topk_accuracies ................ []
[default0]:  retriever_score_scaling ......................... False
[default0]:  retriever_seq_length ............................ 256
[default0]:  retro_add_retriever ............................. False
[default0]:  retro_cyclic_train_iters ........................ None
[default0]:  retro_encoder_attention_dropout ................. 0.1
[default0]:  retro_encoder_hidden_dropout .................... 0.1
[default0]:  retro_encoder_layers ............................ 2
[default0]:  retro_num_neighbors ............................. 2
[default0]:  retro_num_retrieved_chunks ...................... 2
[default0]:  retro_return_doc_ids ............................ False
[default0]:  retro_workdir ................................... None
[default0]:  return_data_index ............................... False
[default0]:  rotary_percent .................................. 1.0
[default0]:  sample_rate ..................................... 1.0
[default0]:  save ............................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_1.3B_MoE128_1BATCH_1GPU_1Node/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-1-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:  save_interval ................................... 1000000
[default0]:  scatter_gather_tensors_in_pipeline .............. True
[default0]:  scattered_embeddings ............................ False
[default0]:  seed ............................................ 1234
[default0]:  seq_length ...................................... 2048
[default0]:  sequence_parallel ............................... False
[default0]:  sgd_momentum .................................... 0.9
[default0]:  short_seq_prob .................................. 0.1
[default0]:  skip_train ...................................... False
[default0]:  split ........................................... 98,2,0
[default0]:  split_transformers .............................. False
[default0]:  squared_relu .................................... False
[default0]:  standalone_embedding_stage ...................... False
[default0]:  start_weight_decay .............................. 0.1
[default0]:  swiglu .......................................... False
[default0]:  swin_backbone_type .............................. tiny
[default0]:  synchronize_each_layer .......................... False
[default0]:  tensor_model_parallel_size ...................... 1
[default0]:  tensorboard_dir ................................. /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_1.3B_MoE128_1BATCH_1GPU_1Node/tensorboard/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-1-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true_n2gpu1212_2023.07.30-22.23.24
[default0]:  tensorboard_log_interval ........................ 1
[default0]:  tensorboard_queue_size .......................... 1
[default0]:  test_data_path .................................. None
[default0]:  tile_factor ..................................... 1
[default0]:  timing_log_level ................................ 0
[default0]:  timing_log_option ............................... minmax
[default0]:  titles_data_path ................................ None
[default0]:  tokenizer_model ................................. None
[default0]:  tokenizer_type .................................. GPT2BPETokenizer
[default0]:  topk ............................................ 1
[default0]:  train_data_exact_num_epochs ..................... None
[default0]:  train_data_path ................................. None
[default0]:  train_desc_path ................................. None
[default0]:  train_doc_idx_path .............................. None
[default0]:  train_idx_path .................................. None
[default0]:  train_iters ..................................... 439453125
[default0]:  train_sample_idx_path ........................... None
[default0]:  train_samples ................................... None
[default0]:  train_shuffle_idx_path .......................... None
[default0]:  train_tokens .................................... 300000000000
[default0]:  transformer_impl ................................ local
[default0]:  transformer_pipeline_model_parallel_size ........ 1
[default0]:  untie_embeddings_and_output_weights ............. False
[default0]:  use_checkpoint_args ............................. False
[default0]:  use_checkpoint_opt_param_scheduler .............. False
[default0]:  use_contiguous_buffers_in_local_ddp ............. True
[default0]:  use_cpu_initialization .......................... None
[default0]:  use_distributed_optimizer ....................... False
[default0]:  use_flash_attn .................................. False
[default0]:  use_one_sent_docs ............................... False
[default0]:  use_pin_memory .................................. False
[default0]:  use_ring_exchange_p2p ........................... False
[default0]:  use_rotary_position_embeddings .................. False
[default0]:  use_tutel ....................................... False
[default0]:  valid_data_path ................................. None
[default0]:  variable_seq_lengths ............................ False
[default0]:  virtual_pipeline_model_parallel_size ............ None
[default0]:  vision_backbone_type ............................ vit
[default0]:  vision_pretraining .............................. False
[default0]:  vision_pretraining_type ......................... classify
[default0]:  vocab_extra_ids ................................. 0
[default0]:  vocab_file ...................................... /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-vocab.json
[default0]:  vocab_size ...................................... None
[default0]:  weight_decay .................................... 0.1
[default0]:  weight_decay_incr_style ......................... constant
[default0]:  world_size ...................................... 1
[default0]:  zero_allgather_bucket_size ...................... 0.0
[default0]:  zero_contigious_gradients ....................... False
[default0]:  zero_reduce_bucket_size ......................... 0.0
[default0]:  zero_reduce_scatter ............................. False
[default0]:  zero_stage ...................................... 1.0
[default0]:-------------------- end of arguments ---------------------
[default0]:setting number of micro-batches to constant 1
[default0]:> building GPT2BPETokenizer tokenizer ...
[default0]: > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
[default0]:> setting tensorboard ...
[default0]:> initializing torch distributed ...
[default0]:[2023-07-30 22:23:47,712] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[default0]:[2023-07-30 22:23:47,712] [INFO] [comm.py:616:init_distributed] cdb=None
[default0]:[2023-07-30 22:23:47,712] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[default0]:> initialized tensor model parallel with size 1
[default0]:> initialized pipeline model parallel with size 1
[default0]:> setting random seeds to 1234 ...
[default0]:> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[default0]:> compiling dataset index builder ...
[default0]:make: Entering directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/data'
[default0]:make: Nothing to be done for 'default'.
[default0]:make: Leaving directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/data'
[default0]:>>> done with dataset index builder. Compilation time: 0.084 seconds
[default0]:> compiling and loading fused kernels ...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Loading extension module scaled_masked_softmax_cuda...
[default0]:Loading extension module scaled_softmax_cuda...
[default0]:n2gpu1212:1380142:1380142 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.80<0>
[default0]:n2gpu1212:1380142:1380142 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1212:1380142:1380142 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.80<0>
[default0]:n2gpu1212:1380142:1380142 [0] NCCL INFO Using network IB
[default0]:NCCL version 2.12.12+cuda11.7
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Connected all rings
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO Connected all trees
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1212:1380142:1380349 [0] NCCL INFO comm 0x1472680090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:>>> done with compiling and loading fused kernels. Compilation time: 3.878 seconds
[default0]:time to initialize megatron (seconds): 7.701
[default0]:[after megatron is initialized] datetime: 2023-07-30 22:23:52 
[default0]:building GPT model ...
[default0]:[2023-07-30 22:23:52,469] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[default0]:[2023-07-30 22:23:52,470] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.53 GB         CA 0.0 GB         Max_CA 1 GB 
[default0]:[2023-07-30 22:23:52,470] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 19.04 GB, percent = 3.8%
[default0]:[2023-07-30 22:23:52,661] [INFO] [utils.py:785:see_memory_usage] After Building Model
[default0]:[2023-07-30 22:23:52,662] [INFO] [utils.py:786:see_memory_usage] MA 2.44 GB         Max_MA 2.44 GB         CA 2.49 GB         Max_CA 2 GB 
[default0]:[2023-07-30 22:23:52,662] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 19.07 GB, percent = 3.8%
[default0]: > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1315819520
[default0]:param_group keyset: dict_keys(['name', 'params', 'wd_mult', 'lr_mult'])
[default0]:> learning rate decay style: cosine
[default0]:DeepSpeed is enabled.
[default0]:[2023-07-30 22:23:52,665] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Connected all rings
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO Connected all trees
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1212:1380142:1380481 [0] NCCL INFO comm 0x1470b00090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:[2023-07-30 22:23:53,161] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[default0]:[2023-07-30 22:23:53,162] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[default0]:[2023-07-30 22:23:53,162] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[default0]:[2023-07-30 22:23:53,171] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[default0]:[2023-07-30 22:23:53,171] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[default0]:[2023-07-30 22:23:53,171] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[default0]:[2023-07-30 22:23:53,171] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[default0]:[2023-07-30 22:23:53,171] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[default0]:[2023-07-30 22:23:53,171] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[default0]:[2023-07-30 22:23:53,171] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
[default0]:Rank: 0 partition count [1, 1] and sizes[(1315176448, False), (643072, False)] 
[default0]:[2023-07-30 22:23:55,596] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[default0]:[2023-07-30 22:23:55,597] [INFO] [utils.py:786:see_memory_usage] MA 7.35 GB         Max_MA 7.36 GB         CA 7.37 GB         Max_CA 7 GB 
[default0]:[2023-07-30 22:23:55,597] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 19.53 GB, percent = 3.9%
[default0]:[2023-07-30 22:23:55,706] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[default0]:[2023-07-30 22:23:55,707] [INFO] [utils.py:786:see_memory_usage] MA 17.16 GB         Max_MA 22.06 GB         CA 22.07 GB         Max_CA 22 GB 
[default0]:[2023-07-30 22:23:55,707] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 19.53 GB, percent = 3.9%
[default0]:[2023-07-30 22:23:55,707] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[default0]:[2023-07-30 22:23:55,752] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[default0]:[2023-07-30 22:23:55,752] [INFO] [utils.py:786:see_memory_usage] MA 17.16 GB         Max_MA 17.16 GB         CA 22.07 GB         Max_CA 22 GB 
[default0]:[2023-07-30 22:23:55,752] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 19.53 GB, percent = 3.9%
[default0]:[2023-07-30 22:23:55,757] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[default0]:[2023-07-30 22:23:55,757] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[default0]:[2023-07-30 22:23:55,757] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x1472e55d3700>
[default0]:[2023-07-30 22:23:55,757] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:55,757] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[default0]:[2023-07-30 22:23:55,758] [INFO] [config.py:964:print]   activation_checkpointing_config  {
[default0]:    "partition_activations": false, 
[default0]:    "contiguous_memory_optimization": false, 
[default0]:    "cpu_checkpointing": false, 
[default0]:    "number_checkpoints": null, 
[default0]:    "synchronize_checkpoint_boundary": false, 
[default0]:    "profile": false
[default0]:}
[default0]:[2023-07-30 22:23:55,758] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[default0]:[2023-07-30 22:23:55,758] [INFO] [config.py:964:print]   amp_enabled .................. False
[default0]:[2023-07-30 22:23:55,758] [INFO] [config.py:964:print]   amp_params ................... False
[default0]:[2023-07-30 22:23:55,758] [INFO] [config.py:964:print]   autotuning_config ............ {
[default0]:    "enabled": false, 
[default0]:    "start_step": null, 
[default0]:    "end_step": null, 
[default0]:    "metric_path": null, 
[default0]:    "arg_mappings": null, 
[default0]:    "metric": "throughput", 
[default0]:    "model_info": null, 
[default0]:    "results_dir": "autotuning_results", 
[default0]:    "exps_dir": "autotuning_exps", 
[default0]:    "overwrite": true, 
[default0]:    "fast": true, 
[default0]:    "start_profile_step": 3, 
[default0]:    "end_profile_step": 5, 
[default0]:    "tuner_type": "gridsearch", 
[default0]:    "tuner_early_stopping": 5, 
[default0]:    "tuner_num_trials": 50, 
[default0]:    "model_info_path": null, 
[default0]:    "mp_size": 1, 
[default0]:    "max_train_batch_size": null, 
[default0]:    "min_train_batch_size": 1, 
[default0]:    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[default0]:    "min_train_micro_batch_size_per_gpu": 1, 
[default0]:    "num_tuning_micro_batch_sizes": 3
[default0]:}
[default0]:[2023-07-30 22:23:55,758] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[default0]:[2023-07-30 22:23:55,758] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[default0]:[2023-07-30 22:23:55,758] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[default0]:[2023-07-30 22:23:55,758] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[default0]:[2023-07-30 22:23:55,758] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1472e5637700>
[default0]:[2023-07-30 22:23:55,758] [INFO] [config.py:964:print]   communication_data_type ...... None
[default0]:[2023-07-30 22:23:55,758] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   curriculum_params_legacy ..... {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 56390977, 'difficulty_step': 8}}
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   disable_allgather ............ False
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   dump_state ................... False
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[default0]:[2023-07-30 22:23:55,759] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   flops_profiler_config ........ {
[default0]:    "enabled": false, 
[default0]:    "recompute_fwd_factor": 0.0, 
[default0]:    "profile_step": 1, 
[default0]:    "module_depth": -1, 
[default0]:    "top_modules": 1, 
[default0]:    "detailed": true, 
[default0]:    "output_file": null
[default0]:}
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   fp16_enabled ................. True
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   global_rank .................. 0
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   gradient_clipping ............ 1
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   loss_scale ................... 0
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   memory_breakdown ............. False
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[default0]:[2023-07-30 22:23:55,760] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   nebula_config ................ {
[default0]:    "enabled": false, 
[default0]:    "persistent_storage_path": null, 
[default0]:    "persistent_time_interval": 100, 
[default0]:    "num_of_version_in_retention": 2, 
[default0]:    "enable_nebula_load": true, 
[default0]:    "load_path": null
[default0]:}
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   optimizer_name ............... None
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   optimizer_params ............. None
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   pld_enabled .................. False
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   pld_params ................... False
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   prescale_gradients ........... False
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   scheduler_name ............... None
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   scheduler_params ............. None
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   sparse_attention ............. None
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   steps_per_print .............. 5
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   train_batch_size ............. 1
[default0]:[2023-07-30 22:23:55,761] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[default0]:[2023-07-30 22:23:55,762] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[default0]:[2023-07-30 22:23:55,762] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[default0]:[2023-07-30 22:23:55,762] [INFO] [config.py:964:print]   world_size ................... 1
[default0]:[2023-07-30 22:23:55,762] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[default0]:[2023-07-30 22:23:55,762] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[default0]:[2023-07-30 22:23:55,762] [INFO] [config.py:964:print]   zero_enabled ................. True
[default0]:[2023-07-30 22:23:55,762] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[default0]:[2023-07-30 22:23:55,762] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[default0]:[2023-07-30 22:23:55,762] [INFO] [config.py:950:print_user_config]   json = {
[default0]:    "train_batch_size": 1, 
[default0]:    "train_micro_batch_size_per_gpu": 1, 
[default0]:    "steps_per_print": 5, 
[default0]:    "zero_optimization": {
[default0]:        "stage": 2
[default0]:    }, 
[default0]:    "gradient_clipping": 1, 
[default0]:    "prescale_gradients": false, 
[default0]:    "fp16": {
[default0]:        "enabled": true, 
[default0]:        "loss_scale": 0, 
[default0]:        "loss_scale_window": 500, 
[default0]:        "hysteresis": 2, 
[default0]:        "min_loss_scale": 1, 
[default0]:        "initial_scale_power": 11
[default0]:    }, 
[default0]:    "bf16": {
[default0]:        "enabled": false
[default0]:    }, 
[default0]:    "curriculum_learning": {
[default0]:        "enabled": false, 
[default0]:        "curriculum_type": "seqlen", 
[default0]:        "min_difficulty": 80, 
[default0]:        "max_difficulty": 2.048000e+03, 
[default0]:        "schedule_type": "fixed_linear", 
[default0]:        "schedule_config": {
[default0]:            "total_curriculum_step": 5.639098e+07, 
[default0]:            "difficulty_step": 8
[default0]:        }
[default0]:    }, 
[default0]:    "wall_clock_breakdown": false
[default0]:}
[default0]:[2023-07-30 22:23:55,763] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_1.3B_MoE128_1BATCH_1GPU_1Node/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-1-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:WARNING: could not find the metadata file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_1.3B_MoE128_1BATCH_1GPU_1Node/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-1-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true 
[default0]:    will not load any checkpoints and will start from random
[default0]:(min, max) time across ranks (ms):
[default0]:    load-checkpoint ................................: (1.14, 1.14)
[default0]:[after model, optimizer, and learning rate scheduler are built] datetime: 2023-07-30 22:23:55 
[default0]:> building train, validation, and test datasets ...
[default0]: > datasets target sizes (minimum size):
[default0]:    train:      439453125
[default0]:    validation: 219726600
[default0]:    test:       50
[default0]:> building train, validation, and test datasets for GPT ...
[default0]:Single data path provided for train, valid & test
[default0]: > building dataset index ...
[default0]:    reading sizes...
[default0]:    reading pointers...
[default0]:    reading document index...
[default0]:    creating numpy buffer of mmap...
[default0]:    creating memory view of numpy buffer...
[default0]: > finished creating indexed dataset in 0.003015 seconds
[default0]:    number of documents: 10000
[default0]: > dataset split:
[default0]:    train:
[default0]:     document indices in [0, 9800) total of 9800 documents
[default0]:    validation:
[default0]:     document indices in [9800, 10000) total of 200 documents
[default0]:    test:
[default0]:     document indices in [10000, 10000) total of 0 documents
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Connected all rings
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO Connected all trees
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1212:1380142:1380489 [0] NCCL INFO comm 0x1470b40090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]: > loading doc-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/153ec6eb4421dfd1cf9815ea8f6e40e4_doc_idx.npy
[default0]: > loading sample-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/153ec6eb4421dfd1cf9815ea8f6e40e4_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/153ec6eb4421dfd1cf9815ea8f6e40e4_shuffle_idx.npy
[default0]:    loaded indexed file in 0.020 seconds
[default0]:    total number of samples: 439466004
[default0]:    total number of epochs: 30909
[default0]: > loading doc-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/de3819301120d9a35080a0a129b3870d_doc_idx.npy
[default0]: > loading sample-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/de3819301120d9a35080a0a129b3870d_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/de3819301120d9a35080a0a129b3870d_shuffle_idx.npy
[default0]:    loaded indexed file in 0.039 seconds
[default0]:    total number of samples: 219726809
[default0]:    total number of epochs: 648115
[default0]:> finished creating GPT datasets ...
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Connected all rings
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO Connected all trees
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1212:1380142:1380497 [0] NCCL INFO comm 0x1472580090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:[after dataloaders are built] datetime: 2023-07-30 22:23:56 
[default0]:done with setup ...
[default0]:(min, max) time across ranks (ms):
[default0]:    model-and-optimizer-setup ......................: (3347.10, 3347.10)
[default0]:    train/valid/test-data-iterators-setup ..........: (1146.24, 1146.24)
[default0]:training ...
[default0]:[before the start of training step] datetime: 2023-07-30 22:23:57 
[default0]:[2023-07-30 22:24:00,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[4.369066666666667e-09, 4.369066666666667e-09], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:00,351] [INFO] [timer.py:215:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=2.3456764163078128, CurrSamplesPerSec=2.333352618906686, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration        5/439453125 | consumed samples:            5 | consumed tokens:        10240 | elapsed time per iteration (ms): 652.8 | learning rate: 4.369E-09 | global batch size:     1 | lm loss: 1.093634E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.532 | TFLOPs: 37.31 |
[default0]:[Rank 0] (after 5 iterations) memory (MB) | allocated: 17699.36279296875 | max allocated: 27735.59033203125 | reserved: 30132.0 | max reserved: 30132.0
[default0]:[2023-07-30 22:24:02,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[9.830400000000001e-09, 9.830400000000001e-09], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:02,590] [INFO] [timer.py:215:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=2.34630970735558, CurrSamplesPerSec=2.339280385992329, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       10/439453125 | consumed samples:           10 | consumed tokens:        20480 | elapsed time per iteration (ms): 447.9 | learning rate: 9.830E-09 | global batch size:     1 | lm loss: 1.093259E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.233 | TFLOPs: 54.37 |
[default0]:[2023-07-30 22:24:04,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=15, skipped=0, lr=[1.5291733333333332e-08, 1.5291733333333332e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:04,840] [INFO] [timer.py:215:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=2.348221190520682, CurrSamplesPerSec=2.348143930678231, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       15/439453125 | consumed samples:           15 | consumed tokens:        30720 | elapsed time per iteration (ms): 449.8 | learning rate: 1.529E-08 | global batch size:     1 | lm loss: 1.092224E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.223 | TFLOPs: 54.15 |
[default0]:[2023-07-30 22:24:07,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[2.0753066666666666e-08, 2.0753066666666666e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:07,076] [INFO] [timer.py:215:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=2.347967204983903, CurrSamplesPerSec=2.340261918485111, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       20/439453125 | consumed samples:           20 | consumed tokens:        40960 | elapsed time per iteration (ms): 447.8 | learning rate: 2.075E-08 | global batch size:     1 | lm loss: 1.092768E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.233 | TFLOPs: 54.39 |
[default0]:[2023-07-30 22:24:09,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=25, skipped=0, lr=[2.62144e-08, 2.62144e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:09,342] [INFO] [timer.py:215:stop] epoch=0/micro_step=25/global_step=25, RunningAvgSamplesPerSec=2.3486867521775796, CurrSamplesPerSec=2.350203064344053, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       25/439453125 | consumed samples:           25 | consumed tokens:        51200 | elapsed time per iteration (ms): 453.2 | learning rate: 2.621E-08 | global batch size:     1 | lm loss: 1.091070E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.206 | TFLOPs: 53.74 |
[default0]:[2023-07-30 22:24:11,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[3.1675733333333335e-08, 3.1675733333333335e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:11,615] [INFO] [timer.py:215:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=2.349076766249899, CurrSamplesPerSec=2.3446418419477117, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       30/439453125 | consumed samples:           30 | consumed tokens:        61440 | elapsed time per iteration (ms): 454.4 | learning rate: 3.168E-08 | global batch size:     1 | lm loss: 1.087918E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.201 | TFLOPs: 53.60 |
[default0]:[2023-07-30 22:24:13,850] [INFO] [logging.py:96:log_dist] [Rank 0] step=35, skipped=0, lr=[3.713706666666667e-08, 3.713706666666667e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:13,851] [INFO] [timer.py:215:stop] epoch=0/micro_step=35/global_step=35, RunningAvgSamplesPerSec=2.3492415664467305, CurrSamplesPerSec=2.3508893665557817, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       35/439453125 | consumed samples:           35 | consumed tokens:        71680 | elapsed time per iteration (ms): 447.2 | learning rate: 3.714E-08 | global batch size:     1 | lm loss: 1.083061E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.236 | TFLOPs: 54.46 |
[default0]:[2023-07-30 22:24:16,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[4.2598400000000004e-08, 4.2598400000000004e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:16,116] [INFO] [timer.py:215:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=2.349609572229926, CurrSamplesPerSec=2.345701341270334, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       40/439453125 | consumed samples:           40 | consumed tokens:        81920 | elapsed time per iteration (ms): 453.2 | learning rate: 4.260E-08 | global batch size:     1 | lm loss: 1.076204E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.207 | TFLOPs: 53.74 |
[default0]:[2023-07-30 22:24:18,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=45, skipped=0, lr=[4.805973333333334e-08, 4.805973333333334e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:18,381] [INFO] [timer.py:215:stop] epoch=0/micro_step=45/global_step=45, RunningAvgSamplesPerSec=2.3491395871848773, CurrSamplesPerSec=2.3363459279283396, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       45/439453125 | consumed samples:           45 | consumed tokens:        92160 | elapsed time per iteration (ms): 453.8 | learning rate: 4.806E-08 | global batch size:     1 | lm loss: 1.068613E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.203 | TFLOPs: 53.66 |
[default0]:[2023-07-30 22:24:20,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[5.3521066666666666e-08, 5.3521066666666666e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:20,606] [INFO] [timer.py:215:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=2.350253628680607, CurrSamplesPerSec=2.3337031526634227, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       50/439453125 | consumed samples:           50 | consumed tokens:       102400 | elapsed time per iteration (ms): 444.1 | learning rate: 5.352E-08 | global batch size:     1 | lm loss: 1.064192E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.252 | TFLOPs: 54.84 |
[default0]:[2023-07-30 22:24:22,837] [INFO] [logging.py:96:log_dist] [Rank 0] step=55, skipped=0, lr=[5.89824e-08, 5.89824e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:22,838] [INFO] [timer.py:215:stop] epoch=0/micro_step=55/global_step=55, RunningAvgSamplesPerSec=2.3504018079321605, CurrSamplesPerSec=2.344628735328266, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       55/439453125 | consumed samples:           55 | consumed tokens:       112640 | elapsed time per iteration (ms): 446.2 | learning rate: 5.898E-08 | global batch size:     1 | lm loss: 1.053232E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.241 | TFLOPs: 54.59 |
[default0]:[2023-07-30 22:24:25,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[6.444373333333333e-08, 6.444373333333333e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:25,091] [INFO] [timer.py:215:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=2.350607647538223, CurrSamplesPerSec=2.348281970487919, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       60/439453125 | consumed samples:           60 | consumed tokens:       122880 | elapsed time per iteration (ms): 451.3 | learning rate: 6.444E-08 | global batch size:     1 | lm loss: 1.045067E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.216 | TFLOPs: 53.97 |
[default0]:[2023-07-30 22:24:27,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=65, skipped=0, lr=[6.990506666666667e-08, 6.990506666666667e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:27,322] [INFO] [timer.py:215:stop] epoch=0/micro_step=65/global_step=65, RunningAvgSamplesPerSec=2.350603301092325, CurrSamplesPerSec=2.3447729162019666, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       65/439453125 | consumed samples:           65 | consumed tokens:       133120 | elapsed time per iteration (ms): 445.9 | learning rate: 6.991E-08 | global batch size:     1 | lm loss: 1.040618E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.243 | TFLOPs: 54.62 |
[default0]:[2023-07-30 22:24:29,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[7.536640000000001e-08, 7.536640000000001e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:29,588] [INFO] [timer.py:215:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=2.3507584407373545, CurrSamplesPerSec=2.3504717414646064, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       70/439453125 | consumed samples:           70 | consumed tokens:       143360 | elapsed time per iteration (ms): 453.7 | learning rate: 7.537E-08 | global batch size:     1 | lm loss: 1.030448E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.204 | TFLOPs: 53.68 |
[default0]:[2023-07-30 22:24:31,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=75, skipped=0, lr=[8.082773333333334e-08, 8.082773333333334e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:31,863] [INFO] [timer.py:215:stop] epoch=0/micro_step=75/global_step=75, RunningAvgSamplesPerSec=2.3508797638974475, CurrSamplesPerSec=2.3496435175770736, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       75/439453125 | consumed samples:           75 | consumed tokens:       153600 | elapsed time per iteration (ms): 454.2 | learning rate: 8.083E-08 | global batch size:     1 | lm loss: 1.024482E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.201 | TFLOPs: 53.62 |
[default0]:[2023-07-30 22:24:34,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[8.628906666666668e-08, 8.628906666666668e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:34,101] [INFO] [timer.py:215:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=2.3508549050954564, CurrSamplesPerSec=2.3532899627731347, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       80/439453125 | consumed samples:           80 | consumed tokens:       163840 | elapsed time per iteration (ms): 447.5 | learning rate: 8.629E-08 | global batch size:     1 | lm loss: 1.025640E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.235 | TFLOPs: 54.43 |
[default0]:[2023-07-30 22:24:36,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=85, skipped=0, lr=[9.175040000000002e-08, 9.175040000000002e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:36,333] [INFO] [timer.py:215:stop] epoch=0/micro_step=85/global_step=85, RunningAvgSamplesPerSec=2.3508061979501926, CurrSamplesPerSec=2.3421646756359378, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       85/439453125 | consumed samples:           85 | consumed tokens:       174080 | elapsed time per iteration (ms): 446.4 | learning rate: 9.175E-08 | global batch size:     1 | lm loss: 1.013608E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.240 | TFLOPs: 54.56 |
[default0]:[2023-07-30 22:24:38,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[9.721173333333333e-08, 9.721173333333333e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:38,601] [INFO] [timer.py:215:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=2.3507744359911955, CurrSamplesPerSec=2.339705789654832, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       90/439453125 | consumed samples:           90 | consumed tokens:       184320 | elapsed time per iteration (ms): 453.5 | learning rate: 9.721E-08 | global batch size:     1 | lm loss: 1.002826E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.205 | TFLOPs: 53.70 |
[default0]:[2023-07-30 22:24:40,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=95, skipped=0, lr=[1.0267306666666667e-07, 1.0267306666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:40,836] [INFO] [timer.py:215:stop] epoch=0/micro_step=95/global_step=95, RunningAvgSamplesPerSec=2.3512373378615874, CurrSamplesPerSec=2.343931674370233, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration       95/439453125 | consumed samples:           95 | consumed tokens:       194560 | elapsed time per iteration (ms): 447.1 | learning rate: 1.027E-07 | global batch size:     1 | lm loss: 9.860664E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.237 | TFLOPs: 54.48 |
[default0]:[2023-07-30 22:24:43,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[1.081344e-07, 1.081344e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:43,068] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=2.3511744601195526, CurrSamplesPerSec=2.3429418267477087, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      100/439453125 | consumed samples:          100 | consumed tokens:       204800 | elapsed time per iteration (ms): 447.3 | learning rate: 1.081E-07 | global batch size:     1 | lm loss: 9.853084E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.236 | TFLOPs: 54.45 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 100 | lm loss value: 9.828688E+00 | lm loss PPL: 1.855858E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:24:50,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=105, skipped=0, lr=[1.1359573333333334e-07, 1.1359573333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:50,227] [INFO] [timer.py:215:stop] epoch=0/micro_step=105/global_step=105, RunningAvgSamplesPerSec=2.351088670443563, CurrSamplesPerSec=2.3563796855916195, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      105/439453125 | consumed samples:          105 | consumed tokens:       215040 | elapsed time per iteration (ms): 1430.7 | learning rate: 1.136E-07 | global batch size:     1 | lm loss: 9.822457E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.699 | TFLOPs: 17.02 |
[default0]:[2023-07-30 22:24:52,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[1.1905706666666667e-07, 1.1905706666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:52,504] [INFO] [timer.py:215:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=2.3512768356434695, CurrSamplesPerSec=2.345521631031563, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      110/439453125 | consumed samples:          110 | consumed tokens:       225280 | elapsed time per iteration (ms): 455.5 | learning rate: 1.191E-07 | global batch size:     1 | lm loss: 9.805839E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.195 | TFLOPs: 53.47 |
[default0]:[2023-07-30 22:24:54,725] [INFO] [logging.py:96:log_dist] [Rank 0] step=115, skipped=0, lr=[1.245184e-07, 1.245184e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:54,726] [INFO] [timer.py:215:stop] epoch=0/micro_step=115/global_step=115, RunningAvgSamplesPerSec=2.3513327331094764, CurrSamplesPerSec=2.35334805986295, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      115/439453125 | consumed samples:          115 | consumed tokens:       235520 | elapsed time per iteration (ms): 444.4 | learning rate: 1.245E-07 | global batch size:     1 | lm loss: 9.711743E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.250 | TFLOPs: 54.81 |
[default0]:[2023-07-30 22:24:56,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[1.2997973333333334e-07, 1.2997973333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:56,978] [INFO] [timer.py:215:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=2.3513785112592926, CurrSamplesPerSec=2.3464847797239026, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      120/439453125 | consumed samples:          120 | consumed tokens:       245760 | elapsed time per iteration (ms): 450.4 | learning rate: 1.300E-07 | global batch size:     1 | lm loss: 9.805524E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.220 | TFLOPs: 54.07 |
[default0]:[2023-07-30 22:24:59,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=125, skipped=0, lr=[1.3544106666666668e-07, 1.3544106666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:59,231] [INFO] [timer.py:215:stop] epoch=0/micro_step=125/global_step=125, RunningAvgSamplesPerSec=2.351531490359797, CurrSamplesPerSec=2.3539239042896787, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      125/439453125 | consumed samples:          125 | consumed tokens:       256000 | elapsed time per iteration (ms): 450.5 | learning rate: 1.354E-07 | global batch size:     1 | lm loss: 9.648094E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.220 | TFLOPs: 54.06 |
[default0]:[2023-07-30 22:25:01,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[1.409024e-07, 1.409024e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:01,505] [INFO] [timer.py:215:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=2.3517525779138877, CurrSamplesPerSec=2.348314839501459, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      130/439453125 | consumed samples:          130 | consumed tokens:       266240 | elapsed time per iteration (ms): 455.0 | learning rate: 1.409E-07 | global batch size:     1 | lm loss: 9.673848E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.198 | TFLOPs: 53.53 |
[default0]:[2023-07-30 22:25:03,728] [INFO] [logging.py:96:log_dist] [Rank 0] step=135, skipped=0, lr=[1.4636373333333334e-07, 1.4636373333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:03,728] [INFO] [timer.py:215:stop] epoch=0/micro_step=135/global_step=135, RunningAvgSamplesPerSec=2.351813371994013, CurrSamplesPerSec=2.3554203031050096, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      135/439453125 | consumed samples:          135 | consumed tokens:       276480 | elapsed time per iteration (ms): 444.6 | learning rate: 1.464E-07 | global batch size:     1 | lm loss: 9.616324E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.249 | TFLOPs: 54.79 |
[default0]:[2023-07-30 22:25:05,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[1.5182506666666668e-07, 1.5182506666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:05,988] [INFO] [timer.py:215:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=2.3519890984763525, CurrSamplesPerSec=2.361936940741867, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      140/439453125 | consumed samples:          140 | consumed tokens:       286720 | elapsed time per iteration (ms): 452.4 | learning rate: 1.518E-07 | global batch size:     1 | lm loss: 9.587741E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.211 | TFLOPs: 53.84 |
[default0]:[2023-07-30 22:25:08,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=145, skipped=0, lr=[1.5728640000000002e-07, 1.5728640000000002e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:08,236] [INFO] [timer.py:215:stop] epoch=0/micro_step=145/global_step=145, RunningAvgSamplesPerSec=2.3522667996685223, CurrSamplesPerSec=2.355966724522409, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      145/439453125 | consumed samples:          145 | consumed tokens:       296960 | elapsed time per iteration (ms): 449.2 | learning rate: 1.573E-07 | global batch size:     1 | lm loss: 9.578017E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.226 | TFLOPs: 54.22 |
[default0]:[2023-07-30 22:25:10,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[1.6274773333333333e-07, 1.6274773333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:10,516] [INFO] [timer.py:215:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=2.352255160913794, CurrSamplesPerSec=2.351614304897159, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      150/439453125 | consumed samples:          150 | consumed tokens:       307200 | elapsed time per iteration (ms): 456.0 | learning rate: 1.627E-07 | global batch size:     1 | lm loss: 9.593816E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.193 | TFLOPs: 53.41 |
[default0]:[2023-07-30 22:25:12,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=155, skipped=0, lr=[1.6820906666666667e-07, 1.6820906666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:12,820] [INFO] [timer.py:215:stop] epoch=0/micro_step=155/global_step=155, RunningAvgSamplesPerSec=2.352417893320928, CurrSamplesPerSec=2.350171459309232, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      155/439453125 | consumed samples:          155 | consumed tokens:       317440 | elapsed time per iteration (ms): 460.7 | learning rate: 1.682E-07 | global batch size:     1 | lm loss: 9.673863E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.171 | TFLOPs: 52.87 |
[default0]:[2023-07-30 22:25:15,058] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[1.7367040000000001e-07, 1.7367040000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:15,061] [INFO] [timer.py:215:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=2.352477246989904, CurrSamplesPerSec=2.348631743987457, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      160/439453125 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 448.1 | learning rate: 1.737E-07 | global batch size:     1 | lm loss: 9.734457E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.232 | TFLOPs: 54.35 |
[default0]:[2023-07-30 22:25:17,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=165, skipped=0, lr=[1.7913173333333336e-07, 1.7913173333333336e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:17,311] [INFO] [timer.py:215:stop] epoch=0/micro_step=165/global_step=165, RunningAvgSamplesPerSec=2.352425010330524, CurrSamplesPerSec=2.354461701511372, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      165/439453125 | consumed samples:          165 | consumed tokens:       337920 | elapsed time per iteration (ms): 450.1 | learning rate: 1.791E-07 | global batch size:     1 | lm loss: 9.547000E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.222 | TFLOPs: 54.11 |
[default0]:[2023-07-30 22:25:19,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[1.845930666666667e-07, 1.845930666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:19,577] [INFO] [timer.py:215:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=2.3521250192258445, CurrSamplesPerSec=2.349734343522621, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      170/439453125 | consumed samples:          170 | consumed tokens:       348160 | elapsed time per iteration (ms): 453.1 | learning rate: 1.846E-07 | global batch size:     1 | lm loss: 9.408024E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.207 | TFLOPs: 53.76 |
[default0]:[2023-07-30 22:25:21,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=175, skipped=0, lr=[1.9005440000000004e-07, 1.9005440000000004e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:21,818] [INFO] [timer.py:215:stop] epoch=0/micro_step=175/global_step=175, RunningAvgSamplesPerSec=2.3523609899482616, CurrSamplesPerSec=2.345659362691284, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      175/439453125 | consumed samples:          175 | consumed tokens:       358400 | elapsed time per iteration (ms): 448.2 | learning rate: 1.901E-07 | global batch size:     1 | lm loss: 9.661653E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.231 | TFLOPs: 54.35 |
[default0]:[2023-07-30 22:25:24,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[1.9551573333333333e-07, 1.9551573333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:24,067] [INFO] [timer.py:215:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=2.3523294771384484, CurrSamplesPerSec=2.3502662769639047, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      180/439453125 | consumed samples:          180 | consumed tokens:       368640 | elapsed time per iteration (ms): 449.8 | learning rate: 1.955E-07 | global batch size:     1 | lm loss: 9.802800E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.223 | TFLOPs: 54.15 |
[default0]:[2023-07-30 22:25:26,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=185, skipped=0, lr=[2.0097706666666667e-07, 2.0097706666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:26,317] [INFO] [timer.py:215:stop] epoch=0/micro_step=185/global_step=185, RunningAvgSamplesPerSec=2.3523511459990405, CurrSamplesPerSec=2.341357978517337, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      185/439453125 | consumed samples:          185 | consumed tokens:       378880 | elapsed time per iteration (ms): 450.0 | learning rate: 2.010E-07 | global batch size:     1 | lm loss: 9.349823E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.222 | TFLOPs: 54.12 |
[default0]:[2023-07-30 22:25:28,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[2.064384e-07, 2.064384e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:28,583] [INFO] [timer.py:215:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=2.352249034904322, CurrSamplesPerSec=2.3209007585265065, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      190/439453125 | consumed samples:          190 | consumed tokens:       389120 | elapsed time per iteration (ms): 453.3 | learning rate: 2.064E-07 | global batch size:     1 | lm loss: 9.450103E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.206 | TFLOPs: 53.73 |
[default0]:[2023-07-30 22:25:30,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=195, skipped=0, lr=[2.1189973333333335e-07, 2.1189973333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:30,812] [INFO] [timer.py:215:stop] epoch=0/micro_step=195/global_step=195, RunningAvgSamplesPerSec=2.352312088247471, CurrSamplesPerSec=2.3594124959919895, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      195/439453125 | consumed samples:          195 | consumed tokens:       399360 | elapsed time per iteration (ms): 445.6 | learning rate: 2.119E-07 | global batch size:     1 | lm loss: 9.415527E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.244 | TFLOPs: 54.66 |
[default0]:[2023-07-30 22:25:33,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[2.173610666666667e-07, 2.173610666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:33,074] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=2.3523871525425895, CurrSamplesPerSec=2.342487771804389, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      200/439453125 | consumed samples:          200 | consumed tokens:       409600 | elapsed time per iteration (ms): 452.5 | learning rate: 2.174E-07 | global batch size:     1 | lm loss: 9.429657E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.210 | TFLOPs: 53.82 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 200 | lm loss value: 9.412098E+00 | lm loss PPL: 1.223551E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:25:40,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=205, skipped=0, lr=[2.228224e-07, 2.228224e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:40,208] [INFO] [timer.py:215:stop] epoch=0/micro_step=205/global_step=205, RunningAvgSamplesPerSec=2.3522317476332435, CurrSamplesPerSec=2.342087512005539, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      205/439453125 | consumed samples:          205 | consumed tokens:       419840 | elapsed time per iteration (ms): 1426.8 | learning rate: 2.228E-07 | global batch size:     1 | lm loss: 9.385232E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.701 | TFLOPs: 17.07 |
[default0]:[2023-07-30 22:25:42,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[2.2828373333333334e-07, 2.2828373333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:42,490] [INFO] [timer.py:215:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=2.3524144039658834, CurrSamplesPerSec=2.343593775231339, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      210/439453125 | consumed samples:          210 | consumed tokens:       430080 | elapsed time per iteration (ms): 456.6 | learning rate: 2.283E-07 | global batch size:     1 | lm loss: 9.347231E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.190 | TFLOPs: 53.34 |
[default0]:[2023-07-30 22:25:44,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=215, skipped=0, lr=[2.3374506666666669e-07, 2.3374506666666669e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:44,743] [INFO] [timer.py:215:stop] epoch=0/micro_step=215/global_step=215, RunningAvgSamplesPerSec=2.3524961203185155, CurrSamplesPerSec=2.357826190098747, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      215/439453125 | consumed samples:          215 | consumed tokens:       440320 | elapsed time per iteration (ms): 450.3 | learning rate: 2.337E-07 | global batch size:     1 | lm loss: 9.411306E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.221 | TFLOPs: 54.09 |
[default0]:[2023-07-30 22:25:46,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[2.392064e-07, 2.392064e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:46,975] [INFO] [timer.py:215:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=2.3524744775551616, CurrSamplesPerSec=2.3532292279734577, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      220/439453125 | consumed samples:          220 | consumed tokens:       450560 | elapsed time per iteration (ms): 446.2 | learning rate: 2.392E-07 | global batch size:     1 | lm loss: 9.246989E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.241 | TFLOPs: 54.58 |
[default0]:[2023-07-30 22:25:49,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=225, skipped=0, lr=[2.446677333333333e-07, 2.446677333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:49,249] [INFO] [timer.py:215:stop] epoch=0/micro_step=225/global_step=225, RunningAvgSamplesPerSec=2.35140957460816, CurrSamplesPerSec=2.3410208209611665, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      225/439453125 | consumed samples:          225 | consumed tokens:       460800 | elapsed time per iteration (ms): 455.0 | learning rate: 2.447E-07 | global batch size:     1 | lm loss: 9.363605E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.198 | TFLOPs: 53.52 |
[default0]:[2023-07-30 22:25:51,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[2.501290666666667e-07, 2.501290666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:51,480] [INFO] [timer.py:215:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=2.3515805454527237, CurrSamplesPerSec=2.3565279633410605, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      230/439453125 | consumed samples:          230 | consumed tokens:       471040 | elapsed time per iteration (ms): 446.2 | learning rate: 2.501E-07 | global batch size:     1 | lm loss: 9.332518E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.241 | TFLOPs: 54.59 |
[default0]:[2023-07-30 22:25:53,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=235, skipped=0, lr=[2.555904e-07, 2.555904e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:53,707] [INFO] [timer.py:215:stop] epoch=0/micro_step=235/global_step=235, RunningAvgSamplesPerSec=2.3516693709570853, CurrSamplesPerSec=2.354383725505715, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      235/439453125 | consumed samples:          235 | consumed tokens:       481280 | elapsed time per iteration (ms): 445.8 | learning rate: 2.556E-07 | global batch size:     1 | lm loss: 9.240614E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.243 | TFLOPs: 54.63 |
[default0]:[2023-07-30 22:25:55,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[2.6105173333333336e-07, 2.6105173333333336e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:55,936] [INFO] [timer.py:215:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=2.351771668354918, CurrSamplesPerSec=2.353596325199219, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      240/439453125 | consumed samples:          240 | consumed tokens:       491520 | elapsed time per iteration (ms): 445.3 | learning rate: 2.611E-07 | global batch size:     1 | lm loss: 9.368568E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.246 | TFLOPs: 54.69 |
[default0]:[2023-07-30 22:25:58,202] [INFO] [logging.py:96:log_dist] [Rank 0] step=245, skipped=0, lr=[2.665130666666667e-07, 2.665130666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:58,204] [INFO] [timer.py:215:stop] epoch=0/micro_step=245/global_step=245, RunningAvgSamplesPerSec=2.351786902295554, CurrSamplesPerSec=2.3403415735954503, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      245/439453125 | consumed samples:          245 | consumed tokens:       501760 | elapsed time per iteration (ms): 453.5 | learning rate: 2.665E-07 | global batch size:     1 | lm loss: 9.425519E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.205 | TFLOPs: 53.71 |
[default0]:[2023-07-30 22:26:00,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[2.7197440000000005e-07, 2.7197440000000005e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:00,503] [INFO] [timer.py:215:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=2.351935254096051, CurrSamplesPerSec=2.3440024097772576, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      250/439453125 | consumed samples:          250 | consumed tokens:       512000 | elapsed time per iteration (ms): 460.2 | learning rate: 2.720E-07 | global batch size:     1 | lm loss: 9.459988E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.173 | TFLOPs: 52.92 |
[default0]:[2023-07-30 22:26:02,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=255, skipped=0, lr=[2.7743573333333336e-07, 2.7743573333333336e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:02,751] [INFO] [timer.py:215:stop] epoch=0/micro_step=255/global_step=255, RunningAvgSamplesPerSec=2.352048189650373, CurrSamplesPerSec=2.3517698953779353, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      255/439453125 | consumed samples:          255 | consumed tokens:       522240 | elapsed time per iteration (ms): 449.3 | learning rate: 2.774E-07 | global batch size:     1 | lm loss: 9.348373E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.226 | TFLOPs: 54.21 |
[default0]:[2023-07-30 22:26:05,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[2.828970666666667e-07, 2.828970666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:05,001] [INFO] [timer.py:215:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=2.352087155426341, CurrSamplesPerSec=2.35053101674674, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      260/439453125 | consumed samples:          260 | consumed tokens:       532480 | elapsed time per iteration (ms): 450.1 | learning rate: 2.829E-07 | global batch size:     1 | lm loss: 9.273328E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.222 | TFLOPs: 54.11 |
[default0]:[2023-07-30 22:26:07,257] [INFO] [logging.py:96:log_dist] [Rank 0] step=265, skipped=0, lr=[2.883584e-07, 2.883584e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:07,258] [INFO] [timer.py:215:stop] epoch=0/micro_step=265/global_step=265, RunningAvgSamplesPerSec=2.3521299070071917, CurrSamplesPerSec=2.350612689810547, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      265/439453125 | consumed samples:          265 | consumed tokens:       542720 | elapsed time per iteration (ms): 451.5 | learning rate: 2.884E-07 | global batch size:     1 | lm loss: 9.345256E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.215 | TFLOPs: 53.95 |
[default0]:[2023-07-30 22:26:09,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[2.9381973333333336e-07, 2.9381973333333336e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:09,530] [INFO] [timer.py:215:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=2.3521118003347974, CurrSamplesPerSec=2.3596819554798434, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      270/439453125 | consumed samples:          270 | consumed tokens:       552960 | elapsed time per iteration (ms): 454.4 | learning rate: 2.938E-07 | global batch size:     1 | lm loss: 9.273663E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.201 | TFLOPs: 53.60 |
[default0]:[2023-07-30 22:26:11,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=275, skipped=0, lr=[2.9928106666666667e-07, 2.9928106666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:11,835] [INFO] [timer.py:215:stop] epoch=0/micro_step=275/global_step=275, RunningAvgSamplesPerSec=2.3521505446694886, CurrSamplesPerSec=2.3604840611340197, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      275/439453125 | consumed samples:          275 | consumed tokens:       563200 | elapsed time per iteration (ms): 460.8 | learning rate: 2.993E-07 | global batch size:     1 | lm loss: 9.244794E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.170 | TFLOPs: 52.85 |
[default0]:[2023-07-30 22:26:14,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[3.0474240000000004e-07, 3.0474240000000004e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:14,114] [INFO] [timer.py:215:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=2.352222189391547, CurrSamplesPerSec=2.3480637435746425, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      280/439453125 | consumed samples:          280 | consumed tokens:       573440 | elapsed time per iteration (ms): 455.6 | learning rate: 3.047E-07 | global batch size:     1 | lm loss: 9.128741E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.195 | TFLOPs: 53.45 |
[default0]:[2023-07-30 22:26:16,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=285, skipped=0, lr=[3.1020373333333335e-07, 3.1020373333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:16,361] [INFO] [timer.py:215:stop] epoch=0/micro_step=285/global_step=285, RunningAvgSamplesPerSec=2.35220573192461, CurrSamplesPerSec=2.3511713803469996, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      285/439453125 | consumed samples:          285 | consumed tokens:       583680 | elapsed time per iteration (ms): 449.8 | learning rate: 3.102E-07 | global batch size:     1 | lm loss: 9.175510E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.223 | TFLOPs: 54.14 |
[default0]:[2023-07-30 22:26:18,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[3.1566506666666667e-07, 3.1566506666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:18,597] [INFO] [timer.py:215:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=2.352130565002082, CurrSamplesPerSec=2.347649749775691, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      290/439453125 | consumed samples:          290 | consumed tokens:       593920 | elapsed time per iteration (ms): 447.1 | learning rate: 3.157E-07 | global batch size:     1 | lm loss: 9.225292E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.237 | TFLOPs: 54.47 |
[default0]:[2023-07-30 22:26:20,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=295, skipped=0, lr=[3.2112640000000003e-07, 3.2112640000000003e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:20,908] [INFO] [timer.py:215:stop] epoch=0/micro_step=295/global_step=295, RunningAvgSamplesPerSec=2.351678641994096, CurrSamplesPerSec=2.3479099574059323, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      295/439453125 | consumed samples:          295 | consumed tokens:       604160 | elapsed time per iteration (ms): 462.3 | learning rate: 3.211E-07 | global batch size:     1 | lm loss: 9.264971E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.163 | TFLOPs: 52.69 |
[default0]:[2023-07-30 22:26:23,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[3.2658773333333335e-07, 3.2658773333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:23,147] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=2.3517127655631893, CurrSamplesPerSec=2.3442919454759674, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      300/439453125 | consumed samples:          300 | consumed tokens:       614400 | elapsed time per iteration (ms): 447.5 | learning rate: 3.266E-07 | global batch size:     1 | lm loss: 9.307765E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.234 | TFLOPs: 54.42 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 300 | lm loss value: 9.208869E+00 | lm loss PPL: 9.985297E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:26:29,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=305, skipped=0, lr=[3.3204906666666666e-07, 3.3204906666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:29,016] [INFO] [timer.py:215:stop] epoch=0/micro_step=305/global_step=305, RunningAvgSamplesPerSec=2.3516257839634718, CurrSamplesPerSec=2.3183427648517396, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      305/439453125 | consumed samples:          305 | consumed tokens:       624640 | elapsed time per iteration (ms): 1173.8 | learning rate: 3.320E-07 | global batch size:     1 | lm loss: 9.304739E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.852 | TFLOPs: 20.75 |
[default0]:[2023-07-30 22:26:31,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[3.375104e-07, 3.375104e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:31,274] [INFO] [timer.py:215:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=2.3516739203663084, CurrSamplesPerSec=2.349989746853741, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      310/439453125 | consumed samples:          310 | consumed tokens:       634880 | elapsed time per iteration (ms): 451.8 | learning rate: 3.375E-07 | global batch size:     1 | lm loss: 9.147787E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.213 | TFLOPs: 53.90 |
[default0]:[2023-07-30 22:26:33,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=315, skipped=0, lr=[3.429717333333334e-07, 3.429717333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:33,538] [INFO] [timer.py:215:stop] epoch=0/micro_step=315/global_step=315, RunningAvgSamplesPerSec=2.3517234359787547, CurrSamplesPerSec=2.3504743758581075, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      315/439453125 | consumed samples:          315 | consumed tokens:       645120 | elapsed time per iteration (ms): 452.5 | learning rate: 3.430E-07 | global batch size:     1 | lm loss: 9.255821E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.210 | TFLOPs: 53.83 |
[default0]:[2023-07-30 22:26:35,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[3.484330666666667e-07, 3.484330666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:35,820] [INFO] [timer.py:215:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=2.351800378165589, CurrSamplesPerSec=2.355921731100631, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      320/439453125 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 456.3 | learning rate: 3.484E-07 | global batch size:     1 | lm loss: 9.209419E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.192 | TFLOPs: 53.38 |
[default0]:[2023-07-30 22:26:38,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=325, skipped=0, lr=[3.538944e-07, 3.538944e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:38,058] [INFO] [timer.py:215:stop] epoch=0/micro_step=325/global_step=325, RunningAvgSamplesPerSec=2.3517216043621794, CurrSamplesPerSec=2.354596519671301, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      325/439453125 | consumed samples:          325 | consumed tokens:       665600 | elapsed time per iteration (ms): 447.8 | learning rate: 3.539E-07 | global batch size:     1 | lm loss: 9.231149E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.233 | TFLOPs: 54.39 |
[default0]:[2023-07-30 22:26:40,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[3.5935573333333334e-07, 3.5935573333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:40,307] [INFO] [timer.py:215:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=2.351676982422272, CurrSamplesPerSec=2.3471360747805807, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      330/439453125 | consumed samples:          330 | consumed tokens:       675840 | elapsed time per iteration (ms): 449.9 | learning rate: 3.594E-07 | global batch size:     1 | lm loss: 9.089160E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.223 | TFLOPs: 54.14 |
[default0]:[2023-07-30 22:26:42,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=335, skipped=0, lr=[3.6481706666666666e-07, 3.6481706666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:42,569] [INFO] [timer.py:215:stop] epoch=0/micro_step=335/global_step=335, RunningAvgSamplesPerSec=2.3516629628215053, CurrSamplesPerSec=2.3521470899476835, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      335/439453125 | consumed samples:          335 | consumed tokens:       686080 | elapsed time per iteration (ms): 452.3 | learning rate: 3.648E-07 | global batch size:     1 | lm loss: 9.170352E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.211 | TFLOPs: 53.85 |
[default0]:[2023-07-30 22:26:44,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[3.7027839999999997e-07, 3.7027839999999997e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:44,804] [INFO] [timer.py:215:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=2.3518009660063868, CurrSamplesPerSec=2.3573504492073885, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      340/439453125 | consumed samples:          340 | consumed tokens:       696320 | elapsed time per iteration (ms): 447.1 | learning rate: 3.703E-07 | global batch size:     1 | lm loss: 9.233064E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.237 | TFLOPs: 54.48 |
[default0]:[2023-07-30 22:26:47,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=345, skipped=0, lr=[3.7573973333333334e-07, 3.7573973333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:47,048] [INFO] [timer.py:215:stop] epoch=0/micro_step=345/global_step=345, RunningAvgSamplesPerSec=2.3517178309749704, CurrSamplesPerSec=2.323578960780675, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      345/439453125 | consumed samples:          345 | consumed tokens:       706560 | elapsed time per iteration (ms): 448.7 | learning rate: 3.757E-07 | global batch size:     1 | lm loss: 9.275185E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.229 | TFLOPs: 54.29 |
[default0]:[2023-07-30 22:26:49,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[3.8120106666666665e-07, 3.8120106666666665e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:49,283] [INFO] [timer.py:215:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=2.351770137888422, CurrSamplesPerSec=2.360981002629314, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      350/439453125 | consumed samples:          350 | consumed tokens:       716800 | elapsed time per iteration (ms): 447.3 | learning rate: 3.812E-07 | global batch size:     1 | lm loss: 9.416413E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.236 | TFLOPs: 54.46 |
[default0]:[2023-07-30 22:26:51,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=355, skipped=0, lr=[3.866624e-07, 3.866624e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:51,543] [INFO] [timer.py:215:stop] epoch=0/micro_step=355/global_step=355, RunningAvgSamplesPerSec=2.3518665680235307, CurrSamplesPerSec=2.3514244596572684, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      355/439453125 | consumed samples:          355 | consumed tokens:       727040 | elapsed time per iteration (ms): 451.8 | learning rate: 3.867E-07 | global batch size:     1 | lm loss: 9.234187E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.213 | TFLOPs: 53.90 |
[default0]:[2023-07-30 22:26:53,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[3.9212373333333333e-07, 3.9212373333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:53,863] [INFO] [timer.py:215:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=2.3509524688671273, CurrSamplesPerSec=2.350432226270705, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      360/439453125 | consumed samples:          360 | consumed tokens:       737280 | elapsed time per iteration (ms): 464.0 | learning rate: 3.921E-07 | global batch size:     1 | lm loss: 9.216299E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.155 | TFLOPs: 52.50 |
[default0]:[2023-07-30 22:26:56,105] [INFO] [logging.py:96:log_dist] [Rank 0] step=365, skipped=0, lr=[3.975850666666667e-07, 3.975850666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:56,105] [INFO] [timer.py:215:stop] epoch=0/micro_step=365/global_step=365, RunningAvgSamplesPerSec=2.351017938330418, CurrSamplesPerSec=2.357469697693568, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      365/439453125 | consumed samples:          365 | consumed tokens:       747520 | elapsed time per iteration (ms): 448.4 | learning rate: 3.976E-07 | global batch size:     1 | lm loss: 9.293594E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.230 | TFLOPs: 54.32 |
[default0]:[2023-07-30 22:26:58,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[4.030464e-07, 4.030464e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:58,361] [INFO] [timer.py:215:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=2.3509957726902084, CurrSamplesPerSec=2.3224621852043192, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      370/439453125 | consumed samples:          370 | consumed tokens:       757760 | elapsed time per iteration (ms): 451.5 | learning rate: 4.030E-07 | global batch size:     1 | lm loss: 9.157961E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.215 | TFLOPs: 53.95 |
[default0]:[2023-07-30 22:27:00,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=375, skipped=0, lr=[4.085077333333334e-07, 4.085077333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:00,605] [INFO] [timer.py:215:stop] epoch=0/micro_step=375/global_step=375, RunningAvgSamplesPerSec=2.3509523264104595, CurrSamplesPerSec=2.3416612418949443, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      375/439453125 | consumed samples:          375 | consumed tokens:       768000 | elapsed time per iteration (ms): 448.3 | learning rate: 4.085E-07 | global batch size:     1 | lm loss: 8.989035E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.231 | TFLOPs: 54.33 |
[default0]:[2023-07-30 22:27:02,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[4.139690666666667e-07, 4.139690666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:02,850] [INFO] [timer.py:215:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=2.3509418058480334, CurrSamplesPerSec=2.3289875746357542, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      380/439453125 | consumed samples:          380 | consumed tokens:       778240 | elapsed time per iteration (ms): 449.3 | learning rate: 4.140E-07 | global batch size:     1 | lm loss: 9.168766E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.226 | TFLOPs: 54.21 |
[default0]:[2023-07-30 22:27:05,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=385, skipped=0, lr=[4.194304e-07, 4.194304e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:05,085] [INFO] [timer.py:215:stop] epoch=0/micro_step=385/global_step=385, RunningAvgSamplesPerSec=2.351044964344487, CurrSamplesPerSec=2.3530417620615505, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      385/439453125 | consumed samples:          385 | consumed tokens:       788480 | elapsed time per iteration (ms): 446.8 | learning rate: 4.194E-07 | global batch size:     1 | lm loss: 9.011583E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.238 | TFLOPs: 54.52 |
[default0]:[2023-07-30 22:27:07,345] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[4.248917333333334e-07, 4.248917333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:07,347] [INFO] [timer.py:215:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=2.351096495678138, CurrSamplesPerSec=2.3451701606833937, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      390/439453125 | consumed samples:          390 | consumed tokens:       798720 | elapsed time per iteration (ms): 452.3 | learning rate: 4.249E-07 | global batch size:     1 | lm loss: 9.016284E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.211 | TFLOPs: 53.85 |
[default0]:[2023-07-30 22:27:09,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=395, skipped=0, lr=[4.303530666666667e-07, 4.303530666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:09,624] [INFO] [timer.py:215:stop] epoch=0/micro_step=395/global_step=395, RunningAvgSamplesPerSec=2.350793248115267, CurrSamplesPerSec=2.3614183247193417, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      395/439453125 | consumed samples:          395 | consumed tokens:       808960 | elapsed time per iteration (ms): 455.4 | learning rate: 4.304E-07 | global batch size:     1 | lm loss: 9.142577E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.196 | TFLOPs: 53.48 |
[default0]:[2023-07-30 22:27:11,888] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[4.3581440000000006e-07, 4.3581440000000006e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:11,889] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=2.3509171735550027, CurrSamplesPerSec=2.3581814644853476, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      400/439453125 | consumed samples:          400 | consumed tokens:       819200 | elapsed time per iteration (ms): 453.2 | learning rate: 4.358E-07 | global batch size:     1 | lm loss: 9.082561E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.207 | TFLOPs: 53.74 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 400 | lm loss value: 9.141830E+00 | lm loss PPL: 9.337842E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:27:18,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=405, skipped=0, lr=[4.412757333333334e-07, 4.412757333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:18,964] [INFO] [timer.py:215:stop] epoch=0/micro_step=405/global_step=405, RunningAvgSamplesPerSec=2.350889798147839, CurrSamplesPerSec=2.3223464519233272, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      405/439453125 | consumed samples:          405 | consumed tokens:       829440 | elapsed time per iteration (ms): 1415.0 | learning rate: 4.413E-07 | global batch size:     1 | lm loss: 8.987225E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.707 | TFLOPs: 17.21 |
[default0]:[2023-07-30 22:27:21,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[4.4673706666666664e-07, 4.4673706666666664e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:21,213] [INFO] [timer.py:215:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=2.3510168638618554, CurrSamplesPerSec=2.348395043756264, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      410/439453125 | consumed samples:          410 | consumed tokens:       839680 | elapsed time per iteration (ms): 449.8 | learning rate: 4.467E-07 | global batch size:     1 | lm loss: 9.229361E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.223 | TFLOPs: 54.15 |
[default0]:[2023-07-30 22:27:23,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=415, skipped=0, lr=[4.521984e-07, 4.521984e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:23,460] [INFO] [timer.py:215:stop] epoch=0/micro_step=415/global_step=415, RunningAvgSamplesPerSec=2.3509966541371794, CurrSamplesPerSec=2.323203151439822, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      415/439453125 | consumed samples:          415 | consumed tokens:       849920 | elapsed time per iteration (ms): 449.5 | learning rate: 4.522E-07 | global batch size:     1 | lm loss: 9.282985E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.225 | TFLOPs: 54.19 |
[default0]:[2023-07-30 22:27:25,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[4.576597333333333e-07, 4.576597333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:25,706] [INFO] [timer.py:215:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=2.3511263491908188, CurrSamplesPerSec=2.353699344277934, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      420/439453125 | consumed samples:          420 | consumed tokens:       860160 | elapsed time per iteration (ms): 449.1 | learning rate: 4.577E-07 | global batch size:     1 | lm loss: 9.052524E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.226 | TFLOPs: 54.23 |
[default0]:[2023-07-30 22:27:27,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=425, skipped=0, lr=[4.631210666666667e-07, 4.631210666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:27,987] [INFO] [timer.py:215:stop] epoch=0/micro_step=425/global_step=425, RunningAvgSamplesPerSec=2.3511560601030874, CurrSamplesPerSec=2.346474277926838, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      425/439453125 | consumed samples:          425 | consumed tokens:       870400 | elapsed time per iteration (ms): 456.3 | learning rate: 4.631E-07 | global batch size:     1 | lm loss: 8.976073E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.191 | TFLOPs: 53.37 |
[default0]:[2023-07-30 22:27:30,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[4.685824e-07, 4.685824e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:30,226] [INFO] [timer.py:215:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=2.3512468278585166, CurrSamplesPerSec=2.3565279633410605, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      430/439453125 | consumed samples:          430 | consumed tokens:       880640 | elapsed time per iteration (ms): 447.6 | learning rate: 4.686E-07 | global batch size:     1 | lm loss: 9.075911E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.234 | TFLOPs: 54.41 |
[default0]:[2023-07-30 22:27:32,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=435, skipped=0, lr=[4.7404373333333337e-07, 4.7404373333333337e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:32,455] [INFO] [timer.py:215:stop] epoch=0/micro_step=435/global_step=435, RunningAvgSamplesPerSec=2.3512830973574026, CurrSamplesPerSec=2.348420026528392, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      435/439453125 | consumed samples:          435 | consumed tokens:       890880 | elapsed time per iteration (ms): 445.8 | learning rate: 4.740E-07 | global batch size:     1 | lm loss: 9.285817E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.243 | TFLOPs: 54.63 |
[default0]:[2023-07-30 22:27:34,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[4.795050666666667e-07, 4.795050666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:34,713] [INFO] [timer.py:215:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=2.3513449512051534, CurrSamplesPerSec=2.3654522372883076, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      440/439453125 | consumed samples:          440 | consumed tokens:       901120 | elapsed time per iteration (ms): 451.6 | learning rate: 4.795E-07 | global batch size:     1 | lm loss: 8.981573E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.214 | TFLOPs: 53.93 |
[default0]:[2023-07-30 22:27:36,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=445, skipped=0, lr=[4.849664e-07, 4.849664e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:36,948] [INFO] [timer.py:215:stop] epoch=0/micro_step=445/global_step=445, RunningAvgSamplesPerSec=2.351340388413255, CurrSamplesPerSec=2.3402384147345106, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      445/439453125 | consumed samples:          445 | consumed tokens:       911360 | elapsed time per iteration (ms): 447.2 | learning rate: 4.850E-07 | global batch size:     1 | lm loss: 9.046446E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.236 | TFLOPs: 54.47 |
[default0]:[2023-07-30 22:27:39,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[4.904277333333333e-07, 4.904277333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:39,179] [INFO] [timer.py:215:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=2.3513184413469017, CurrSamplesPerSec=2.329260477302741, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      450/439453125 | consumed samples:          450 | consumed tokens:       921600 | elapsed time per iteration (ms): 446.1 | learning rate: 4.904E-07 | global batch size:     1 | lm loss: 8.965501E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.242 | TFLOPs: 54.60 |
[default0]:[2023-07-30 22:27:41,406] [INFO] [logging.py:96:log_dist] [Rank 0] step=455, skipped=0, lr=[4.958890666666667e-07, 4.958890666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:41,407] [INFO] [timer.py:215:stop] epoch=0/micro_step=455/global_step=455, RunningAvgSamplesPerSec=2.3514911924462134, CurrSamplesPerSec=2.3632770879594855, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      455/439453125 | consumed samples:          455 | consumed tokens:       931840 | elapsed time per iteration (ms): 445.4 | learning rate: 4.959E-07 | global batch size:     1 | lm loss: 9.027013E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.245 | TFLOPs: 54.69 |
[default0]:[2023-07-30 22:27:43,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[5.013504e-07, 5.013504e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:43,659] [INFO] [timer.py:215:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=2.351494110923585, CurrSamplesPerSec=2.3402710589600666, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      460/439453125 | consumed samples:          460 | consumed tokens:       942080 | elapsed time per iteration (ms): 450.6 | learning rate: 5.014E-07 | global batch size:     1 | lm loss: 9.173840E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.219 | TFLOPs: 54.06 |
[default0]:[2023-07-30 22:27:45,982] [INFO] [logging.py:96:log_dist] [Rank 0] step=465, skipped=0, lr=[5.068117333333334e-07, 5.068117333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:45,983] [INFO] [timer.py:215:stop] epoch=0/micro_step=465/global_step=465, RunningAvgSamplesPerSec=2.3515190680734235, CurrSamplesPerSec=2.333159221290843, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      465/439453125 | consumed samples:          465 | consumed tokens:       952320 | elapsed time per iteration (ms): 465.0 | learning rate: 5.068E-07 | global batch size:     1 | lm loss: 9.228680E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.150 | TFLOPs: 52.37 |
[default0]:[2023-07-30 22:27:48,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[5.122730666666667e-07, 5.122730666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:48,243] [INFO] [timer.py:215:stop] epoch=0/micro_step=470/global_step=470, RunningAvgSamplesPerSec=2.3514521744978296, CurrSamplesPerSec=2.334390244581173, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      470/439453125 | consumed samples:          470 | consumed tokens:       962560 | elapsed time per iteration (ms): 451.7 | learning rate: 5.123E-07 | global batch size:     1 | lm loss: 8.979321E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.214 | TFLOPs: 53.92 |
[default0]:[2023-07-30 22:27:50,512] [INFO] [logging.py:96:log_dist] [Rank 0] step=475, skipped=0, lr=[5.177344000000001e-07, 5.177344000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:50,513] [INFO] [timer.py:215:stop] epoch=0/micro_step=475/global_step=475, RunningAvgSamplesPerSec=2.3511459879202765, CurrSamplesPerSec=2.2362346116730025, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      475/439453125 | consumed samples:          475 | consumed tokens:       972800 | elapsed time per iteration (ms): 453.9 | learning rate: 5.177E-07 | global batch size:     1 | lm loss: 9.063705E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.203 | TFLOPs: 53.66 |
[default0]:[2023-07-30 22:27:52,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[5.231957333333334e-07, 5.231957333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:52,748] [INFO] [timer.py:215:stop] epoch=0/micro_step=480/global_step=480, RunningAvgSamplesPerSec=2.3511947209486057, CurrSamplesPerSec=2.355720605051683, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      480/439453125 | consumed samples:          480 | consumed tokens:       983040 | elapsed time per iteration (ms): 447.0 | learning rate: 5.232E-07 | global batch size:     1 | lm loss: 9.032301E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.237 | TFLOPs: 54.49 |
[default0]:[2023-07-30 22:27:55,042] [INFO] [logging.py:96:log_dist] [Rank 0] step=485, skipped=0, lr=[5.286570666666667e-07, 5.286570666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:55,042] [INFO] [timer.py:215:stop] epoch=0/micro_step=485/global_step=485, RunningAvgSamplesPerSec=2.351223890588669, CurrSamplesPerSec=2.3654909250463305, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      485/439453125 | consumed samples:          485 | consumed tokens:       993280 | elapsed time per iteration (ms): 459.0 | learning rate: 5.287E-07 | global batch size:     1 | lm loss: 8.680569E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.179 | TFLOPs: 53.07 |
[default0]:[2023-07-30 22:27:57,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[5.341184e-07, 5.341184e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:57,284] [INFO] [timer.py:215:stop] epoch=0/micro_step=490/global_step=490, RunningAvgSamplesPerSec=2.35124603491508, CurrSamplesPerSec=2.334809972267095, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      490/439453125 | consumed samples:          490 | consumed tokens:      1003520 | elapsed time per iteration (ms): 448.1 | learning rate: 5.341E-07 | global batch size:     1 | lm loss: 8.965179E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.231 | TFLOPs: 54.35 |
[default0]:[2023-07-30 22:27:59,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=495, skipped=0, lr=[5.395797333333334e-07, 5.395797333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:59,544] [INFO] [timer.py:215:stop] epoch=0/micro_step=495/global_step=495, RunningAvgSamplesPerSec=2.351290178278006, CurrSamplesPerSec=2.3412887095432695, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      495/439453125 | consumed samples:          495 | consumed tokens:      1013760 | elapsed time per iteration (ms): 453.5 | learning rate: 5.396E-07 | global batch size:     1 | lm loss: 8.823873E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.205 | TFLOPs: 53.70 |
[default0]:[2023-07-30 22:28:01,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[5.450410666666667e-07, 5.450410666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:01,780] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=500, RunningAvgSamplesPerSec=2.351362722614155, CurrSamplesPerSec=2.3453144081557795, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      500/439453125 | consumed samples:          500 | consumed tokens:      1024000 | elapsed time per iteration (ms): 445.6 | learning rate: 5.450E-07 | global batch size:     1 | lm loss: 8.941534E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.244 | TFLOPs: 54.65 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 500 | lm loss value: 8.854793E+00 | lm loss PPL: 7.007895E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:28:09,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=505, skipped=0, lr=[5.505024e-07, 5.505024e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:09,311] [INFO] [timer.py:215:stop] epoch=0/micro_step=505/global_step=505, RunningAvgSamplesPerSec=2.351377008378016, CurrSamplesPerSec=2.3461277713513478, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      505/439453125 | consumed samples:          505 | consumed tokens:      1034240 | elapsed time per iteration (ms): 1506.4 | learning rate: 5.505E-07 | global batch size:     1 | lm loss: 9.066156E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.664 | TFLOPs: 16.17 |
[default0]:[2023-07-30 22:28:11,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[5.559637333333333e-07, 5.559637333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:11,557] [INFO] [timer.py:215:stop] epoch=0/micro_step=510/global_step=510, RunningAvgSamplesPerSec=2.351532162548952, CurrSamplesPerSec=2.358723862043213, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      510/439453125 | consumed samples:          510 | consumed tokens:      1044480 | elapsed time per iteration (ms): 449.3 | learning rate: 5.560E-07 | global batch size:     1 | lm loss: 8.790094E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.226 | TFLOPs: 54.21 |
[default0]:[2023-07-30 22:28:13,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=515, skipped=0, lr=[5.614250666666667e-07, 5.614250666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:13,805] [INFO] [timer.py:215:stop] epoch=0/micro_step=515/global_step=515, RunningAvgSamplesPerSec=2.3515619422554885, CurrSamplesPerSec=2.357312027265053, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      515/439453125 | consumed samples:          515 | consumed tokens:      1054720 | elapsed time per iteration (ms): 449.5 | learning rate: 5.614E-07 | global batch size:     1 | lm loss: 8.952905E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.225 | TFLOPs: 54.18 |
[default0]:[2023-07-30 22:28:16,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[5.668864e-07, 5.668864e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:16,094] [INFO] [timer.py:215:stop] epoch=0/micro_step=520/global_step=520, RunningAvgSamplesPerSec=2.351577265972622, CurrSamplesPerSec=2.3584307507183304, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      520/439453125 | consumed samples:          520 | consumed tokens:      1064960 | elapsed time per iteration (ms): 457.7 | learning rate: 5.669E-07 | global batch size:     1 | lm loss: 8.936292E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.185 | TFLOPs: 53.22 |
[default0]:[2023-07-30 22:28:18,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=525, skipped=0, lr=[5.723477333333333e-07, 5.723477333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:18,350] [INFO] [timer.py:215:stop] epoch=0/micro_step=525/global_step=525, RunningAvgSamplesPerSec=2.3515558875183538, CurrSamplesPerSec=2.3312982733857734, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      525/439453125 | consumed samples:          525 | consumed tokens:      1075200 | elapsed time per iteration (ms): 451.4 | learning rate: 5.723E-07 | global batch size:     1 | lm loss: 8.798553E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.215 | TFLOPs: 53.96 |
[default0]:[2023-07-30 22:28:20,662] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[5.778090666666667e-07, 5.778090666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:20,663] [INFO] [timer.py:215:stop] epoch=0/micro_step=530/global_step=530, RunningAvgSamplesPerSec=2.3515164021297688, CurrSamplesPerSec=2.34874485094335, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      530/439453125 | consumed samples:          530 | consumed tokens:      1085440 | elapsed time per iteration (ms): 462.5 | learning rate: 5.778E-07 | global batch size:     1 | lm loss: 8.856497E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.162 | TFLOPs: 52.66 |
[default0]:[2023-07-30 22:28:22,931] [INFO] [logging.py:96:log_dist] [Rank 0] step=535, skipped=0, lr=[5.832704000000001e-07, 5.832704000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:22,932] [INFO] [timer.py:215:stop] epoch=0/micro_step=535/global_step=535, RunningAvgSamplesPerSec=2.3515592369253477, CurrSamplesPerSec=2.357108014485554, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      535/439453125 | consumed samples:          535 | consumed tokens:      1095680 | elapsed time per iteration (ms): 453.9 | learning rate: 5.833E-07 | global batch size:     1 | lm loss: 8.691628E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.203 | TFLOPs: 53.66 |
[default0]: iteration      540/439453125 | consumed samples:          540 | consumed tokens:      1105920 | elapsed time per iteration (ms): 450.0 | learning rate: 5.887E-07 | global batch size:     1 | lm loss: 9.116637E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.222 | TFLOPs: 54.12 |
[default0]:[2023-07-30 22:28:27,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=545, skipped=0, lr=[5.941930666666667e-07, 5.941930666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:27,431] [INFO] [timer.py:215:stop] epoch=0/micro_step=545/global_step=545, RunningAvgSamplesPerSec=2.351598276901166, CurrSamplesPerSec=2.344419049343817, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      545/439453125 | consumed samples:          545 | consumed tokens:      1116160 | elapsed time per iteration (ms): 449.8 | learning rate: 5.942E-07 | global batch size:     1 | lm loss: 9.040608E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.223 | TFLOPs: 54.15 |
[default0]:[2023-07-30 22:28:29,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[5.996544e-07, 5.996544e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:29,690] [INFO] [timer.py:215:stop] epoch=0/micro_step=550/global_step=550, RunningAvgSamplesPerSec=2.351541793350191, CurrSamplesPerSec=2.30995931153442, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      550/439453125 | consumed samples:          550 | consumed tokens:      1126400 | elapsed time per iteration (ms): 451.8 | learning rate: 5.997E-07 | global batch size:     1 | lm loss: 8.916797E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.214 | TFLOPs: 53.91 |
[default0]:[2023-07-30 22:28:31,943] [INFO] [logging.py:96:log_dist] [Rank 0] step=555, skipped=0, lr=[6.051157333333333e-07, 6.051157333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:31,945] [INFO] [timer.py:215:stop] epoch=0/micro_step=555/global_step=555, RunningAvgSamplesPerSec=2.3515577357627095, CurrSamplesPerSec=2.3470874777913013, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      555/439453125 | consumed samples:          555 | consumed tokens:      1136640 | elapsed time per iteration (ms): 450.8 | learning rate: 6.051E-07 | global batch size:     1 | lm loss: 8.882161E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.218 | TFLOPs: 54.03 |
[default0]:[2023-07-30 22:28:34,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[6.105770666666668e-07, 6.105770666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:34,176] [INFO] [timer.py:215:stop] epoch=0/micro_step=560/global_step=560, RunningAvgSamplesPerSec=2.351580580272827, CurrSamplesPerSec=2.350662750300257, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      560/439453125 | consumed samples:          560 | consumed tokens:      1146880 | elapsed time per iteration (ms): 447.8 | learning rate: 6.106E-07 | global batch size:     1 | lm loss: 8.717286E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.233 | TFLOPs: 54.39 |
[default0]:[2023-07-30 22:28:36,418] [INFO] [logging.py:96:log_dist] [Rank 0] step=565, skipped=0, lr=[6.160384000000001e-07, 6.160384000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:36,421] [INFO] [timer.py:215:stop] epoch=0/micro_step=565/global_step=565, RunningAvgSamplesPerSec=2.3516279300103986, CurrSamplesPerSec=2.359155040486331, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      565/439453125 | consumed samples:          565 | consumed tokens:      1157120 | elapsed time per iteration (ms): 447.7 | learning rate: 6.160E-07 | global batch size:     1 | lm loss: 9.148701E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.234 | TFLOPs: 54.40 |
[default0]:[2023-07-30 22:28:38,662] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[6.214997333333334e-07, 6.214997333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:38,664] [INFO] [timer.py:215:stop] epoch=0/micro_step=570/global_step=570, RunningAvgSamplesPerSec=2.3516439895555923, CurrSamplesPerSec=2.3371218263221976, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      570/439453125 | consumed samples:          570 | consumed tokens:      1167360 | elapsed time per iteration (ms): 448.5 | learning rate: 6.215E-07 | global batch size:     1 | lm loss: 8.885434E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.230 | TFLOPs: 54.31 |
[default0]:[2023-07-30 22:28:40,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=575, skipped=0, lr=[6.269610666666667e-07, 6.269610666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:40,943] [INFO] [timer.py:215:stop] epoch=0/micro_step=575/global_step=575, RunningAvgSamplesPerSec=2.3516106394010827, CurrSamplesPerSec=2.3470874777913013, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      575/439453125 | consumed samples:          575 | consumed tokens:      1177600 | elapsed time per iteration (ms): 455.5 | learning rate: 6.270E-07 | global batch size:     1 | lm loss: 9.258933E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.195 | TFLOPs: 53.47 |
[default0]:[2023-07-30 22:28:43,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[6.324224e-07, 6.324224e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:43,185] [INFO] [timer.py:215:stop] epoch=0/micro_step=580/global_step=580, RunningAvgSamplesPerSec=2.3516293032294313, CurrSamplesPerSec=2.3475301787441345, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      580/439453125 | consumed samples:          580 | consumed tokens:      1187840 | elapsed time per iteration (ms): 448.4 | learning rate: 6.324E-07 | global batch size:     1 | lm loss: 8.813595E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.230 | TFLOPs: 54.32 |
[default0]:[2023-07-30 22:28:45,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=585, skipped=0, lr=[6.378837333333333e-07, 6.378837333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:45,438] [INFO] [timer.py:215:stop] epoch=0/micro_step=585/global_step=585, RunningAvgSamplesPerSec=2.3516389943574976, CurrSamplesPerSec=2.3473449333987753, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      585/439453125 | consumed samples:          585 | consumed tokens:      1198080 | elapsed time per iteration (ms): 450.9 | learning rate: 6.379E-07 | global batch size:     1 | lm loss: 8.974319E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.218 | TFLOPs: 54.02 |
[default0]:[2023-07-30 22:28:47,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[6.433450666666666e-07, 6.433450666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:47,675] [INFO] [timer.py:215:stop] epoch=0/micro_step=590/global_step=590, RunningAvgSamplesPerSec=2.351603180867743, CurrSamplesPerSec=2.3392203721622833, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      590/439453125 | consumed samples:          590 | consumed tokens:      1208320 | elapsed time per iteration (ms): 447.2 | learning rate: 6.433E-07 | global batch size:     1 | lm loss: 8.888208E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.236 | TFLOPs: 54.47 |
[default0]:[2023-07-30 22:28:49,928] [INFO] [logging.py:96:log_dist] [Rank 0] step=595, skipped=0, lr=[6.488064000000001e-07, 6.488064000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:49,930] [INFO] [timer.py:215:stop] epoch=0/micro_step=595/global_step=595, RunningAvgSamplesPerSec=2.3515995327207837, CurrSamplesPerSec=2.3595770846202053, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      595/439453125 | consumed samples:          595 | consumed tokens:      1218560 | elapsed time per iteration (ms): 451.2 | learning rate: 6.488E-07 | global batch size:     1 | lm loss: 8.886542E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.216 | TFLOPs: 53.98 |
[default0]:[2023-07-30 22:28:52,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[6.542677333333334e-07, 6.542677333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:52,200] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=600, RunningAvgSamplesPerSec=2.3516304155515337, CurrSamplesPerSec=2.3412168309790755, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      600/439453125 | consumed samples:          600 | consumed tokens:      1228800 | elapsed time per iteration (ms): 453.8 | learning rate: 6.543E-07 | global batch size:     1 | lm loss: 8.919379E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.204 | TFLOPs: 53.67 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 600 | lm loss value: 8.871549E+00 | lm loss PPL: 7.126309E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:28:59,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=605, skipped=0, lr=[6.597290666666667e-07, 6.597290666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:59,455] [INFO] [timer.py:215:stop] epoch=0/micro_step=605/global_step=605, RunningAvgSamplesPerSec=2.3516680662810687, CurrSamplesPerSec=2.3500568981833485, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      605/439453125 | consumed samples:          605 | consumed tokens:      1239040 | elapsed time per iteration (ms): 1451.3 | learning rate: 6.597E-07 | global batch size:     1 | lm loss: 8.663087E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.689 | TFLOPs: 16.78 |
[default0]:[2023-07-30 22:29:01,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[6.651904e-07, 6.651904e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:01,702] [INFO] [timer.py:215:stop] epoch=0/micro_step=610/global_step=610, RunningAvgSamplesPerSec=2.351718774919821, CurrSamplesPerSec=2.3376115844391228, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      610/439453125 | consumed samples:          610 | consumed tokens:      1249280 | elapsed time per iteration (ms): 455.3 | learning rate: 6.652E-07 | global batch size:     1 | lm loss: 8.922058E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.197 | TFLOPs: 53.50 |
[default0]:[2023-07-30 22:29:03,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=615, skipped=0, lr=[6.706517333333333e-07, 6.706517333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:03,978] [INFO] [timer.py:215:stop] epoch=0/micro_step=615/global_step=615, RunningAvgSamplesPerSec=2.3517737695927954, CurrSamplesPerSec=2.3623559892423156, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      615/439453125 | consumed samples:          615 | consumed tokens:      1259520 | elapsed time per iteration (ms): 448.8 | learning rate: 6.707E-07 | global batch size:     1 | lm loss: 9.136212E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.228 | TFLOPs: 54.27 |
[default0]:[2023-07-30 22:29:06,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[6.761130666666667e-07, 6.761130666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:06,197] [INFO] [timer.py:215:stop] epoch=0/micro_step=620/global_step=620, RunningAvgSamplesPerSec=2.3518226340510977, CurrSamplesPerSec=2.3501938461210883, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      620/439453125 | consumed samples:          620 | consumed tokens:      1269760 | elapsed time per iteration (ms): 444.2 | learning rate: 6.761E-07 | global batch size:     1 | lm loss: 8.979086E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.251 | TFLOPs: 54.83 |
[default0]:[2023-07-30 22:29:08,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=625, skipped=0, lr=[6.815744000000001e-07, 6.815744000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:08,448] [INFO] [timer.py:215:stop] epoch=0/micro_step=625/global_step=625, RunningAvgSamplesPerSec=2.3518283915824902, CurrSamplesPerSec=2.3303449593969323, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      625/439453125 | consumed samples:          625 | consumed tokens:      1280000 | elapsed time per iteration (ms): 450.4 | learning rate: 6.816E-07 | global batch size:     1 | lm loss: 8.695578E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.220 | TFLOPs: 54.08 |
[default0]:[2023-07-30 22:29:10,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[6.870357333333333e-07, 6.870357333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:10,713] [INFO] [timer.py:215:stop] epoch=0/micro_step=630/global_step=630, RunningAvgSamplesPerSec=2.3518428328785888, CurrSamplesPerSec=2.3532820406649533, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      630/439453125 | consumed samples:          630 | consumed tokens:      1290240 | elapsed time per iteration (ms): 453.1 | learning rate: 6.870E-07 | global batch size:     1 | lm loss: 8.723802E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.207 | TFLOPs: 53.76 |
[default0]:[2023-07-30 22:29:12,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=635, skipped=0, lr=[6.924970666666667e-07, 6.924970666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:12,941] [INFO] [timer.py:215:stop] epoch=0/micro_step=635/global_step=635, RunningAvgSamplesPerSec=2.3518650170578206, CurrSamplesPerSec=2.3546864070224527, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      635/439453125 | consumed samples:          635 | consumed tokens:      1300480 | elapsed time per iteration (ms): 445.1 | learning rate: 6.925E-07 | global batch size:     1 | lm loss: 8.905809E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.247 | TFLOPs: 54.72 |
[default0]:[2023-07-30 22:29:15,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[6.979584e-07, 6.979584e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:15,191] [INFO] [timer.py:215:stop] epoch=0/micro_step=640/global_step=640, RunningAvgSamplesPerSec=2.351888873446927, CurrSamplesPerSec=2.354849014813588, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      640/439453125 | consumed samples:          640 | consumed tokens:      1310720 | elapsed time per iteration (ms): 449.9 | learning rate: 6.980E-07 | global batch size:     1 | lm loss: 8.863191E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.223 | TFLOPs: 54.14 |
[default0]:[2023-07-30 22:29:17,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=645, skipped=0, lr=[7.034197333333333e-07, 7.034197333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:17,435] [INFO] [timer.py:215:stop] epoch=0/micro_step=645/global_step=645, RunningAvgSamplesPerSec=2.3518851383757076, CurrSamplesPerSec=2.3372689926955887, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      645/439453125 | consumed samples:          645 | consumed tokens:      1320960 | elapsed time per iteration (ms): 449.2 | learning rate: 7.034E-07 | global batch size:     1 | lm loss: 8.453261E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.226 | TFLOPs: 54.22 |
[default0]:[2023-07-30 22:29:19,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=0, lr=[7.088810666666666e-07, 7.088810666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:19,691] [INFO] [timer.py:215:stop] epoch=0/micro_step=650/global_step=650, RunningAvgSamplesPerSec=2.351821002340528, CurrSamplesPerSec=2.314707925892773, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      650/439453125 | consumed samples:          650 | consumed tokens:      1331200 | elapsed time per iteration (ms): 451.0 | learning rate: 7.089E-07 | global batch size:     1 | lm loss: 8.837932E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.217 | TFLOPs: 54.01 |
[default0]:[2023-07-30 22:29:21,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=655, skipped=0, lr=[7.143424e-07, 7.143424e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:21,957] [INFO] [timer.py:215:stop] epoch=0/micro_step=655/global_step=655, RunningAvgSamplesPerSec=2.3518276831786067, CurrSamplesPerSec=2.34779561664631, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      655/439453125 | consumed samples:          655 | consumed tokens:      1341440 | elapsed time per iteration (ms): 453.1 | learning rate: 7.143E-07 | global batch size:     1 | lm loss: 9.007778E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.207 | TFLOPs: 53.76 |
[default0]:[2023-07-30 22:29:24,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[7.198037333333334e-07, 7.198037333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:24,195] [INFO] [timer.py:215:stop] epoch=0/micro_step=660/global_step=660, RunningAvgSamplesPerSec=2.351820670535761, CurrSamplesPerSec=2.3463115120920826, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      660/439453125 | consumed samples:          660 | consumed tokens:      1351680 | elapsed time per iteration (ms): 447.6 | learning rate: 7.198E-07 | global batch size:     1 | lm loss: 8.765342E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.234 | TFLOPs: 54.42 |
[default0]:[2023-07-30 22:29:26,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=665, skipped=0, lr=[7.252650666666667e-07, 7.252650666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:26,424] [INFO] [timer.py:215:stop] epoch=0/micro_step=665/global_step=665, RunningAvgSamplesPerSec=2.3518767686145554, CurrSamplesPerSec=2.3648560722328282, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      665/439453125 | consumed samples:          665 | consumed tokens:      1361920 | elapsed time per iteration (ms): 445.8 | learning rate: 7.253E-07 | global batch size:     1 | lm loss: 9.029831E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.243 | TFLOPs: 54.63 |
[default0]:[2023-07-30 22:29:28,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[7.307264e-07, 7.307264e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:28,693] [INFO] [timer.py:215:stop] epoch=0/micro_step=670/global_step=670, RunningAvgSamplesPerSec=2.35187661419463, CurrSamplesPerSec=2.3313578814084073, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      670/439453125 | consumed samples:          670 | consumed tokens:      1372160 | elapsed time per iteration (ms): 454.0 | learning rate: 7.307E-07 | global batch size:     1 | lm loss: 8.503636E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.203 | TFLOPs: 53.65 |
[default0]:[2023-07-30 22:29:30,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=675, skipped=0, lr=[7.361877333333333e-07, 7.361877333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:30,937] [INFO] [timer.py:215:stop] epoch=0/micro_step=675/global_step=675, RunningAvgSamplesPerSec=2.3519208817516066, CurrSamplesPerSec=2.3430334441826988, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      675/439453125 | consumed samples:          675 | consumed tokens:      1382400 | elapsed time per iteration (ms): 448.6 | learning rate: 7.362E-07 | global batch size:     1 | lm loss: 8.769307E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.229 | TFLOPs: 54.29 |
[default0]:[2023-07-30 22:29:33,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[7.416490666666667e-07, 7.416490666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:33,164] [INFO] [timer.py:215:stop] epoch=0/micro_step=680/global_step=680, RunningAvgSamplesPerSec=2.351980486014171, CurrSamplesPerSec=2.3623466754379936, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      680/439453125 | consumed samples:          680 | consumed tokens:      1392640 | elapsed time per iteration (ms): 445.2 | learning rate: 7.416E-07 | global batch size:     1 | lm loss: 8.801396E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.246 | TFLOPs: 54.70 |
[default0]:[2023-07-30 22:29:35,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=685, skipped=0, lr=[7.471104e-07, 7.471104e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:35,423] [INFO] [timer.py:215:stop] epoch=0/micro_step=685/global_step=685, RunningAvgSamplesPerSec=2.3519716585108634, CurrSamplesPerSec=2.3457341380806866, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      685/439453125 | consumed samples:          685 | consumed tokens:      1402880 | elapsed time per iteration (ms): 452.1 | learning rate: 7.471E-07 | global batch size:     1 | lm loss: 8.456219E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.212 | TFLOPs: 53.88 |
[default0]:[2023-07-30 22:29:37,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[7.525717333333334e-07, 7.525717333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:37,696] [INFO] [timer.py:215:stop] epoch=0/micro_step=690/global_step=690, RunningAvgSamplesPerSec=2.3520229847019896, CurrSamplesPerSec=2.3623253870181773, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      690/439453125 | consumed samples:          690 | consumed tokens:      1413120 | elapsed time per iteration (ms): 454.4 | learning rate: 7.526E-07 | global batch size:     1 | lm loss: 8.594158E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.201 | TFLOPs: 53.60 |
[default0]:[2023-07-30 22:29:39,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=695, skipped=0, lr=[7.580330666666667e-07, 7.580330666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:39,926] [INFO] [timer.py:215:stop] epoch=0/micro_step=695/global_step=695, RunningAvgSamplesPerSec=2.3519953910335345, CurrSamplesPerSec=2.319673654310719, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      695/439453125 | consumed samples:          695 | consumed tokens:      1423360 | elapsed time per iteration (ms): 446.2 | learning rate: 7.580E-07 | global batch size:     1 | lm loss: 8.696286E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.241 | TFLOPs: 54.58 |
[default0]:[2023-07-30 22:29:42,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[7.634944e-07, 7.634944e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:42,205] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=700, RunningAvgSamplesPerSec=2.351985905656953, CurrSamplesPerSec=2.341399803278615, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      700/439453125 | consumed samples:          700 | consumed tokens:      1433600 | elapsed time per iteration (ms): 455.8 | learning rate: 7.635E-07 | global batch size:     1 | lm loss: 8.515749E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.194 | TFLOPs: 53.43 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 700 | lm loss value: 8.735198E+00 | lm loss PPL: 6.217965E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:29:49,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=705, skipped=0, lr=[7.689557333333334e-07, 7.689557333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:49,307] [INFO] [timer.py:215:stop] epoch=0/micro_step=705/global_step=705, RunningAvgSamplesPerSec=2.3519798740679416, CurrSamplesPerSec=2.3360323300372823, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      705/439453125 | consumed samples:          705 | consumed tokens:      1443840 | elapsed time per iteration (ms): 1420.2 | learning rate: 7.690E-07 | global batch size:     1 | lm loss: 8.469505E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.704 | TFLOPs: 17.15 |
[default0]:[2023-07-30 22:29:51,538] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[7.744170666666667e-07, 7.744170666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:51,540] [INFO] [timer.py:215:stop] epoch=0/micro_step=710/global_step=710, RunningAvgSamplesPerSec=2.3520141691154994, CurrSamplesPerSec=2.3406524109651117, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      710/439453125 | consumed samples:          710 | consumed tokens:      1454080 | elapsed time per iteration (ms): 446.9 | learning rate: 7.744E-07 | global batch size:     1 | lm loss: 8.725770E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.238 | TFLOPs: 54.50 |
[default0]:[2023-07-30 22:29:53,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=715, skipped=0, lr=[7.798784e-07, 7.798784e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:53,813] [INFO] [timer.py:215:stop] epoch=0/micro_step=715/global_step=715, RunningAvgSamplesPerSec=2.3519958761068906, CurrSamplesPerSec=2.3372090820237403, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      715/439453125 | consumed samples:          715 | consumed tokens:      1464320 | elapsed time per iteration (ms): 454.6 | learning rate: 7.799E-07 | global batch size:     1 | lm loss: 8.472447E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.200 | TFLOPs: 53.57 |
[default0]:[2023-07-30 22:29:56,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[7.853397333333334e-07, 7.853397333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:56,062] [INFO] [timer.py:215:stop] epoch=0/micro_step=720/global_step=720, RunningAvgSamplesPerSec=2.3520057242445995, CurrSamplesPerSec=2.3492092296955716, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      720/439453125 | consumed samples:          720 | consumed tokens:      1474560 | elapsed time per iteration (ms): 449.2 | learning rate: 7.853E-07 | global batch size:     1 | lm loss: 8.599866E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.226 | TFLOPs: 54.22 |
[default0]:[2023-07-30 22:29:58,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=725, skipped=0, lr=[7.908010666666668e-07, 7.908010666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:58,397] [INFO] [timer.py:215:stop] epoch=0/micro_step=725/global_step=725, RunningAvgSamplesPerSec=2.352005030828196, CurrSamplesPerSec=2.3522407479067473, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      725/439453125 | consumed samples:          725 | consumed tokens:      1484800 | elapsed time per iteration (ms): 467.4 | learning rate: 7.908E-07 | global batch size:     1 | lm loss: 8.652943E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.140 | TFLOPs: 52.11 |
[default0]:[2023-07-30 22:30:00,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[7.962624000000001e-07, 7.962624000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:00,681] [INFO] [timer.py:215:stop] epoch=0/micro_step=730/global_step=730, RunningAvgSamplesPerSec=2.352053439446584, CurrSamplesPerSec=2.3624371555028856, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      730/439453125 | consumed samples:          730 | consumed tokens:      1495040 | elapsed time per iteration (ms): 456.7 | learning rate: 7.963E-07 | global batch size:     1 | lm loss: 8.547907E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.189 | TFLOPs: 53.33 |
[default0]:[2023-07-30 22:30:02,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=735, skipped=0, lr=[8.017237333333334e-07, 8.017237333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:02,922] [INFO] [timer.py:215:stop] epoch=0/micro_step=735/global_step=735, RunningAvgSamplesPerSec=2.352066532037498, CurrSamplesPerSec=2.3551438813372023, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      735/439453125 | consumed samples:          735 | consumed tokens:      1505280 | elapsed time per iteration (ms): 448.3 | learning rate: 8.017E-07 | global batch size:     1 | lm loss: 8.902203E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.231 | TFLOPs: 54.33 |
[default0]:[2023-07-30 22:30:05,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[8.071850666666667e-07, 8.071850666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:05,187] [INFO] [timer.py:215:stop] epoch=0/micro_step=740/global_step=740, RunningAvgSamplesPerSec=2.3520731115456166, CurrSamplesPerSec=2.3493197604696507, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      740/439453125 | consumed samples:          740 | consumed tokens:      1515520 | elapsed time per iteration (ms): 452.6 | learning rate: 8.072E-07 | global batch size:     1 | lm loss: 8.628072E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.209 | TFLOPs: 53.81 |
[default0]:[2023-07-30 22:30:07,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=745, skipped=0, lr=[8.126464000000001e-07, 8.126464000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:07,462] [INFO] [timer.py:215:stop] epoch=0/micro_step=745/global_step=745, RunningAvgSamplesPerSec=2.3520785036679452, CurrSamplesPerSec=2.3483805802890303, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      745/439453125 | consumed samples:          745 | consumed tokens:      1525760 | elapsed time per iteration (ms): 455.5 | learning rate: 8.126E-07 | global batch size:     1 | lm loss: 8.571335E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.195 | TFLOPs: 53.46 |
[default0]:[2023-07-30 22:30:09,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[8.181077333333335e-07, 8.181077333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:09,698] [INFO] [timer.py:215:stop] epoch=0/micro_step=750/global_step=750, RunningAvgSamplesPerSec=2.352125330723971, CurrSamplesPerSec=2.347618213375976, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      750/439453125 | consumed samples:          750 | consumed tokens:      1536000 | elapsed time per iteration (ms): 446.8 | learning rate: 8.181E-07 | global batch size:     1 | lm loss: 8.832187E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.238 | TFLOPs: 54.51 |
[default0]:[2023-07-30 22:30:11,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=755, skipped=0, lr=[8.235690666666668e-07, 8.235690666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:11,941] [INFO] [timer.py:215:stop] epoch=0/micro_step=755/global_step=755, RunningAvgSamplesPerSec=2.352086866054801, CurrSamplesPerSec=2.344083629768934, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      755/439453125 | consumed samples:          755 | consumed tokens:      1546240 | elapsed time per iteration (ms): 448.5 | learning rate: 8.236E-07 | global batch size:     1 | lm loss: 8.469173E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.230 | TFLOPs: 54.30 |
[default0]:[2023-07-30 22:30:14,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[8.290304000000001e-07, 8.290304000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:14,174] [INFO] [timer.py:215:stop] epoch=0/micro_step=760/global_step=760, RunningAvgSamplesPerSec=2.3520741533526435, CurrSamplesPerSec=2.3429588408724515, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      760/439453125 | consumed samples:          760 | consumed tokens:      1556480 | elapsed time per iteration (ms): 446.7 | learning rate: 8.290E-07 | global batch size:     1 | lm loss: 8.368581E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.239 | TFLOPs: 54.53 |
[default0]:[2023-07-30 22:30:16,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=765, skipped=0, lr=[8.344917333333334e-07, 8.344917333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:16,427] [INFO] [timer.py:215:stop] epoch=0/micro_step=765/global_step=765, RunningAvgSamplesPerSec=2.352083314551846, CurrSamplesPerSec=2.361980834089349, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      765/439453125 | consumed samples:          765 | consumed tokens:      1566720 | elapsed time per iteration (ms): 450.8 | learning rate: 8.345E-07 | global batch size:     1 | lm loss: 8.482203E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.218 | TFLOPs: 54.03 |
[default0]:[2023-07-30 22:30:18,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[8.399530666666668e-07, 8.399530666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:18,703] [INFO] [timer.py:215:stop] epoch=0/micro_step=770/global_step=770, RunningAvgSamplesPerSec=2.3518404281323866, CurrSamplesPerSec=2.3493473947883445, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      770/439453125 | consumed samples:          770 | consumed tokens:      1576960 | elapsed time per iteration (ms): 454.8 | learning rate: 8.400E-07 | global batch size:     1 | lm loss: 8.908797E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.199 | TFLOPs: 53.55 |
[default0]:[2023-07-30 22:30:20,957] [INFO] [logging.py:96:log_dist] [Rank 0] step=775, skipped=0, lr=[8.454144000000001e-07, 8.454144000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:20,959] [INFO] [timer.py:215:stop] epoch=0/micro_step=775/global_step=775, RunningAvgSamplesPerSec=2.3518345349181597, CurrSamplesPerSec=2.3521721526142887, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      775/439453125 | consumed samples:          775 | consumed tokens:      1587200 | elapsed time per iteration (ms): 451.8 | learning rate: 8.454E-07 | global batch size:     1 | lm loss: 8.567203E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.214 | TFLOPs: 53.91 |
[default0]:[2023-07-30 22:30:23,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=0, lr=[8.508757333333333e-07, 8.508757333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:23,200] [INFO] [timer.py:215:stop] epoch=0/micro_step=780/global_step=780, RunningAvgSamplesPerSec=2.3518418572603443, CurrSamplesPerSec=2.3449118709128447, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      780/439453125 | consumed samples:          780 | consumed tokens:      1597440 | elapsed time per iteration (ms): 447.9 | learning rate: 8.509E-07 | global batch size:     1 | lm loss: 8.244027E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.233 | TFLOPs: 54.37 |
[default0]:[2023-07-30 22:30:25,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=785, skipped=0, lr=[8.563370666666667e-07, 8.563370666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:25,491] [INFO] [timer.py:215:stop] epoch=0/micro_step=785/global_step=785, RunningAvgSamplesPerSec=2.351833158628249, CurrSamplesPerSec=2.344194988905842, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      785/439453125 | consumed samples:          785 | consumed tokens:      1607680 | elapsed time per iteration (ms): 458.1 | learning rate: 8.563E-07 | global batch size:     1 | lm loss: 8.450889E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.183 | TFLOPs: 53.16 |
[default0]:[2023-07-30 22:30:27,722] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=0, lr=[8.617984e-07, 8.617984e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:27,724] [INFO] [timer.py:215:stop] epoch=0/micro_step=790/global_step=790, RunningAvgSamplesPerSec=2.3518409055200387, CurrSamplesPerSec=2.3458941996740372, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      790/439453125 | consumed samples:          790 | consumed tokens:      1617920 | elapsed time per iteration (ms): 446.6 | learning rate: 8.618E-07 | global batch size:     1 | lm loss: 8.682536E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.239 | TFLOPs: 54.53 |
[default0]: iteration      795/439453125 | consumed samples:          795 | consumed tokens:      1628160 | elapsed time per iteration (ms): 447.9 | learning rate: 8.673E-07 | global batch size:     1 | lm loss: 8.527299E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.233 | TFLOPs: 54.38 |
[default0]:[2023-07-30 22:30:32,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=0, lr=[8.727210666666667e-07, 8.727210666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:32,186] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=800, RunningAvgSamplesPerSec=2.351918799961122, CurrSamplesPerSec=2.3490684496039806, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      800/439453125 | consumed samples:          800 | consumed tokens:      1638400 | elapsed time per iteration (ms): 444.3 | learning rate: 8.727E-07 | global batch size:     1 | lm loss: 8.538525E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.251 | TFLOPs: 54.82 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 800 | lm loss value: 8.526489E+00 | lm loss PPL: 5.046697E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:30:39,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=805, skipped=0, lr=[8.781824e-07, 8.781824e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:39,607] [INFO] [timer.py:215:stop] epoch=0/micro_step=805/global_step=805, RunningAvgSamplesPerSec=2.3519103605868286, CurrSamplesPerSec=2.347020496050532, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      805/439453125 | consumed samples:          805 | consumed tokens:      1648640 | elapsed time per iteration (ms): 1484.2 | learning rate: 8.782E-07 | global batch size:     1 | lm loss: 8.556335E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.674 | TFLOPs: 16.41 |
[default0]:[2023-07-30 22:30:41,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=0, lr=[8.836437333333333e-07, 8.836437333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:41,912] [INFO] [timer.py:215:stop] epoch=0/micro_step=810/global_step=810, RunningAvgSamplesPerSec=2.3519061975708544, CurrSamplesPerSec=2.3562949638266963, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      810/439453125 | consumed samples:          810 | consumed tokens:      1658880 | elapsed time per iteration (ms): 461.4 | learning rate: 8.836E-07 | global batch size:     1 | lm loss: 8.615478E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.167 | TFLOPs: 52.79 |
[default0]:[2023-07-30 22:30:44,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=815, skipped=0, lr=[8.891050666666666e-07, 8.891050666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:44,154] [INFO] [timer.py:215:stop] epoch=0/micro_step=815/global_step=815, RunningAvgSamplesPerSec=2.351925478871913, CurrSamplesPerSec=2.3532477121458393, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      815/439453125 | consumed samples:          815 | consumed tokens:      1669120 | elapsed time per iteration (ms): 447.8 | learning rate: 8.891E-07 | global batch size:     1 | lm loss: 8.380070E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.233 | TFLOPs: 54.38 |
[default0]:[2023-07-30 22:30:46,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=0, lr=[8.945664e-07, 8.945664e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:46,404] [INFO] [timer.py:215:stop] epoch=0/micro_step=820/global_step=820, RunningAvgSamplesPerSec=2.3519418886929007, CurrSamplesPerSec=2.3568192775417134, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      820/439453125 | consumed samples:          820 | consumed tokens:      1679360 | elapsed time per iteration (ms): 450.1 | learning rate: 8.946E-07 | global batch size:     1 | lm loss: 8.609116E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.222 | TFLOPs: 54.11 |
[default0]:[2023-07-30 22:30:48,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=825, skipped=0, lr=[9.000277333333334e-07, 9.000277333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:48,650] [INFO] [timer.py:215:stop] epoch=0/micro_step=825/global_step=825, RunningAvgSamplesPerSec=2.3519173803987465, CurrSamplesPerSec=2.3469233136558207, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      825/439453125 | consumed samples:          825 | consumed tokens:      1689600 | elapsed time per iteration (ms): 449.5 | learning rate: 9.000E-07 | global batch size:     1 | lm loss: 8.635686E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.225 | TFLOPs: 54.19 |
[default0]:[2023-07-30 22:30:50,953] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=0, lr=[9.054890666666667e-07, 9.054890666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:50,954] [INFO] [timer.py:215:stop] epoch=0/micro_step=830/global_step=830, RunningAvgSamplesPerSec=2.351910528003539, CurrSamplesPerSec=2.347902071478752, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      830/439453125 | consumed samples:          830 | consumed tokens:      1699840 | elapsed time per iteration (ms): 461.0 | learning rate: 9.055E-07 | global batch size:     1 | lm loss: 8.297144E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.169 | TFLOPs: 52.83 |
[default0]:[2023-07-30 22:30:53,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=835, skipped=0, lr=[9.109504e-07, 9.109504e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:53,265] [INFO] [timer.py:215:stop] epoch=0/micro_step=835/global_step=835, RunningAvgSamplesPerSec=2.351934469111014, CurrSamplesPerSec=2.3543176481880534, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      835/439453125 | consumed samples:          835 | consumed tokens:      1710080 | elapsed time per iteration (ms): 461.7 | learning rate: 9.110E-07 | global batch size:     1 | lm loss: 8.449166E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.166 | TFLOPs: 52.75 |
[default0]:[2023-07-30 22:30:55,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=0, lr=[9.164117333333333e-07, 9.164117333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:55,516] [INFO] [timer.py:215:stop] epoch=0/micro_step=840/global_step=840, RunningAvgSamplesPerSec=2.3519444533030542, CurrSamplesPerSec=2.3470782839926763, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      840/439453125 | consumed samples:          840 | consumed tokens:      1720320 | elapsed time per iteration (ms): 450.4 | learning rate: 9.164E-07 | global batch size:     1 | lm loss: 8.490459E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.220 | TFLOPs: 54.08 |
[default0]:[2023-07-30 22:30:57,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=845, skipped=0, lr=[9.218730666666667e-07, 9.218730666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:57,807] [INFO] [timer.py:215:stop] epoch=0/micro_step=845/global_step=845, RunningAvgSamplesPerSec=2.3519589187249728, CurrSamplesPerSec=2.3424903883315125, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      845/439453125 | consumed samples:          845 | consumed tokens:      1730560 | elapsed time per iteration (ms): 458.8 | learning rate: 9.219E-07 | global batch size:     1 | lm loss: 8.157019E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.180 | TFLOPs: 53.09 |
[default0]: iteration      850/439453125 | consumed samples:          850 | consumed tokens:      1740800 | elapsed time per iteration (ms): 462.1 | learning rate: 9.273E-07 | global batch size:     1 | lm loss: 8.778065E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.164 | TFLOPs: 52.71 |
[default0]:[2023-07-30 22:31:02,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=855, skipped=0, lr=[9.327957333333334e-07, 9.327957333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:02,371] [INFO] [timer.py:215:stop] epoch=0/micro_step=855/global_step=855, RunningAvgSamplesPerSec=2.3516847241435537, CurrSamplesPerSec=2.3471846737823254, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      855/439453125 | consumed samples:          855 | consumed tokens:      1751040 | elapsed time per iteration (ms): 449.9 | learning rate: 9.328E-07 | global batch size:     1 | lm loss: 8.569673E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.223 | TFLOPs: 54.13 |
[default0]:[2023-07-30 22:31:04,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[9.382570666666667e-07, 9.382570666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:04,592] [INFO] [timer.py:215:stop] epoch=0/micro_step=860/global_step=860, RunningAvgSamplesPerSec=2.3517006359845762, CurrSamplesPerSec=2.349057924667577, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      860/439453125 | consumed samples:          860 | consumed tokens:      1761280 | elapsed time per iteration (ms): 444.3 | learning rate: 9.383E-07 | global batch size:     1 | lm loss: 8.415021E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.251 | TFLOPs: 54.82 |
[default0]:[2023-07-30 22:31:06,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=865, skipped=0, lr=[9.437184e-07, 9.437184e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:06,841] [INFO] [timer.py:215:stop] epoch=0/micro_step=865/global_step=865, RunningAvgSamplesPerSec=2.3517231415092446, CurrSamplesPerSec=2.351069900302579, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      865/439453125 | consumed samples:          865 | consumed tokens:      1771520 | elapsed time per iteration (ms): 449.6 | learning rate: 9.437E-07 | global batch size:     1 | lm loss: 8.164989E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.224 | TFLOPs: 54.17 |
[default0]:[2023-07-30 22:31:09,070] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[9.491797333333334e-07, 9.491797333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:09,072] [INFO] [timer.py:215:stop] epoch=0/micro_step=870/global_step=870, RunningAvgSamplesPerSec=2.351708244000157, CurrSamplesPerSec=2.3190054542991656, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      870/439453125 | consumed samples:          870 | consumed tokens:      1781760 | elapsed time per iteration (ms): 446.6 | learning rate: 9.492E-07 | global batch size:     1 | lm loss: 8.264153E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.239 | TFLOPs: 54.53 |
[default0]:[2023-07-30 22:31:11,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=875, skipped=0, lr=[9.546410666666666e-07, 9.546410666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:11,311] [INFO] [timer.py:215:stop] epoch=0/micro_step=875/global_step=875, RunningAvgSamplesPerSec=2.3517359236218507, CurrSamplesPerSec=2.364273534625867, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      875/439453125 | consumed samples:          875 | consumed tokens:      1792000 | elapsed time per iteration (ms): 447.4 | learning rate: 9.546E-07 | global batch size:     1 | lm loss: 8.516792E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.235 | TFLOPs: 54.43 |
[default0]:[2023-07-30 22:31:13,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[9.601024000000002e-07, 9.601024000000002e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:13,539] [INFO] [timer.py:215:stop] epoch=0/micro_step=880/global_step=880, RunningAvgSamplesPerSec=2.35176062881487, CurrSamplesPerSec=2.3435086609668145, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      880/439453125 | consumed samples:          880 | consumed tokens:      1802240 | elapsed time per iteration (ms): 445.7 | learning rate: 9.601E-07 | global batch size:     1 | lm loss: 8.308240E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.244 | TFLOPs: 54.65 |
[default0]:[2023-07-30 22:31:15,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=885, skipped=0, lr=[9.655637333333335e-07, 9.655637333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:15,773] [INFO] [timer.py:215:stop] epoch=0/micro_step=885/global_step=885, RunningAvgSamplesPerSec=2.351762391189592, CurrSamplesPerSec=2.3445933481876837, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      885/439453125 | consumed samples:          885 | consumed tokens:      1812480 | elapsed time per iteration (ms): 446.7 | learning rate: 9.656E-07 | global batch size:     1 | lm loss: 8.233719E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.239 | TFLOPs: 54.53 |
[default0]:[2023-07-30 22:31:18,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[9.710250666666668e-07, 9.710250666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:18,080] [INFO] [timer.py:215:stop] epoch=0/micro_step=890/global_step=890, RunningAvgSamplesPerSec=2.3517558907381835, CurrSamplesPerSec=2.3447886460973058, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      890/439453125 | consumed samples:          890 | consumed tokens:      1822720 | elapsed time per iteration (ms): 461.3 | learning rate: 9.710E-07 | global batch size:     1 | lm loss: 8.381570E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.168 | TFLOPs: 52.80 |
[default0]:[2023-07-30 22:31:20,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=895, skipped=0, lr=[9.764864e-07, 9.764864e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:20,323] [INFO] [timer.py:215:stop] epoch=0/micro_step=895/global_step=895, RunningAvgSamplesPerSec=2.351777365786404, CurrSamplesPerSec=2.3408509721585053, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      895/439453125 | consumed samples:          895 | consumed tokens:      1832960 | elapsed time per iteration (ms): 448.7 | learning rate: 9.765E-07 | global batch size:     1 | lm loss: 8.347670E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.229 | TFLOPs: 54.29 |
[default0]:[2023-07-30 22:31:22,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[9.819477333333334e-07, 9.819477333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:22,555] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=900, RunningAvgSamplesPerSec=2.3518110136207553, CurrSamplesPerSec=2.3490395062558105, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      900/439453125 | consumed samples:          900 | consumed tokens:      1843200 | elapsed time per iteration (ms): 446.2 | learning rate: 9.819E-07 | global batch size:     1 | lm loss: 8.646690E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.241 | TFLOPs: 54.58 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 900 | lm loss value: 8.470325E+00 | lm loss PPL: 4.771068E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:31:29,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=905, skipped=0, lr=[9.874090666666667e-07, 9.874090666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:29,935] [INFO] [timer.py:215:stop] epoch=0/micro_step=905/global_step=905, RunningAvgSamplesPerSec=2.351828730864777, CurrSamplesPerSec=2.3551068536840094, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      905/439453125 | consumed samples:          905 | consumed tokens:      1853440 | elapsed time per iteration (ms): 1476.4 | learning rate: 9.874E-07 | global batch size:     1 | lm loss: 8.373654E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.677 | TFLOPs: 16.50 |
[default0]:[2023-07-30 22:31:32,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[9.928704e-07, 9.928704e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:32,175] [INFO] [timer.py:215:stop] epoch=0/micro_step=910/global_step=910, RunningAvgSamplesPerSec=2.3518659850770756, CurrSamplesPerSec=2.3465320389740008, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      910/439453125 | consumed samples:          910 | consumed tokens:      1863680 | elapsed time per iteration (ms): 448.1 | learning rate: 9.929E-07 | global batch size:     1 | lm loss: 8.315346E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.232 | TFLOPs: 54.36 |
[default0]:[2023-07-30 22:31:34,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=915, skipped=0, lr=[9.983317333333334e-07, 9.983317333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:34,449] [INFO] [timer.py:215:stop] epoch=0/micro_step=915/global_step=915, RunningAvgSamplesPerSec=2.351882439579455, CurrSamplesPerSec=2.3526801164699793, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      915/439453125 | consumed samples:          915 | consumed tokens:      1873920 | elapsed time per iteration (ms): 454.5 | learning rate: 9.983E-07 | global batch size:     1 | lm loss: 8.330588E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.200 | TFLOPs: 53.58 |
[default0]:[2023-07-30 22:31:36,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[1.0037930666666667e-06, 1.0037930666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:36,707] [INFO] [timer.py:215:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=2.351907971089649, CurrSamplesPerSec=2.3543599372440718, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      920/439453125 | consumed samples:          920 | consumed tokens:      1884160 | elapsed time per iteration (ms): 451.4 | learning rate: 1.004E-06 | global batch size:     1 | lm loss: 8.283200E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.215 | TFLOPs: 53.95 |
[default0]:[2023-07-30 22:31:38,929] [INFO] [logging.py:96:log_dist] [Rank 0] step=925, skipped=0, lr=[1.0092544000000002e-06, 1.0092544000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:38,931] [INFO] [timer.py:215:stop] epoch=0/micro_step=925/global_step=925, RunningAvgSamplesPerSec=2.3519122382568134, CurrSamplesPerSec=2.3433790372728933, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      925/439453125 | consumed samples:          925 | consumed tokens:      1894400 | elapsed time per iteration (ms): 445.0 | learning rate: 1.009E-06 | global batch size:     1 | lm loss: 8.267802E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.247 | TFLOPs: 54.73 |
[default0]:[2023-07-30 22:31:41,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=0, lr=[1.0147157333333335e-06, 1.0147157333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:41,176] [INFO] [timer.py:215:stop] epoch=0/micro_step=930/global_step=930, RunningAvgSamplesPerSec=2.351917863540858, CurrSamplesPerSec=2.3565160474752442, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      930/439453125 | consumed samples:          930 | consumed tokens:      1904640 | elapsed time per iteration (ms): 449.1 | learning rate: 1.015E-06 | global batch size:     1 | lm loss: 8.268530E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.227 | TFLOPs: 54.23 |
[default0]:[2023-07-30 22:31:43,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=935, skipped=0, lr=[1.0201770666666668e-06, 1.0201770666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:43,430] [INFO] [timer.py:215:stop] epoch=0/micro_step=935/global_step=935, RunningAvgSamplesPerSec=2.351939978276188, CurrSamplesPerSec=2.3389503480288774, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      935/439453125 | consumed samples:          935 | consumed tokens:      1914880 | elapsed time per iteration (ms): 450.9 | learning rate: 1.020E-06 | global batch size:     1 | lm loss: 8.584425E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.218 | TFLOPs: 54.02 |
[default0]:[2023-07-30 22:31:45,693] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=0, lr=[1.0256384000000001e-06, 1.0256384000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:45,695] [INFO] [timer.py:215:stop] epoch=0/micro_step=940/global_step=940, RunningAvgSamplesPerSec=2.35193970007377, CurrSamplesPerSec=2.339404337345097, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      940/439453125 | consumed samples:          940 | consumed tokens:      1925120 | elapsed time per iteration (ms): 452.7 | learning rate: 1.026E-06 | global batch size:     1 | lm loss: 8.266578E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.209 | TFLOPs: 53.80 |
[default0]:[2023-07-30 22:31:47,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=945, skipped=0, lr=[1.0310997333333332e-06, 1.0310997333333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:47,964] [INFO] [timer.py:215:stop] epoch=0/micro_step=945/global_step=945, RunningAvgSamplesPerSec=2.3519199682353724, CurrSamplesPerSec=2.3445763103157793, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      945/439453125 | consumed samples:          945 | consumed tokens:      1935360 | elapsed time per iteration (ms): 453.9 | learning rate: 1.031E-06 | global batch size:     1 | lm loss: 8.122429E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.203 | TFLOPs: 53.66 |
[default0]:[2023-07-30 22:31:50,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=0, lr=[1.0365610666666666e-06, 1.0365610666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:50,214] [INFO] [timer.py:215:stop] epoch=0/micro_step=950/global_step=950, RunningAvgSamplesPerSec=2.351843128671065, CurrSamplesPerSec=2.278647405143372, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      950/439453125 | consumed samples:          950 | consumed tokens:      1945600 | elapsed time per iteration (ms): 450.3 | learning rate: 1.037E-06 | global batch size:     1 | lm loss: 8.330403E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.221 | TFLOPs: 54.09 |
[default0]:[2023-07-30 22:31:52,498] [INFO] [logging.py:96:log_dist] [Rank 0] step=955, skipped=0, lr=[1.0420224e-06, 1.0420224e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:52,500] [INFO] [timer.py:215:stop] epoch=0/micro_step=955/global_step=955, RunningAvgSamplesPerSec=2.351869285432615, CurrSamplesPerSec=2.3452973598025935, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      955/439453125 | consumed samples:          955 | consumed tokens:      1955840 | elapsed time per iteration (ms): 457.0 | learning rate: 1.042E-06 | global batch size:     1 | lm loss: 8.414505E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.188 | TFLOPs: 53.29 |
[default0]:[2023-07-30 22:31:54,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=0, lr=[1.0474837333333334e-06, 1.0474837333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:54,756] [INFO] [timer.py:215:stop] epoch=0/micro_step=960/global_step=960, RunningAvgSamplesPerSec=2.351887232232909, CurrSamplesPerSec=2.3589599956356215, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      960/439453125 | consumed samples:          960 | consumed tokens:      1966080 | elapsed time per iteration (ms): 450.8 | learning rate: 1.047E-06 | global batch size:     1 | lm loss: 8.327376E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.218 | TFLOPs: 54.03 |
[default0]:[2023-07-30 22:31:57,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=965, skipped=0, lr=[1.0529450666666667e-06, 1.0529450666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:57,013] [INFO] [timer.py:215:stop] epoch=0/micro_step=965/global_step=965, RunningAvgSamplesPerSec=2.351902140331377, CurrSamplesPerSec=2.3550513143872314, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      965/439453125 | consumed samples:          965 | consumed tokens:      1976320 | elapsed time per iteration (ms): 451.3 | learning rate: 1.053E-06 | global batch size:     1 | lm loss: 8.392496E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.216 | TFLOPs: 53.97 |
[default0]:[2023-07-30 22:31:59,248] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=0, lr=[1.0584064e-06, 1.0584064e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:59,249] [INFO] [timer.py:215:stop] epoch=0/micro_step=970/global_step=970, RunningAvgSamplesPerSec=2.351925160393514, CurrSamplesPerSec=2.3519321005930434, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      970/439453125 | consumed samples:          970 | consumed tokens:      1986560 | elapsed time per iteration (ms): 447.3 | learning rate: 1.058E-06 | global batch size:     1 | lm loss: 8.248714E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.236 | TFLOPs: 54.45 |
[default0]:[2023-07-30 22:32:01,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=975, skipped=0, lr=[1.0638677333333333e-06, 1.0638677333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:01,526] [INFO] [timer.py:215:stop] epoch=0/micro_step=975/global_step=975, RunningAvgSamplesPerSec=2.3518843118438717, CurrSamplesPerSec=2.343578061363323, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      975/439453125 | consumed samples:          975 | consumed tokens:      1996800 | elapsed time per iteration (ms): 455.6 | learning rate: 1.064E-06 | global batch size:     1 | lm loss: 8.226602E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.195 | TFLOPs: 53.46 |
[default0]:[2023-07-30 22:32:03,757] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=0, lr=[1.0693290666666667e-06, 1.0693290666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:03,759] [INFO] [timer.py:215:stop] epoch=0/micro_step=980/global_step=980, RunningAvgSamplesPerSec=2.351894037112791, CurrSamplesPerSec=2.348182054336824, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      980/439453125 | consumed samples:          980 | consumed tokens:      2007040 | elapsed time per iteration (ms): 446.7 | learning rate: 1.069E-06 | global batch size:     1 | lm loss: 8.381593E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.239 | TFLOPs: 54.53 |
[default0]:[2023-07-30 22:32:05,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=985, skipped=0, lr=[1.0747904e-06, 1.0747904e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:06,000] [INFO] [timer.py:215:stop] epoch=0/micro_step=985/global_step=985, RunningAvgSamplesPerSec=2.35189824478417, CurrSamplesPerSec=2.354435268388412, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      985/439453125 | consumed samples:          985 | consumed tokens:      2017280 | elapsed time per iteration (ms): 448.1 | learning rate: 1.075E-06 | global batch size:     1 | lm loss: 8.075011E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.232 | TFLOPs: 54.36 |
[default0]:[2023-07-30 22:32:08,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=0, lr=[1.0802517333333333e-06, 1.0802517333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:08,243] [INFO] [timer.py:215:stop] epoch=0/micro_step=990/global_step=990, RunningAvgSamplesPerSec=2.3519179846310747, CurrSamplesPerSec=2.348784309421224, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      990/439453125 | consumed samples:          990 | consumed tokens:      2027520 | elapsed time per iteration (ms): 448.8 | learning rate: 1.080E-06 | global batch size:     1 | lm loss: 7.996090E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.228 | TFLOPs: 54.27 |
[default0]:[2023-07-30 22:32:10,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=995, skipped=0, lr=[1.0857130666666666e-06, 1.0857130666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:10,518] [INFO] [timer.py:215:stop] epoch=0/micro_step=995/global_step=995, RunningAvgSamplesPerSec=2.3518972988039604, CurrSamplesPerSec=2.318699057550871, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration      995/439453125 | consumed samples:          995 | consumed tokens:      2037760 | elapsed time per iteration (ms): 455.2 | learning rate: 1.086E-06 | global batch size:     1 | lm loss: 8.448824E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.197 | TFLOPs: 53.50 |
[default0]:[2023-07-30 22:32:12,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.0911744000000001e-06, 1.0911744000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:12,744] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=2.3519264645135722, CurrSamplesPerSec=2.3501569739523536, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1000/439453125 | consumed samples:         1000 | consumed tokens:      2048000 | elapsed time per iteration (ms): 444.7 | learning rate: 1.091E-06 | global batch size:     1 | lm loss: 8.338339E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.249 | TFLOPs: 54.77 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1000 | lm loss value: 8.394428E+00 | lm loss PPL: 4.422358E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:32:20,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=1005, skipped=0, lr=[1.0966357333333334e-06, 1.0966357333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:20,047] [INFO] [timer.py:215:stop] epoch=0/micro_step=1005/global_step=1005, RunningAvgSamplesPerSec=2.3518906166122022, CurrSamplesPerSec=2.343104125385531, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1005/439453125 | consumed samples:         1005 | consumed tokens:      2058240 | elapsed time per iteration (ms): 1460.6 | learning rate: 1.097E-06 | global batch size:     1 | lm loss: 7.903140E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.685 | TFLOPs: 16.68 |
[default0]:[2023-07-30 22:32:22,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=0, lr=[1.1020970666666668e-06, 1.1020970666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:22,271] [INFO] [timer.py:215:stop] epoch=0/micro_step=1010/global_step=1010, RunningAvgSamplesPerSec=2.3518774736722547, CurrSamplesPerSec=2.3317467041587028, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1010/439453125 | consumed samples:         1010 | consumed tokens:      2068480 | elapsed time per iteration (ms): 444.7 | learning rate: 1.102E-06 | global batch size:     1 | lm loss: 8.170723E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.249 | TFLOPs: 54.77 |
[default0]:[2023-07-30 22:32:24,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=1015, skipped=0, lr=[1.1075584e-06, 1.1075584e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:24,508] [INFO] [timer.py:215:stop] epoch=0/micro_step=1015/global_step=1015, RunningAvgSamplesPerSec=2.3518740629781423, CurrSamplesPerSec=2.3466790799267736, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1015/439453125 | consumed samples:         1015 | consumed tokens:      2078720 | elapsed time per iteration (ms): 447.5 | learning rate: 1.108E-06 | global batch size:     1 | lm loss: 8.412723E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.235 | TFLOPs: 54.43 |
[default0]:[2023-07-30 22:32:26,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=0, lr=[1.1130197333333334e-06, 1.1130197333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:26,749] [INFO] [timer.py:215:stop] epoch=0/micro_step=1020/global_step=1020, RunningAvgSamplesPerSec=2.351880703521493, CurrSamplesPerSec=2.349815961231407, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1020/439453125 | consumed samples:         1020 | consumed tokens:      2088960 | elapsed time per iteration (ms): 448.3 | learning rate: 1.113E-06 | global batch size:     1 | lm loss: 8.536679E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.231 | TFLOPs: 54.33 |
[default0]:[2023-07-30 22:32:28,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=1025, skipped=0, lr=[1.1184810666666667e-06, 1.1184810666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:28,992] [INFO] [timer.py:215:stop] epoch=0/micro_step=1025/global_step=1025, RunningAvgSamplesPerSec=2.3518634627125, CurrSamplesPerSec=2.336206685277729, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1025/439453125 | consumed samples:         1025 | consumed tokens:      2099200 | elapsed time per iteration (ms): 448.6 | learning rate: 1.118E-06 | global batch size:     1 | lm loss: 8.271790E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.229 | TFLOPs: 54.29 |
[default0]:[2023-07-30 22:32:31,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=0, lr=[1.1239424e-06, 1.1239424e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:31,255] [INFO] [timer.py:215:stop] epoch=0/micro_step=1030/global_step=1030, RunningAvgSamplesPerSec=2.351852693646986, CurrSamplesPerSec=2.354859591707947, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1030/439453125 | consumed samples:         1030 | consumed tokens:      2109440 | elapsed time per iteration (ms): 452.6 | learning rate: 1.124E-06 | global batch size:     1 | lm loss: 7.926437E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.209 | TFLOPs: 53.81 |
[default0]:[2023-07-30 22:32:33,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=1035, skipped=0, lr=[1.1294037333333333e-06, 1.1294037333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:33,504] [INFO] [timer.py:215:stop] epoch=0/micro_step=1035/global_step=1035, RunningAvgSamplesPerSec=2.3518538401249787, CurrSamplesPerSec=2.3481097518988134, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1035/439453125 | consumed samples:         1035 | consumed tokens:      2119680 | elapsed time per iteration (ms): 449.7 | learning rate: 1.129E-06 | global batch size:     1 | lm loss: 8.083620E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.224 | TFLOPs: 54.16 |
[default0]:[2023-07-30 22:32:35,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=0, lr=[1.1348650666666666e-06, 1.1348650666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:35,745] [INFO] [timer.py:215:stop] epoch=0/micro_step=1040/global_step=1040, RunningAvgSamplesPerSec=2.3518602200636556, CurrSamplesPerSec=2.3500292471710926, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1040/439453125 | consumed samples:         1040 | consumed tokens:      2129920 | elapsed time per iteration (ms): 448.2 | learning rate: 1.135E-06 | global batch size:     1 | lm loss: 8.174411E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.231 | TFLOPs: 54.34 |
[default0]:[2023-07-30 22:32:37,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=1045, skipped=0, lr=[1.1403264000000002e-06, 1.1403264000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:37,974] [INFO] [timer.py:215:stop] epoch=0/micro_step=1045/global_step=1045, RunningAvgSamplesPerSec=2.3518682988998614, CurrSamplesPerSec=2.346382391212165, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1045/439453125 | consumed samples:         1045 | consumed tokens:      2140160 | elapsed time per iteration (ms): 445.9 | learning rate: 1.140E-06 | global batch size:     1 | lm loss: 8.196710E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.243 | TFLOPs: 54.62 |
[default0]:[2023-07-30 22:32:40,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=0, lr=[1.1457877333333335e-06, 1.1457877333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:40,232] [INFO] [timer.py:215:stop] epoch=0/micro_step=1050/global_step=1050, RunningAvgSamplesPerSec=2.3518485993094824, CurrSamplesPerSec=2.333918718296285, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1050/439453125 | consumed samples:         1050 | consumed tokens:      2150400 | elapsed time per iteration (ms): 451.5 | learning rate: 1.146E-06 | global batch size:     1 | lm loss: 8.047968E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.215 | TFLOPs: 53.95 |
[default0]:[2023-07-30 22:32:42,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=1055, skipped=0, lr=[1.1512490666666668e-06, 1.1512490666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:42,527] [INFO] [timer.py:215:stop] epoch=0/micro_step=1055/global_step=1055, RunningAvgSamplesPerSec=2.3518884910036544, CurrSamplesPerSec=2.349838341271073, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1055/439453125 | consumed samples:         1055 | consumed tokens:      2160640 | elapsed time per iteration (ms): 459.3 | learning rate: 1.151E-06 | global batch size:     1 | lm loss: 8.177579E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.177 | TFLOPs: 53.03 |
[default0]:[2023-07-30 22:32:44,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=0, lr=[1.1567104000000001e-06, 1.1567104000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:44,850] [INFO] [timer.py:215:stop] epoch=0/micro_step=1060/global_step=1060, RunningAvgSamplesPerSec=2.3515529275706166, CurrSamplesPerSec=2.3500542647255753, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1060/439453125 | consumed samples:         1060 | consumed tokens:      2170880 | elapsed time per iteration (ms): 464.4 | learning rate: 1.157E-06 | global batch size:     1 | lm loss: 8.139772E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.153 | TFLOPs: 52.45 |
[default0]:[2023-07-30 22:32:47,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=1065, skipped=0, lr=[1.1621717333333334e-06, 1.1621717333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:47,240] [INFO] [timer.py:215:stop] epoch=0/micro_step=1065/global_step=1065, RunningAvgSamplesPerSec=2.3515471091850983, CurrSamplesPerSec=2.3508300731988925, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1065/439453125 | consumed samples:         1065 | consumed tokens:      2181120 | elapsed time per iteration (ms): 477.7 | learning rate: 1.162E-06 | global batch size:     1 | lm loss: 8.187866E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.093 | TFLOPs: 50.98 |
[default0]:[2023-07-30 22:32:49,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=0, lr=[1.1676330666666667e-06, 1.1676330666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:49,464] [INFO] [timer.py:215:stop] epoch=0/micro_step=1070/global_step=1070, RunningAvgSamplesPerSec=2.3515244025805586, CurrSamplesPerSec=2.3472332747966607, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1070/439453125 | consumed samples:         1070 | consumed tokens:      2191360 | elapsed time per iteration (ms): 444.9 | learning rate: 1.168E-06 | global batch size:     1 | lm loss: 8.297260E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.248 | TFLOPs: 54.74 |
[default0]:[2023-07-30 22:32:51,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=1075, skipped=0, lr=[1.1730944e-06, 1.1730944e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:51,713] [INFO] [timer.py:215:stop] epoch=0/micro_step=1075/global_step=1075, RunningAvgSamplesPerSec=2.351570671887096, CurrSamplesPerSec=2.357062977461727, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1075/439453125 | consumed samples:         1075 | consumed tokens:      2201600 | elapsed time per iteration (ms): 449.8 | learning rate: 1.173E-06 | global batch size:     1 | lm loss: 8.208447E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.223 | TFLOPs: 54.15 |
[default0]:[2023-07-30 22:32:53,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=0, lr=[1.1785557333333334e-06, 1.1785557333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:53,950] [INFO] [timer.py:215:stop] epoch=0/micro_step=1080/global_step=1080, RunningAvgSamplesPerSec=2.3516245139586616, CurrSamplesPerSec=2.3489461030426, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1080/439453125 | consumed samples:         1080 | consumed tokens:      2211840 | elapsed time per iteration (ms): 447.7 | learning rate: 1.179E-06 | global batch size:     1 | lm loss: 8.143130E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.234 | TFLOPs: 54.41 |
[default0]:[2023-07-30 22:32:56,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=1085, skipped=0, lr=[1.1840170666666667e-06, 1.1840170666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:56,182] [INFO] [timer.py:215:stop] epoch=0/micro_step=1085/global_step=1085, RunningAvgSamplesPerSec=2.351679127500903, CurrSamplesPerSec=2.350787910853643, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1085/439453125 | consumed samples:         1085 | consumed tokens:      2222080 | elapsed time per iteration (ms): 446.5 | learning rate: 1.184E-06 | global batch size:     1 | lm loss: 8.253374E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.240 | TFLOPs: 54.55 |
[default0]:[2023-07-30 22:32:58,406] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=0, lr=[1.1894784000000002e-06, 1.1894784000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:58,407] [INFO] [timer.py:215:stop] epoch=0/micro_step=1090/global_step=1090, RunningAvgSamplesPerSec=2.351713848001646, CurrSamplesPerSec=2.354694338588813, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1090/439453125 | consumed samples:         1090 | consumed tokens:      2232320 | elapsed time per iteration (ms): 444.5 | learning rate: 1.189E-06 | global batch size:     1 | lm loss: 8.397025E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.250 | TFLOPs: 54.79 |
[default0]:[2023-07-30 22:33:00,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=1095, skipped=0, lr=[1.1949397333333335e-06, 1.1949397333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:00,687] [INFO] [timer.py:215:stop] epoch=0/micro_step=1095/global_step=1095, RunningAvgSamplesPerSec=2.3517026038951547, CurrSamplesPerSec=2.327510354839139, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1095/439453125 | consumed samples:         1095 | consumed tokens:      2242560 | elapsed time per iteration (ms): 456.2 | learning rate: 1.195E-06 | global batch size:     1 | lm loss: 8.056571E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.192 | TFLOPs: 53.39 |
[default0]:[2023-07-30 22:33:02,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=0, lr=[1.2004010666666668e-06, 1.2004010666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:02,935] [INFO] [timer.py:215:stop] epoch=0/micro_step=1100/global_step=1100, RunningAvgSamplesPerSec=2.351728324990902, CurrSamplesPerSec=2.3414403224410845, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1100/439453125 | consumed samples:         1100 | consumed tokens:      2252800 | elapsed time per iteration (ms): 449.7 | learning rate: 1.200E-06 | global batch size:     1 | lm loss: 8.261648E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.223 | TFLOPs: 54.15 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1100 | lm loss value: 8.226508E+00 | lm loss PPL: 3.738756E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:33:10,076] [INFO] [logging.py:96:log_dist] [Rank 0] step=1105, skipped=0, lr=[1.2058624000000002e-06, 1.2058624000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:10,077] [INFO] [timer.py:215:stop] epoch=0/micro_step=1105/global_step=1105, RunningAvgSamplesPerSec=2.351728875657648, CurrSamplesPerSec=2.355233810485365, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1105/439453125 | consumed samples:         1105 | consumed tokens:      2263040 | elapsed time per iteration (ms): 1428.1 | learning rate: 1.206E-06 | global batch size:     1 | lm loss: 8.061520E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.700 | TFLOPs: 17.05 |
[default0]:[2023-07-30 22:33:12,325] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=0, lr=[1.2113237333333333e-06, 1.2113237333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:12,328] [INFO] [timer.py:215:stop] epoch=0/micro_step=1110/global_step=1110, RunningAvgSamplesPerSec=2.3516982679333, CurrSamplesPerSec=2.3489724130553973, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1110/439453125 | consumed samples:         1110 | consumed tokens:      2273280 | elapsed time per iteration (ms): 450.1 | learning rate: 1.211E-06 | global batch size:     1 | lm loss: 8.158682E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.221 | TFLOPs: 54.11 |
[default0]:[2023-07-30 22:33:14,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=1115, skipped=0, lr=[1.2167850666666666e-06, 1.2167850666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:14,568] [INFO] [timer.py:215:stop] epoch=0/micro_step=1115/global_step=1115, RunningAvgSamplesPerSec=2.3517159670024292, CurrSamplesPerSec=2.3496711595128454, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1115/439453125 | consumed samples:         1115 | consumed tokens:      2283520 | elapsed time per iteration (ms): 448.1 | learning rate: 1.217E-06 | global batch size:     1 | lm loss: 7.894022E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.232 | TFLOPs: 54.35 |
[default0]:[2023-07-30 22:33:16,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=0, lr=[1.2222464e-06, 1.2222464e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:16,801] [INFO] [timer.py:215:stop] epoch=0/micro_step=1120/global_step=1120, RunningAvgSamplesPerSec=2.351728633424002, CurrSamplesPerSec=2.3551385916011762, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1120/439453125 | consumed samples:         1120 | consumed tokens:      2293760 | elapsed time per iteration (ms): 446.4 | learning rate: 1.222E-06 | global batch size:     1 | lm loss: 7.827985E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.240 | TFLOPs: 54.56 |
[default0]:[2023-07-30 22:33:19,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=1125, skipped=0, lr=[1.2277077333333334e-06, 1.2277077333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:19,033] [INFO] [timer.py:215:stop] epoch=0/micro_step=1125/global_step=1125, RunningAvgSamplesPerSec=2.351691816943724, CurrSamplesPerSec=2.3241866905902753, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1125/439453125 | consumed samples:         1125 | consumed tokens:      2304000 | elapsed time per iteration (ms): 446.5 | learning rate: 1.228E-06 | global batch size:     1 | lm loss: 8.141915E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.239 | TFLOPs: 54.54 |
[default0]:[2023-07-30 22:33:21,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=0, lr=[1.2331690666666667e-06, 1.2331690666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:21,274] [INFO] [timer.py:215:stop] epoch=0/micro_step=1130/global_step=1130, RunningAvgSamplesPerSec=2.3517227732661303, CurrSamplesPerSec=2.358570002671053, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1130/439453125 | consumed samples:         1130 | consumed tokens:      2314240 | elapsed time per iteration (ms): 448.4 | learning rate: 1.233E-06 | global batch size:     1 | lm loss: 7.820308E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.230 | TFLOPs: 54.32 |
[default0]:[2023-07-30 22:33:23,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=1135, skipped=0, lr=[1.2386304e-06, 1.2386304e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:23,532] [INFO] [timer.py:215:stop] epoch=0/micro_step=1135/global_step=1135, RunningAvgSamplesPerSec=2.351762158078207, CurrSamplesPerSec=2.3513255940675095, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1135/439453125 | consumed samples:         1135 | consumed tokens:      2324480 | elapsed time per iteration (ms): 451.3 | learning rate: 1.239E-06 | global batch size:     1 | lm loss: 8.048288E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.216 | TFLOPs: 53.96 |
[default0]:[2023-07-30 22:33:25,789] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=0, lr=[1.2440917333333334e-06, 1.2440917333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:25,791] [INFO] [timer.py:215:stop] epoch=0/micro_step=1140/global_step=1140, RunningAvgSamplesPerSec=2.3517675373363223, CurrSamplesPerSec=2.3461093988600323, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1140/439453125 | consumed samples:         1140 | consumed tokens:      2334720 | elapsed time per iteration (ms): 451.7 | learning rate: 1.244E-06 | global batch size:     1 | lm loss: 7.859527E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.214 | TFLOPs: 53.92 |
[default0]:[2023-07-30 22:33:28,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=1145, skipped=0, lr=[1.2495530666666667e-06, 1.2495530666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:28,027] [INFO] [timer.py:215:stop] epoch=0/micro_step=1145/global_step=1145, RunningAvgSamplesPerSec=2.3517885735261137, CurrSamplesPerSec=2.355531419130089, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1145/439453125 | consumed samples:         1145 | consumed tokens:      2344960 | elapsed time per iteration (ms): 447.4 | learning rate: 1.250E-06 | global batch size:     1 | lm loss: 8.033244E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.235 | TFLOPs: 54.43 |
[default0]:[2023-07-30 22:33:30,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=0, lr=[1.2550144e-06, 1.2550144e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:30,261] [INFO] [timer.py:215:stop] epoch=0/micro_step=1150/global_step=1150, RunningAvgSamplesPerSec=2.3517975138520244, CurrSamplesPerSec=2.3495408532237185, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1150/439453125 | consumed samples:         1150 | consumed tokens:      2355200 | elapsed time per iteration (ms): 446.9 | learning rate: 1.255E-06 | global batch size:     1 | lm loss: 8.002387E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.238 | TFLOPs: 54.50 |
[default0]:[2023-07-30 22:33:32,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=1155, skipped=0, lr=[1.2604757333333333e-06, 1.2604757333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:32,504] [INFO] [timer.py:215:stop] epoch=0/micro_step=1155/global_step=1155, RunningAvgSamplesPerSec=2.351809776944403, CurrSamplesPerSec=2.3542343960484957, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1155/439453125 | consumed samples:         1155 | consumed tokens:      2365440 | elapsed time per iteration (ms): 448.3 | learning rate: 1.260E-06 | global batch size:     1 | lm loss: 8.606442E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.230 | TFLOPs: 54.32 |
[default0]:[2023-07-30 22:33:34,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=0, lr=[1.2659370666666666e-06, 1.2659370666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:34,741] [INFO] [timer.py:215:stop] epoch=0/micro_step=1160/global_step=1160, RunningAvgSamplesPerSec=2.3518319350859134, CurrSamplesPerSec=2.348221493907048, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1160/439453125 | consumed samples:         1160 | consumed tokens:      2375680 | elapsed time per iteration (ms): 447.4 | learning rate: 1.266E-06 | global batch size:     1 | lm loss: 7.949646E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.235 | TFLOPs: 54.44 |
[default0]:[2023-07-30 22:33:36,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=1165, skipped=0, lr=[1.2713984000000001e-06, 1.2713984000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:36,997] [INFO] [timer.py:215:stop] epoch=0/micro_step=1165/global_step=1165, RunningAvgSamplesPerSec=2.351838604412479, CurrSamplesPerSec=2.345100665962554, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1165/439453125 | consumed samples:         1165 | consumed tokens:      2385920 | elapsed time per iteration (ms): 451.4 | learning rate: 1.271E-06 | global batch size:     1 | lm loss: 8.000771E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.216 | TFLOPs: 53.96 |
[default0]:[2023-07-30 22:33:39,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=0, lr=[1.2768597333333335e-06, 1.2768597333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:33:39,240] [INFO] [timer.py:215:stop] epoch=0/micro_step=1170/global_step=1170, RunningAvgSamplesPerSec=2.3518382718851707, CurrSamplesPerSec=2.3416024131160427, MemAllocated=17.28GB, MaxMemAllocated=27.09GB
[default0]: iteration     1170/439453125 | consumed samples:         1170 | consumed tokens:      2396160 | elapsed time per iteration (ms): 448.6 | learning rate: 1.277E-06 | global batch size:     1 | lm loss: 8.066772E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.229 | TFLOPs: 54.29 |
slurmstepd: error: *** STEP 4505026.0 ON n2gpu1212 CANCELLED AT 2023-07-30T22:33:40 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 4505026 ON n2gpu1212 CANCELLED AT 2023-07-30T22:33:40 DUE TO TIME LIMIT ***
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1380142 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1380142 closing signal SIGTERM
