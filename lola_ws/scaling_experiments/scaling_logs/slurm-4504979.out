cpu-bind=MASK - n2gpu1227, task  0  0 [3528377]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||--------|--------||-B------|--------||--------|--------||--------|--------|  set
Total estimated parameters in the Dense GPT-2 model: 125226240 (0.13B)
Total Estimated Parameters in the Sparse(MoE) GPT-2 model: 3752054016 (3.75B)
LAUNCHER: python -u -m torch.distributed.run --nproc_per_node 1 --nnodes 1 --rdzv_id=7569 --rdzv_endpoint n2gpu1227:6006 --rdzv_backend c10d --max_restarts 0 --tee 3
CMD: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/pretrain_gpt.py --override-opt_param-scheduler --adam-beta1 0.9 --adam-beta2 0.95 --tensor-model-parallel-size 1 --moe-expert-parallel-size 1 --num-experts 128 --moe-loss-coeff 0.01 --moe-train-capacity-factor 1.0 --moe-eval-capacity-factor 1.0 --moe-min-capacity 4 --init-method-std 0.01 --lr-decay-tokens 300000000000 --lr-warmup-tokens 375000000 --micro-batch-size 2 --exit-duration-in-mins 30000000 --global-batch-size 2 --num-layers 12 --hidden-size 768 --num-attention-heads 12 --seq-length 2048 --max-position-embeddings 2048 --train-tokens 300000000000 --train-iters 219726562 --lr 2.0e-4 --min-lr 2e-06 --lr-decay-style cosine --split 98,2,0 --log-interval 5 --eval-interval 100 --eval-iters 50 --save-interval 1000000 --weight-decay 0.1 --clip-grad 1.0 --hysteresis 2 --num-workers 0 --fp16 --load /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_2BATCH_1GPU_1Node/checkpoint/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true --save /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_2BATCH_1GPU_1Node/checkpoint/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true --tensorboard-queue-size 1 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_2BATCH_1GPU_1Node/tensorboard/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true_n2gpu1227_2023.07.30-22.21.10 --checkpoint-activations --create-moe-param-group --vocab-file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-vocab.json --merge-file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-merges.txt --data-path /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/meg-gpt2-oscar-en-10k_text_document --data-impl mmap --deepspeed --deepspeed_config /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_2BATCH_1GPU_1Node/ds_config_gpt_gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true.json --pipeline-model-parallel-size 1 --no-pipeline-parallel --deepspeed-activation-checkpointing
cpu-bind=MASK - n2gpu1227, task  0  0 [3528617]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||--------|--------||-B------|--------||--------|--------||--------|--------|  set
[default0]:[2023-07-30 22:21:13,897] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.10.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:**** Git info for Megatron: git_hash=6e47d55 git_branch=main ****
[default0]:using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
[default0]:using torch.float16 for parameters ...
[default0]:------------------------ arguments ------------------------
[default0]:  accumulate_allreduce_grads_in_fp32 .............. False
[default0]:  adam_beta1 ...................................... 0.9
[default0]:  adam_beta2 ...................................... 0.95
[default0]:  adam_eps ........................................ 1e-08
[default0]:  add_bias_linear ................................. True
[default0]:  add_position_embedding .......................... True
[default0]:  adlr_autoresume ................................. False
[default0]:  adlr_autoresume_interval ........................ 1000
[default0]:  aml_data_download_path .......................... None
[default0]:  apply_layernorm_1p .............................. False
[default0]:  apply_query_key_layer_scaling ................... True
[default0]:  apply_residual_connection_post_layernorm ........ False
[default0]:  async_tensor_model_parallel_allreduce ........... False
[default0]:  attention_dropout ............................... 0.1
[default0]:  attention_softmax_in_fp32 ....................... False
[default0]:  barrier_with_L1_time ............................ True
[default0]:  bert_binary_head ................................ True
[default0]:  bert_embedder_type .............................. megatron
[default0]:  bert_load ....................................... None
[default0]:  bf16 ............................................ False
[default0]:  bias_dropout_fusion ............................. True
[default0]:  bias_gelu_fusion ................................ True
[default0]:  biencoder_projection_dim ........................ 0
[default0]:  biencoder_shared_query_context_model ............ False
[default0]:  block_data_path ................................. None
[default0]:  checkpoint_activations .......................... True
[default0]:  checkpoint_in_cpu ............................... False
[default0]:  checkpoint_num_layers ........................... 1
[default0]:  classes_fraction ................................ 1.0
[default0]:  clip_grad ....................................... 1.0
[default0]:  compression_training ............................ False
[default0]:  consumed_train_samples .......................... 0
[default0]:  consumed_train_tokens ........................... 0
[default0]:  consumed_valid_samples .......................... 0
[default0]:  contigious_checkpointing ........................ False
[default0]:  cpu_optimizer ................................... False
[default0]:  cpu_torch_adam .................................. False
[default0]:  create_moe_param_group .......................... True
[default0]:  curriculum_learning_legacy ...................... False
[default0]:  data_cache_path ................................. None
[default0]:  data_efficiency_curriculum_learning ............. False
[default0]:  data_impl ....................................... mmap
[default0]:  data_parallel_random_init ....................... False
[default0]:  data_parallel_size .............................. 1
[default0]:  data_path ....................................... ['/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/meg-gpt2-oscar-en-10k_text_document']
[default0]:  data_per_class_fraction ......................... 1.0
[default0]:  data_sharding ................................... True
[default0]:  dataloader_type ................................. single
[default0]:  DDP_impl ........................................ local
[default0]:  decoder_num_layers .............................. None
[default0]:  decoder_seq_length .............................. None
[default0]:  deepscale ....................................... False
[default0]:  deepscale_config ................................ None
[default0]:  deepspeed ....................................... True
[default0]:  deepspeed_activation_checkpointing .............. True
[default0]:  deepspeed_config ................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_2BATCH_1GPU_1Node/ds_config_gpt_gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true.json
[default0]:  deepspeed_mpi ................................... False
[default0]:  dino_bottleneck_size ............................ 256
[default0]:  dino_freeze_last_layer .......................... 1
[default0]:  dino_head_hidden_size ........................... 2048
[default0]:  dino_local_crops_number ......................... 10
[default0]:  dino_local_img_size ............................. 96
[default0]:  dino_norm_last_layer ............................ False
[default0]:  dino_teacher_temp ............................... 0.07
[default0]:  dino_warmup_teacher_temp ........................ 0.04
[default0]:  dino_warmup_teacher_temp_epochs ................. 30
[default0]:  distribute_checkpointed_activations ............. False
[default0]:  distribute_saved_activations .................... False
[default0]:  distributed_backend ............................. nccl
[default0]:  distributed_timeout_minutes ..................... 10
[default0]:  ds_inference .................................... False
[default0]:  ds_pipeline_enabled ............................. False
[default0]:  embedding_path .................................. None
[default0]:  embedding_weights_in_fp32 ....................... False
[default0]:  empty_unused_memory_level ....................... 0
[default0]:  enable_expert_tensor_parallelism ................ False
[default0]:  encoder_num_layers .............................. 12
[default0]:  encoder_seq_length .............................. 2048
[default0]:  end_weight_decay ................................ 0.1
[default0]:  eod_mask_loss ................................... False
[default0]:  eval_interval ................................... 100
[default0]:  eval_iters ...................................... 50
[default0]:  evidence_data_path .............................. None
[default0]:  exit_duration_in_mins ........................... 30000000
[default0]:  exit_interval ................................... None
[default0]:  exit_on_missing_checkpoint ...................... False
[default0]:  exit_signal_handler ............................. False
[default0]:  expert_interval ................................. 2
[default0]:  ffn_hidden_size ................................. 3072
[default0]:  finetune ........................................ False
[default0]:  fp16 ............................................ True
[default0]:  fp16_lm_cross_entropy ........................... False
[default0]:  fp32_residual_connection ........................ False
[default0]:  fp8_amax_compute_algo ........................... most_recent
[default0]:  fp8_amax_history_len ............................ 1
[default0]:  fp8_e4m3 ........................................ False
[default0]:  fp8_hybrid ...................................... False
[default0]:  fp8_interval .................................... 1
[default0]:  fp8_margin ...................................... 0
[default0]:  fp8_wgrad ....................................... True
[default0]:  global_batch_size ............................... 2
[default0]:  gradient_accumulation_fusion .................... True
[default0]:  head_lr_mult .................................... 1.0
[default0]:  hidden_dropout .................................. 0.1
[default0]:  hidden_size ..................................... 768
[default0]:  hidden_size_teacher ............................. None
[default0]:  hysteresis ...................................... 2
[default0]:  ict_head_size ................................... None
[default0]:  ict_load ........................................ None
[default0]:  img_h ........................................... 224
[default0]:  img_w ........................................... 224
[default0]:  indexer_batch_size .............................. 128
[default0]:  indexer_log_interval ............................ 1000
[default0]:  inference ....................................... False
[default0]:  inference_batch_times_seqlen_threshold .......... 512
[default0]:  init_method_std ................................. 0.01
[default0]:  init_method_xavier_uniform ...................... False
[default0]:  initial_loss_scale .............................. 4294967296
[default0]:  iter_per_epoch .................................. 1250
[default0]:  kd .............................................. False
[default0]:  kd_alpha_ce ..................................... 1
[default0]:  kd_beta_ce ...................................... 1
[default0]:  kd_temp ......................................... 1.0
[default0]:  kv_channels ..................................... 64
[default0]:  layernorm_epsilon ............................... 1e-05
[default0]:  lazy_mpu_init ................................... None
[default0]:  load ............................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_2BATCH_1GPU_1Node/checkpoint/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:  load_teacher .................................... None
[default0]:  local_rank ...................................... None
[default0]:  log_batch_size_to_tensorboard ................... True
[default0]:  log_interval .................................... 5
[default0]:  log_learning_rate_to_tensorboard ................ True
[default0]:  log_loss_scale_to_tensorboard ................... True
[default0]:  log_memory_to_tensorboard ....................... False
[default0]:  log_num_zeros_in_grad ........................... False
[default0]:  log_optimizer_states_to_tensorboard ............. False
[default0]:  log_params_norm ................................. False
[default0]:  log_timers_to_tensorboard ....................... True
[default0]:  log_validation_ppl_to_tensorboard ............... True
[default0]:  log_world_size_to_tensorboard ................... False
[default0]:  loss_scale ...................................... None
[default0]:  loss_scale_window ............................... 1000
[default0]:  lr .............................................. 0.0002
[default0]:  lr_decay_iters .................................. None
[default0]:  lr_decay_samples ................................ None
[default0]:  lr_decay_style .................................. cosine
[default0]:  lr_decay_tokens ................................. 300000000000
[default0]:  lr_warmup_fraction .............................. None
[default0]:  lr_warmup_iters ................................. 0
[default0]:  lr_warmup_samples ............................... 0
[default0]:  lr_warmup_tokens ................................ 375000000
[default0]:  make_vocab_size_divisible_by .................... 128
[default0]:  mask_factor ..................................... 1.0
[default0]:  mask_prob ....................................... 0.15
[default0]:  mask_type ....................................... random
[default0]:  masked_softmax_fusion ........................... True
[default0]:  max_position_embeddings ......................... 2048
[default0]:  max_tokens_to_oom ............................... 12000
[default0]:  memory_centric_tiled_linear ..................... False
[default0]:  merge_file ...................................... /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-merges.txt
[default0]:  micro_batch_size ................................ 2
[default0]:  min_loss_scale .................................. 1.0
[default0]:  min_lr .......................................... 2e-06
[default0]:  mlp_type ........................................ standard
[default0]:  mmap_warmup ..................................... False
[default0]:  moe_eval_capacity_factor ........................ 1.0
[default0]:  moe_expert_parallel_size ........................ 1
[default0]:  moe_loss_coeff .................................. 0.01
[default0]:  moe_min_capacity ................................ 4
[default0]:  moe_token_dropping .............................. True
[default0]:  moe_train_capacity_factor ....................... 1.0
[default0]:  mos ............................................. False
[default0]:  no_load_lr_state ................................ False
[default0]:  no_load_optim ................................... None
[default0]:  no_load_rng ..................................... None
[default0]:  no_persist_layer_norm ........................... False
[default0]:  no_pipeline_parallel ............................ True
[default0]:  no_save_optim ................................... None
[default0]:  no_save_rng ..................................... None
[default0]:  normalization ................................... layernorm
[default0]:  num_attention_heads ............................. 12
[default0]:  num_attention_heads_teacher ..................... None
[default0]:  num_channels .................................... 3
[default0]:  num_classes ..................................... 1000
[default0]:  num_experts ..................................... [128]
[default0]:  num_experts_switch .............................. None
[default0]:  num_experts_teacher ............................. [1]
[default0]:  num_layers ...................................... 12
[default0]:  num_layers_per_virtual_pipeline_stage ........... None
[default0]:  num_layers_teacher .............................. None
[default0]:  num_workers ..................................... 0
[default0]:  onnx_safe ....................................... None
[default0]:  openai_gelu ..................................... False
[default0]:  optimizer ....................................... adam
[default0]:  output_bert_embeddings .......................... False
[default0]:  overlap_p2p_comm ................................ False
[default0]:  override_opt_param_scheduler .................... True
[default0]:  params_dtype .................................... torch.float16
[default0]:  partition_activations ........................... False
[default0]:  patch_dim ....................................... 16
[default0]:  perform_initialization .......................... True
[default0]:  pipeline_model_parallel_size .................... 1
[default0]:  pipeline_model_parallel_split_rank .............. None
[default0]:  profile_backward ................................ False
[default0]:  query_in_block_prob ............................. 0.1
[default0]:  rampup_batch_size ............................... None
[default0]:  random_ltd ...................................... False
[default0]:  rank ............................................ 0
[default0]:  recompute_granularity ........................... None
[default0]:  recompute_method ................................ None
[default0]:  recompute_num_layers ............................ 1
[default0]:  remote_device ................................... none
[default0]:  reset_attention_mask ............................ False
[default0]:  reset_iteration ................................. False
[default0]:  reset_position_ids .............................. False
[default0]:  retriever_report_topk_accuracies ................ []
[default0]:  retriever_score_scaling ......................... False
[default0]:  retriever_seq_length ............................ 256
[default0]:  retro_add_retriever ............................. False
[default0]:  retro_cyclic_train_iters ........................ None
[default0]:  retro_encoder_attention_dropout ................. 0.1
[default0]:  retro_encoder_hidden_dropout .................... 0.1
[default0]:  retro_encoder_layers ............................ 2
[default0]:  retro_num_neighbors ............................. 2
[default0]:  retro_num_retrieved_chunks ...................... 2
[default0]:  retro_return_doc_ids ............................ False
[default0]:  retro_workdir ................................... None
[default0]:  return_data_index ............................... False
[default0]:  rotary_percent .................................. 1.0
[default0]:  sample_rate ..................................... 1.0
[default0]:  save ............................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_2BATCH_1GPU_1Node/checkpoint/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:  save_interval ................................... 1000000
[default0]:  scatter_gather_tensors_in_pipeline .............. True
[default0]:  scattered_embeddings ............................ False
[default0]:  seed ............................................ 1234
[default0]:  seq_length ...................................... 2048
[default0]:  sequence_parallel ............................... False
[default0]:  sgd_momentum .................................... 0.9
[default0]:  short_seq_prob .................................. 0.1
[default0]:  skip_train ...................................... False
[default0]:  split ........................................... 98,2,0
[default0]:  split_transformers .............................. False
[default0]:  squared_relu .................................... False
[default0]:  standalone_embedding_stage ...................... False
[default0]:  start_weight_decay .............................. 0.1
[default0]:  swiglu .......................................... False
[default0]:  swin_backbone_type .............................. tiny
[default0]:  synchronize_each_layer .......................... False
[default0]:  tensor_model_parallel_size ...................... 1
[default0]:  tensorboard_dir ................................. /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_2BATCH_1GPU_1Node/tensorboard/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true_n2gpu1227_2023.07.30-22.21.10
[default0]:  tensorboard_log_interval ........................ 1
[default0]:  tensorboard_queue_size .......................... 1
[default0]:  test_data_path .................................. None
[default0]:  tile_factor ..................................... 1
[default0]:  timing_log_level ................................ 0
[default0]:  timing_log_option ............................... minmax
[default0]:  titles_data_path ................................ None
[default0]:  tokenizer_model ................................. None
[default0]:  tokenizer_type .................................. GPT2BPETokenizer
[default0]:  topk ............................................ 1
[default0]:  train_data_exact_num_epochs ..................... None
[default0]:  train_data_path ................................. None
[default0]:  train_desc_path ................................. None
[default0]:  train_doc_idx_path .............................. None
[default0]:  train_idx_path .................................. None
[default0]:  train_iters ..................................... 219726562
[default0]:  train_sample_idx_path ........................... None
[default0]:  train_samples ................................... None
[default0]:  train_shuffle_idx_path .......................... None
[default0]:  train_tokens .................................... 300000000000
[default0]:  transformer_impl ................................ local
[default0]:  transformer_pipeline_model_parallel_size ........ 1
[default0]:  untie_embeddings_and_output_weights ............. False
[default0]:  use_checkpoint_args ............................. False
[default0]:  use_checkpoint_opt_param_scheduler .............. False
[default0]:  use_contiguous_buffers_in_local_ddp ............. True
[default0]:  use_cpu_initialization .......................... None
[default0]:  use_distributed_optimizer ....................... False
[default0]:  use_flash_attn .................................. False
[default0]:  use_one_sent_docs ............................... False
[default0]:  use_pin_memory .................................. False
[default0]:  use_ring_exchange_p2p ........................... False
[default0]:  use_rotary_position_embeddings .................. False
[default0]:  use_tutel ....................................... False
[default0]:  valid_data_path ................................. None
[default0]:  variable_seq_lengths ............................ False
[default0]:  virtual_pipeline_model_parallel_size ............ None
[default0]:  vision_backbone_type ............................ vit
[default0]:  vision_pretraining .............................. False
[default0]:  vision_pretraining_type ......................... classify
[default0]:  vocab_extra_ids ................................. 0
[default0]:  vocab_file ...................................... /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-vocab.json
[default0]:  vocab_size ...................................... None
[default0]:  weight_decay .................................... 0.1
[default0]:  weight_decay_incr_style ......................... constant
[default0]:  world_size ...................................... 1
[default0]:  zero_allgather_bucket_size ...................... 0.0
[default0]:  zero_contigious_gradients ....................... False
[default0]:  zero_reduce_bucket_size ......................... 0.0
[default0]:  zero_reduce_scatter ............................. False
[default0]:  zero_stage ...................................... 1.0
[default0]:-------------------- end of arguments ---------------------
[default0]:setting number of micro-batches to constant 1
[default0]:> building GPT2BPETokenizer tokenizer ...
[default0]: > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
[default0]:> setting tensorboard ...
[default0]:> initializing torch distributed ...
[default0]:[2023-07-30 22:21:34,080] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[default0]:[2023-07-30 22:21:34,080] [INFO] [comm.py:616:init_distributed] cdb=None
[default0]:[2023-07-30 22:21:34,080] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[default0]:> initialized tensor model parallel with size 1
[default0]:> initialized pipeline model parallel with size 1
[default0]:> setting random seeds to 1234 ...
[default0]:> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[default0]:> compiling dataset index builder ...
[default0]:make: Entering directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/data'
[default0]:make: Nothing to be done for 'default'.
[default0]:make: Leaving directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/data'
[default0]:>>> done with dataset index builder. Compilation time: 0.089 seconds
[default0]:> compiling and loading fused kernels ...
[default0]:Loading extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Loading extension module scaled_masked_softmax_cuda...
[default0]:Loading extension module scaled_softmax_cuda...
[default0]:n2gpu1227:3528636:3528636 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.95<0>
[default0]:n2gpu1227:3528636:3528636 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1227:3528636:3528636 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.95<0>
[default0]:n2gpu1227:3528636:3528636 [0] NCCL INFO Using network IB
[default0]:NCCL version 2.12.12+cuda11.7
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Setting affinity for GPU 0 to 020000,00000000,00000000
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Connected all rings
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO Connected all trees
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1227:3528636:3528841 [0] NCCL INFO comm 0x14ad340090d0 rank 0 nranks 1 cudaDev 0 busId c4000 - Init COMPLETE
[default0]:>>> done with compiling and loading fused kernels. Compilation time: 2.262 seconds
[default0]:time to initialize megatron (seconds): 5.655
[default0]:[after megatron is initialized] datetime: 2023-07-30 22:21:37 
[default0]:building GPT model ...
[default0]:[2023-07-30 22:21:37,201] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[default0]:[2023-07-30 22:21:37,202] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.4 GB         CA 0.0 GB         Max_CA 0 GB 
[default0]:[2023-07-30 22:21:37,202] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 26.97 GB, percent = 5.4%
[default0]:[2023-07-30 22:21:37,290] [INFO] [utils.py:785:see_memory_usage] After Building Model
[default0]:[2023-07-30 22:21:37,291] [INFO] [utils.py:786:see_memory_usage] MA 0.24 GB         Max_MA 0.24 GB         CA 0.25 GB         Max_CA 0 GB 
[default0]:[2023-07-30 22:21:37,291] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 26.98 GB, percent = 5.4%
[default0]: > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 125262336
[default0]:param_group keyset: dict_keys(['name', 'params', 'wd_mult', 'lr_mult'])
[default0]:> learning rate decay style: cosine
[default0]:DeepSpeed is enabled.
[default0]:[2023-07-30 22:21:37,293] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Setting affinity for GPU 0 to 020000,00000000,00000000
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Connected all rings
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO Connected all trees
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1227:3528636:3528973 [0] NCCL INFO comm 0x14ac680090d0 rank 0 nranks 1 cudaDev 0 busId c4000 - Init COMPLETE
[default0]:[2023-07-30 22:21:37,707] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[default0]:[2023-07-30 22:21:37,707] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[default0]:[2023-07-30 22:21:37,707] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[default0]:[2023-07-30 22:21:37,710] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[default0]:[2023-07-30 22:21:37,710] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[default0]:[2023-07-30 22:21:37,710] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[default0]:[2023-07-30 22:21:37,710] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[default0]:[2023-07-30 22:21:37,710] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[default0]:[2023-07-30 22:21:37,710] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[default0]:[2023-07-30 22:21:37,711] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
[default0]:Rank: 0 partition count [1, 1] and sizes[(125140992, False), (121344, False)] 
[default0]:[2023-07-30 22:21:38,015] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[default0]:[2023-07-30 22:21:38,016] [INFO] [utils.py:786:see_memory_usage] MA 0.7 GB         Max_MA 0.7 GB         CA 0.7 GB         Max_CA 1 GB 
[default0]:[2023-07-30 22:21:38,016] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 27.22 GB, percent = 5.4%
[default0]:[2023-07-30 22:21:38,069] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[default0]:[2023-07-30 22:21:38,069] [INFO] [utils.py:786:see_memory_usage] MA 1.64 GB         Max_MA 2.1 GB         CA 2.11 GB         Max_CA 2 GB 
[default0]:[2023-07-30 22:21:38,070] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 27.22 GB, percent = 5.4%
[default0]:[2023-07-30 22:21:38,070] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[default0]:[2023-07-30 22:21:38,111] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[default0]:[2023-07-30 22:21:38,112] [INFO] [utils.py:786:see_memory_usage] MA 1.64 GB         Max_MA 1.64 GB         CA 2.11 GB         Max_CA 2 GB 
[default0]:[2023-07-30 22:21:38,112] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 27.22 GB, percent = 5.4%
[default0]:[2023-07-30 22:21:38,114] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[default0]:[2023-07-30 22:21:38,114] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[default0]:[2023-07-30 22:21:38,114] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x14adb615b700>
[default0]:[2023-07-30 22:21:38,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:38,115] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[default0]:[2023-07-30 22:21:38,115] [INFO] [config.py:964:print]   activation_checkpointing_config  {
[default0]:    "partition_activations": false, 
[default0]:    "contiguous_memory_optimization": false, 
[default0]:    "cpu_checkpointing": false, 
[default0]:    "number_checkpoints": null, 
[default0]:    "synchronize_checkpoint_boundary": false, 
[default0]:    "profile": false
[default0]:}
[default0]:[2023-07-30 22:21:38,115] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[default0]:[2023-07-30 22:21:38,115] [INFO] [config.py:964:print]   amp_enabled .................. False
[default0]:[2023-07-30 22:21:38,115] [INFO] [config.py:964:print]   amp_params ................... False
[default0]:[2023-07-30 22:21:38,115] [INFO] [config.py:964:print]   autotuning_config ............ {
[default0]:    "enabled": false, 
[default0]:    "start_step": null, 
[default0]:    "end_step": null, 
[default0]:    "metric_path": null, 
[default0]:    "arg_mappings": null, 
[default0]:    "metric": "throughput", 
[default0]:    "model_info": null, 
[default0]:    "results_dir": "autotuning_results", 
[default0]:    "exps_dir": "autotuning_exps", 
[default0]:    "overwrite": true, 
[default0]:    "fast": true, 
[default0]:    "start_profile_step": 3, 
[default0]:    "end_profile_step": 5, 
[default0]:    "tuner_type": "gridsearch", 
[default0]:    "tuner_early_stopping": 5, 
[default0]:    "tuner_num_trials": 50, 
[default0]:    "model_info_path": null, 
[default0]:    "mp_size": 1, 
[default0]:    "max_train_batch_size": null, 
[default0]:    "min_train_batch_size": 1, 
[default0]:    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[default0]:    "min_train_micro_batch_size_per_gpu": 1, 
[default0]:    "num_tuning_micro_batch_sizes": 3
[default0]:}
[default0]:[2023-07-30 22:21:38,115] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[default0]:[2023-07-30 22:21:38,115] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[default0]:[2023-07-30 22:21:38,115] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[default0]:[2023-07-30 22:21:38,115] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14adb61b9c30>
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   communication_data_type ...... None
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   curriculum_params_legacy ..... {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 28195488, 'difficulty_step': 8}}
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   disable_allgather ............ False
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   dump_state ................... False
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[default0]:[2023-07-30 22:21:38,116] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   flops_profiler_config ........ {
[default0]:    "enabled": false, 
[default0]:    "recompute_fwd_factor": 0.0, 
[default0]:    "profile_step": 1, 
[default0]:    "module_depth": -1, 
[default0]:    "top_modules": 1, 
[default0]:    "detailed": true, 
[default0]:    "output_file": null
[default0]:}
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   fp16_enabled ................. True
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   global_rank .................. 0
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   gradient_clipping ............ 1
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[default0]:[2023-07-30 22:21:38,117] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   loss_scale ................... 0
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   memory_breakdown ............. False
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   nebula_config ................ {
[default0]:    "enabled": false, 
[default0]:    "persistent_storage_path": null, 
[default0]:    "persistent_time_interval": 100, 
[default0]:    "num_of_version_in_retention": 2, 
[default0]:    "enable_nebula_load": true, 
[default0]:    "load_path": null
[default0]:}
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   optimizer_name ............... None
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   optimizer_params ............. None
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   pld_enabled .................. False
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   pld_params ................... False
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   prescale_gradients ........... False
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   scheduler_name ............... None
[default0]:[2023-07-30 22:21:38,118] [INFO] [config.py:964:print]   scheduler_params ............. None
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   sparse_attention ............. None
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   steps_per_print .............. 5
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   train_batch_size ............. 2
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  2
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   world_size ................... 1
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   zero_enabled ................. True
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[default0]:[2023-07-30 22:21:38,119] [INFO] [config.py:950:print_user_config]   json = {
[default0]:    "train_batch_size": 2, 
[default0]:    "train_micro_batch_size_per_gpu": 2, 
[default0]:    "steps_per_print": 5, 
[default0]:    "zero_optimization": {
[default0]:        "stage": 2
[default0]:    }, 
[default0]:    "gradient_clipping": 1, 
[default0]:    "prescale_gradients": false, 
[default0]:    "fp16": {
[default0]:        "enabled": true, 
[default0]:        "loss_scale": 0, 
[default0]:        "loss_scale_window": 500, 
[default0]:        "hysteresis": 2, 
[default0]:        "min_loss_scale": 1, 
[default0]:        "initial_scale_power": 11
[default0]:    }, 
[default0]:    "bf16": {
[default0]:        "enabled": false
[default0]:    }, 
[default0]:    "curriculum_learning": {
[default0]:        "enabled": false, 
[default0]:        "curriculum_type": "seqlen", 
[default0]:        "min_difficulty": 80, 
[default0]:        "max_difficulty": 2.048000e+03, 
[default0]:        "schedule_type": "fixed_linear", 
[default0]:        "schedule_config": {
[default0]:            "total_curriculum_step": 2.819549e+07, 
[default0]:            "difficulty_step": 8
[default0]:        }
[default0]:    }, 
[default0]:    "wall_clock_breakdown": false
[default0]:}
[default0]:[2023-07-30 22:21:38,120] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_2BATCH_1GPU_1Node/checkpoint/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:WARNING: could not find the metadata file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.35B_MoE128_2BATCH_1GPU_1Node/checkpoint/gpt-0.35B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true 
[default0]:    will not load any checkpoints and will start from random
[default0]:(min, max) time across ranks (ms):
[default0]:    load-checkpoint ................................: (0.66, 0.66)
[default0]:[after model, optimizer, and learning rate scheduler are built] datetime: 2023-07-30 22:21:38 
[default0]:> building train, validation, and test datasets ...
[default0]: > datasets target sizes (minimum size):
[default0]:    train:      439453124
[default0]:    validation: 219726600
[default0]:    test:       100
[default0]:> building train, validation, and test datasets for GPT ...
[default0]:Single data path provided for train, valid & test
[default0]: > building dataset index ...
[default0]:    reading sizes...
[default0]:    reading pointers...
[default0]:    reading document index...
[default0]:    creating numpy buffer of mmap...
[default0]:    creating memory view of numpy buffer...
[default0]: > finished creating indexed dataset in 0.000603 seconds
[default0]:    number of documents: 10000
[default0]: > dataset split:
[default0]:    train:
[default0]:     document indices in [0, 9800) total of 9800 documents
[default0]:    validation:
[default0]:     document indices in [9800, 10000) total of 200 documents
[default0]:    test:
[default0]:     document indices in [10000, 10000) total of 0 documents
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Setting affinity for GPU 0 to 020000,00000000,00000000
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Connected all rings
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO Connected all trees
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1227:3528636:3528983 [0] NCCL INFO comm 0x14ab840090d0 rank 0 nranks 1 cudaDev 0 busId c4000 - Init COMPLETE
[default0]: > loading doc-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/71654a473c8694dd2b48f0197bdd21a9_doc_idx.npy
[default0]: > loading sample-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/71654a473c8694dd2b48f0197bdd21a9_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/71654a473c8694dd2b48f0197bdd21a9_shuffle_idx.npy
[default0]:    loaded indexed file in 0.063 seconds
[default0]:    total number of samples: 439466004
[default0]:    total number of epochs: 30909
[default0]: > loading doc-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/de3819301120d9a35080a0a129b3870d_doc_idx.npy
[default0]: > loading sample-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/de3819301120d9a35080a0a129b3870d_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/de3819301120d9a35080a0a129b3870d_shuffle_idx.npy
[default0]:    loaded indexed file in 0.007 seconds
[default0]:    total number of samples: 219726809
[default0]:    total number of epochs: 648115
[default0]:> finished creating GPT datasets ...
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Setting affinity for GPU 0 to 020000,00000000,00000000
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Connected all rings
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO Connected all trees
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1227:3528636:3528990 [0] NCCL INFO comm 0x14ab84303de0 rank 0 nranks 1 cudaDev 0 busId c4000 - Init COMPLETE
[default0]:[after dataloaders are built] datetime: 2023-07-30 22:21:39 
[default0]:done with setup ...
[default0]:(min, max) time across ranks (ms):
[default0]:    model-and-optimizer-setup ......................: (966.95, 966.95)
[default0]:    train/valid/test-data-iterators-setup ..........: (1097.97, 1097.97)
[default0]:training ...
[default0]:[before the start of training step] datetime: 2023-07-30 22:21:39 
[default0]:[2023-07-30 22:21:41,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[8.738133333333334e-09, 8.738133333333334e-09], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:41,370] [INFO] [timer.py:215:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=12.295456521972799, CurrSamplesPerSec=12.310679782626881, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration        5/219726562 | consumed samples:           10 | consumed tokens:        20480 | elapsed time per iteration (ms): 394.8 | learning rate: 8.738E-09 | global batch size:     2 | lm loss: 1.085263E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 5.066 | TFLOPs: 12.59 |
[default0]:[Rank 0] (after 5 iterations) memory (MB) | allocated: 1866.31103515625 | max allocated: 4091.39208984375 | reserved: 5748.0 | max reserved: 5748.0
[default0]:[2023-07-30 22:21:42,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.9660800000000002e-08, 1.9660800000000002e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:42,289] [INFO] [timer.py:215:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=12.288592528570408, CurrSamplesPerSec=12.202765352360586, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       10/219726562 | consumed samples:           20 | consumed tokens:        40960 | elapsed time per iteration (ms): 183.9 | learning rate: 1.966E-08 | global batch size:     2 | lm loss: 1.085148E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.873 | TFLOPs: 27.02 |
[default0]:[2023-07-30 22:21:43,200] [INFO] [logging.py:96:log_dist] [Rank 0] step=15, skipped=0, lr=[3.0583466666666664e-08, 3.0583466666666664e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:43,201] [INFO] [timer.py:215:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=12.27473126630693, CurrSamplesPerSec=12.187128895552485, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       15/219726562 | consumed samples:           30 | consumed tokens:        61440 | elapsed time per iteration (ms): 183.7 | learning rate: 3.058E-08 | global batch size:     2 | lm loss: 1.085522E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.885 | TFLOPs: 27.05 |
[default0]:[2023-07-30 22:21:44,133] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[4.150613333333333e-08, 4.150613333333333e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:44,135] [INFO] [timer.py:215:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=12.259307155453218, CurrSamplesPerSec=12.000526451317558, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       20/219726562 | consumed samples:           40 | consumed tokens:        81920 | elapsed time per iteration (ms): 185.5 | learning rate: 4.151E-08 | global batch size:     2 | lm loss: 1.085320E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.784 | TFLOPs: 26.80 |
[default0]:[2023-07-30 22:21:45,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=25, skipped=0, lr=[5.24288e-08, 5.24288e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:45,042] [INFO] [timer.py:215:stop] epoch=0/micro_step=25/global_step=25, RunningAvgSamplesPerSec=12.27218767801982, CurrSamplesPerSec=12.254479689832792, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       25/219726562 | consumed samples:           50 | consumed tokens:       102400 | elapsed time per iteration (ms): 180.9 | learning rate: 5.243E-08 | global batch size:     2 | lm loss: 1.084817E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 11.053 | TFLOPs: 27.47 |
[default0]:[2023-07-30 22:21:45,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[6.335146666666667e-08, 6.335146666666667e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:45,943] [INFO] [timer.py:215:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=12.285685293249369, CurrSamplesPerSec=12.269015376109916, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       30/219726562 | consumed samples:           60 | consumed tokens:       122880 | elapsed time per iteration (ms): 180.4 | learning rate: 6.335E-08 | global batch size:     2 | lm loss: 1.083857E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 11.084 | TFLOPs: 27.54 |
[default0]:[2023-07-30 22:21:46,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=35, skipped=0, lr=[7.427413333333334e-08, 7.427413333333334e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:46,848] [INFO] [timer.py:215:stop] epoch=0/micro_step=35/global_step=35, RunningAvgSamplesPerSec=12.294963944421646, CurrSamplesPerSec=12.375005716460604, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       35/219726562 | consumed samples:           70 | consumed tokens:       143360 | elapsed time per iteration (ms): 181.0 | learning rate: 7.427E-08 | global batch size:     2 | lm loss: 1.083020E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 11.052 | TFLOPs: 27.46 |
[default0]:[2023-07-30 22:21:47,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[8.519680000000001e-08, 8.519680000000001e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:47,791] [INFO] [timer.py:215:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=12.29841961730915, CurrSamplesPerSec=12.172079957281824, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       40/219726562 | consumed samples:           80 | consumed tokens:       163840 | elapsed time per iteration (ms): 188.5 | learning rate: 8.520E-08 | global batch size:     2 | lm loss: 1.081932E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.609 | TFLOPs: 26.36 |
[default0]:[2023-07-30 22:21:48,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=45, skipped=0, lr=[9.611946666666668e-08, 9.611946666666668e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:48,710] [INFO] [timer.py:215:stop] epoch=0/micro_step=45/global_step=45, RunningAvgSamplesPerSec=12.299082777455089, CurrSamplesPerSec=12.361328997063145, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       45/219726562 | consumed samples:           90 | consumed tokens:       184320 | elapsed time per iteration (ms): 183.8 | learning rate: 9.612E-08 | global batch size:     2 | lm loss: 1.080620E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.879 | TFLOPs: 27.03 |
[default0]:[2023-07-30 22:21:49,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[1.0704213333333333e-07, 1.0704213333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:49,628] [INFO] [timer.py:215:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=12.3048188638945, CurrSamplesPerSec=12.340670802482666, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       50/219726562 | consumed samples:          100 | consumed tokens:       204800 | elapsed time per iteration (ms): 184.0 | learning rate: 1.070E-07 | global batch size:     2 | lm loss: 1.078279E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.868 | TFLOPs: 27.00 |
[default0]:[2023-07-30 22:21:50,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=55, skipped=0, lr=[1.179648e-07, 1.179648e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:50,546] [INFO] [timer.py:215:stop] epoch=0/micro_step=55/global_step=55, RunningAvgSamplesPerSec=12.310818862206515, CurrSamplesPerSec=12.384615725314132, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       55/219726562 | consumed samples:          110 | consumed tokens:       225280 | elapsed time per iteration (ms): 183.7 | learning rate: 1.180E-07 | global batch size:     2 | lm loss: 1.077341E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.888 | TFLOPs: 27.05 |
[default0]:[2023-07-30 22:21:51,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.2888746666666666e-07, 1.2888746666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:51,505] [INFO] [timer.py:215:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=12.316400177139334, CurrSamplesPerSec=12.320334863227465, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       60/219726562 | consumed samples:          120 | consumed tokens:       245760 | elapsed time per iteration (ms): 192.0 | learning rate: 1.289E-07 | global batch size:     2 | lm loss: 1.076054E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.416 | TFLOPs: 25.88 |
[default0]:[2023-07-30 22:21:52,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=65, skipped=0, lr=[1.3981013333333334e-07, 1.3981013333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:52,427] [INFO] [timer.py:215:stop] epoch=0/micro_step=65/global_step=65, RunningAvgSamplesPerSec=12.317285055479514, CurrSamplesPerSec=12.250470969305944, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       65/219726562 | consumed samples:          130 | consumed tokens:       266240 | elapsed time per iteration (ms): 184.3 | learning rate: 1.398E-07 | global batch size:     2 | lm loss: 1.073481E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.851 | TFLOPs: 26.96 |
[default0]:[2023-07-30 22:21:53,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.5073280000000002e-07, 1.5073280000000002e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:53,354] [INFO] [timer.py:215:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=12.313912941999428, CurrSamplesPerSec=12.320678675079606, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       70/219726562 | consumed samples:          140 | consumed tokens:       286720 | elapsed time per iteration (ms): 185.1 | learning rate: 1.507E-07 | global batch size:     2 | lm loss: 1.070953E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.803 | TFLOPs: 26.84 |
[default0]:[2023-07-30 22:21:54,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=75, skipped=0, lr=[1.6165546666666667e-07, 1.6165546666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:54,273] [INFO] [timer.py:215:stop] epoch=0/micro_step=75/global_step=75, RunningAvgSamplesPerSec=12.313930632031447, CurrSamplesPerSec=12.203546453183122, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       75/219726562 | consumed samples:          150 | consumed tokens:       307200 | elapsed time per iteration (ms): 183.9 | learning rate: 1.617E-07 | global batch size:     2 | lm loss: 1.069119E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.877 | TFLOPs: 27.03 |
[default0]:[2023-07-30 22:21:55,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.7257813333333336e-07, 1.7257813333333336e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:55,195] [INFO] [timer.py:215:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=12.31551980241907, CurrSamplesPerSec=12.333630820668068, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       80/219726562 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 184.3 | learning rate: 1.726E-07 | global batch size:     2 | lm loss: 1.069131E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.853 | TFLOPs: 26.97 |
[default0]:[2023-07-30 22:21:56,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=85, skipped=0, lr=[1.8350080000000004e-07, 1.8350080000000004e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:56,112] [INFO] [timer.py:215:stop] epoch=0/micro_step=85/global_step=85, RunningAvgSamplesPerSec=12.319324904261988, CurrSamplesPerSec=12.390085105503056, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       85/219726562 | consumed samples:          170 | consumed tokens:       348160 | elapsed time per iteration (ms): 183.6 | learning rate: 1.835E-07 | global batch size:     2 | lm loss: 1.064141E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.891 | TFLOPs: 27.06 |
[default0]:[2023-07-30 22:21:57,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1.9442346666666667e-07, 1.9442346666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:57,056] [INFO] [timer.py:215:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=12.324291710192917, CurrSamplesPerSec=12.36318725451906, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       90/219726562 | consumed samples:          180 | consumed tokens:       368640 | elapsed time per iteration (ms): 188.4 | learning rate: 1.944E-07 | global batch size:     2 | lm loss: 1.057578E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.617 | TFLOPs: 26.38 |
[default0]:[2023-07-30 22:21:58,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=95, skipped=0, lr=[2.0534613333333335e-07, 2.0534613333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:58,007] [INFO] [timer.py:215:stop] epoch=0/micro_step=95/global_step=95, RunningAvgSamplesPerSec=12.329872977299058, CurrSamplesPerSec=12.284485005755188, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration       95/219726562 | consumed samples:          190 | consumed tokens:       389120 | elapsed time per iteration (ms): 190.4 | learning rate: 2.053E-07 | global batch size:     2 | lm loss: 1.059176E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.502 | TFLOPs: 26.09 |
[default0]:[2023-07-30 22:21:58,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[2.162688e-07, 2.162688e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:21:58,926] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=12.330937594760071, CurrSamplesPerSec=12.299777276972726, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      100/219726562 | consumed samples:          200 | consumed tokens:       409600 | elapsed time per iteration (ms): 183.7 | learning rate: 2.163E-07 | global batch size:     2 | lm loss: 1.057011E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.887 | TFLOPs: 27.05 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 100 | lm loss value: 1.053298E+01 | lm loss PPL: 3.753316E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:22:03,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=105, skipped=0, lr=[2.271914666666667e-07, 2.271914666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:03,157] [INFO] [timer.py:215:stop] epoch=0/micro_step=105/global_step=105, RunningAvgSamplesPerSec=12.321845353968898, CurrSamplesPerSec=12.11724039307397, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      105/219726562 | consumed samples:          210 | consumed tokens:       430080 | elapsed time per iteration (ms): 846.2 | learning rate: 2.272E-07 | global batch size:     2 | lm loss: 1.055561E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.363 | TFLOPs: 5.87 |
[default0]:[2023-07-30 22:22:04,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[2.3811413333333334e-07, 2.3811413333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:04,098] [INFO] [timer.py:215:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=12.322194390267818, CurrSamplesPerSec=12.07868140539124, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      110/219726562 | consumed samples:          220 | consumed tokens:       450560 | elapsed time per iteration (ms): 188.8 | learning rate: 2.381E-07 | global batch size:     2 | lm loss: 1.050898E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.594 | TFLOPs: 26.32 |
[default0]:[2023-07-30 22:22:05,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=115, skipped=0, lr=[2.490368e-07, 2.490368e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:05,041] [INFO] [timer.py:215:stop] epoch=0/micro_step=115/global_step=115, RunningAvgSamplesPerSec=12.321267539487026, CurrSamplesPerSec=12.385548288030238, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      115/219726562 | consumed samples:          230 | consumed tokens:       471040 | elapsed time per iteration (ms): 188.0 | learning rate: 2.490E-07 | global batch size:     2 | lm loss: 1.051102E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.637 | TFLOPs: 26.43 |
[default0]:[2023-07-30 22:22:06,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[2.599594666666667e-07, 2.599594666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:06,015] [INFO] [timer.py:215:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=12.322900113007503, CurrSamplesPerSec=12.25784617722614, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      120/219726562 | consumed samples:          240 | consumed tokens:       491520 | elapsed time per iteration (ms): 194.6 | learning rate: 2.600E-07 | global batch size:     2 | lm loss: 1.047389E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.275 | TFLOPs: 25.53 |
[default0]:[2023-07-30 22:22:06,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=125, skipped=0, lr=[2.7088213333333336e-07, 2.7088213333333336e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:06,932] [INFO] [timer.py:215:stop] epoch=0/micro_step=125/global_step=125, RunningAvgSamplesPerSec=12.32274445111884, CurrSamplesPerSec=12.31774784220094, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      125/219726562 | consumed samples:          250 | consumed tokens:       512000 | elapsed time per iteration (ms): 183.6 | learning rate: 2.709E-07 | global batch size:     2 | lm loss: 1.049218E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.893 | TFLOPs: 27.07 |
[default0]:[2023-07-30 22:22:08,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[2.818048e-07, 2.818048e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:08,072] [INFO] [timer.py:215:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=12.324737161141757, CurrSamplesPerSec=12.224567150824312, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      130/219726562 | consumed samples:          260 | consumed tokens:       532480 | elapsed time per iteration (ms): 228.0 | learning rate: 2.818E-07 | global batch size:     2 | lm loss: 1.044098E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.772 | TFLOPs: 21.80 |
[default0]:[2023-07-30 22:22:08,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=135, skipped=0, lr=[2.9272746666666667e-07, 2.9272746666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:08,992] [INFO] [timer.py:215:stop] epoch=0/micro_step=135/global_step=135, RunningAvgSamplesPerSec=12.32513227547421, CurrSamplesPerSec=12.3353719239168, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      135/219726562 | consumed samples:          270 | consumed tokens:       552960 | elapsed time per iteration (ms): 184.2 | learning rate: 2.927E-07 | global batch size:     2 | lm loss: 1.042804E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.858 | TFLOPs: 26.98 |
[default0]:[2023-07-30 22:22:09,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[3.0365013333333335e-07, 3.0365013333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:09,990] [INFO] [timer.py:215:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=12.327338010559362, CurrSamplesPerSec=12.327993745333984, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      140/219726562 | consumed samples:          280 | consumed tokens:       573440 | elapsed time per iteration (ms): 199.4 | learning rate: 3.037E-07 | global batch size:     2 | lm loss: 1.039082E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.029 | TFLOPs: 24.92 |
[default0]:[2023-07-30 22:22:10,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=145, skipped=0, lr=[3.1457280000000004e-07, 3.1457280000000004e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:10,922] [INFO] [timer.py:215:stop] epoch=0/micro_step=145/global_step=145, RunningAvgSamplesPerSec=12.327625327795756, CurrSamplesPerSec=12.34016249350894, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      145/219726562 | consumed samples:          290 | consumed tokens:       593920 | elapsed time per iteration (ms): 186.7 | learning rate: 3.146E-07 | global batch size:     2 | lm loss: 1.037996E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.714 | TFLOPs: 26.62 |
[default0]:[2023-07-30 22:22:11,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[3.2549546666666666e-07, 3.2549546666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:11,838] [INFO] [timer.py:215:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=12.329234174794335, CurrSamplesPerSec=12.277473026057736, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      150/219726562 | consumed samples:          300 | consumed tokens:       614400 | elapsed time per iteration (ms): 182.9 | learning rate: 3.255E-07 | global batch size:     2 | lm loss: 1.039087E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.936 | TFLOPs: 27.17 |
[default0]:[2023-07-30 22:22:12,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=155, skipped=0, lr=[3.3641813333333335e-07, 3.3641813333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:12,781] [INFO] [timer.py:215:stop] epoch=0/micro_step=155/global_step=155, RunningAvgSamplesPerSec=12.3270292194087, CurrSamplesPerSec=12.209656690153658, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      155/219726562 | consumed samples:          310 | consumed tokens:       634880 | elapsed time per iteration (ms): 188.5 | learning rate: 3.364E-07 | global batch size:     2 | lm loss: 1.035645E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.613 | TFLOPs: 26.37 |
[default0]:[2023-07-30 22:22:13,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[3.4734080000000003e-07, 3.4734080000000003e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:13,719] [INFO] [timer.py:215:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=12.32796565202355, CurrSamplesPerSec=12.36637674322611, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      160/219726562 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 187.6 | learning rate: 3.473E-07 | global batch size:     2 | lm loss: 1.033934E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.662 | TFLOPs: 26.49 |
[default0]:[2023-07-30 22:22:14,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=165, skipped=0, lr=[3.582634666666667e-07, 3.582634666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:14,742] [INFO] [timer.py:215:stop] epoch=0/micro_step=165/global_step=165, RunningAvgSamplesPerSec=12.329147362236244, CurrSamplesPerSec=12.313318057783851, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      165/219726562 | consumed samples:          330 | consumed tokens:       675840 | elapsed time per iteration (ms): 204.6 | learning rate: 3.583E-07 | global batch size:     2 | lm loss: 1.032026E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.773 | TFLOPs: 24.28 |
[default0]:[2023-07-30 22:22:15,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[3.691861333333334e-07, 3.691861333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:15,981] [INFO] [timer.py:215:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=12.332648644224905, CurrSamplesPerSec=12.422304031180863, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      170/219726562 | consumed samples:          340 | consumed tokens:       696320 | elapsed time per iteration (ms): 248.7 | learning rate: 3.692E-07 | global batch size:     2 | lm loss: 1.032520E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.040 | TFLOPs: 19.98 |
[default0]:[2023-07-30 22:22:16,903] [INFO] [logging.py:96:log_dist] [Rank 0] step=175, skipped=0, lr=[3.801088000000001e-07, 3.801088000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:16,904] [INFO] [timer.py:215:stop] epoch=0/micro_step=175/global_step=175, RunningAvgSamplesPerSec=12.333573274527259, CurrSamplesPerSec=12.336786932307009, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      175/219726562 | consumed samples:          350 | consumed tokens:       716800 | elapsed time per iteration (ms): 183.9 | learning rate: 3.801E-07 | global batch size:     2 | lm loss: 1.037001E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.876 | TFLOPs: 27.02 |
[default0]:[2023-07-30 22:22:17,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[3.9103146666666665e-07, 3.9103146666666665e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:17,922] [INFO] [timer.py:215:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=12.33578577314922, CurrSamplesPerSec=12.423978258132836, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      180/219726562 | consumed samples:          360 | consumed tokens:       737280 | elapsed time per iteration (ms): 203.3 | learning rate: 3.910E-07 | global batch size:     2 | lm loss: 1.031143E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.838 | TFLOPs: 24.45 |
[default0]:[2023-07-30 22:22:18,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=185, skipped=0, lr=[4.0195413333333333e-07, 4.0195413333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:18,880] [INFO] [timer.py:215:stop] epoch=0/micro_step=185/global_step=185, RunningAvgSamplesPerSec=12.335377375546996, CurrSamplesPerSec=12.297883943833765, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      185/219726562 | consumed samples:          370 | consumed tokens:       757760 | elapsed time per iteration (ms): 191.9 | learning rate: 4.020E-07 | global batch size:     2 | lm loss: 1.032603E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.424 | TFLOPs: 25.90 |
[default0]:[2023-07-30 22:22:19,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[4.128768e-07, 4.128768e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:19,823] [INFO] [timer.py:215:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=12.336305190878376, CurrSamplesPerSec=12.429224637359056, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      190/219726562 | consumed samples:          380 | consumed tokens:       778240 | elapsed time per iteration (ms): 188.3 | learning rate: 4.129E-07 | global batch size:     2 | lm loss: 1.028181E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.621 | TFLOPs: 26.39 |
[default0]:[2023-07-30 22:22:20,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=195, skipped=0, lr=[4.237994666666667e-07, 4.237994666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:20,783] [INFO] [timer.py:215:stop] epoch=0/micro_step=195/global_step=195, RunningAvgSamplesPerSec=12.33654158076655, CurrSamplesPerSec=12.269876768932606, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      195/219726562 | consumed samples:          390 | consumed tokens:       798720 | elapsed time per iteration (ms): 192.2 | learning rate: 4.238E-07 | global batch size:     2 | lm loss: 1.026752E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.404 | TFLOPs: 25.85 |
[default0]:[2023-07-30 22:22:21,750] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[4.347221333333334e-07, 4.347221333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:21,751] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=12.337385962358244, CurrSamplesPerSec=12.373800950831424, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      200/219726562 | consumed samples:          400 | consumed tokens:       819200 | elapsed time per iteration (ms): 193.6 | learning rate: 4.347E-07 | global batch size:     2 | lm loss: 1.028284E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.330 | TFLOPs: 25.67 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 200 | lm loss value: 1.024771E+01 | lm loss PPL: 2.821799E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:22:26,230] [INFO] [logging.py:96:log_dist] [Rank 0] step=205, skipped=0, lr=[4.456448e-07, 4.456448e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:26,231] [INFO] [timer.py:215:stop] epoch=0/micro_step=205/global_step=205, RunningAvgSamplesPerSec=12.333493969062323, CurrSamplesPerSec=12.434973124735027, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      205/219726562 | consumed samples:          410 | consumed tokens:       839680 | elapsed time per iteration (ms): 895.6 | learning rate: 4.456E-07 | global batch size:     2 | lm loss: 1.026838E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.233 | TFLOPs: 5.55 |
[default0]:[2023-07-30 22:22:27,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[4.565674666666667e-07, 4.565674666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:27,189] [INFO] [timer.py:215:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=12.3331789710162, CurrSamplesPerSec=12.303078479973014, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      210/219726562 | consumed samples:          420 | consumed tokens:       860160 | elapsed time per iteration (ms): 191.6 | learning rate: 4.566E-07 | global batch size:     2 | lm loss: 1.029401E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.436 | TFLOPs: 25.93 |
[default0]:[2023-07-30 22:22:28,232] [INFO] [logging.py:96:log_dist] [Rank 0] step=215, skipped=0, lr=[4.6749013333333337e-07, 4.6749013333333337e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:28,232] [INFO] [timer.py:215:stop] epoch=0/micro_step=215/global_step=215, RunningAvgSamplesPerSec=12.334239061047581, CurrSamplesPerSec=12.429519302290434, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      215/219726562 | consumed samples:          430 | consumed tokens:       880640 | elapsed time per iteration (ms): 208.9 | learning rate: 4.675E-07 | global batch size:     2 | lm loss: 1.026669E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.575 | TFLOPs: 23.79 |
[default0]:[2023-07-30 22:22:29,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[4.784128e-07, 4.784128e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:29,154] [INFO] [timer.py:215:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=12.334023873697879, CurrSamplesPerSec=12.31199877886975, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      220/219726562 | consumed samples:          440 | consumed tokens:       901120 | elapsed time per iteration (ms): 184.3 | learning rate: 4.784E-07 | global batch size:     2 | lm loss: 1.027740E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.855 | TFLOPs: 26.97 |
[default0]:[2023-07-30 22:22:30,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=225, skipped=0, lr=[4.893354666666666e-07, 4.893354666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:30,129] [INFO] [timer.py:215:stop] epoch=0/micro_step=225/global_step=225, RunningAvgSamplesPerSec=12.334682190446083, CurrSamplesPerSec=12.458760452094875, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      225/219726562 | consumed samples:          450 | consumed tokens:       921600 | elapsed time per iteration (ms): 194.8 | learning rate: 4.893E-07 | global batch size:     2 | lm loss: 1.025782E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.267 | TFLOPs: 25.51 |
[default0]:[2023-07-30 22:22:31,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[5.002581333333334e-07, 5.002581333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:31,048] [INFO] [timer.py:215:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=12.334940897810915, CurrSamplesPerSec=12.288516021668878, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      230/219726562 | consumed samples:          460 | consumed tokens:       942080 | elapsed time per iteration (ms): 183.8 | learning rate: 5.003E-07 | global batch size:     2 | lm loss: 1.028184E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.880 | TFLOPs: 27.03 |
[default0]:[2023-07-30 22:22:31,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=235, skipped=0, lr=[5.111808e-07, 5.111808e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:31,962] [INFO] [timer.py:215:stop] epoch=0/micro_step=235/global_step=235, RunningAvgSamplesPerSec=12.336303391774278, CurrSamplesPerSec=12.461425562194169, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      235/219726562 | consumed samples:          470 | consumed tokens:       962560 | elapsed time per iteration (ms): 183.0 | learning rate: 5.112E-07 | global batch size:     2 | lm loss: 1.027746E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.926 | TFLOPs: 27.15 |
[default0]:[2023-07-30 22:22:32,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[5.221034666666667e-07, 5.221034666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:32,890] [INFO] [timer.py:215:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=12.337999597568205, CurrSamplesPerSec=12.29981334611905, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      240/219726562 | consumed samples:          480 | consumed tokens:       983040 | elapsed time per iteration (ms): 185.7 | learning rate: 5.221E-07 | global batch size:     2 | lm loss: 1.024110E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.769 | TFLOPs: 26.76 |
[default0]:[2023-07-30 22:22:33,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=245, skipped=0, lr=[5.330261333333334e-07, 5.330261333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:33,858] [INFO] [timer.py:215:stop] epoch=0/micro_step=245/global_step=245, RunningAvgSamplesPerSec=12.33706961454127, CurrSamplesPerSec=12.27134859485103, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      245/219726562 | consumed samples:          490 | consumed tokens:      1003520 | elapsed time per iteration (ms): 193.1 | learning rate: 5.330E-07 | global batch size:     2 | lm loss: 1.021966E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.357 | TFLOPs: 25.73 |
[default0]:[2023-07-30 22:22:34,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[5.439488000000001e-07, 5.439488000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:34,806] [INFO] [timer.py:215:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=12.337651356080825, CurrSamplesPerSec=12.233213362810693, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      250/219726562 | consumed samples:          500 | consumed tokens:      1024000 | elapsed time per iteration (ms): 189.7 | learning rate: 5.439E-07 | global batch size:     2 | lm loss: 1.021639E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.540 | TFLOPs: 26.19 |
[default0]:[2023-07-30 22:22:35,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=255, skipped=0, lr=[5.548714666666667e-07, 5.548714666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:35,729] [INFO] [timer.py:215:stop] epoch=0/micro_step=255/global_step=255, RunningAvgSamplesPerSec=12.337930849696098, CurrSamplesPerSec=12.38536542152665, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      255/219726562 | consumed samples:          510 | consumed tokens:      1044480 | elapsed time per iteration (ms): 184.6 | learning rate: 5.549E-07 | global batch size:     2 | lm loss: 1.020285E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.837 | TFLOPs: 26.93 |
[default0]:[2023-07-30 22:22:36,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[5.657941333333333e-07, 5.657941333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:36,638] [INFO] [timer.py:215:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=12.339580635278162, CurrSamplesPerSec=12.341633073414743, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      260/219726562 | consumed samples:          520 | consumed tokens:      1064960 | elapsed time per iteration (ms): 182.2 | learning rate: 5.658E-07 | global batch size:     2 | lm loss: 1.022330E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.974 | TFLOPs: 27.27 |
[default0]:[2023-07-30 22:22:37,581] [INFO] [logging.py:96:log_dist] [Rank 0] step=265, skipped=0, lr=[5.767168e-07, 5.767168e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:37,582] [INFO] [timer.py:215:stop] epoch=0/micro_step=265/global_step=265, RunningAvgSamplesPerSec=12.339738635602961, CurrSamplesPerSec=12.376977657249848, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      265/219726562 | consumed samples:          530 | consumed tokens:      1085440 | elapsed time per iteration (ms): 188.2 | learning rate: 5.767E-07 | global batch size:     2 | lm loss: 1.019264E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.627 | TFLOPs: 26.41 |
[default0]:[2023-07-30 22:22:38,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[5.876394666666667e-07, 5.876394666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:38,590] [INFO] [timer.py:215:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=12.340818886507337, CurrSamplesPerSec=12.34748255021505, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      270/219726562 | consumed samples:          540 | consumed tokens:      1105920 | elapsed time per iteration (ms): 201.8 | learning rate: 5.876E-07 | global batch size:     2 | lm loss: 1.020935E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.910 | TFLOPs: 24.63 |
[default0]:[2023-07-30 22:22:39,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=275, skipped=0, lr=[5.985621333333333e-07, 5.985621333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:39,514] [INFO] [timer.py:215:stop] epoch=0/micro_step=275/global_step=275, RunningAvgSamplesPerSec=12.34048340688628, CurrSamplesPerSec=12.26135788935175, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      275/219726562 | consumed samples:          550 | consumed tokens:      1126400 | elapsed time per iteration (ms): 184.7 | learning rate: 5.986E-07 | global batch size:     2 | lm loss: 1.022547E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.829 | TFLOPs: 26.91 |
[default0]:[2023-07-30 22:22:40,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[6.094848000000001e-07, 6.094848000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:40,448] [INFO] [timer.py:215:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=12.340637497298276, CurrSamplesPerSec=12.235925621233072, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      280/219726562 | consumed samples:          560 | consumed tokens:      1146880 | elapsed time per iteration (ms): 186.8 | learning rate: 6.095E-07 | global batch size:     2 | lm loss: 1.018117E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.705 | TFLOPs: 26.60 |
[default0]:[2023-07-30 22:22:41,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=285, skipped=0, lr=[6.204074666666667e-07, 6.204074666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:41,381] [INFO] [timer.py:215:stop] epoch=0/micro_step=285/global_step=285, RunningAvgSamplesPerSec=12.338987590205782, CurrSamplesPerSec=12.256055262212067, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      285/219726562 | consumed samples:          570 | consumed tokens:      1167360 | elapsed time per iteration (ms): 186.8 | learning rate: 6.204E-07 | global batch size:     2 | lm loss: 1.021222E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.709 | TFLOPs: 26.61 |
[default0]:[2023-07-30 22:22:42,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[6.313301333333333e-07, 6.313301333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:42,309] [INFO] [timer.py:215:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=12.33865224852322, CurrSamplesPerSec=12.33791191416448, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      290/219726562 | consumed samples:          580 | consumed tokens:      1187840 | elapsed time per iteration (ms): 185.4 | learning rate: 6.313E-07 | global batch size:     2 | lm loss: 1.024504E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.790 | TFLOPs: 26.81 |
[default0]:[2023-07-30 22:22:43,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=295, skipped=0, lr=[6.422528000000001e-07, 6.422528000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:43,229] [INFO] [timer.py:215:stop] epoch=0/micro_step=295/global_step=295, RunningAvgSamplesPerSec=12.338051390043024, CurrSamplesPerSec=12.164190164077057, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      295/219726562 | consumed samples:          590 | consumed tokens:      1208320 | elapsed time per iteration (ms): 184.2 | learning rate: 6.423E-07 | global batch size:     2 | lm loss: 1.022537E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.860 | TFLOPs: 26.98 |
[default0]:[2023-07-30 22:22:44,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[6.531754666666667e-07, 6.531754666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:44,254] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=12.33654547398757, CurrSamplesPerSec=12.121057293420165, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      300/219726562 | consumed samples:          600 | consumed tokens:      1228800 | elapsed time per iteration (ms): 205.0 | learning rate: 6.532E-07 | global batch size:     2 | lm loss: 1.020840E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.757 | TFLOPs: 24.24 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 300 | lm loss value: 1.016658E+01 | lm loss PPL: 2.601904E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:22:49,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=305, skipped=0, lr=[6.640981333333333e-07, 6.640981333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:49,670] [INFO] [timer.py:215:stop] epoch=0/micro_step=305/global_step=305, RunningAvgSamplesPerSec=12.333971903631527, CurrSamplesPerSec=12.382988648273622, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      305/219726562 | consumed samples:          610 | consumed tokens:      1249280 | elapsed time per iteration (ms): 1083.1 | learning rate: 6.641E-07 | global batch size:     2 | lm loss: 1.017631E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.847 | TFLOPs: 4.59 |
[default0]:[2023-07-30 22:22:50,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[6.750208e-07, 6.750208e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:50,611] [INFO] [timer.py:215:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=12.33613104286076, CurrSamplesPerSec=12.439398657686274, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      310/219726562 | consumed samples:          620 | consumed tokens:      1269760 | elapsed time per iteration (ms): 189.0 | learning rate: 6.750E-07 | global batch size:     2 | lm loss: 1.025045E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.581 | TFLOPs: 26.29 |
[default0]:[2023-07-30 22:22:51,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=315, skipped=0, lr=[6.859434666666668e-07, 6.859434666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:51,602] [INFO] [timer.py:215:stop] epoch=0/micro_step=315/global_step=315, RunningAvgSamplesPerSec=12.3370826795848, CurrSamplesPerSec=12.370479933344638, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      315/219726562 | consumed samples:          630 | consumed tokens:      1290240 | elapsed time per iteration (ms): 197.5 | learning rate: 6.859E-07 | global batch size:     2 | lm loss: 1.017845E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.129 | TFLOPs: 25.17 |
[default0]:[2023-07-30 22:22:52,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[6.968661333333334e-07, 6.968661333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:52,652] [INFO] [timer.py:215:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=12.337581747579554, CurrSamplesPerSec=12.257470041658934, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      320/219726562 | consumed samples:          640 | consumed tokens:      1310720 | elapsed time per iteration (ms): 210.1 | learning rate: 6.969E-07 | global batch size:     2 | lm loss: 1.021122E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.521 | TFLOPs: 23.66 |
[default0]:[2023-07-30 22:22:53,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=325, skipped=0, lr=[7.077888e-07, 7.077888e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:53,605] [INFO] [timer.py:215:stop] epoch=0/micro_step=325/global_step=325, RunningAvgSamplesPerSec=12.336779630088895, CurrSamplesPerSec=12.104790916004207, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      325/219726562 | consumed samples:          650 | consumed tokens:      1331200 | elapsed time per iteration (ms): 190.8 | learning rate: 7.078E-07 | global batch size:     2 | lm loss: 1.014920E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.485 | TFLOPs: 26.05 |
[default0]:[2023-07-30 22:22:54,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[7.187114666666667e-07, 7.187114666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:54,586] [INFO] [timer.py:215:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=12.335942888079934, CurrSamplesPerSec=12.268243814404343, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      330/219726562 | consumed samples:          660 | consumed tokens:      1351680 | elapsed time per iteration (ms): 196.2 | learning rate: 7.187E-07 | global batch size:     2 | lm loss: 1.018946E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.196 | TFLOPs: 25.33 |
[default0]:[2023-07-30 22:22:55,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=335, skipped=0, lr=[7.296341333333333e-07, 7.296341333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:55,607] [INFO] [timer.py:215:stop] epoch=0/micro_step=335/global_step=335, RunningAvgSamplesPerSec=12.335991678775937, CurrSamplesPerSec=12.289452171594496, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      335/219726562 | consumed samples:          670 | consumed tokens:      1372160 | elapsed time per iteration (ms): 204.3 | learning rate: 7.296E-07 | global batch size:     2 | lm loss: 1.015738E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.792 | TFLOPs: 24.33 |
[default0]:[2023-07-30 22:22:56,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[7.405567999999999e-07, 7.405567999999999e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:56,541] [INFO] [timer.py:215:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=12.336437658483288, CurrSamplesPerSec=12.298965777104, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      340/219726562 | consumed samples:          680 | consumed tokens:      1392640 | elapsed time per iteration (ms): 186.5 | learning rate: 7.406E-07 | global batch size:     2 | lm loss: 1.015928E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.721 | TFLOPs: 26.64 |
[default0]:[2023-07-30 22:22:57,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=345, skipped=0, lr=[7.514794666666667e-07, 7.514794666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:57,498] [INFO] [timer.py:215:stop] epoch=0/micro_step=345/global_step=345, RunningAvgSamplesPerSec=12.336337018161688, CurrSamplesPerSec=12.243319059253485, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      345/219726562 | consumed samples:          690 | consumed tokens:      1413120 | elapsed time per iteration (ms): 191.4 | learning rate: 7.515E-07 | global batch size:     2 | lm loss: 1.011334E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.451 | TFLOPs: 25.97 |
[default0]:[2023-07-30 22:22:58,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[7.624021333333333e-07, 7.624021333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:58,501] [INFO] [timer.py:215:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=12.336395145167051, CurrSamplesPerSec=12.341142841170058, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      350/219726562 | consumed samples:          700 | consumed tokens:      1433600 | elapsed time per iteration (ms): 200.5 | learning rate: 7.624E-07 | global batch size:     2 | lm loss: 1.011584E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.976 | TFLOPs: 24.79 |
[default0]:[2023-07-30 22:22:59,479] [INFO] [logging.py:96:log_dist] [Rank 0] step=355, skipped=0, lr=[7.733248e-07, 7.733248e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:59,480] [INFO] [timer.py:215:stop] epoch=0/micro_step=355/global_step=355, RunningAvgSamplesPerSec=12.33667750862307, CurrSamplesPerSec=12.216520233419402, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      355/219726562 | consumed samples:          710 | consumed tokens:      1454080 | elapsed time per iteration (ms): 195.9 | learning rate: 7.733E-07 | global batch size:     2 | lm loss: 1.008519E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.211 | TFLOPs: 25.37 |
[default0]:[2023-07-30 22:23:00,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[7.842474666666667e-07, 7.842474666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:00,430] [INFO] [timer.py:215:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=12.337106930144715, CurrSamplesPerSec=12.360727915715023, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      360/219726562 | consumed samples:          720 | consumed tokens:      1474560 | elapsed time per iteration (ms): 190.0 | learning rate: 7.842E-07 | global batch size:     2 | lm loss: 1.010453E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.524 | TFLOPs: 26.15 |
[default0]:[2023-07-30 22:23:01,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=365, skipped=0, lr=[7.951701333333334e-07, 7.951701333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:01,405] [INFO] [timer.py:215:stop] epoch=0/micro_step=365/global_step=365, RunningAvgSamplesPerSec=12.336668877591135, CurrSamplesPerSec=12.285438534055643, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      365/219726562 | consumed samples:          730 | consumed tokens:      1495040 | elapsed time per iteration (ms): 194.7 | learning rate: 7.952E-07 | global batch size:     2 | lm loss: 1.009800E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.270 | TFLOPs: 25.52 |
[default0]:[2023-07-30 22:23:02,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[8.060928e-07, 8.060928e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:02,347] [INFO] [timer.py:215:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=12.336707654858742, CurrSamplesPerSec=12.329207727977542, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      370/219726562 | consumed samples:          740 | consumed tokens:      1515520 | elapsed time per iteration (ms): 188.6 | learning rate: 8.061E-07 | global batch size:     2 | lm loss: 1.013719E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.605 | TFLOPs: 26.35 |
[default0]:[2023-07-30 22:23:03,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=375, skipped=0, lr=[8.170154666666668e-07, 8.170154666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:03,347] [INFO] [timer.py:215:stop] epoch=0/micro_step=375/global_step=375, RunningAvgSamplesPerSec=12.336786543176421, CurrSamplesPerSec=12.359689821970305, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      375/219726562 | consumed samples:          750 | consumed tokens:      1536000 | elapsed time per iteration (ms): 200.0 | learning rate: 8.170E-07 | global batch size:     2 | lm loss: 1.013101E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.000 | TFLOPs: 24.85 |
[default0]:[2023-07-30 22:23:04,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[8.279381333333334e-07, 8.279381333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:04,315] [INFO] [timer.py:215:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=12.336080203405938, CurrSamplesPerSec=12.239353351425487, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      380/219726562 | consumed samples:          760 | consumed tokens:      1556480 | elapsed time per iteration (ms): 194.1 | learning rate: 8.279E-07 | global batch size:     2 | lm loss: 1.004790E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.302 | TFLOPs: 25.60 |
[default0]:[2023-07-30 22:23:05,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=385, skipped=0, lr=[8.388608e-07, 8.388608e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:05,273] [INFO] [timer.py:215:stop] epoch=0/micro_step=385/global_step=385, RunningAvgSamplesPerSec=12.336144089670205, CurrSamplesPerSec=12.31264934683693, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      385/219726562 | consumed samples:          770 | consumed tokens:      1576960 | elapsed time per iteration (ms): 191.0 | learning rate: 8.389E-07 | global batch size:     2 | lm loss: 1.013291E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.472 | TFLOPs: 26.02 |
[default0]:[2023-07-30 22:23:06,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[8.497834666666668e-07, 8.497834666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:06,283] [INFO] [timer.py:215:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=12.336681440705869, CurrSamplesPerSec=12.3769411341166, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      390/219726562 | consumed samples:          780 | consumed tokens:      1597440 | elapsed time per iteration (ms): 202.1 | learning rate: 8.498E-07 | global batch size:     2 | lm loss: 1.004640E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.898 | TFLOPs: 24.59 |
[default0]:[2023-07-30 22:23:07,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=395, skipped=0, lr=[8.607061333333334e-07, 8.607061333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:07,453] [INFO] [timer.py:215:stop] epoch=0/micro_step=395/global_step=395, RunningAvgSamplesPerSec=12.33733139125846, CurrSamplesPerSec=12.400048484991826, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      395/219726562 | consumed samples:          790 | consumed tokens:      1617920 | elapsed time per iteration (ms): 234.2 | learning rate: 8.607E-07 | global batch size:     2 | lm loss: 1.008159E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.539 | TFLOPs: 21.22 |
[default0]:[2023-07-30 22:23:08,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[8.716288000000001e-07, 8.716288000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:08,432] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=12.33858670081194, CurrSamplesPerSec=12.433277505224622, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      400/219726562 | consumed samples:          800 | consumed tokens:      1638400 | elapsed time per iteration (ms): 195.3 | learning rate: 8.716E-07 | global batch size:     2 | lm loss: 1.006441E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.240 | TFLOPs: 25.44 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 400 | lm loss value: 1.005565E+01 | lm loss PPL: 2.328700E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:23:11,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=405, skipped=0, lr=[8.825514666666668e-07, 8.825514666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:11,829] [INFO] [timer.py:215:stop] epoch=0/micro_step=405/global_step=405, RunningAvgSamplesPerSec=12.338279043213431, CurrSamplesPerSec=12.310697849159387, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      405/219726562 | consumed samples:          810 | consumed tokens:      1658880 | elapsed time per iteration (ms): 679.4 | learning rate: 8.826E-07 | global batch size:     2 | lm loss: 1.009019E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.944 | TFLOPs: 7.31 |
[default0]:[2023-07-30 22:23:12,750] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[8.934741333333333e-07, 8.934741333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:12,751] [INFO] [timer.py:215:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=12.338074701300567, CurrSamplesPerSec=12.33667807398235, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      410/219726562 | consumed samples:          820 | consumed tokens:      1679360 | elapsed time per iteration (ms): 184.5 | learning rate: 8.935E-07 | global batch size:     2 | lm loss: 1.002309E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.840 | TFLOPs: 26.93 |
[default0]:[2023-07-30 22:23:13,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=415, skipped=0, lr=[9.043968e-07, 9.043968e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:13,688] [INFO] [timer.py:215:stop] epoch=0/micro_step=415/global_step=415, RunningAvgSamplesPerSec=12.337750135417194, CurrSamplesPerSec=12.336659931115014, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      415/219726562 | consumed samples:          830 | consumed tokens:      1699840 | elapsed time per iteration (ms): 187.5 | learning rate: 9.044E-07 | global batch size:     2 | lm loss: 1.003311E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.665 | TFLOPs: 26.50 |
[default0]:[2023-07-30 22:23:14,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[9.153194666666666e-07, 9.153194666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:14,639] [INFO] [timer.py:215:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=12.336074309755618, CurrSamplesPerSec=12.248378171199123, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      420/219726562 | consumed samples:          840 | consumed tokens:      1720320 | elapsed time per iteration (ms): 190.2 | learning rate: 9.153E-07 | global batch size:     2 | lm loss: 1.004252E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.516 | TFLOPs: 26.13 |
[default0]:[2023-07-30 22:23:15,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=425, skipped=0, lr=[9.262421333333334e-07, 9.262421333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:15,588] [INFO] [timer.py:215:stop] epoch=0/micro_step=425/global_step=425, RunningAvgSamplesPerSec=12.336293868348623, CurrSamplesPerSec=12.34186912505315, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      425/219726562 | consumed samples:          850 | consumed tokens:      1740800 | elapsed time per iteration (ms): 189.8 | learning rate: 9.262E-07 | global batch size:     2 | lm loss: 1.005149E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.536 | TFLOPs: 26.18 |
[default0]:[2023-07-30 22:23:16,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[9.371648e-07, 9.371648e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:16,508] [INFO] [timer.py:215:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=12.336167296362547, CurrSamplesPerSec=12.353701423783129, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      430/219726562 | consumed samples:          860 | consumed tokens:      1761280 | elapsed time per iteration (ms): 184.0 | learning rate: 9.372E-07 | global batch size:     2 | lm loss: 1.004386E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.867 | TFLOPs: 27.00 |
[default0]:[2023-07-30 22:23:17,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=435, skipped=0, lr=[9.480874666666667e-07, 9.480874666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:17,479] [INFO] [timer.py:215:stop] epoch=0/micro_step=435/global_step=435, RunningAvgSamplesPerSec=12.33623469937604, CurrSamplesPerSec=12.334972877672348, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      435/219726562 | consumed samples:          870 | consumed tokens:      1781760 | elapsed time per iteration (ms): 194.0 | learning rate: 9.481E-07 | global batch size:     2 | lm loss: 9.962267E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.307 | TFLOPs: 25.61 |
[default0]:[2023-07-30 22:23:18,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[9.590101333333334e-07, 9.590101333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:18,505] [INFO] [timer.py:215:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=12.336414344902998, CurrSamplesPerSec=12.382769299807215, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      440/219726562 | consumed samples:          880 | consumed tokens:      1802240 | elapsed time per iteration (ms): 205.2 | learning rate: 9.590E-07 | global batch size:     2 | lm loss: 9.982626E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.746 | TFLOPs: 24.22 |
[default0]:[2023-07-30 22:23:19,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=445, skipped=0, lr=[9.699328e-07, 9.699328e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:19,460] [INFO] [timer.py:215:stop] epoch=0/micro_step=445/global_step=445, RunningAvgSamplesPerSec=12.33601476776472, CurrSamplesPerSec=12.255321138312341, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      445/219726562 | consumed samples:          890 | consumed tokens:      1822720 | elapsed time per iteration (ms): 191.1 | learning rate: 9.699E-07 | global batch size:     2 | lm loss: 9.963049E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.466 | TFLOPs: 26.01 |
[default0]:[2023-07-30 22:23:20,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[9.808554666666666e-07, 9.808554666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:20,616] [INFO] [timer.py:215:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=12.33632211095358, CurrSamplesPerSec=12.201274437506454, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      450/219726562 | consumed samples:          900 | consumed tokens:      1843200 | elapsed time per iteration (ms): 231.2 | learning rate: 9.809E-07 | global batch size:     2 | lm loss: 1.001803E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.649 | TFLOPs: 21.49 |
[default0]:[2023-07-30 22:23:21,560] [INFO] [logging.py:96:log_dist] [Rank 0] step=455, skipped=0, lr=[9.917781333333335e-07, 9.917781333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:21,561] [INFO] [timer.py:215:stop] epoch=0/micro_step=455/global_step=455, RunningAvgSamplesPerSec=12.33568313864619, CurrSamplesPerSec=12.215826416193389, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      455/219726562 | consumed samples:          910 | consumed tokens:      1863680 | elapsed time per iteration (ms): 188.9 | learning rate: 9.918E-07 | global batch size:     2 | lm loss: 9.982607E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.589 | TFLOPs: 26.31 |
[default0]:[2023-07-30 22:23:22,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[1.0027008e-06, 1.0027008e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:22,489] [INFO] [timer.py:215:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=12.336001515826016, CurrSamplesPerSec=12.32491456344344, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      460/219726562 | consumed samples:          920 | consumed tokens:      1884160 | elapsed time per iteration (ms): 185.6 | learning rate: 1.003E-06 | global batch size:     2 | lm loss: 9.949390E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.775 | TFLOPs: 26.77 |
[default0]:[2023-07-30 22:23:23,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=465, skipped=0, lr=[1.0136234666666667e-06, 1.0136234666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:23,457] [INFO] [timer.py:215:stop] epoch=0/micro_step=465/global_step=465, RunningAvgSamplesPerSec=12.336100976696496, CurrSamplesPerSec=12.25557180487762, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      465/219726562 | consumed samples:          930 | consumed tokens:      1904640 | elapsed time per iteration (ms): 193.6 | learning rate: 1.014E-06 | global batch size:     2 | lm loss: 9.946902E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.331 | TFLOPs: 25.67 |
[default0]:[2023-07-30 22:23:24,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[1.0245461333333334e-06, 1.0245461333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:24,391] [INFO] [timer.py:215:stop] epoch=0/micro_step=470/global_step=470, RunningAvgSamplesPerSec=12.335338281492469, CurrSamplesPerSec=12.272048733530928, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      470/219726562 | consumed samples:          940 | consumed tokens:      1925120 | elapsed time per iteration (ms): 187.0 | learning rate: 1.025E-06 | global batch size:     2 | lm loss: 9.978492E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.697 | TFLOPs: 26.58 |
[default0]:[2023-07-30 22:23:25,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=475, skipped=0, lr=[1.0354688000000002e-06, 1.0354688000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:25,576] [INFO] [timer.py:215:stop] epoch=0/micro_step=475/global_step=475, RunningAvgSamplesPerSec=12.336244731029998, CurrSamplesPerSec=12.42892998639851, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      475/219726562 | consumed samples:          950 | consumed tokens:      1945600 | elapsed time per iteration (ms): 236.8 | learning rate: 1.035E-06 | global batch size:     2 | lm loss: 9.947588E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.446 | TFLOPs: 20.99 |
[default0]:[2023-07-30 22:23:26,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[1.0463914666666668e-06, 1.0463914666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:26,541] [INFO] [timer.py:215:stop] epoch=0/micro_step=480/global_step=480, RunningAvgSamplesPerSec=12.337546071362327, CurrSamplesPerSec=12.384816854413653, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      480/219726562 | consumed samples:          960 | consumed tokens:      1966080 | elapsed time per iteration (ms): 192.9 | learning rate: 1.046E-06 | global batch size:     2 | lm loss: 9.970090E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.366 | TFLOPs: 25.76 |
[default0]:[2023-07-30 22:23:27,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=485, skipped=0, lr=[1.0573141333333335e-06, 1.0573141333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:27,593] [INFO] [timer.py:215:stop] epoch=0/micro_step=485/global_step=485, RunningAvgSamplesPerSec=12.33775100206183, CurrSamplesPerSec=12.246661537495749, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      485/219726562 | consumed samples:          970 | consumed tokens:      1986560 | elapsed time per iteration (ms): 210.7 | learning rate: 1.057E-06 | global batch size:     2 | lm loss: 9.964443E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.491 | TFLOPs: 23.58 |
[default0]:[2023-07-30 22:23:28,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[1.0682368e-06, 1.0682368e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:28,517] [INFO] [timer.py:215:stop] epoch=0/micro_step=490/global_step=490, RunningAvgSamplesPerSec=12.33795720626796, CurrSamplesPerSec=12.349736622824452, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      490/219726562 | consumed samples:          980 | consumed tokens:      2007040 | elapsed time per iteration (ms): 184.9 | learning rate: 1.068E-06 | global batch size:     2 | lm loss: 9.932918E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.816 | TFLOPs: 26.88 |
[default0]:[2023-07-30 22:23:29,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=495, skipped=0, lr=[1.0791594666666667e-06, 1.0791594666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:29,511] [INFO] [timer.py:215:stop] epoch=0/micro_step=495/global_step=495, RunningAvgSamplesPerSec=12.338039309226515, CurrSamplesPerSec=12.296153406409132, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      495/219726562 | consumed samples:          990 | consumed tokens:      2027520 | elapsed time per iteration (ms): 198.5 | learning rate: 1.079E-06 | global batch size:     2 | lm loss: 9.855535E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.073 | TFLOPs: 25.03 |
[default0]:[2023-07-30 22:23:30,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[1.0900821333333333e-06, 1.0900821333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:30,427] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=500, RunningAvgSamplesPerSec=12.338242167864781, CurrSamplesPerSec=12.36152937050365, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      500/219726562 | consumed samples:         1000 | consumed tokens:      2048000 | elapsed time per iteration (ms): 182.9 | learning rate: 1.090E-06 | global batch size:     2 | lm loss: 9.992462E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.932 | TFLOPs: 27.16 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 500 | lm loss value: 9.938642E+00 | lm loss PPL: 2.071558E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:23:36,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=505, skipped=0, lr=[1.1010048e-06, 1.1010048e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:36,052] [INFO] [timer.py:215:stop] epoch=0/micro_step=505/global_step=505, RunningAvgSamplesPerSec=12.33584316060297, CurrSamplesPerSec=12.41039509774623, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      505/219726562 | consumed samples:         1010 | consumed tokens:      2068480 | elapsed time per iteration (ms): 1125.3 | learning rate: 1.101E-06 | global batch size:     2 | lm loss: 9.852674E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.777 | TFLOPs: 4.42 |
[default0]:[2023-07-30 22:23:36,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[1.1119274666666666e-06, 1.1119274666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:36,996] [INFO] [timer.py:215:stop] epoch=0/micro_step=510/global_step=510, RunningAvgSamplesPerSec=12.336357760219684, CurrSamplesPerSec=12.36101934179543, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      510/219726562 | consumed samples:         1020 | consumed tokens:      2088960 | elapsed time per iteration (ms): 188.7 | learning rate: 1.112E-06 | global batch size:     2 | lm loss: 1.000105E+01 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.596 | TFLOPs: 26.33 |
[default0]:[2023-07-30 22:23:37,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=515, skipped=0, lr=[1.1228501333333334e-06, 1.1228501333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:37,976] [INFO] [timer.py:215:stop] epoch=0/micro_step=515/global_step=515, RunningAvgSamplesPerSec=12.3360740831122, CurrSamplesPerSec=12.288552024797989, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      515/219726562 | consumed samples:         1030 | consumed tokens:      2109440 | elapsed time per iteration (ms): 195.8 | learning rate: 1.123E-06 | global batch size:     2 | lm loss: 9.855570E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.217 | TFLOPs: 25.39 |
[default0]:[2023-07-30 22:23:38,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[1.1337728e-06, 1.1337728e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:38,974] [INFO] [timer.py:215:stop] epoch=0/micro_step=520/global_step=520, RunningAvgSamplesPerSec=12.3364269155957, CurrSamplesPerSec=12.323176410789785, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      520/219726562 | consumed samples:         1040 | consumed tokens:      2129920 | elapsed time per iteration (ms): 199.4 | learning rate: 1.134E-06 | global batch size:     2 | lm loss: 9.864088E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.029 | TFLOPs: 24.92 |
[default0]:[2023-07-30 22:23:39,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=525, skipped=0, lr=[1.1446954666666667e-06, 1.1446954666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:39,892] [INFO] [timer.py:215:stop] epoch=0/micro_step=525/global_step=525, RunningAvgSamplesPerSec=12.336011124528168, CurrSamplesPerSec=12.25038151886127, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      525/219726562 | consumed samples:         1050 | consumed tokens:      2150400 | elapsed time per iteration (ms): 184.0 | learning rate: 1.145E-06 | global batch size:     2 | lm loss: 9.894487E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.867 | TFLOPs: 27.00 |
[default0]:[2023-07-30 22:23:40,834] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[1.1556181333333333e-06, 1.1556181333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:40,834] [INFO] [timer.py:215:stop] epoch=0/micro_step=530/global_step=530, RunningAvgSamplesPerSec=12.336755456659644, CurrSamplesPerSec=12.374147753254837, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      530/219726562 | consumed samples:         1060 | consumed tokens:      2170880 | elapsed time per iteration (ms): 188.2 | learning rate: 1.156E-06 | global batch size:     2 | lm loss: 9.879772E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.629 | TFLOPs: 26.41 |
[default0]:[2023-07-30 22:23:41,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=535, skipped=0, lr=[1.1665408000000002e-06, 1.1665408000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:41,834] [INFO] [timer.py:215:stop] epoch=0/micro_step=535/global_step=535, RunningAvgSamplesPerSec=12.336320536579644, CurrSamplesPerSec=12.314004917611655, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      535/219726562 | consumed samples:         1070 | consumed tokens:      2191360 | elapsed time per iteration (ms): 200.1 | learning rate: 1.167E-06 | global batch size:     2 | lm loss: 9.922722E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.993 | TFLOPs: 24.83 |
[default0]:[2023-07-30 22:23:42,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[1.1774634666666668e-06, 1.1774634666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:42,869] [INFO] [timer.py:215:stop] epoch=0/micro_step=540/global_step=540, RunningAvgSamplesPerSec=12.336711425972485, CurrSamplesPerSec=12.470929290436525, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      540/219726562 | consumed samples:         1080 | consumed tokens:      2211840 | elapsed time per iteration (ms): 206.9 | learning rate: 1.177E-06 | global batch size:     2 | lm loss: 9.935294E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.667 | TFLOPs: 24.02 |
[default0]:[2023-07-30 22:23:43,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=545, skipped=0, lr=[1.1883861333333334e-06, 1.1883861333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:43,825] [INFO] [timer.py:215:stop] epoch=0/micro_step=545/global_step=545, RunningAvgSamplesPerSec=12.33697675401201, CurrSamplesPerSec=12.363351245235856, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      545/219726562 | consumed samples:         1090 | consumed tokens:      2232320 | elapsed time per iteration (ms): 191.2 | learning rate: 1.188E-06 | global batch size:     2 | lm loss: 9.955357E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.461 | TFLOPs: 25.99 |
[default0]:[2023-07-30 22:23:44,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[1.1993088e-06, 1.1993088e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:44,758] [INFO] [timer.py:215:stop] epoch=0/micro_step=550/global_step=550, RunningAvgSamplesPerSec=12.336836859456179, CurrSamplesPerSec=12.328283629907883, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      550/219726562 | consumed samples:         1100 | consumed tokens:      2252800 | elapsed time per iteration (ms): 186.5 | learning rate: 1.199E-06 | global batch size:     2 | lm loss: 9.890404E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.725 | TFLOPs: 26.65 |
[default0]:[2023-07-30 22:23:45,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=555, skipped=0, lr=[1.2102314666666667e-06, 1.2102314666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:45,715] [INFO] [timer.py:215:stop] epoch=0/micro_step=555/global_step=555, RunningAvgSamplesPerSec=12.337110172143781, CurrSamplesPerSec=12.290586544336234, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      555/219726562 | consumed samples:         1110 | consumed tokens:      2273280 | elapsed time per iteration (ms): 191.4 | learning rate: 1.210E-06 | global batch size:     2 | lm loss: 9.874123E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.451 | TFLOPs: 25.97 |
[default0]:[2023-07-30 22:23:46,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[1.2211541333333335e-06, 1.2211541333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:46,651] [INFO] [timer.py:215:stop] epoch=0/micro_step=560/global_step=560, RunningAvgSamplesPerSec=12.337548083849788, CurrSamplesPerSec=12.44953391836559, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      560/219726562 | consumed samples:         1120 | consumed tokens:      2293760 | elapsed time per iteration (ms): 187.3 | learning rate: 1.221E-06 | global batch size:     2 | lm loss: 9.754471E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.679 | TFLOPs: 26.54 |
[default0]:[2023-07-30 22:23:47,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=565, skipped=0, lr=[1.2320768000000001e-06, 1.2320768000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:47,579] [INFO] [timer.py:215:stop] epoch=0/micro_step=565/global_step=565, RunningAvgSamplesPerSec=12.33791033480475, CurrSamplesPerSec=12.34030772037136, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      565/219726562 | consumed samples:         1130 | consumed tokens:      2314240 | elapsed time per iteration (ms): 186.1 | learning rate: 1.232E-06 | global batch size:     2 | lm loss: 9.849876E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.749 | TFLOPs: 26.71 |
[default0]:[2023-07-30 22:23:48,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[1.2429994666666668e-06, 1.2429994666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:48,593] [INFO] [timer.py:215:stop] epoch=0/micro_step=570/global_step=570, RunningAvgSamplesPerSec=12.337976002394754, CurrSamplesPerSec=12.41954530110921, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      570/219726562 | consumed samples:         1140 | consumed tokens:      2334720 | elapsed time per iteration (ms): 202.2 | learning rate: 1.243E-06 | global batch size:     2 | lm loss: 9.837954E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.892 | TFLOPs: 24.58 |
[default0]:[2023-07-30 22:23:49,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=575, skipped=0, lr=[1.2539221333333334e-06, 1.2539221333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:49,560] [INFO] [timer.py:215:stop] epoch=0/micro_step=575/global_step=575, RunningAvgSamplesPerSec=12.33825851299487, CurrSamplesPerSec=12.295558646648502, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      575/219726562 | consumed samples:         1150 | consumed tokens:      2355200 | elapsed time per iteration (ms): 193.7 | learning rate: 1.254E-06 | global batch size:     2 | lm loss: 9.854017E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.327 | TFLOPs: 25.66 |
[default0]:[2023-07-30 22:23:50,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[1.2648448e-06, 1.2648448e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:50,559] [INFO] [timer.py:215:stop] epoch=0/micro_step=580/global_step=580, RunningAvgSamplesPerSec=12.338294258816495, CurrSamplesPerSec=12.356303570665775, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      580/219726562 | consumed samples:         1160 | consumed tokens:      2375680 | elapsed time per iteration (ms): 199.8 | learning rate: 1.265E-06 | global batch size:     2 | lm loss: 9.911162E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.010 | TFLOPs: 24.87 |
[default0]:[2023-07-30 22:23:51,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=585, skipped=0, lr=[1.2757674666666667e-06, 1.2757674666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:51,485] [INFO] [timer.py:215:stop] epoch=0/micro_step=585/global_step=585, RunningAvgSamplesPerSec=12.33882142394544, CurrSamplesPerSec=12.301256285459965, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      585/219726562 | consumed samples:         1170 | consumed tokens:      2396160 | elapsed time per iteration (ms): 185.2 | learning rate: 1.276E-06 | global batch size:     2 | lm loss: 9.833904E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.799 | TFLOPs: 26.83 |
[default0]:[2023-07-30 22:23:52,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[1.2866901333333333e-06, 1.2866901333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:52,424] [INFO] [timer.py:215:stop] epoch=0/micro_step=590/global_step=590, RunningAvgSamplesPerSec=12.338761063240609, CurrSamplesPerSec=12.32136635635368, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      590/219726562 | consumed samples:         1180 | consumed tokens:      2416640 | elapsed time per iteration (ms): 187.7 | learning rate: 1.287E-06 | global batch size:     2 | lm loss: 9.813126E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.658 | TFLOPs: 26.48 |
[default0]:[2023-07-30 22:23:53,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=595, skipped=0, lr=[1.2976128000000001e-06, 1.2976128000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:53,420] [INFO] [timer.py:215:stop] epoch=0/micro_step=595/global_step=595, RunningAvgSamplesPerSec=12.339232345089002, CurrSamplesPerSec=12.349700260284783, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      595/219726562 | consumed samples:         1190 | consumed tokens:      2437120 | elapsed time per iteration (ms): 199.4 | learning rate: 1.298E-06 | global batch size:     2 | lm loss: 9.838475E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.031 | TFLOPs: 24.92 |
[default0]:[2023-07-30 22:23:54,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[1.3085354666666668e-06, 1.3085354666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:54,692] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=600, RunningAvgSamplesPerSec=12.339079046288074, CurrSamplesPerSec=12.28925412798474, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      600/219726562 | consumed samples:         1200 | consumed tokens:      2457600 | elapsed time per iteration (ms): 254.3 | learning rate: 1.309E-06 | global batch size:     2 | lm loss: 9.830211E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 7.865 | TFLOPs: 19.54 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 600 | lm loss value: 9.846188E+00 | lm loss PPL: 1.888622E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:23:59,213] [INFO] [logging.py:96:log_dist] [Rank 0] step=605, skipped=0, lr=[1.3194581333333334e-06, 1.3194581333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:59,214] [INFO] [timer.py:215:stop] epoch=0/micro_step=605/global_step=605, RunningAvgSamplesPerSec=12.337956964652953, CurrSamplesPerSec=12.422193658447013, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      605/219726562 | consumed samples:         1210 | consumed tokens:      2478080 | elapsed time per iteration (ms): 904.4 | learning rate: 1.319E-06 | global batch size:     2 | lm loss: 9.743942E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.211 | TFLOPs: 5.49 |
[default0]:[2023-07-30 22:24:00,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[1.3303808e-06, 1.3303808e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:00,142] [INFO] [timer.py:215:stop] epoch=0/micro_step=610/global_step=610, RunningAvgSamplesPerSec=12.337768325500837, CurrSamplesPerSec=12.196520143590966, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      610/219726562 | consumed samples:         1220 | consumed tokens:      2498560 | elapsed time per iteration (ms): 185.7 | learning rate: 1.330E-06 | global batch size:     2 | lm loss: 9.831052E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.772 | TFLOPs: 26.77 |
[default0]:[2023-07-30 22:24:01,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=615, skipped=0, lr=[1.3413034666666667e-06, 1.3413034666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:01,077] [INFO] [timer.py:215:stop] epoch=0/micro_step=615/global_step=615, RunningAvgSamplesPerSec=12.338402777905747, CurrSamplesPerSec=12.371465104172044, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      615/219726562 | consumed samples:         1230 | consumed tokens:      2519040 | elapsed time per iteration (ms): 186.8 | learning rate: 1.341E-06 | global batch size:     2 | lm loss: 9.830083E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.708 | TFLOPs: 26.61 |
[default0]:[2023-07-30 22:24:02,032] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[1.3522261333333335e-06, 1.3522261333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:02,033] [INFO] [timer.py:215:stop] epoch=0/micro_step=620/global_step=620, RunningAvgSamplesPerSec=12.338944972407468, CurrSamplesPerSec=12.388090691736407, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      620/219726562 | consumed samples:         1240 | consumed tokens:      2539520 | elapsed time per iteration (ms): 191.2 | learning rate: 1.352E-06 | global batch size:     2 | lm loss: 9.817187E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.461 | TFLOPs: 25.99 |
[default0]:[2023-07-30 22:24:02,953] [INFO] [logging.py:96:log_dist] [Rank 0] step=625, skipped=0, lr=[1.3631488000000001e-06, 1.3631488000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:02,953] [INFO] [timer.py:215:stop] epoch=0/micro_step=625/global_step=625, RunningAvgSamplesPerSec=12.339844399180873, CurrSamplesPerSec=12.494612572946984, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      625/219726562 | consumed samples:         1250 | consumed tokens:      2560000 | elapsed time per iteration (ms): 184.7 | learning rate: 1.363E-06 | global batch size:     2 | lm loss: 9.777071E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.828 | TFLOPs: 26.91 |
[default0]:[2023-07-30 22:24:03,877] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[1.3740714666666665e-06, 1.3740714666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:03,878] [INFO] [timer.py:215:stop] epoch=0/micro_step=630/global_step=630, RunningAvgSamplesPerSec=12.34012783497594, CurrSamplesPerSec=12.286248249761265, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      630/219726562 | consumed samples:         1260 | consumed tokens:      2580480 | elapsed time per iteration (ms): 184.4 | learning rate: 1.374E-06 | global batch size:     2 | lm loss: 9.799130E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.844 | TFLOPs: 26.94 |
[default0]:[2023-07-30 22:24:04,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=635, skipped=0, lr=[1.3849941333333334e-06, 1.3849941333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:04,833] [INFO] [timer.py:215:stop] epoch=0/micro_step=635/global_step=635, RunningAvgSamplesPerSec=12.339302616335704, CurrSamplesPerSec=12.18990932341316, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      635/219726562 | consumed samples:         1270 | consumed tokens:      2600960 | elapsed time per iteration (ms): 191.0 | learning rate: 1.385E-06 | global batch size:     2 | lm loss: 9.748898E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.471 | TFLOPs: 26.02 |
[default0]:[2023-07-30 22:24:05,760] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[1.3959168e-06, 1.3959168e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:05,761] [INFO] [timer.py:215:stop] epoch=0/micro_step=640/global_step=640, RunningAvgSamplesPerSec=12.339433167184163, CurrSamplesPerSec=12.369531398608308, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      640/219726562 | consumed samples:         1280 | consumed tokens:      2621440 | elapsed time per iteration (ms): 185.4 | learning rate: 1.396E-06 | global batch size:     2 | lm loss: 9.862317E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.785 | TFLOPs: 26.80 |
[default0]:[2023-07-30 22:24:06,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=645, skipped=0, lr=[1.4068394666666666e-06, 1.4068394666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:06,697] [INFO] [timer.py:215:stop] epoch=0/micro_step=645/global_step=645, RunningAvgSamplesPerSec=12.339451484572802, CurrSamplesPerSec=12.362768186895943, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      645/219726562 | consumed samples:         1290 | consumed tokens:      2641920 | elapsed time per iteration (ms): 187.1 | learning rate: 1.407E-06 | global batch size:     2 | lm loss: 9.724927E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.691 | TFLOPs: 26.56 |
[default0]:[2023-07-30 22:24:07,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=0, lr=[1.4177621333333333e-06, 1.4177621333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:07,612] [INFO] [timer.py:215:stop] epoch=0/micro_step=650/global_step=650, RunningAvgSamplesPerSec=12.339858547887172, CurrSamplesPerSec=12.367124232823578, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      650/219726562 | consumed samples:         1300 | consumed tokens:      2662400 | elapsed time per iteration (ms): 183.1 | learning rate: 1.418E-06 | global batch size:     2 | lm loss: 9.768245E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.925 | TFLOPs: 27.15 |
[default0]:[2023-07-30 22:24:08,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=655, skipped=0, lr=[1.4286848e-06, 1.4286848e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:08,537] [INFO] [timer.py:215:stop] epoch=0/micro_step=655/global_step=655, RunningAvgSamplesPerSec=12.339488640004996, CurrSamplesPerSec=12.293270259417856, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      655/219726562 | consumed samples:         1310 | consumed tokens:      2682880 | elapsed time per iteration (ms): 185.0 | learning rate: 1.429E-06 | global batch size:     2 | lm loss: 9.758565E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.813 | TFLOPs: 26.87 |
[default0]:[2023-07-30 22:24:09,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[1.4396074666666667e-06, 1.4396074666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:09,507] [INFO] [timer.py:215:stop] epoch=0/micro_step=660/global_step=660, RunningAvgSamplesPerSec=12.33990308556616, CurrSamplesPerSec=12.316843545725243, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      660/219726562 | consumed samples:         1320 | consumed tokens:      2703360 | elapsed time per iteration (ms): 194.1 | learning rate: 1.440E-06 | global batch size:     2 | lm loss: 9.796596E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.303 | TFLOPs: 25.60 |
[default0]:[2023-07-30 22:24:10,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=665, skipped=0, lr=[1.4505301333333334e-06, 1.4505301333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:10,563] [INFO] [timer.py:215:stop] epoch=0/micro_step=665/global_step=665, RunningAvgSamplesPerSec=12.340453881626427, CurrSamplesPerSec=12.512280178392972, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      665/219726562 | consumed samples:         1330 | consumed tokens:      2723840 | elapsed time per iteration (ms): 211.1 | learning rate: 1.451E-06 | global batch size:     2 | lm loss: 9.721762E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.473 | TFLOPs: 23.54 |
[default0]:[2023-07-30 22:24:11,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[1.4614528e-06, 1.4614528e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:11,505] [INFO] [timer.py:215:stop] epoch=0/micro_step=670/global_step=670, RunningAvgSamplesPerSec=12.3408600158652, CurrSamplesPerSec=12.400653396701973, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      670/219726562 | consumed samples:         1340 | consumed tokens:      2744320 | elapsed time per iteration (ms): 188.4 | learning rate: 1.461E-06 | global batch size:     2 | lm loss: 9.687823E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.616 | TFLOPs: 26.38 |
[default0]:[2023-07-30 22:24:12,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=675, skipped=0, lr=[1.4723754666666666e-06, 1.4723754666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:12,529] [INFO] [timer.py:215:stop] epoch=0/micro_step=675/global_step=675, RunningAvgSamplesPerSec=12.34096206588716, CurrSamplesPerSec=12.365647572157197, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      675/219726562 | consumed samples:         1350 | consumed tokens:      2764800 | elapsed time per iteration (ms): 204.8 | learning rate: 1.472E-06 | global batch size:     2 | lm loss: 9.769737E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.767 | TFLOPs: 24.27 |
[default0]:[2023-07-30 22:24:13,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[1.4832981333333335e-06, 1.4832981333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:13,502] [INFO] [timer.py:215:stop] epoch=0/micro_step=680/global_step=680, RunningAvgSamplesPerSec=12.341153258133303, CurrSamplesPerSec=12.418368615840118, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      680/219726562 | consumed samples:         1360 | consumed tokens:      2785280 | elapsed time per iteration (ms): 194.6 | learning rate: 1.483E-06 | global batch size:     2 | lm loss: 9.734631E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.279 | TFLOPs: 25.54 |
[default0]:[2023-07-30 22:24:14,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=685, skipped=0, lr=[1.4942208e-06, 1.4942208e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:14,498] [INFO] [timer.py:215:stop] epoch=0/micro_step=685/global_step=685, RunningAvgSamplesPerSec=12.339646226930771, CurrSamplesPerSec=11.505381269861735, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      685/219726562 | consumed samples:         1370 | consumed tokens:      2805760 | elapsed time per iteration (ms): 199.4 | learning rate: 1.494E-06 | global batch size:     2 | lm loss: 9.629272E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.028 | TFLOPs: 24.92 |
[default0]:[2023-07-30 22:24:15,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[1.5051434666666667e-06, 1.5051434666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:15,438] [INFO] [timer.py:215:stop] epoch=0/micro_step=690/global_step=690, RunningAvgSamplesPerSec=12.339941308799776, CurrSamplesPerSec=12.419876284019674, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      690/219726562 | consumed samples:         1380 | consumed tokens:      2826240 | elapsed time per iteration (ms): 187.6 | learning rate: 1.505E-06 | global batch size:     2 | lm loss: 9.753710E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.660 | TFLOPs: 26.49 |
[default0]:[2023-07-30 22:24:16,384] [INFO] [logging.py:96:log_dist] [Rank 0] step=695, skipped=0, lr=[1.5160661333333333e-06, 1.5160661333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:16,385] [INFO] [timer.py:215:stop] epoch=0/micro_step=695/global_step=695, RunningAvgSamplesPerSec=12.340329200825174, CurrSamplesPerSec=12.348191403702717, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      695/219726562 | consumed samples:         1390 | consumed tokens:      2846720 | elapsed time per iteration (ms): 189.3 | learning rate: 1.516E-06 | global batch size:     2 | lm loss: 9.774898E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.563 | TFLOPs: 26.25 |
[default0]:[2023-07-30 22:24:17,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[1.5269888e-06, 1.5269888e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:17,332] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=700, RunningAvgSamplesPerSec=12.340447696918277, CurrSamplesPerSec=12.258150684831419, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      700/219726562 | consumed samples:         1400 | consumed tokens:      2867200 | elapsed time per iteration (ms): 189.6 | learning rate: 1.527E-06 | global batch size:     2 | lm loss: 9.673677E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.549 | TFLOPs: 26.21 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 700 | lm loss value: 9.656570E+00 | lm loss PPL: 1.562411E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:24:23,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=705, skipped=0, lr=[1.5379114666666668e-06, 1.5379114666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:23,131] [INFO] [timer.py:215:stop] epoch=0/micro_step=705/global_step=705, RunningAvgSamplesPerSec=12.339852012835198, CurrSamplesPerSec=12.441391175380051, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      705/219726562 | consumed samples:         1410 | consumed tokens:      2887680 | elapsed time per iteration (ms): 1159.8 | learning rate: 1.538E-06 | global batch size:     2 | lm loss: 9.751587E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.724 | TFLOPs: 4.28 |
[default0]:[2023-07-30 22:24:24,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[1.5488341333333334e-06, 1.5488341333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:24,126] [INFO] [timer.py:215:stop] epoch=0/micro_step=710/global_step=710, RunningAvgSamplesPerSec=12.340424155605438, CurrSamplesPerSec=12.462851123699838, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      710/219726562 | consumed samples:         1420 | consumed tokens:      2908160 | elapsed time per iteration (ms): 198.9 | learning rate: 1.549E-06 | global batch size:     2 | lm loss: 9.677288E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.053 | TFLOPs: 24.98 |
[default0]:[2023-07-30 22:24:25,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=715, skipped=0, lr=[1.5597568e-06, 1.5597568e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:25,096] [INFO] [timer.py:215:stop] epoch=0/micro_step=715/global_step=715, RunningAvgSamplesPerSec=12.339773575289424, CurrSamplesPerSec=12.175631053446681, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      715/219726562 | consumed samples:         1430 | consumed tokens:      2928640 | elapsed time per iteration (ms): 194.6 | learning rate: 1.560E-06 | global batch size:     2 | lm loss: 9.745746E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.276 | TFLOPs: 25.53 |
[default0]:[2023-07-30 22:24:26,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[1.5706794666666667e-06, 1.5706794666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:26,102] [INFO] [timer.py:215:stop] epoch=0/micro_step=720/global_step=720, RunningAvgSamplesPerSec=12.340237988801551, CurrSamplesPerSec=12.426205127141618, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      720/219726562 | consumed samples:         1440 | consumed tokens:      2949120 | elapsed time per iteration (ms): 200.4 | learning rate: 1.571E-06 | global batch size:     2 | lm loss: 9.672800E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.980 | TFLOPs: 24.80 |
[default0]:[2023-07-30 22:24:27,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=725, skipped=0, lr=[1.5816021333333335e-06, 1.5816021333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:27,047] [INFO] [timer.py:215:stop] epoch=0/micro_step=725/global_step=725, RunningAvgSamplesPerSec=12.340530438331008, CurrSamplesPerSec=12.346555710751234, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      725/219726562 | consumed samples:         1450 | consumed tokens:      2969600 | elapsed time per iteration (ms): 191.5 | learning rate: 1.582E-06 | global batch size:     2 | lm loss: 9.619339E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.444 | TFLOPs: 25.95 |
[default0]:[2023-07-30 22:24:27,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[1.5925248000000002e-06, 1.5925248000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:27,970] [INFO] [timer.py:215:stop] epoch=0/micro_step=730/global_step=730, RunningAvgSamplesPerSec=12.340730777923165, CurrSamplesPerSec=12.366832518815006, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      730/219726562 | consumed samples:         1460 | consumed tokens:      2990080 | elapsed time per iteration (ms): 182.2 | learning rate: 1.593E-06 | global batch size:     2 | lm loss: 9.703279E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.977 | TFLOPs: 27.27 |
[default0]:[2023-07-30 22:24:28,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=735, skipped=0, lr=[1.6034474666666668e-06, 1.6034474666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:28,912] [INFO] [timer.py:215:stop] epoch=0/micro_step=735/global_step=735, RunningAvgSamplesPerSec=12.34080635696358, CurrSamplesPerSec=12.229165056979289, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      735/219726562 | consumed samples:         1470 | consumed tokens:      3010560 | elapsed time per iteration (ms): 188.9 | learning rate: 1.603E-06 | global batch size:     2 | lm loss: 9.581029E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.590 | TFLOPs: 26.31 |
[default0]:[2023-07-30 22:24:29,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[1.6143701333333334e-06, 1.6143701333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:29,856] [INFO] [timer.py:215:stop] epoch=0/micro_step=740/global_step=740, RunningAvgSamplesPerSec=12.340942439263308, CurrSamplesPerSec=12.294855822924697, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      740/219726562 | consumed samples:         1480 | consumed tokens:      3031040 | elapsed time per iteration (ms): 188.3 | learning rate: 1.614E-06 | global batch size:     2 | lm loss: 9.751099E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.623 | TFLOPs: 26.40 |
[default0]:[2023-07-30 22:24:30,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=745, skipped=0, lr=[1.6252928000000003e-06, 1.6252928000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:30,797] [INFO] [timer.py:215:stop] epoch=0/micro_step=745/global_step=745, RunningAvgSamplesPerSec=12.34155912431894, CurrSamplesPerSec=12.41934304245929, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      745/219726562 | consumed samples:         1490 | consumed tokens:      3051520 | elapsed time per iteration (ms): 188.9 | learning rate: 1.625E-06 | global batch size:     2 | lm loss: 9.612910E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.588 | TFLOPs: 26.31 |
[default0]:[2023-07-30 22:24:31,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[1.636215466666667e-06, 1.636215466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:31,715] [INFO] [timer.py:215:stop] epoch=0/micro_step=750/global_step=750, RunningAvgSamplesPerSec=12.341846257459121, CurrSamplesPerSec=12.388840808731226, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      750/219726562 | consumed samples:         1500 | consumed tokens:      3072000 | elapsed time per iteration (ms): 183.1 | learning rate: 1.636E-06 | global batch size:     2 | lm loss: 9.661868E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.924 | TFLOPs: 27.14 |
[default0]:[2023-07-30 22:24:32,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=755, skipped=0, lr=[1.6471381333333335e-06, 1.6471381333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:32,638] [INFO] [timer.py:215:stop] epoch=0/micro_step=755/global_step=755, RunningAvgSamplesPerSec=12.342010461344694, CurrSamplesPerSec=12.266342239483413, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      755/219726562 | consumed samples:         1510 | consumed tokens:      3092480 | elapsed time per iteration (ms): 184.5 | learning rate: 1.647E-06 | global batch size:     2 | lm loss: 9.631085E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.842 | TFLOPs: 26.94 |
[default0]:[2023-07-30 22:24:33,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[1.6580608000000002e-06, 1.6580608000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:33,571] [INFO] [timer.py:215:stop] epoch=0/micro_step=760/global_step=760, RunningAvgSamplesPerSec=12.342179163157507, CurrSamplesPerSec=12.346446679962026, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      760/219726562 | consumed samples:         1520 | consumed tokens:      3112960 | elapsed time per iteration (ms): 186.9 | learning rate: 1.658E-06 | global batch size:     2 | lm loss: 9.627432E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.698 | TFLOPs: 26.58 |
[default0]:[2023-07-30 22:24:34,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=765, skipped=0, lr=[1.6689834666666668e-06, 1.6689834666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:34,546] [INFO] [timer.py:215:stop] epoch=0/micro_step=765/global_step=765, RunningAvgSamplesPerSec=12.342027386324288, CurrSamplesPerSec=12.36196657146815, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      765/219726562 | consumed samples:         1530 | consumed tokens:      3133440 | elapsed time per iteration (ms): 195.1 | learning rate: 1.669E-06 | global batch size:     2 | lm loss: 9.797470E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.250 | TFLOPs: 25.47 |
[default0]:[2023-07-30 22:24:35,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[1.6799061333333336e-06, 1.6799061333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:35,509] [INFO] [timer.py:215:stop] epoch=0/micro_step=770/global_step=770, RunningAvgSamplesPerSec=12.342337874149615, CurrSamplesPerSec=12.374001728812459, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      770/219726562 | consumed samples:         1540 | consumed tokens:      3153920 | elapsed time per iteration (ms): 191.9 | learning rate: 1.680E-06 | global batch size:     2 | lm loss: 9.644164E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.420 | TFLOPs: 25.89 |
[default0]:[2023-07-30 22:24:36,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=775, skipped=0, lr=[1.6908288000000003e-06, 1.6908288000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:36,450] [INFO] [timer.py:215:stop] epoch=0/micro_step=775/global_step=775, RunningAvgSamplesPerSec=12.342576934764923, CurrSamplesPerSec=12.376119420601174, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      775/219726562 | consumed samples:         1550 | consumed tokens:      3174400 | elapsed time per iteration (ms): 188.6 | learning rate: 1.691E-06 | global batch size:     2 | lm loss: 9.647127E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.602 | TFLOPs: 26.34 |
[default0]:[2023-07-30 22:24:37,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=0, lr=[1.7017514666666667e-06, 1.7017514666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:37,380] [INFO] [timer.py:215:stop] epoch=0/micro_step=780/global_step=780, RunningAvgSamplesPerSec=12.343006355781002, CurrSamplesPerSec=12.461925397280503, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      780/219726562 | consumed samples:         1560 | consumed tokens:      3194880 | elapsed time per iteration (ms): 185.9 | learning rate: 1.702E-06 | global batch size:     2 | lm loss: 9.528069E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.758 | TFLOPs: 26.73 |
[default0]:[2023-07-30 22:24:38,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=785, skipped=0, lr=[1.7126741333333333e-06, 1.7126741333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:38,319] [INFO] [timer.py:215:stop] epoch=0/micro_step=785/global_step=785, RunningAvgSamplesPerSec=12.343413620438916, CurrSamplesPerSec=12.352828221545494, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      785/219726562 | consumed samples:         1570 | consumed tokens:      3215360 | elapsed time per iteration (ms): 187.6 | learning rate: 1.713E-06 | global batch size:     2 | lm loss: 9.615634E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.662 | TFLOPs: 26.49 |
[default0]:[2023-07-30 22:24:39,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=0, lr=[1.7235968e-06, 1.7235968e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:39,328] [INFO] [timer.py:215:stop] epoch=0/micro_step=790/global_step=790, RunningAvgSamplesPerSec=12.344249963065607, CurrSamplesPerSec=12.458871475589035, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      790/219726562 | consumed samples:         1580 | consumed tokens:      3235840 | elapsed time per iteration (ms): 201.8 | learning rate: 1.724E-06 | global batch size:     2 | lm loss: 9.498102E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.913 | TFLOPs: 24.63 |
[default0]:[2023-07-30 22:24:40,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=795, skipped=0, lr=[1.7345194666666666e-06, 1.7345194666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:40,311] [INFO] [timer.py:215:stop] epoch=0/micro_step=795/global_step=795, RunningAvgSamplesPerSec=12.344961184111574, CurrSamplesPerSec=12.40727462047259, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      795/219726562 | consumed samples:         1590 | consumed tokens:      3256320 | elapsed time per iteration (ms): 196.6 | learning rate: 1.735E-06 | global batch size:     2 | lm loss: 9.633456E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.170 | TFLOPs: 25.27 |
[default0]:[2023-07-30 22:24:41,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=0, lr=[1.7454421333333334e-06, 1.7454421333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:41,375] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=800, RunningAvgSamplesPerSec=12.345338901522728, CurrSamplesPerSec=12.33758528539261, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      800/219726562 | consumed samples:         1600 | consumed tokens:      3276800 | elapsed time per iteration (ms): 212.8 | learning rate: 1.745E-06 | global batch size:     2 | lm loss: 9.537727E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.399 | TFLOPs: 23.35 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 800 | lm loss value: 9.560107E+00 | lm loss PPL: 1.418737E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:24:45,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=805, skipped=0, lr=[1.7563648e-06, 1.7563648e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:45,278] [INFO] [timer.py:215:stop] epoch=0/micro_step=805/global_step=805, RunningAvgSamplesPerSec=12.345178914114788, CurrSamplesPerSec=12.316753123380312, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      805/219726562 | consumed samples:         1610 | consumed tokens:      3297280 | elapsed time per iteration (ms): 780.7 | learning rate: 1.756E-06 | global batch size:     2 | lm loss: 9.490613E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.562 | TFLOPs: 6.37 |
[default0]:[2023-07-30 22:24:46,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=0, lr=[1.7672874666666667e-06, 1.7672874666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:46,198] [INFO] [timer.py:215:stop] epoch=0/micro_step=810/global_step=810, RunningAvgSamplesPerSec=12.345396187809664, CurrSamplesPerSec=12.360618634477555, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      810/219726562 | consumed samples:         1620 | consumed tokens:      3317760 | elapsed time per iteration (ms): 183.7 | learning rate: 1.767E-06 | global batch size:     2 | lm loss: 9.606225E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.886 | TFLOPs: 27.05 |
[default0]:[2023-07-30 22:24:47,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=815, skipped=0, lr=[1.7782101333333333e-06, 1.7782101333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:47,136] [INFO] [timer.py:215:stop] epoch=0/micro_step=815/global_step=815, RunningAvgSamplesPerSec=12.34571134239146, CurrSamplesPerSec=12.28993830569958, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      815/219726562 | consumed samples:         1630 | consumed tokens:      3338240 | elapsed time per iteration (ms): 187.7 | learning rate: 1.778E-06 | global batch size:     2 | lm loss: 9.573727E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.657 | TFLOPs: 26.48 |
[default0]:[2023-07-30 22:24:48,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=0, lr=[1.7891328e-06, 1.7891328e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:48,055] [INFO] [timer.py:215:stop] epoch=0/micro_step=820/global_step=820, RunningAvgSamplesPerSec=12.345580455673359, CurrSamplesPerSec=12.22551139974, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      820/219726562 | consumed samples:         1640 | consumed tokens:      3358720 | elapsed time per iteration (ms): 183.9 | learning rate: 1.789E-06 | global batch size:     2 | lm loss: 9.492642E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.873 | TFLOPs: 27.02 |
[default0]:[2023-07-30 22:24:48,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=825, skipped=0, lr=[1.8000554666666668e-06, 1.8000554666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:48,980] [INFO] [timer.py:215:stop] epoch=0/micro_step=825/global_step=825, RunningAvgSamplesPerSec=12.345550925166735, CurrSamplesPerSec=12.41346203647115, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      825/219726562 | consumed samples:         1650 | consumed tokens:      3379200 | elapsed time per iteration (ms): 184.9 | learning rate: 1.800E-06 | global batch size:     2 | lm loss: 9.642299E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.816 | TFLOPs: 26.88 |
[default0]:[2023-07-30 22:24:49,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=0, lr=[1.8109781333333334e-06, 1.8109781333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:49,894] [INFO] [timer.py:215:stop] epoch=0/micro_step=830/global_step=830, RunningAvgSamplesPerSec=12.345632170286784, CurrSamplesPerSec=12.382422013960952, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      830/219726562 | consumed samples:         1660 | consumed tokens:      3399680 | elapsed time per iteration (ms): 183.2 | learning rate: 1.811E-06 | global batch size:     2 | lm loss: 9.490079E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.920 | TFLOPs: 27.13 |
[default0]:[2023-07-30 22:24:50,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=835, skipped=0, lr=[1.8219008e-06, 1.8219008e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:50,806] [INFO] [timer.py:215:stop] epoch=0/micro_step=835/global_step=835, RunningAvgSamplesPerSec=12.34593571378408, CurrSamplesPerSec=12.445525840249486, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      835/219726562 | consumed samples:         1670 | consumed tokens:      3420160 | elapsed time per iteration (ms): 182.1 | learning rate: 1.822E-06 | global batch size:     2 | lm loss: 9.463720E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.984 | TFLOPs: 27.29 |
[default0]:[2023-07-30 22:24:51,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=0, lr=[1.8328234666666666e-06, 1.8328234666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:51,723] [INFO] [timer.py:215:stop] epoch=0/micro_step=840/global_step=840, RunningAvgSamplesPerSec=12.345992600880393, CurrSamplesPerSec=12.34154228672733, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      840/219726562 | consumed samples:         1680 | consumed tokens:      3440640 | elapsed time per iteration (ms): 183.3 | learning rate: 1.833E-06 | global batch size:     2 | lm loss: 9.611397E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.909 | TFLOPs: 27.11 |
[default0]:[2023-07-30 22:24:52,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=845, skipped=0, lr=[1.8437461333333335e-06, 1.8437461333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:52,661] [INFO] [timer.py:215:stop] epoch=0/micro_step=845/global_step=845, RunningAvgSamplesPerSec=12.3461611137835, CurrSamplesPerSec=12.364863809157413, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      845/219726562 | consumed samples:         1690 | consumed tokens:      3461120 | elapsed time per iteration (ms): 188.1 | learning rate: 1.844E-06 | global batch size:     2 | lm loss: 9.550023E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.632 | TFLOPs: 26.42 |
[default0]:[2023-07-30 22:24:53,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=0, lr=[1.8546688000000001e-06, 1.8546688000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:53,583] [INFO] [timer.py:215:stop] epoch=0/micro_step=850/global_step=850, RunningAvgSamplesPerSec=12.346276001785553, CurrSamplesPerSec=12.34950027014414, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      850/219726562 | consumed samples:         1700 | consumed tokens:      3481600 | elapsed time per iteration (ms): 184.3 | learning rate: 1.855E-06 | global batch size:     2 | lm loss: 9.544377E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.854 | TFLOPs: 26.97 |
[default0]:[2023-07-30 22:24:54,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=855, skipped=0, lr=[1.8655914666666667e-06, 1.8655914666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:54,524] [INFO] [timer.py:215:stop] epoch=0/micro_step=855/global_step=855, RunningAvgSamplesPerSec=12.34609284379261, CurrSamplesPerSec=12.167401328337924, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      855/219726562 | consumed samples:         1710 | consumed tokens:      3502080 | elapsed time per iteration (ms): 188.1 | learning rate: 1.866E-06 | global batch size:     2 | lm loss: 9.535898E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.633 | TFLOPs: 26.42 |
[default0]:[2023-07-30 22:24:55,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[1.8765141333333334e-06, 1.8765141333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:55,572] [INFO] [timer.py:215:stop] epoch=0/micro_step=860/global_step=860, RunningAvgSamplesPerSec=12.346284005707526, CurrSamplesPerSec=12.346410336793552, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      860/219726562 | consumed samples:         1720 | consumed tokens:      3522560 | elapsed time per iteration (ms): 209.3 | learning rate: 1.877E-06 | global batch size:     2 | lm loss: 9.444933E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.557 | TFLOPs: 23.75 |
[default0]:[2023-07-30 22:24:56,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=865, skipped=0, lr=[1.8874368e-06, 1.8874368e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:56,527] [INFO] [timer.py:215:stop] epoch=0/micro_step=865/global_step=865, RunningAvgSamplesPerSec=12.346650340424702, CurrSamplesPerSec=12.37926078089352, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      865/219726562 | consumed samples:         1730 | consumed tokens:      3543040 | elapsed time per iteration (ms): 191.2 | learning rate: 1.887E-06 | global batch size:     2 | lm loss: 9.495915E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.460 | TFLOPs: 25.99 |
[default0]:[2023-07-30 22:24:57,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[1.8983594666666668e-06, 1.8983594666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:57,447] [INFO] [timer.py:215:stop] epoch=0/micro_step=870/global_step=870, RunningAvgSamplesPerSec=12.347253528279703, CurrSamplesPerSec=12.420869338608513, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      870/219726562 | consumed samples:         1740 | consumed tokens:      3563520 | elapsed time per iteration (ms): 183.7 | learning rate: 1.898E-06 | global batch size:     2 | lm loss: 9.502274E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.888 | TFLOPs: 27.06 |
[default0]:[2023-07-30 22:24:58,493] [INFO] [logging.py:96:log_dist] [Rank 0] step=875, skipped=0, lr=[1.9092821333333333e-06, 1.9092821333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:58,494] [INFO] [timer.py:215:stop] epoch=0/micro_step=875/global_step=875, RunningAvgSamplesPerSec=12.347560995410863, CurrSamplesPerSec=12.388676141121428, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      875/219726562 | consumed samples:         1750 | consumed tokens:      3584000 | elapsed time per iteration (ms): 209.3 | learning rate: 1.909E-06 | global batch size:     2 | lm loss: 9.495992E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.554 | TFLOPs: 23.74 |
[default0]:[2023-07-30 22:24:59,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[1.9202048000000003e-06, 1.9202048000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:59,435] [INFO] [timer.py:215:stop] epoch=0/micro_step=880/global_step=880, RunningAvgSamplesPerSec=12.347857109171187, CurrSamplesPerSec=12.34948208954345, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      880/219726562 | consumed samples:         1760 | consumed tokens:      3604480 | elapsed time per iteration (ms): 188.1 | learning rate: 1.920E-06 | global batch size:     2 | lm loss: 9.469385E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.632 | TFLOPs: 26.42 |
[default0]:[2023-07-30 22:25:00,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=885, skipped=0, lr=[1.931127466666667e-06, 1.931127466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:00,464] [INFO] [timer.py:215:stop] epoch=0/micro_step=885/global_step=885, RunningAvgSamplesPerSec=12.348056036596482, CurrSamplesPerSec=12.34871855264901, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      885/219726562 | consumed samples:         1770 | consumed tokens:      3624960 | elapsed time per iteration (ms): 206.1 | learning rate: 1.931E-06 | global batch size:     2 | lm loss: 9.481246E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.704 | TFLOPs: 24.11 |
[default0]:[2023-07-30 22:25:01,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[1.9420501333333336e-06, 1.9420501333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:01,400] [INFO] [timer.py:215:stop] epoch=0/micro_step=890/global_step=890, RunningAvgSamplesPerSec=12.34817451652033, CurrSamplesPerSec=12.35353768896023, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      890/219726562 | consumed samples:         1780 | consumed tokens:      3645440 | elapsed time per iteration (ms): 187.0 | learning rate: 1.942E-06 | global batch size:     2 | lm loss: 9.503217E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.693 | TFLOPs: 26.57 |
[default0]:[2023-07-30 22:25:02,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=895, skipped=0, lr=[1.9529728e-06, 1.9529728e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:02,393] [INFO] [timer.py:215:stop] epoch=0/micro_step=895/global_step=895, RunningAvgSamplesPerSec=12.348245913910313, CurrSamplesPerSec=12.306814764173158, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      895/219726562 | consumed samples:         1790 | consumed tokens:      3665920 | elapsed time per iteration (ms): 199.1 | learning rate: 1.953E-06 | global batch size:     2 | lm loss: 9.457449E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.046 | TFLOPs: 24.96 |
[default0]:[2023-07-30 22:25:03,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[1.963895466666667e-06, 1.963895466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:03,355] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=900, RunningAvgSamplesPerSec=12.348441470976995, CurrSamplesPerSec=12.297811828382606, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      900/219726562 | consumed samples:         1800 | consumed tokens:      3686400 | elapsed time per iteration (ms): 191.8 | learning rate: 1.964E-06 | global batch size:     2 | lm loss: 9.476443E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.428 | TFLOPs: 25.91 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 900 | lm loss value: 9.433829E+00 | lm loss PPL: 1.250432E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:25:08,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=905, skipped=0, lr=[1.9748181333333335e-06, 1.9748181333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:08,712] [INFO] [timer.py:215:stop] epoch=0/micro_step=905/global_step=905, RunningAvgSamplesPerSec=12.3471335175311, CurrSamplesPerSec=12.109001989150613, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      905/219726562 | consumed samples:         1810 | consumed tokens:      3706880 | elapsed time per iteration (ms): 1071.6 | learning rate: 1.975E-06 | global batch size:     2 | lm loss: 9.620393E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.866 | TFLOPs: 4.64 |
[default0]:[2023-07-30 22:25:09,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[1.9857408e-06, 1.9857408e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:09,667] [INFO] [timer.py:215:stop] epoch=0/micro_step=910/global_step=910, RunningAvgSamplesPerSec=12.34702884022823, CurrSamplesPerSec=12.320660579240574, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      910/219726562 | consumed samples:         1820 | consumed tokens:      3727360 | elapsed time per iteration (ms): 190.8 | learning rate: 1.986E-06 | global batch size:     2 | lm loss: 9.494185E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.482 | TFLOPs: 26.05 |
[default0]:[2023-07-30 22:25:10,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=915, skipped=0, lr=[1.9966634666666667e-06, 1.9966634666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:10,580] [INFO] [timer.py:215:stop] epoch=0/micro_step=915/global_step=915, RunningAvgSamplesPerSec=12.347139888242605, CurrSamplesPerSec=12.322814357359219, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      915/219726562 | consumed samples:         1830 | consumed tokens:      3747840 | elapsed time per iteration (ms): 182.9 | learning rate: 1.997E-06 | global batch size:     2 | lm loss: 9.439923E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.936 | TFLOPs: 27.17 |
[default0]:[2023-07-30 22:25:11,498] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[2.0075861333333333e-06, 2.0075861333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:11,499] [INFO] [timer.py:215:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=12.347287383294386, CurrSamplesPerSec=12.354247237874187, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      920/219726562 | consumed samples:         1840 | consumed tokens:      3768320 | elapsed time per iteration (ms): 183.6 | learning rate: 2.008E-06 | global batch size:     2 | lm loss: 9.443185E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.892 | TFLOPs: 27.06 |
[default0]:[2023-07-30 22:25:12,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=925, skipped=0, lr=[2.0185088000000004e-06, 2.0185088000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:12,422] [INFO] [timer.py:215:stop] epoch=0/micro_step=925/global_step=925, RunningAvgSamplesPerSec=12.347081341774555, CurrSamplesPerSec=12.351500463074075, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      925/219726562 | consumed samples:         1850 | consumed tokens:      3788800 | elapsed time per iteration (ms): 184.9 | learning rate: 2.019E-06 | global batch size:     2 | lm loss: 9.380910E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.814 | TFLOPs: 26.87 |
[default0]:[2023-07-30 22:25:13,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=0, lr=[2.029431466666667e-06, 2.029431466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:13,341] [INFO] [timer.py:215:stop] epoch=0/micro_step=930/global_step=930, RunningAvgSamplesPerSec=12.347445065036775, CurrSamplesPerSec=12.33636965249489, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      930/219726562 | consumed samples:         1860 | consumed tokens:      3809280 | elapsed time per iteration (ms): 184.2 | learning rate: 2.029E-06 | global batch size:     2 | lm loss: 9.443526E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.856 | TFLOPs: 26.98 |
[default0]:[2023-07-30 22:25:14,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=935, skipped=0, lr=[2.0403541333333337e-06, 2.0403541333333337e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:14,284] [INFO] [timer.py:215:stop] epoch=0/micro_step=935/global_step=935, RunningAvgSamplesPerSec=12.347788702909746, CurrSamplesPerSec=12.430256025747791, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      935/219726562 | consumed samples:         1870 | consumed tokens:      3829760 | elapsed time per iteration (ms): 187.9 | learning rate: 2.040E-06 | global batch size:     2 | lm loss: 9.500501E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.642 | TFLOPs: 26.44 |
[default0]:[2023-07-30 22:25:15,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=0, lr=[2.0512768000000003e-06, 2.0512768000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:15,257] [INFO] [timer.py:215:stop] epoch=0/micro_step=940/global_step=940, RunningAvgSamplesPerSec=12.347867698960652, CurrSamplesPerSec=12.365410610074102, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      940/219726562 | consumed samples:         1880 | consumed tokens:      3850240 | elapsed time per iteration (ms): 195.1 | learning rate: 2.051E-06 | global batch size:     2 | lm loss: 9.271553E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.249 | TFLOPs: 25.47 |
[default0]:[2023-07-30 22:25:16,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=945, skipped=0, lr=[2.0621994666666665e-06, 2.0621994666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:16,210] [INFO] [timer.py:215:stop] epoch=0/micro_step=945/global_step=945, RunningAvgSamplesPerSec=12.348360278457752, CurrSamplesPerSec=12.37531607287748, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      945/219726562 | consumed samples:         1890 | consumed tokens:      3870720 | elapsed time per iteration (ms): 190.0 | learning rate: 2.062E-06 | global batch size:     2 | lm loss: 9.419383E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.527 | TFLOPs: 26.16 |
[default0]:[2023-07-30 22:25:17,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=0, lr=[2.073122133333333e-06, 2.073122133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:17,129] [INFO] [timer.py:215:stop] epoch=0/micro_step=950/global_step=950, RunningAvgSamplesPerSec=12.34874566679377, CurrSamplesPerSec=12.37856662220071, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      950/219726562 | consumed samples:         1900 | consumed tokens:      3891200 | elapsed time per iteration (ms): 183.9 | learning rate: 2.073E-06 | global batch size:     2 | lm loss: 9.424638E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.878 | TFLOPs: 27.03 |
[default0]:[2023-07-30 22:25:18,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=955, skipped=0, lr=[2.0840448e-06, 2.0840448e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:18,045] [INFO] [timer.py:215:stop] epoch=0/micro_step=955/global_step=955, RunningAvgSamplesPerSec=12.348628138552698, CurrSamplesPerSec=12.22877285257583, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      955/219726562 | consumed samples:         1910 | consumed tokens:      3911680 | elapsed time per iteration (ms): 183.1 | learning rate: 2.084E-06 | global batch size:     2 | lm loss: 9.276154E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.923 | TFLOPs: 27.14 |
[default0]:[2023-07-30 22:25:18,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=0, lr=[2.094967466666667e-06, 2.094967466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:18,965] [INFO] [timer.py:215:stop] epoch=0/micro_step=960/global_step=960, RunningAvgSamplesPerSec=12.34850724745795, CurrSamplesPerSec=12.213976622151298, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      960/219726562 | consumed samples:         1920 | consumed tokens:      3932160 | elapsed time per iteration (ms): 184.1 | learning rate: 2.095E-06 | global batch size:     2 | lm loss: 9.444500E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.861 | TFLOPs: 26.99 |
[default0]:[2023-07-30 22:25:19,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=965, skipped=0, lr=[2.1058901333333334e-06, 2.1058901333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:19,884] [INFO] [timer.py:215:stop] epoch=0/micro_step=965/global_step=965, RunningAvgSamplesPerSec=12.348515460861334, CurrSamplesPerSec=12.334319948596022, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      965/219726562 | consumed samples:         1930 | consumed tokens:      3952640 | elapsed time per iteration (ms): 183.8 | learning rate: 2.106E-06 | global batch size:     2 | lm loss: 9.364941E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.884 | TFLOPs: 27.04 |
[default0]:[2023-07-30 22:25:20,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=0, lr=[2.1168128e-06, 2.1168128e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:20,803] [INFO] [timer.py:215:stop] epoch=0/micro_step=970/global_step=970, RunningAvgSamplesPerSec=12.348577822461378, CurrSamplesPerSec=12.392647680166405, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      970/219726562 | consumed samples:         1940 | consumed tokens:      3973120 | elapsed time per iteration (ms): 183.6 | learning rate: 2.117E-06 | global batch size:     2 | lm loss: 9.291486E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.895 | TFLOPs: 27.07 |
[default0]:[2023-07-30 22:25:21,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=975, skipped=0, lr=[2.1277354666666667e-06, 2.1277354666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:21,720] [INFO] [timer.py:215:stop] epoch=0/micro_step=975/global_step=975, RunningAvgSamplesPerSec=12.348637376588657, CurrSamplesPerSec=12.362731747575678, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      975/219726562 | consumed samples:         1950 | consumed tokens:      3993600 | elapsed time per iteration (ms): 183.4 | learning rate: 2.128E-06 | global batch size:     2 | lm loss: 9.365691E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.905 | TFLOPs: 27.10 |
[default0]:[2023-07-30 22:25:22,646] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=0, lr=[2.1386581333333333e-06, 2.1386581333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:22,646] [INFO] [timer.py:215:stop] epoch=0/micro_step=980/global_step=980, RunningAvgSamplesPerSec=12.348754519073372, CurrSamplesPerSec=12.35819672976433, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      980/219726562 | consumed samples:         1960 | consumed tokens:      4014080 | elapsed time per iteration (ms): 185.3 | learning rate: 2.139E-06 | global batch size:     2 | lm loss: 9.320635E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.793 | TFLOPs: 26.82 |
[default0]:[2023-07-30 22:25:23,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=985, skipped=0, lr=[2.1495808e-06, 2.1495808e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:23,564] [INFO] [timer.py:215:stop] epoch=0/micro_step=985/global_step=985, RunningAvgSamplesPerSec=12.349076600229932, CurrSamplesPerSec=12.33636965249489, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      985/219726562 | consumed samples:         1970 | consumed tokens:      4034560 | elapsed time per iteration (ms): 183.6 | learning rate: 2.150E-06 | global batch size:     2 | lm loss: 9.349944E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.892 | TFLOPs: 27.06 |
[default0]:[2023-07-30 22:25:24,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=0, lr=[2.1605034666666666e-06, 2.1605034666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:24,484] [INFO] [timer.py:215:stop] epoch=0/micro_step=990/global_step=990, RunningAvgSamplesPerSec=12.348694284263674, CurrSamplesPerSec=11.888465141459363, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      990/219726562 | consumed samples:         1980 | consumed tokens:      4055040 | elapsed time per iteration (ms): 184.6 | learning rate: 2.161E-06 | global batch size:     2 | lm loss: 9.306221E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.837 | TFLOPs: 26.93 |
[default0]:[2023-07-30 22:25:25,409] [INFO] [logging.py:96:log_dist] [Rank 0] step=995, skipped=0, lr=[2.171426133333333e-06, 2.171426133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:25,410] [INFO] [timer.py:215:stop] epoch=0/micro_step=995/global_step=995, RunningAvgSamplesPerSec=12.348966408937002, CurrSamplesPerSec=12.369914443202497, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration      995/219726562 | consumed samples:         1990 | consumed tokens:      4075520 | elapsed time per iteration (ms): 184.7 | learning rate: 2.171E-06 | global batch size:     2 | lm loss: 9.254839E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.830 | TFLOPs: 26.91 |
[default0]:[2023-07-30 22:25:26,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[2.1823488000000003e-06, 2.1823488000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:26,332] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=12.34882050147659, CurrSamplesPerSec=12.199056488531857, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1000/219726562 | consumed samples:         2000 | consumed tokens:      4096000 | elapsed time per iteration (ms): 184.2 | learning rate: 2.182E-06 | global batch size:     2 | lm loss: 9.366602E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.860 | TFLOPs: 26.99 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1000 | lm loss value: 9.350126E+00 | lm loss PPL: 1.150028E+04 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:25:30,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=1005, skipped=0, lr=[2.193271466666667e-06, 2.193271466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:30,727] [INFO] [timer.py:215:stop] epoch=0/micro_step=1005/global_step=1005, RunningAvgSamplesPerSec=12.3490718348674, CurrSamplesPerSec=12.416695900330229, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1005/219726562 | consumed samples:         2010 | consumed tokens:      4116480 | elapsed time per iteration (ms): 879.1 | learning rate: 2.193E-06 | global batch size:     2 | lm loss: 9.316640E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.275 | TFLOPs: 5.65 |
[default0]:[2023-07-30 22:25:31,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=0, lr=[2.2041941333333335e-06, 2.2041941333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:31,652] [INFO] [timer.py:215:stop] epoch=0/micro_step=1010/global_step=1010, RunningAvgSamplesPerSec=12.349174470352226, CurrSamplesPerSec=12.257470041658934, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1010/219726562 | consumed samples:         2020 | consumed tokens:      4136960 | elapsed time per iteration (ms): 184.8 | learning rate: 2.204E-06 | global batch size:     2 | lm loss: 9.270128E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.824 | TFLOPs: 26.89 |
[default0]:[2023-07-30 22:25:32,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=1015, skipped=0, lr=[2.2151168e-06, 2.2151168e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:32,664] [INFO] [timer.py:215:stop] epoch=0/micro_step=1015/global_step=1015, RunningAvgSamplesPerSec=12.349757765603911, CurrSamplesPerSec=12.509555963828111, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1015/219726562 | consumed samples:         2030 | consumed tokens:      4157440 | elapsed time per iteration (ms): 202.4 | learning rate: 2.215E-06 | global batch size:     2 | lm loss: 9.401907E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.880 | TFLOPs: 24.55 |
[default0]:[2023-07-30 22:25:33,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=0, lr=[2.2260394666666668e-06, 2.2260394666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:33,661] [INFO] [timer.py:215:stop] epoch=0/micro_step=1020/global_step=1020, RunningAvgSamplesPerSec=12.350310897160357, CurrSamplesPerSec=12.416732658465428, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1020/219726562 | consumed samples:         2040 | consumed tokens:      4177920 | elapsed time per iteration (ms): 200.5 | learning rate: 2.226E-06 | global batch size:     2 | lm loss: 9.431332E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.973 | TFLOPs: 24.78 |
[default0]:[2023-07-30 22:25:34,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=1025, skipped=0, lr=[2.2369621333333334e-06, 2.2369621333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:34,619] [INFO] [timer.py:215:stop] epoch=0/micro_step=1025/global_step=1025, RunningAvgSamplesPerSec=12.350120077595959, CurrSamplesPerSec=11.843766236700025, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1025/219726562 | consumed samples:         2050 | consumed tokens:      4198400 | elapsed time per iteration (ms): 190.8 | learning rate: 2.237E-06 | global batch size:     2 | lm loss: 9.336380E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.480 | TFLOPs: 26.04 |
[default0]:[2023-07-30 22:25:35,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=0, lr=[2.2478848e-06, 2.2478848e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:35,533] [INFO] [timer.py:215:stop] epoch=0/micro_step=1030/global_step=1030, RunningAvgSamplesPerSec=12.350381669027332, CurrSamplesPerSec=12.357850820850668, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1030/219726562 | consumed samples:         2060 | consumed tokens:      4218880 | elapsed time per iteration (ms): 182.4 | learning rate: 2.248E-06 | global batch size:     2 | lm loss: 9.311937E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.962 | TFLOPs: 27.24 |
[default0]:[2023-07-30 22:25:36,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=1035, skipped=0, lr=[2.2588074666666667e-06, 2.2588074666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:36,525] [INFO] [timer.py:215:stop] epoch=0/micro_step=1035/global_step=1035, RunningAvgSamplesPerSec=12.350604811243866, CurrSamplesPerSec=12.376995918897306, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1035/219726562 | consumed samples:         2070 | consumed tokens:      4239360 | elapsed time per iteration (ms): 198.5 | learning rate: 2.259E-06 | global batch size:     2 | lm loss: 9.255353E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.076 | TFLOPs: 25.04 |
[default0]:[2023-07-30 22:25:37,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=0, lr=[2.2697301333333333e-06, 2.2697301333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:37,484] [INFO] [timer.py:215:stop] epoch=0/micro_step=1040/global_step=1040, RunningAvgSamplesPerSec=12.35098977244654, CurrSamplesPerSec=12.430311283527772, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1040/219726562 | consumed samples:         2080 | consumed tokens:      4259840 | elapsed time per iteration (ms): 192.4 | learning rate: 2.270E-06 | global batch size:     2 | lm loss: 9.312108E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.394 | TFLOPs: 25.83 |
[default0]:[2023-07-30 22:25:38,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=1045, skipped=0, lr=[2.2806528000000003e-06, 2.2806528000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:38,414] [INFO] [timer.py:215:stop] epoch=0/micro_step=1045/global_step=1045, RunningAvgSamplesPerSec=12.350979684557341, CurrSamplesPerSec=12.268925654646292, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1045/219726562 | consumed samples:         2090 | consumed tokens:      4280320 | elapsed time per iteration (ms): 185.4 | learning rate: 2.281E-06 | global batch size:     2 | lm loss: 9.227692E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.788 | TFLOPs: 26.81 |
[default0]:[2023-07-30 22:25:39,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=0, lr=[2.291575466666667e-06, 2.291575466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:39,328] [INFO] [timer.py:215:stop] epoch=0/micro_step=1050/global_step=1050, RunningAvgSamplesPerSec=12.35107616607354, CurrSamplesPerSec=12.343085842088225, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1050/219726562 | consumed samples:         2100 | consumed tokens:      4300800 | elapsed time per iteration (ms): 182.6 | learning rate: 2.292E-06 | global batch size:     2 | lm loss: 9.139566E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.951 | TFLOPs: 27.21 |
[default0]:[2023-07-30 22:25:40,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=1055, skipped=0, lr=[2.3024981333333336e-06, 2.3024981333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:40,252] [INFO] [timer.py:215:stop] epoch=0/micro_step=1055/global_step=1055, RunningAvgSamplesPerSec=12.351154911643679, CurrSamplesPerSec=12.25824024877105, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1055/219726562 | consumed samples:         2110 | consumed tokens:      4321280 | elapsed time per iteration (ms): 185.1 | learning rate: 2.302E-06 | global batch size:     2 | lm loss: 9.145372E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.804 | TFLOPs: 26.85 |
[default0]:[2023-07-30 22:25:41,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=0, lr=[2.3134208000000002e-06, 2.3134208000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:41,240] [INFO] [timer.py:215:stop] epoch=0/micro_step=1060/global_step=1060, RunningAvgSamplesPerSec=12.350348769175342, CurrSamplesPerSec=11.468416972792511, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1060/219726562 | consumed samples:         2120 | consumed tokens:      4341760 | elapsed time per iteration (ms): 197.4 | learning rate: 2.313E-06 | global batch size:     2 | lm loss: 9.156701E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.131 | TFLOPs: 25.17 |
[default0]:[2023-07-30 22:25:42,208] [INFO] [logging.py:96:log_dist] [Rank 0] step=1065, skipped=0, lr=[2.324343466666667e-06, 2.324343466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:42,208] [INFO] [timer.py:215:stop] epoch=0/micro_step=1065/global_step=1065, RunningAvgSamplesPerSec=12.350656430256711, CurrSamplesPerSec=12.376466352998804, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1065/219726562 | consumed samples:         2130 | consumed tokens:      4362240 | elapsed time per iteration (ms): 193.8 | learning rate: 2.324E-06 | global batch size:     2 | lm loss: 9.179848E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.322 | TFLOPs: 25.65 |
[default0]:[2023-07-30 22:25:43,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=0, lr=[2.3352661333333335e-06, 2.3352661333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:43,148] [INFO] [timer.py:215:stop] epoch=0/micro_step=1070/global_step=1070, RunningAvgSamplesPerSec=12.350790923151198, CurrSamplesPerSec=12.270702383895358, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1070/219726562 | consumed samples:         2140 | consumed tokens:      4382720 | elapsed time per iteration (ms): 187.7 | learning rate: 2.335E-06 | global batch size:     2 | lm loss: 9.304896E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.657 | TFLOPs: 26.48 |
[default0]:[2023-07-30 22:25:44,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=1075, skipped=0, lr=[2.3461888e-06, 2.3461888e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:44,121] [INFO] [timer.py:215:stop] epoch=0/micro_step=1075/global_step=1075, RunningAvgSamplesPerSec=12.35061747163931, CurrSamplesPerSec=12.1670660176488, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1075/219726562 | consumed samples:         2150 | consumed tokens:      4403200 | elapsed time per iteration (ms): 194.8 | learning rate: 2.346E-06 | global batch size:     2 | lm loss: 9.199588E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.265 | TFLOPs: 25.51 |
[default0]:[2023-07-30 22:25:45,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=0, lr=[2.3571114666666667e-06, 2.3571114666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:45,058] [INFO] [timer.py:215:stop] epoch=0/micro_step=1080/global_step=1080, RunningAvgSamplesPerSec=12.350102216546118, CurrSamplesPerSec=12.263473053892216, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1080/219726562 | consumed samples:         2160 | consumed tokens:      4423680 | elapsed time per iteration (ms): 187.2 | learning rate: 2.357E-06 | global batch size:     2 | lm loss: 9.151413E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.686 | TFLOPs: 26.55 |
[default0]:[2023-07-30 22:25:45,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=1085, skipped=0, lr=[2.3680341333333334e-06, 2.3680341333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:45,973] [INFO] [timer.py:215:stop] epoch=0/micro_step=1085/global_step=1085, RunningAvgSamplesPerSec=12.35028463860712, CurrSamplesPerSec=12.284395057895608, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1085/219726562 | consumed samples:         2170 | consumed tokens:      4444160 | elapsed time per iteration (ms): 183.4 | learning rate: 2.368E-06 | global batch size:     2 | lm loss: 9.216597E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.907 | TFLOPs: 27.10 |
[default0]:[2023-07-30 22:25:46,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=0, lr=[2.3789568000000004e-06, 2.3789568000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:46,893] [INFO] [timer.py:215:stop] epoch=0/micro_step=1090/global_step=1090, RunningAvgSamplesPerSec=12.350459974382053, CurrSamplesPerSec=12.380832059130865, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1090/219726562 | consumed samples:         2180 | consumed tokens:      4464640 | elapsed time per iteration (ms): 183.7 | learning rate: 2.379E-06 | global batch size:     2 | lm loss: 9.204525E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.887 | TFLOPs: 27.05 |
[default0]:[2023-07-30 22:25:47,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=1095, skipped=0, lr=[2.389879466666667e-06, 2.389879466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:47,804] [INFO] [timer.py:215:stop] epoch=0/micro_step=1095/global_step=1095, RunningAvgSamplesPerSec=12.3507300217789, CurrSamplesPerSec=12.380777240384857, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1095/219726562 | consumed samples:         2190 | consumed tokens:      4485120 | elapsed time per iteration (ms): 182.2 | learning rate: 2.390E-06 | global batch size:     2 | lm loss: 9.088257E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.976 | TFLOPs: 27.27 |
[default0]:[2023-07-30 22:25:48,725] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=0, lr=[2.4008021333333337e-06, 2.4008021333333337e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:48,725] [INFO] [timer.py:215:stop] epoch=0/micro_step=1100/global_step=1100, RunningAvgSamplesPerSec=12.350578038273433, CurrSamplesPerSec=12.322144614593661, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1100/219726562 | consumed samples:         2200 | consumed tokens:      4505600 | elapsed time per iteration (ms): 184.5 | learning rate: 2.401E-06 | global batch size:     2 | lm loss: 9.094957E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.839 | TFLOPs: 26.93 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1100 | lm loss value: 9.194913E+00 | lm loss PPL: 9.846909E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:25:54,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=1105, skipped=0, lr=[2.4117248000000003e-06, 2.4117248000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:54,349] [INFO] [timer.py:215:stop] epoch=0/micro_step=1105/global_step=1105, RunningAvgSamplesPerSec=12.349496874669775, CurrSamplesPerSec=12.26053353132728, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1105/219726562 | consumed samples:         2210 | consumed tokens:      4526080 | elapsed time per iteration (ms): 1124.9 | learning rate: 2.412E-06 | global batch size:     2 | lm loss: 9.169075E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.778 | TFLOPs: 4.42 |
[default0]:[2023-07-30 22:25:55,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=0, lr=[2.4226474666666665e-06, 2.4226474666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:55,372] [INFO] [timer.py:215:stop] epoch=0/micro_step=1110/global_step=1110, RunningAvgSamplesPerSec=12.349393911107837, CurrSamplesPerSec=12.283459678234168, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1110/219726562 | consumed samples:         2220 | consumed tokens:      4546560 | elapsed time per iteration (ms): 204.0 | learning rate: 2.423E-06 | global batch size:     2 | lm loss: 9.123057E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.805 | TFLOPs: 24.36 |
[default0]: iteration     1115/219726562 | consumed samples:         2230 | consumed tokens:      4567040 | elapsed time per iteration (ms): 189.3 | learning rate: 2.434E-06 | global batch size:     2 | lm loss: 9.137871E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.563 | TFLOPs: 26.25 |
[default0]:[2023-07-30 22:25:57,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=0, lr=[2.4444928e-06, 2.4444928e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:57,287] [INFO] [timer.py:215:stop] epoch=0/micro_step=1120/global_step=1120, RunningAvgSamplesPerSec=12.349827367484425, CurrSamplesPerSec=12.367944752260952, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1120/219726562 | consumed samples:         2240 | consumed tokens:      4587520 | elapsed time per iteration (ms): 194.0 | learning rate: 2.444E-06 | global batch size:     2 | lm loss: 9.125596E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.312 | TFLOPs: 25.62 |
[default0]:[2023-07-30 22:25:58,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=1125, skipped=0, lr=[2.455415466666667e-06, 2.455415466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:58,220] [INFO] [timer.py:215:stop] epoch=0/micro_step=1125/global_step=1125, RunningAvgSamplesPerSec=12.349938174906859, CurrSamplesPerSec=12.350172990003387, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1125/219726562 | consumed samples:         2250 | consumed tokens:      4608000 | elapsed time per iteration (ms): 186.6 | learning rate: 2.455E-06 | global batch size:     2 | lm loss: 9.152506E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.719 | TFLOPs: 26.63 |
[default0]:[2023-07-30 22:25:59,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=0, lr=[2.4663381333333335e-06, 2.4663381333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:59,137] [INFO] [timer.py:215:stop] epoch=0/micro_step=1130/global_step=1130, RunningAvgSamplesPerSec=12.350037427896336, CurrSamplesPerSec=12.245338627389442, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1130/219726562 | consumed samples:         2260 | consumed tokens:      4628480 | elapsed time per iteration (ms): 184.5 | learning rate: 2.466E-06 | global batch size:     2 | lm loss: 9.121123E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.838 | TFLOPs: 26.93 |
[default0]:[2023-07-30 22:26:00,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=1135, skipped=0, lr=[2.4772608e-06, 2.4772608e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:00,073] [INFO] [timer.py:215:stop] epoch=0/micro_step=1135/global_step=1135, RunningAvgSamplesPerSec=12.350318259999606, CurrSamplesPerSec=12.358833981088878, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1135/219726562 | consumed samples:         2270 | consumed tokens:      4648960 | elapsed time per iteration (ms): 186.0 | learning rate: 2.477E-06 | global batch size:     2 | lm loss: 9.097429E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.752 | TFLOPs: 26.72 |
[default0]:[2023-07-30 22:26:00,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=0, lr=[2.4881834666666667e-06, 2.4881834666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:00,986] [INFO] [timer.py:215:stop] epoch=0/micro_step=1140/global_step=1140, RunningAvgSamplesPerSec=12.350501595865053, CurrSamplesPerSec=12.403990637055788, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1140/219726562 | consumed samples:         2280 | consumed tokens:      4669440 | elapsed time per iteration (ms): 182.5 | learning rate: 2.488E-06 | global batch size:     2 | lm loss: 9.062010E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.958 | TFLOPs: 27.23 |
[default0]:[2023-07-30 22:26:01,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=1145, skipped=0, lr=[2.4991061333333333e-06, 2.4991061333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:01,900] [INFO] [timer.py:215:stop] epoch=0/micro_step=1145/global_step=1145, RunningAvgSamplesPerSec=12.350653503607813, CurrSamplesPerSec=12.361893702493125, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1145/219726562 | consumed samples:         2290 | consumed tokens:      4689920 | elapsed time per iteration (ms): 182.9 | learning rate: 2.499E-06 | global batch size:     2 | lm loss: 9.158620E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.933 | TFLOPs: 27.16 |
[default0]:[2023-07-30 22:26:02,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=0, lr=[2.5100288e-06, 2.5100288e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:02,813] [INFO] [timer.py:215:stop] epoch=0/micro_step=1150/global_step=1150, RunningAvgSamplesPerSec=12.350917523996626, CurrSamplesPerSec=12.396841605916004, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1150/219726562 | consumed samples:         2300 | consumed tokens:      4710400 | elapsed time per iteration (ms): 182.3 | learning rate: 2.510E-06 | global batch size:     2 | lm loss: 9.146936E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.972 | TFLOPs: 27.26 |
[default0]:[2023-07-30 22:26:03,734] [INFO] [logging.py:96:log_dist] [Rank 0] step=1155, skipped=0, lr=[2.5209514666666666e-06, 2.5209514666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:03,735] [INFO] [timer.py:215:stop] epoch=0/micro_step=1155/global_step=1155, RunningAvgSamplesPerSec=12.350517980331944, CurrSamplesPerSec=12.326671319936814, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1155/219726562 | consumed samples:         2310 | consumed tokens:      4730880 | elapsed time per iteration (ms): 184.6 | learning rate: 2.521E-06 | global batch size:     2 | lm loss: 9.136688E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.836 | TFLOPs: 26.93 |
[default0]:[2023-07-30 22:26:04,657] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=0, lr=[2.5318741333333332e-06, 2.5318741333333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:04,658] [INFO] [timer.py:215:stop] epoch=0/micro_step=1160/global_step=1160, RunningAvgSamplesPerSec=12.35047330867484, CurrSamplesPerSec=12.092715632752912, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1160/219726562 | consumed samples:         2320 | consumed tokens:      4751360 | elapsed time per iteration (ms): 185.0 | learning rate: 2.532E-06 | global batch size:     2 | lm loss: 9.059776E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.812 | TFLOPs: 26.87 |
[default0]:[2023-07-30 22:26:05,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=1165, skipped=0, lr=[2.5427968000000003e-06, 2.5427968000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:05,581] [INFO] [timer.py:215:stop] epoch=0/micro_step=1165/global_step=1165, RunningAvgSamplesPerSec=12.350434071511227, CurrSamplesPerSec=12.167083665119058, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1165/219726562 | consumed samples:         2330 | consumed tokens:      4771840 | elapsed time per iteration (ms): 184.3 | learning rate: 2.543E-06 | global batch size:     2 | lm loss: 9.022563E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.853 | TFLOPs: 26.97 |
[default0]:[2023-07-30 22:26:06,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=0, lr=[2.553719466666667e-06, 2.553719466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:06,513] [INFO] [timer.py:215:stop] epoch=0/micro_step=1170/global_step=1170, RunningAvgSamplesPerSec=12.350334425030825, CurrSamplesPerSec=12.444233462296172, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1170/219726562 | consumed samples:         2340 | consumed tokens:      4792320 | elapsed time per iteration (ms): 186.5 | learning rate: 2.554E-06 | global batch size:     2 | lm loss: 9.050859E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.723 | TFLOPs: 26.65 |
[default0]:[2023-07-30 22:26:07,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=1175, skipped=0, lr=[2.5646421333333335e-06, 2.5646421333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:07,441] [INFO] [timer.py:215:stop] epoch=0/micro_step=1175/global_step=1175, RunningAvgSamplesPerSec=12.35034305321038, CurrSamplesPerSec=12.405072875260268, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1175/219726562 | consumed samples:         2350 | consumed tokens:      4812800 | elapsed time per iteration (ms): 185.4 | learning rate: 2.565E-06 | global batch size:     2 | lm loss: 8.968829E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.788 | TFLOPs: 26.81 |
[default0]:[2023-07-30 22:26:08,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=0, lr=[2.5755648e-06, 2.5755648e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:08,387] [INFO] [timer.py:215:stop] epoch=0/micro_step=1180/global_step=1180, RunningAvgSamplesPerSec=12.350872843984993, CurrSamplesPerSec=12.525096156143244, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1180/219726562 | consumed samples:         2360 | consumed tokens:      4833280 | elapsed time per iteration (ms): 189.0 | learning rate: 2.576E-06 | global batch size:     2 | lm loss: 8.973904E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.583 | TFLOPs: 26.30 |
[default0]:[2023-07-30 22:26:09,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=1185, skipped=0, lr=[2.586487466666667e-06, 2.586487466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:09,352] [INFO] [timer.py:215:stop] epoch=0/micro_step=1185/global_step=1185, RunningAvgSamplesPerSec=12.351259246617245, CurrSamplesPerSec=12.38463400950778, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1185/219726562 | consumed samples:         2370 | consumed tokens:      4853760 | elapsed time per iteration (ms): 193.1 | learning rate: 2.586E-06 | global batch size:     2 | lm loss: 8.948872E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.356 | TFLOPs: 25.73 |
[default0]:[2023-07-30 22:26:10,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=0, lr=[2.5974101333333334e-06, 2.5974101333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:10,273] [INFO] [timer.py:215:stop] epoch=0/micro_step=1190/global_step=1190, RunningAvgSamplesPerSec=12.351648391037399, CurrSamplesPerSec=12.34063449331009, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1190/219726562 | consumed samples:         2380 | consumed tokens:      4874240 | elapsed time per iteration (ms): 184.3 | learning rate: 2.597E-06 | global batch size:     2 | lm loss: 9.024088E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.853 | TFLOPs: 26.97 |
[default0]:[2023-07-30 22:26:11,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=1195, skipped=0, lr=[2.6083328e-06, 2.6083328e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:11,239] [INFO] [timer.py:215:stop] epoch=0/micro_step=1195/global_step=1195, RunningAvgSamplesPerSec=12.351807949397516, CurrSamplesPerSec=12.313806081886453, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1195/219726562 | consumed samples:         2390 | consumed tokens:      4894720 | elapsed time per iteration (ms): 193.3 | learning rate: 2.608E-06 | global batch size:     2 | lm loss: 9.097531E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.344 | TFLOPs: 25.70 |
[default0]:[2023-07-30 22:26:12,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=0, lr=[2.6192554666666667e-06, 2.6192554666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:12,160] [INFO] [timer.py:215:stop] epoch=0/micro_step=1200/global_step=1200, RunningAvgSamplesPerSec=12.351790892491117, CurrSamplesPerSec=12.122265895953758, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1200/219726562 | consumed samples:         2400 | consumed tokens:      4915200 | elapsed time per iteration (ms): 184.1 | learning rate: 2.619E-06 | global batch size:     2 | lm loss: 8.934201E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.866 | TFLOPs: 27.00 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1200 | lm loss value: 9.029410E+00 | lm loss PPL: 8.344938E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:26:16,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=1205, skipped=0, lr=[2.6301781333333333e-06, 2.6301781333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:16,413] [INFO] [timer.py:215:stop] epoch=0/micro_step=1205/global_step=1205, RunningAvgSamplesPerSec=12.351486237394127, CurrSamplesPerSec=12.39977354463255, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1205/219726562 | consumed samples:         2410 | consumed tokens:      4935680 | elapsed time per iteration (ms): 850.5 | learning rate: 2.630E-06 | global batch size:     2 | lm loss: 9.033492E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.351 | TFLOPs: 5.84 |
[default0]:[2023-07-30 22:26:17,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=0, lr=[2.6411008000000004e-06, 2.6411008000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:17,628] [INFO] [timer.py:215:stop] epoch=0/micro_step=1210/global_step=1210, RunningAvgSamplesPerSec=12.351758707984567, CurrSamplesPerSec=12.352464423711243, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1210/219726562 | consumed samples:         2420 | consumed tokens:      4956160 | elapsed time per iteration (ms): 243.0 | learning rate: 2.641E-06 | global batch size:     2 | lm loss: 8.973297E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.231 | TFLOPs: 20.45 |
[default0]:[2023-07-30 22:26:18,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=1215, skipped=0, lr=[2.652023466666667e-06, 2.652023466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:18,547] [INFO] [timer.py:215:stop] epoch=0/micro_step=1215/global_step=1215, RunningAvgSamplesPerSec=12.351595609427912, CurrSamplesPerSec=12.112516370493692, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1215/219726562 | consumed samples:         2430 | consumed tokens:      4976640 | elapsed time per iteration (ms): 184.3 | learning rate: 2.652E-06 | global batch size:     2 | lm loss: 8.972031E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.851 | TFLOPs: 26.96 |
[default0]:[2023-07-30 22:26:19,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=0, lr=[2.6629461333333336e-06, 2.6629461333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:19,531] [INFO] [timer.py:215:stop] epoch=0/micro_step=1220/global_step=1220, RunningAvgSamplesPerSec=12.351896054889941, CurrSamplesPerSec=12.426241941600303, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1220/219726562 | consumed samples:         2440 | consumed tokens:      4997120 | elapsed time per iteration (ms): 196.1 | learning rate: 2.663E-06 | global batch size:     2 | lm loss: 9.179826E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.201 | TFLOPs: 25.35 |
[default0]:[2023-07-30 22:26:20,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=1225, skipped=0, lr=[2.6738688000000003e-06, 2.6738688000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:20,453] [INFO] [timer.py:215:stop] epoch=0/micro_step=1225/global_step=1225, RunningAvgSamplesPerSec=12.351971769111085, CurrSamplesPerSec=12.338565223594545, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1225/219726562 | consumed samples:         2450 | consumed tokens:      5017600 | elapsed time per iteration (ms): 184.6 | learning rate: 2.674E-06 | global batch size:     2 | lm loss: 8.986610E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.834 | TFLOPs: 26.92 |
[default0]:[2023-07-30 22:26:21,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=0, lr=[2.684791466666667e-06, 2.684791466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:21,370] [INFO] [timer.py:215:stop] epoch=0/micro_step=1230/global_step=1230, RunningAvgSamplesPerSec=12.352153049930047, CurrSamplesPerSec=12.391055098384168, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1230/219726562 | consumed samples:         2460 | consumed tokens:      5038080 | elapsed time per iteration (ms): 183.7 | learning rate: 2.685E-06 | global batch size:     2 | lm loss: 8.939641E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.890 | TFLOPs: 27.06 |
[default0]:[2023-07-30 22:26:22,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=1235, skipped=0, lr=[2.6957141333333335e-06, 2.6957141333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:22,290] [INFO] [timer.py:215:stop] epoch=0/micro_step=1235/global_step=1235, RunningAvgSamplesPerSec=12.352282193540928, CurrSamplesPerSec=12.347555249391352, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1235/219726562 | consumed samples:         2470 | consumed tokens:      5058560 | elapsed time per iteration (ms): 183.9 | learning rate: 2.696E-06 | global batch size:     2 | lm loss: 8.971487E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.875 | TFLOPs: 27.02 |
[default0]:[2023-07-30 22:26:23,213] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=0, lr=[2.7066368e-06, 2.7066368e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:23,213] [INFO] [timer.py:215:stop] epoch=0/micro_step=1240/global_step=1240, RunningAvgSamplesPerSec=12.352487799575684, CurrSamplesPerSec=12.443181295640604, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1240/219726562 | consumed samples:         2480 | consumed tokens:      5079040 | elapsed time per iteration (ms): 184.4 | learning rate: 2.707E-06 | global batch size:     2 | lm loss: 8.975095E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.846 | TFLOPs: 26.95 |
[default0]:[2023-07-30 22:26:24,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=1245, skipped=0, lr=[2.7175594666666668e-06, 2.7175594666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:24,137] [INFO] [timer.py:215:stop] epoch=0/micro_step=1245/global_step=1245, RunningAvgSamplesPerSec=12.352418167605821, CurrSamplesPerSec=12.363296581180206, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1245/219726562 | consumed samples:         2490 | consumed tokens:      5099520 | elapsed time per iteration (ms): 184.9 | learning rate: 2.718E-06 | global batch size:     2 | lm loss: 9.064902E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.819 | TFLOPs: 26.88 |
[default0]:[2023-07-30 22:26:25,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=0, lr=[2.7284821333333334e-06, 2.7284821333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:25,062] [INFO] [timer.py:215:stop] epoch=0/micro_step=1250/global_step=1250, RunningAvgSamplesPerSec=12.352443115383828, CurrSamplesPerSec=12.344230138163539, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1250/219726562 | consumed samples:         2500 | consumed tokens:      5120000 | elapsed time per iteration (ms): 184.8 | learning rate: 2.728E-06 | global batch size:     2 | lm loss: 8.925404E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.820 | TFLOPs: 26.89 |
[default0]:[2023-07-30 22:26:25,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=1255, skipped=0, lr=[2.7394048000000005e-06, 2.7394048000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:25,978] [INFO] [timer.py:215:stop] epoch=0/micro_step=1255/global_step=1255, RunningAvgSamplesPerSec=12.352553324284179, CurrSamplesPerSec=12.2555538997829, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1255/219726562 | consumed samples:         2510 | consumed tokens:      5140480 | elapsed time per iteration (ms): 183.4 | learning rate: 2.739E-06 | global batch size:     2 | lm loss: 8.888448E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.903 | TFLOPs: 27.09 |
[default0]:[2023-07-30 22:26:26,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=0, lr=[2.750327466666667e-06, 2.750327466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:26,889] [INFO] [timer.py:215:stop] epoch=0/micro_step=1260/global_step=1260, RunningAvgSamplesPerSec=12.352885121117552, CurrSamplesPerSec=12.434899392530706, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1260/219726562 | consumed samples:         2520 | consumed tokens:      5160960 | elapsed time per iteration (ms): 182.0 | learning rate: 2.750E-06 | global batch size:     2 | lm loss: 8.998749E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.990 | TFLOPs: 27.31 |
[default0]:[2023-07-30 22:26:27,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=1265, skipped=0, lr=[2.7612501333333337e-06, 2.7612501333333337e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:27,824] [INFO] [timer.py:215:stop] epoch=0/micro_step=1265/global_step=1265, RunningAvgSamplesPerSec=12.35330041296971, CurrSamplesPerSec=12.400360098805436, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1265/219726562 | consumed samples:         2530 | consumed tokens:      5181440 | elapsed time per iteration (ms): 187.1 | learning rate: 2.761E-06 | global batch size:     2 | lm loss: 8.713728E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.688 | TFLOPs: 26.56 |
[default0]:[2023-07-30 22:26:28,757] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=0, lr=[2.7721728000000003e-06, 2.7721728000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:28,758] [INFO] [timer.py:215:stop] epoch=0/micro_step=1270/global_step=1270, RunningAvgSamplesPerSec=12.353143319920493, CurrSamplesPerSec=12.226331054787133, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1270/219726562 | consumed samples:         2540 | consumed tokens:      5201920 | elapsed time per iteration (ms): 186.8 | learning rate: 2.772E-06 | global batch size:     2 | lm loss: 8.780602E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.706 | TFLOPs: 26.60 |
[default0]:[2023-07-30 22:26:29,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=1275, skipped=0, lr=[2.783095466666667e-06, 2.783095466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:29,744] [INFO] [timer.py:215:stop] epoch=0/micro_step=1275/global_step=1275, RunningAvgSamplesPerSec=12.353294088387358, CurrSamplesPerSec=12.341941758143134, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1275/219726562 | consumed samples:         2550 | consumed tokens:      5222400 | elapsed time per iteration (ms): 197.1 | learning rate: 2.783E-06 | global batch size:     2 | lm loss: 8.864702E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.146 | TFLOPs: 25.21 |
[default0]:[2023-07-30 22:26:30,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=0, lr=[2.7940181333333336e-06, 2.7940181333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:30,745] [INFO] [timer.py:215:stop] epoch=0/micro_step=1280/global_step=1280, RunningAvgSamplesPerSec=12.353385417477178, CurrSamplesPerSec=12.321945515918365, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1280/219726562 | consumed samples:         2560 | consumed tokens:      5242880 | elapsed time per iteration (ms): 200.1 | learning rate: 2.794E-06 | global batch size:     2 | lm loss: 8.855881E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.996 | TFLOPs: 24.84 |
[default0]:[2023-07-30 22:26:31,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=1285, skipped=0, lr=[2.8049408000000002e-06, 2.8049408000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:31,684] [INFO] [timer.py:215:stop] epoch=0/micro_step=1285/global_step=1285, RunningAvgSamplesPerSec=12.353691398498626, CurrSamplesPerSec=12.452102661540517, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1285/219726562 | consumed samples:         2570 | consumed tokens:      5263360 | elapsed time per iteration (ms): 187.8 | learning rate: 2.805E-06 | global batch size:     2 | lm loss: 8.888060E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.647 | TFLOPs: 26.46 |
[default0]:[2023-07-30 22:26:32,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=0, lr=[2.815863466666667e-06, 2.815863466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:32,598] [INFO] [timer.py:215:stop] epoch=0/micro_step=1290/global_step=1290, RunningAvgSamplesPerSec=12.353826713798478, CurrSamplesPerSec=12.37913290370565, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1290/219726562 | consumed samples:         2580 | consumed tokens:      5283840 | elapsed time per iteration (ms): 183.1 | learning rate: 2.816E-06 | global batch size:     2 | lm loss: 8.922559E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.923 | TFLOPs: 27.14 |
[default0]:[2023-07-30 22:26:33,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=1295, skipped=0, lr=[2.8267861333333335e-06, 2.8267861333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:33,519] [INFO] [timer.py:215:stop] epoch=0/micro_step=1295/global_step=1295, RunningAvgSamplesPerSec=12.354034211266358, CurrSamplesPerSec=12.395046478055354, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1295/219726562 | consumed samples:         2590 | consumed tokens:      5304320 | elapsed time per iteration (ms): 184.0 | learning rate: 2.827E-06 | global batch size:     2 | lm loss: 8.923912E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.869 | TFLOPs: 27.01 |
[default0]:[2023-07-30 22:26:34,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=0, lr=[2.8377088000000005e-06, 2.8377088000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:34,430] [INFO] [timer.py:215:stop] epoch=0/micro_step=1300/global_step=1300, RunningAvgSamplesPerSec=12.354225286638636, CurrSamplesPerSec=12.384762000375, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1300/219726562 | consumed samples:         2600 | consumed tokens:      5324800 | elapsed time per iteration (ms): 182.4 | learning rate: 2.838E-06 | global batch size:     2 | lm loss: 8.901437E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.966 | TFLOPs: 27.25 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1300 | lm loss value: 8.874675E+00 | lm loss PPL: 7.148621E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:26:40,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=1305, skipped=0, lr=[2.848631466666667e-06, 2.848631466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:40,323] [INFO] [timer.py:215:stop] epoch=0/micro_step=1305/global_step=1305, RunningAvgSamplesPerSec=12.353766823982296, CurrSamplesPerSec=12.359726243360518, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1305/219726562 | consumed samples:         2610 | consumed tokens:      5345280 | elapsed time per iteration (ms): 1178.3 | learning rate: 2.849E-06 | global batch size:     2 | lm loss: 8.801172E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.697 | TFLOPs: 4.22 |
[default0]:[2023-07-30 22:26:41,245] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=0, lr=[2.859554133333334e-06, 2.859554133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:41,245] [INFO] [timer.py:215:stop] epoch=0/micro_step=1310/global_step=1310, RunningAvgSamplesPerSec=12.353755001563679, CurrSamplesPerSec=12.316789292158967, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1310/219726562 | consumed samples:         2620 | consumed tokens:      5365760 | elapsed time per iteration (ms): 184.3 | learning rate: 2.860E-06 | global batch size:     2 | lm loss: 8.752358E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.854 | TFLOPs: 26.97 |
[default0]:[2023-07-30 22:26:42,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=1315, skipped=0, lr=[2.8704768000000004e-06, 2.8704768000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:42,175] [INFO] [timer.py:215:stop] epoch=0/micro_step=1315/global_step=1315, RunningAvgSamplesPerSec=12.35388726397831, CurrSamplesPerSec=12.163572826796202, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1315/219726562 | consumed samples:         2630 | consumed tokens:      5386240 | elapsed time per iteration (ms): 186.1 | learning rate: 2.870E-06 | global batch size:     2 | lm loss: 8.852850E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.747 | TFLOPs: 26.70 |
[default0]:[2023-07-30 22:26:43,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=0, lr=[2.881399466666667e-06, 2.881399466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:43,145] [INFO] [timer.py:215:stop] epoch=0/micro_step=1320/global_step=1320, RunningAvgSamplesPerSec=12.353931062903907, CurrSamplesPerSec=12.249540748528789, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1320/219726562 | consumed samples:         2640 | consumed tokens:      5406720 | elapsed time per iteration (ms): 193.8 | learning rate: 2.881E-06 | global batch size:     2 | lm loss: 8.748684E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.321 | TFLOPs: 25.65 |
[default0]:[2023-07-30 22:26:44,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=1325, skipped=0, lr=[2.8923221333333337e-06, 2.8923221333333337e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:44,191] [INFO] [timer.py:215:stop] epoch=0/micro_step=1325/global_step=1325, RunningAvgSamplesPerSec=12.354194249575155, CurrSamplesPerSec=12.233017127607603, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1325/219726562 | consumed samples:         2650 | consumed tokens:      5427200 | elapsed time per iteration (ms): 209.4 | learning rate: 2.892E-06 | global batch size:     2 | lm loss: 8.690359E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.552 | TFLOPs: 23.74 |
[default0]:[2023-07-30 22:26:45,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=0, lr=[2.9032448000000003e-06, 2.9032448000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:45,501] [INFO] [timer.py:215:stop] epoch=0/micro_step=1330/global_step=1330, RunningAvgSamplesPerSec=12.354367696114705, CurrSamplesPerSec=12.404650963331445, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1330/219726562 | consumed samples:         2660 | consumed tokens:      5447680 | elapsed time per iteration (ms): 262.0 | learning rate: 2.903E-06 | global batch size:     2 | lm loss: 8.771198E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 7.634 | TFLOPs: 18.97 |
[default0]:[2023-07-30 22:26:46,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=1335, skipped=0, lr=[2.914167466666667e-06, 2.914167466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:46,579] [INFO] [timer.py:215:stop] epoch=0/micro_step=1335/global_step=1335, RunningAvgSamplesPerSec=12.354620791409312, CurrSamplesPerSec=12.372997904058693, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1335/219726562 | consumed samples:         2670 | consumed tokens:      5468160 | elapsed time per iteration (ms): 215.5 | learning rate: 2.914E-06 | global batch size:     2 | lm loss: 8.783659E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.279 | TFLOPs: 23.06 |
[default0]:[2023-07-30 22:26:47,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=0, lr=[2.9250901333333336e-06, 2.9250901333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:47,696] [INFO] [timer.py:215:stop] epoch=0/micro_step=1340/global_step=1340, RunningAvgSamplesPerSec=12.3549535907208, CurrSamplesPerSec=12.480744624503812, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1340/219726562 | consumed samples:         2680 | consumed tokens:      5488640 | elapsed time per iteration (ms): 223.6 | learning rate: 2.925E-06 | global batch size:     2 | lm loss: 8.790146E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.946 | TFLOPs: 22.23 |
[default0]:[2023-07-30 22:26:48,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=1345, skipped=0, lr=[2.9360128000000006e-06, 2.9360128000000006e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:48,895] [INFO] [timer.py:215:stop] epoch=0/micro_step=1345/global_step=1345, RunningAvgSamplesPerSec=12.35518072018552, CurrSamplesPerSec=12.355266219898372, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1345/219726562 | consumed samples:         2690 | consumed tokens:      5509120 | elapsed time per iteration (ms): 239.8 | learning rate: 2.936E-06 | global batch size:     2 | lm loss: 8.744791E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.340 | TFLOPs: 20.72 |
[default0]:[2023-07-30 22:26:50,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=0, lr=[2.9469354666666672e-06, 2.9469354666666672e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:50,033] [INFO] [timer.py:215:stop] epoch=0/micro_step=1350/global_step=1350, RunningAvgSamplesPerSec=12.35441853680413, CurrSamplesPerSec=11.154426010214841, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1350/219726562 | consumed samples:         2700 | consumed tokens:      5529600 | elapsed time per iteration (ms): 228.1 | learning rate: 2.947E-06 | global batch size:     2 | lm loss: 8.815348E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.768 | TFLOPs: 21.79 |
[default0]:[2023-07-30 22:26:51,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=1355, skipped=0, lr=[2.957858133333334e-06, 2.957858133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:51,183] [INFO] [timer.py:215:stop] epoch=0/micro_step=1355/global_step=1355, RunningAvgSamplesPerSec=12.354545284289989, CurrSamplesPerSec=12.295847007831659, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1355/219726562 | consumed samples:         2710 | consumed tokens:      5550080 | elapsed time per iteration (ms): 229.2 | learning rate: 2.958E-06 | global batch size:     2 | lm loss: 8.809573E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.725 | TFLOPs: 21.68 |
[default0]:[2023-07-30 22:26:52,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=0, lr=[2.9687807999999997e-06, 2.9687807999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:52,290] [INFO] [timer.py:215:stop] epoch=0/micro_step=1360/global_step=1360, RunningAvgSamplesPerSec=12.35482888976375, CurrSamplesPerSec=12.372632917254796, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1360/219726562 | consumed samples:         2720 | consumed tokens:      5570560 | elapsed time per iteration (ms): 221.7 | learning rate: 2.969E-06 | global batch size:     2 | lm loss: 8.710728E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.022 | TFLOPs: 22.42 |
[default0]:[2023-07-30 22:26:53,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=1365, skipped=0, lr=[2.9797034666666667e-06, 2.9797034666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:53,440] [INFO] [timer.py:215:stop] epoch=0/micro_step=1365/global_step=1365, RunningAvgSamplesPerSec=12.354949058278589, CurrSamplesPerSec=12.367106000294855, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1365/219726562 | consumed samples:         2730 | consumed tokens:      5591040 | elapsed time per iteration (ms): 230.2 | learning rate: 2.980E-06 | global batch size:     2 | lm loss: 8.742631E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.688 | TFLOPs: 21.59 |
[default0]:[2023-07-30 22:26:54,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=0, lr=[2.9906261333333333e-06, 2.9906261333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:54,596] [INFO] [timer.py:215:stop] epoch=0/micro_step=1370/global_step=1370, RunningAvgSamplesPerSec=12.355053851605721, CurrSamplesPerSec=12.355466396833883, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1370/219726562 | consumed samples:         2740 | consumed tokens:      5611520 | elapsed time per iteration (ms): 231.0 | learning rate: 2.991E-06 | global batch size:     2 | lm loss: 8.720279E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.659 | TFLOPs: 21.52 |
[default0]:[2023-07-30 22:26:55,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=1375, skipped=0, lr=[3.0015488e-06, 3.0015488e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:55,735] [INFO] [timer.py:215:stop] epoch=0/micro_step=1375/global_step=1375, RunningAvgSamplesPerSec=12.3551447623281, CurrSamplesPerSec=12.26053353132728, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1375/219726562 | consumed samples:         2750 | consumed tokens:      5632000 | elapsed time per iteration (ms): 227.7 | learning rate: 3.002E-06 | global batch size:     2 | lm loss: 8.661370E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.785 | TFLOPs: 21.83 |
[default0]:[2023-07-30 22:26:56,832] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=0, lr=[3.0124714666666666e-06, 3.0124714666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:56,833] [INFO] [timer.py:215:stop] epoch=0/micro_step=1380/global_step=1380, RunningAvgSamplesPerSec=12.355319677255052, CurrSamplesPerSec=12.261949344921995, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1380/219726562 | consumed samples:         2760 | consumed tokens:      5652480 | elapsed time per iteration (ms): 219.4 | learning rate: 3.012E-06 | global batch size:     2 | lm loss: 8.721650E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.116 | TFLOPs: 22.65 |
[default0]:[2023-07-30 22:26:57,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=1385, skipped=0, lr=[3.0233941333333332e-06, 3.0233941333333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:57,948] [INFO] [timer.py:215:stop] epoch=0/micro_step=1385/global_step=1385, RunningAvgSamplesPerSec=12.355523583916211, CurrSamplesPerSec=12.384816854413653, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1385/219726562 | consumed samples:         2770 | consumed tokens:      5672960 | elapsed time per iteration (ms): 223.1 | learning rate: 3.023E-06 | global batch size:     2 | lm loss: 8.786672E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.964 | TFLOPs: 22.27 |
[default0]:[2023-07-30 22:26:59,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=0, lr=[3.0343168e-06, 3.0343168e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:59,022] [INFO] [timer.py:215:stop] epoch=0/micro_step=1390/global_step=1390, RunningAvgSamplesPerSec=12.355745905452517, CurrSamplesPerSec=12.459204557944172, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1390/219726562 | consumed samples:         2780 | consumed tokens:      5693440 | elapsed time per iteration (ms): 214.7 | learning rate: 3.034E-06 | global batch size:     2 | lm loss: 8.786633E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.313 | TFLOPs: 23.14 |
[default0]:[2023-07-30 22:27:00,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=1395, skipped=0, lr=[3.0452394666666665e-06, 3.0452394666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:00,104] [INFO] [timer.py:215:stop] epoch=0/micro_step=1395/global_step=1395, RunningAvgSamplesPerSec=12.355978867850094, CurrSamplesPerSec=12.402908587667186, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1395/219726562 | consumed samples:         2790 | consumed tokens:      5713920 | elapsed time per iteration (ms): 216.6 | learning rate: 3.045E-06 | global batch size:     2 | lm loss: 8.628493E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.232 | TFLOPs: 22.94 |
[default0]:[2023-07-30 22:27:01,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=0, lr=[3.056162133333333e-06, 3.056162133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:01,184] [INFO] [timer.py:215:stop] epoch=0/micro_step=1400/global_step=1400, RunningAvgSamplesPerSec=12.3560385600965, CurrSamplesPerSec=12.319792041474214, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1400/219726562 | consumed samples:         2800 | consumed tokens:      5734400 | elapsed time per iteration (ms): 215.8 | learning rate: 3.056E-06 | global batch size:     2 | lm loss: 8.632773E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.270 | TFLOPs: 23.03 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1400 | lm loss value: 8.686569E+00 | lm loss PPL: 5.922827E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:27:04,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=1405, skipped=0, lr=[3.0670847999999997e-06, 3.0670847999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:04,092] [INFO] [timer.py:215:stop] epoch=0/micro_step=1405/global_step=1405, RunningAvgSamplesPerSec=12.355953214569773, CurrSamplesPerSec=12.29752337503555, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1405/219726562 | consumed samples:         2810 | consumed tokens:      5754880 | elapsed time per iteration (ms): 581.8 | learning rate: 3.067E-06 | global batch size:     2 | lm loss: 8.581203E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.438 | TFLOPs: 8.54 |
[default0]:[2023-07-30 22:27:05,015] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=0, lr=[3.078007466666667e-06, 3.078007466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:05,016] [INFO] [timer.py:215:stop] epoch=0/micro_step=1410/global_step=1410, RunningAvgSamplesPerSec=12.355950193158723, CurrSamplesPerSec=12.306255079556491, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1410/219726562 | consumed samples:         2820 | consumed tokens:      5775360 | elapsed time per iteration (ms): 184.7 | learning rate: 3.078E-06 | global batch size:     2 | lm loss: 8.635526E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.831 | TFLOPs: 26.91 |
[default0]:[2023-07-30 22:27:05,931] [INFO] [logging.py:96:log_dist] [Rank 0] step=1415, skipped=0, lr=[3.0889301333333334e-06, 3.0889301333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:05,931] [INFO] [timer.py:215:stop] epoch=0/micro_step=1415/global_step=1415, RunningAvgSamplesPerSec=12.35595538489602, CurrSamplesPerSec=12.238192706927617, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1415/219726562 | consumed samples:         2830 | consumed tokens:      5795840 | elapsed time per iteration (ms): 183.1 | learning rate: 3.089E-06 | global batch size:     2 | lm loss: 8.737372E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.923 | TFLOPs: 27.14 |
[default0]:[2023-07-30 22:27:06,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=0, lr=[3.0998528e-06, 3.0998528e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:06,874] [INFO] [timer.py:215:stop] epoch=0/micro_step=1420/global_step=1420, RunningAvgSamplesPerSec=12.355962016019197, CurrSamplesPerSec=12.359362039117462, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1420/219726562 | consumed samples:         2840 | consumed tokens:      5816320 | elapsed time per iteration (ms): 188.6 | learning rate: 3.100E-06 | global batch size:     2 | lm loss: 8.638570E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.606 | TFLOPs: 26.35 |
[default0]:[2023-07-30 22:27:07,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=1425, skipped=0, lr=[3.1107754666666667e-06, 3.1107754666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:07,790] [INFO] [timer.py:215:stop] epoch=0/micro_step=1425/global_step=1425, RunningAvgSamplesPerSec=12.356003286188512, CurrSamplesPerSec=12.400396760283764, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1425/219726562 | consumed samples:         2850 | consumed tokens:      5836800 | elapsed time per iteration (ms): 183.2 | learning rate: 3.111E-06 | global batch size:     2 | lm loss: 8.603444E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.914 | TFLOPs: 27.12 |
[default0]:[2023-07-30 22:27:08,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=0, lr=[3.1216981333333333e-06, 3.1216981333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:08,705] [INFO] [timer.py:215:stop] epoch=0/micro_step=1430/global_step=1430, RunningAvgSamplesPerSec=12.356018458995488, CurrSamplesPerSec=12.376557654231007, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1430/219726562 | consumed samples:         2860 | consumed tokens:      5857280 | elapsed time per iteration (ms): 183.0 | learning rate: 3.122E-06 | global batch size:     2 | lm loss: 8.562946E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.930 | TFLOPs: 27.16 |
[default0]:[2023-07-30 22:27:09,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=1435, skipped=0, lr=[3.1326208e-06, 3.1326208e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:09,649] [INFO] [timer.py:215:stop] epoch=0/micro_step=1435/global_step=1435, RunningAvgSamplesPerSec=12.356203944106483, CurrSamplesPerSec=12.416346708900106, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1435/219726562 | consumed samples:         2870 | consumed tokens:      5877760 | elapsed time per iteration (ms): 188.8 | learning rate: 3.133E-06 | global batch size:     2 | lm loss: 8.558255E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.595 | TFLOPs: 26.33 |
[default0]:[2023-07-30 22:27:10,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=0, lr=[3.1435434666666666e-06, 3.1435434666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:10,565] [INFO] [timer.py:215:stop] epoch=0/micro_step=1440/global_step=1440, RunningAvgSamplesPerSec=12.356305507176254, CurrSamplesPerSec=12.377671637738834, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1440/219726562 | consumed samples:         2880 | consumed tokens:      5898240 | elapsed time per iteration (ms): 183.1 | learning rate: 3.144E-06 | global batch size:     2 | lm loss: 8.566832E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.920 | TFLOPs: 27.13 |
[default0]:[2023-07-30 22:27:11,482] [INFO] [logging.py:96:log_dist] [Rank 0] step=1445, skipped=0, lr=[3.154466133333333e-06, 3.154466133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:11,483] [INFO] [timer.py:215:stop] epoch=0/micro_step=1445/global_step=1445, RunningAvgSamplesPerSec=12.356241237173688, CurrSamplesPerSec=12.322180815044074, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1445/219726562 | consumed samples:         2890 | consumed tokens:      5918720 | elapsed time per iteration (ms): 184.7 | learning rate: 3.154E-06 | global batch size:     2 | lm loss: 8.508557E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.826 | TFLOPs: 26.90 |
[default0]:[2023-07-30 22:27:12,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=0, lr=[3.1653888e-06, 3.1653888e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:12,411] [INFO] [timer.py:215:stop] epoch=0/micro_step=1450/global_step=1450, RunningAvgSamplesPerSec=12.356109412348573, CurrSamplesPerSec=12.299687105032419, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1450/219726562 | consumed samples:         2900 | consumed tokens:      5939200 | elapsed time per iteration (ms): 184.2 | learning rate: 3.165E-06 | global batch size:     2 | lm loss: 8.620361E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.856 | TFLOPs: 26.97 |
[default0]:[2023-07-30 22:27:13,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=1455, skipped=0, lr=[3.176311466666667e-06, 3.176311466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:13,341] [INFO] [timer.py:215:stop] epoch=0/micro_step=1455/global_step=1455, RunningAvgSamplesPerSec=12.356138802362016, CurrSamplesPerSec=12.297559430963913, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1455/219726562 | consumed samples:         2910 | consumed tokens:      5959680 | elapsed time per iteration (ms): 186.2 | learning rate: 3.176E-06 | global batch size:     2 | lm loss: 8.421968E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.743 | TFLOPs: 26.69 |
[default0]:[2023-07-30 22:27:14,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=0, lr=[3.1872341333333335e-06, 3.1872341333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:14,262] [INFO] [timer.py:215:stop] epoch=0/micro_step=1460/global_step=1460, RunningAvgSamplesPerSec=12.356135572579703, CurrSamplesPerSec=12.354210848767615, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1460/219726562 | consumed samples:         2920 | consumed tokens:      5980160 | elapsed time per iteration (ms): 184.2 | learning rate: 3.187E-06 | global batch size:     2 | lm loss: 8.853358E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.859 | TFLOPs: 26.98 |
[default0]:[2023-07-30 22:27:15,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=1465, skipped=0, lr=[3.1981568e-06, 3.1981568e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:15,188] [INFO] [timer.py:215:stop] epoch=0/micro_step=1465/global_step=1465, RunningAvgSamplesPerSec=12.355815443903992, CurrSamplesPerSec=12.306850874606269, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1465/219726562 | consumed samples:         2930 | consumed tokens:      6000640 | elapsed time per iteration (ms): 185.7 | learning rate: 3.198E-06 | global batch size:     2 | lm loss: 8.528713E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.768 | TFLOPs: 26.75 |
[default0]:[2023-07-30 22:27:16,108] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=0, lr=[3.2090794666666668e-06, 3.2090794666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:16,108] [INFO] [timer.py:215:stop] epoch=0/micro_step=1470/global_step=1470, RunningAvgSamplesPerSec=12.35568846098978, CurrSamplesPerSec=12.111764366156512, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1470/219726562 | consumed samples:         2940 | consumed tokens:      6021120 | elapsed time per iteration (ms): 183.5 | learning rate: 3.209E-06 | global batch size:     2 | lm loss: 8.572927E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.902 | TFLOPs: 27.09 |
[default0]:[2023-07-30 22:27:17,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=1475, skipped=0, lr=[3.2200021333333334e-06, 3.2200021333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:17,026] [INFO] [timer.py:215:stop] epoch=0/micro_step=1475/global_step=1475, RunningAvgSamplesPerSec=12.35569017818602, CurrSamplesPerSec=12.320425338170281, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1475/219726562 | consumed samples:         2950 | consumed tokens:      6041600 | elapsed time per iteration (ms): 183.7 | learning rate: 3.220E-06 | global batch size:     2 | lm loss: 8.601082E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.888 | TFLOPs: 27.06 |
[default0]:[2023-07-30 22:27:17,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=0, lr=[3.2309248e-06, 3.2309248e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:17,964] [INFO] [timer.py:215:stop] epoch=0/micro_step=1480/global_step=1480, RunningAvgSamplesPerSec=12.355724624564827, CurrSamplesPerSec=12.354119876939027, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1480/219726562 | consumed samples:         2960 | consumed tokens:      6062080 | elapsed time per iteration (ms): 187.4 | learning rate: 3.231E-06 | global batch size:     2 | lm loss: 8.546022E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.670 | TFLOPs: 26.51 |
[default0]:[2023-07-30 22:27:18,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=1485, skipped=0, lr=[3.2418474666666667e-06, 3.2418474666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:18,887] [INFO] [timer.py:215:stop] epoch=0/micro_step=1485/global_step=1485, RunningAvgSamplesPerSec=12.355874562890392, CurrSamplesPerSec=12.451769958793982, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1485/219726562 | consumed samples:         2970 | consumed tokens:      6082560 | elapsed time per iteration (ms): 184.5 | learning rate: 3.242E-06 | global batch size:     2 | lm loss: 8.635046E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.840 | TFLOPs: 26.93 |
[default0]:[2023-07-30 22:27:19,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=0, lr=[3.2527701333333333e-06, 3.2527701333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:19,796] [INFO] [timer.py:215:stop] epoch=0/micro_step=1490/global_step=1490, RunningAvgSamplesPerSec=12.356229740992967, CurrSamplesPerSec=12.441834043777606, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1490/219726562 | consumed samples:         2980 | consumed tokens:      6103040 | elapsed time per iteration (ms): 181.8 | learning rate: 3.253E-06 | global batch size:     2 | lm loss: 8.421481E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 11.001 | TFLOPs: 27.33 |
[default0]:[2023-07-30 22:27:20,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=1495, skipped=0, lr=[3.2636928e-06, 3.2636928e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:20,733] [INFO] [timer.py:215:stop] epoch=0/micro_step=1495/global_step=1495, RunningAvgSamplesPerSec=12.35648617734974, CurrSamplesPerSec=12.342795260992649, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1495/219726562 | consumed samples:         2990 | consumed tokens:      6123520 | elapsed time per iteration (ms): 187.5 | learning rate: 3.264E-06 | global batch size:     2 | lm loss: 8.424826E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.668 | TFLOPs: 26.51 |
[default0]:[2023-07-30 22:27:21,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=0, lr=[3.274615466666667e-06, 3.274615466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:21,644] [INFO] [timer.py:215:stop] epoch=0/micro_step=1500/global_step=1500, RunningAvgSamplesPerSec=12.3568746823905, CurrSamplesPerSec=12.46724093857193, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1500/219726562 | consumed samples:         3000 | consumed tokens:      6144000 | elapsed time per iteration (ms): 182.2 | learning rate: 3.275E-06 | global batch size:     2 | lm loss: 8.624628E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.977 | TFLOPs: 27.28 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1500 | lm loss value: 8.496782E+00 | lm loss PPL: 4.898980E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:27:27,693] [INFO] [logging.py:96:log_dist] [Rank 0] step=1505, skipped=0, lr=[3.2855381333333336e-06, 3.2855381333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:27,694] [INFO] [timer.py:215:stop] epoch=0/micro_step=1505/global_step=1505, RunningAvgSamplesPerSec=12.356463636864607, CurrSamplesPerSec=12.370224544297619, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1505/219726562 | consumed samples:         3010 | consumed tokens:      6164480 | elapsed time per iteration (ms): 1210.5 | learning rate: 3.286E-06 | global batch size:     2 | lm loss: 8.378471E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.652 | TFLOPs: 4.11 |
[default0]:[2023-07-30 22:27:28,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=0, lr=[3.2964608000000002e-06, 3.2964608000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:28,714] [INFO] [timer.py:215:stop] epoch=0/micro_step=1510/global_step=1510, RunningAvgSamplesPerSec=12.356596707283815, CurrSamplesPerSec=12.425321645569245, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1510/219726562 | consumed samples:         3020 | consumed tokens:      6184960 | elapsed time per iteration (ms): 203.7 | learning rate: 3.296E-06 | global batch size:     2 | lm loss: 8.543459E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.820 | TFLOPs: 24.40 |
[default0]:[2023-07-30 22:27:29,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=1515, skipped=0, lr=[3.307383466666667e-06, 3.307383466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:29,759] [INFO] [timer.py:215:stop] epoch=0/micro_step=1515/global_step=1515, RunningAvgSamplesPerSec=12.35674691048515, CurrSamplesPerSec=12.448019041655105, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1515/219726562 | consumed samples:         3030 | consumed tokens:      6205440 | elapsed time per iteration (ms): 208.8 | learning rate: 3.307E-06 | global batch size:     2 | lm loss: 8.474326E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.577 | TFLOPs: 23.80 |
[default0]:[2023-07-30 22:27:30,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=0, lr=[3.3183061333333335e-06, 3.3183061333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:30,777] [INFO] [timer.py:215:stop] epoch=0/micro_step=1520/global_step=1520, RunningAvgSamplesPerSec=12.356957762112396, CurrSamplesPerSec=12.381946812263557, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1520/219726562 | consumed samples:         3040 | consumed tokens:      6225920 | elapsed time per iteration (ms): 203.6 | learning rate: 3.318E-06 | global batch size:     2 | lm loss: 8.478366E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.822 | TFLOPs: 24.41 |
[default0]:[2023-07-30 22:27:31,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=1525, skipped=0, lr=[3.3292288e-06, 3.3292288e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:31,728] [INFO] [timer.py:215:stop] epoch=0/micro_step=1525/global_step=1525, RunningAvgSamplesPerSec=12.357099862201533, CurrSamplesPerSec=12.425432073895449, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1525/219726562 | consumed samples:         3050 | consumed tokens:      6246400 | elapsed time per iteration (ms): 190.1 | learning rate: 3.329E-06 | global batch size:     2 | lm loss: 8.467729E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.520 | TFLOPs: 26.14 |
[default0]:[2023-07-30 22:27:32,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=0, lr=[3.3401514666666667e-06, 3.3401514666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:32,664] [INFO] [timer.py:215:stop] epoch=0/micro_step=1530/global_step=1530, RunningAvgSamplesPerSec=12.357280015786877, CurrSamplesPerSec=12.339835745565969, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1530/219726562 | consumed samples:         3060 | consumed tokens:      6266880 | elapsed time per iteration (ms): 187.4 | learning rate: 3.340E-06 | global batch size:     2 | lm loss: 8.553993E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.671 | TFLOPs: 26.52 |
[default0]:[2023-07-30 22:27:33,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=1535, skipped=0, lr=[3.3510741333333334e-06, 3.3510741333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:33,601] [INFO] [timer.py:215:stop] epoch=0/micro_step=1535/global_step=1535, RunningAvgSamplesPerSec=12.357316907181648, CurrSamplesPerSec=12.2707382827185, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1535/219726562 | consumed samples:         3070 | consumed tokens:      6287360 | elapsed time per iteration (ms): 187.0 | learning rate: 3.351E-06 | global batch size:     2 | lm loss: 8.403726E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.693 | TFLOPs: 26.57 |
[default0]:[2023-07-30 22:27:34,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=0, lr=[3.3619968e-06, 3.3619968e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:34,557] [INFO] [timer.py:215:stop] epoch=0/micro_step=1540/global_step=1540, RunningAvgSamplesPerSec=12.357528864311671, CurrSamplesPerSec=12.35643097662936, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1540/219726562 | consumed samples:         3080 | consumed tokens:      6307840 | elapsed time per iteration (ms): 191.3 | learning rate: 3.362E-06 | global batch size:     2 | lm loss: 8.447043E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.454 | TFLOPs: 25.98 |
[default0]:[2023-07-30 22:27:35,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=1545, skipped=0, lr=[3.372919466666667e-06, 3.372919466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:35,490] [INFO] [timer.py:215:stop] epoch=0/micro_step=1545/global_step=1545, RunningAvgSamplesPerSec=12.357213328783919, CurrSamplesPerSec=12.207044881067064, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1545/219726562 | consumed samples:         3090 | consumed tokens:      6328320 | elapsed time per iteration (ms): 186.7 | learning rate: 3.373E-06 | global batch size:     2 | lm loss: 8.432278E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.713 | TFLOPs: 26.62 |
[default0]:[2023-07-30 22:27:36,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=0, lr=[3.3838421333333337e-06, 3.3838421333333337e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:36,402] [INFO] [timer.py:215:stop] epoch=0/micro_step=1550/global_step=1550, RunningAvgSamplesPerSec=12.357392861218464, CurrSamplesPerSec=12.310607517027119, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1550/219726562 | consumed samples:         3100 | consumed tokens:      6348800 | elapsed time per iteration (ms): 182.9 | learning rate: 3.384E-06 | global batch size:     2 | lm loss: 8.273787E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.937 | TFLOPs: 27.18 |
[default0]:[2023-07-30 22:27:37,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=1555, skipped=0, lr=[3.3947648000000003e-06, 3.3947648000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:37,363] [INFO] [timer.py:215:stop] epoch=0/micro_step=1555/global_step=1555, RunningAvgSamplesPerSec=12.35755881738494, CurrSamplesPerSec=12.377470740646745, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1555/219726562 | consumed samples:         3110 | consumed tokens:      6369280 | elapsed time per iteration (ms): 192.2 | learning rate: 3.395E-06 | global batch size:     2 | lm loss: 8.359048E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.408 | TFLOPs: 25.86 |
[default0]:[2023-07-30 22:27:38,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=0, lr=[3.405687466666667e-06, 3.405687466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:38,331] [INFO] [timer.py:215:stop] epoch=0/micro_step=1560/global_step=1560, RunningAvgSamplesPerSec=12.357931611436513, CurrSamplesPerSec=12.50787724441078, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1560/219726562 | consumed samples:         3120 | consumed tokens:      6389760 | elapsed time per iteration (ms): 193.0 | learning rate: 3.406E-06 | global batch size:     2 | lm loss: 8.320966E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.365 | TFLOPs: 25.76 |
[default0]:[2023-07-30 22:27:39,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=1565, skipped=0, lr=[3.4166101333333336e-06, 3.4166101333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:39,289] [INFO] [timer.py:215:stop] epoch=0/micro_step=1565/global_step=1565, RunningAvgSamplesPerSec=12.358186223036096, CurrSamplesPerSec=12.361347212562038, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1565/219726562 | consumed samples:         3130 | consumed tokens:      6410240 | elapsed time per iteration (ms): 191.8 | learning rate: 3.417E-06 | global batch size:     2 | lm loss: 8.288911E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.430 | TFLOPs: 25.92 |
[default0]:[2023-07-30 22:27:40,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=0, lr=[3.4275328e-06, 3.4275328e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:40,397] [INFO] [timer.py:215:stop] epoch=0/micro_step=1570/global_step=1570, RunningAvgSamplesPerSec=12.358518087578926, CurrSamplesPerSec=12.41221304016513, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1570/219726562 | consumed samples:         3140 | consumed tokens:      6430720 | elapsed time per iteration (ms): 221.4 | learning rate: 3.428E-06 | global batch size:     2 | lm loss: 8.366077E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.034 | TFLOPs: 22.45 |
[default0]:[2023-07-30 22:27:41,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=1575, skipped=0, lr=[3.438455466666667e-06, 3.438455466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:41,415] [INFO] [timer.py:215:stop] epoch=0/micro_step=1575/global_step=1575, RunningAvgSamplesPerSec=12.358529127041956, CurrSamplesPerSec=12.405256324181398, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1575/219726562 | consumed samples:         3150 | consumed tokens:      6451200 | elapsed time per iteration (ms): 203.7 | learning rate: 3.438E-06 | global batch size:     2 | lm loss: 8.367799E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.816 | TFLOPs: 24.39 |
[default0]:[2023-07-30 22:27:42,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=0, lr=[3.4493781333333334e-06, 3.4493781333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:42,501] [INFO] [timer.py:215:stop] epoch=0/micro_step=1580/global_step=1580, RunningAvgSamplesPerSec=12.358719517924674, CurrSamplesPerSec=12.405586545912724, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1580/219726562 | consumed samples:         3160 | consumed tokens:      6471680 | elapsed time per iteration (ms): 217.2 | learning rate: 3.449E-06 | global batch size:     2 | lm loss: 8.253426E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.208 | TFLOPs: 22.88 |
[default0]:[2023-07-30 22:27:43,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=1585, skipped=0, lr=[3.4603008e-06, 3.4603008e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:43,509] [INFO] [timer.py:215:stop] epoch=0/micro_step=1585/global_step=1585, RunningAvgSamplesPerSec=12.358654260475566, CurrSamplesPerSec=12.437886246367357, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1585/219726562 | consumed samples:         3170 | consumed tokens:      6492160 | elapsed time per iteration (ms): 201.6 | learning rate: 3.460E-06 | global batch size:     2 | lm loss: 8.294615E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.922 | TFLOPs: 24.65 |
[default0]:[2023-07-30 22:27:44,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=0, lr=[3.471223466666667e-06, 3.471223466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:44,589] [INFO] [timer.py:215:stop] epoch=0/micro_step=1590/global_step=1590, RunningAvgSamplesPerSec=12.358706135728102, CurrSamplesPerSec=12.383957530297014, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1590/219726562 | consumed samples:         3180 | consumed tokens:      6512640 | elapsed time per iteration (ms): 216.3 | learning rate: 3.471E-06 | global batch size:     2 | lm loss: 8.347237E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.248 | TFLOPs: 22.98 |
[default0]:[2023-07-30 22:27:45,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=1595, skipped=0, lr=[3.4821461333333338e-06, 3.4821461333333338e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:45,692] [INFO] [timer.py:215:stop] epoch=0/micro_step=1595/global_step=1595, RunningAvgSamplesPerSec=12.358766201078332, CurrSamplesPerSec=12.287922000495113, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1595/219726562 | consumed samples:         3190 | consumed tokens:      6533120 | elapsed time per iteration (ms): 220.7 | learning rate: 3.482E-06 | global batch size:     2 | lm loss: 8.526864E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.064 | TFLOPs: 22.52 |
[default0]:[2023-07-30 22:27:46,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=0, lr=[3.4930688000000004e-06, 3.4930688000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:46,775] [INFO] [timer.py:215:stop] epoch=0/micro_step=1600/global_step=1600, RunningAvgSamplesPerSec=12.358899430395885, CurrSamplesPerSec=12.368911281463754, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1600/219726562 | consumed samples:         3200 | consumed tokens:      6553600 | elapsed time per iteration (ms): 216.6 | learning rate: 3.493E-06 | global batch size:     2 | lm loss: 8.236201E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.234 | TFLOPs: 22.94 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1600 | lm loss value: 8.308297E+00 | lm loss PPL: 4.057398E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:27:50,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=1605, skipped=0, lr=[3.503991466666667e-06, 3.503991466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:50,224] [INFO] [timer.py:215:stop] epoch=0/micro_step=1605/global_step=1605, RunningAvgSamplesPerSec=12.359065852154279, CurrSamplesPerSec=12.473747994426773, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1605/219726562 | consumed samples:         3210 | consumed tokens:      6574080 | elapsed time per iteration (ms): 689.4 | learning rate: 3.504E-06 | global batch size:     2 | lm loss: 8.203279E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.901 | TFLOPs: 7.21 |
[default0]:[2023-07-30 22:27:51,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=0, lr=[3.5149141333333336e-06, 3.5149141333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:51,198] [INFO] [timer.py:215:stop] epoch=0/micro_step=1610/global_step=1610, RunningAvgSamplesPerSec=12.359140786460818, CurrSamplesPerSec=12.258437294045269, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1610/219726562 | consumed samples:         3220 | consumed tokens:      6594560 | elapsed time per iteration (ms): 195.0 | learning rate: 3.515E-06 | global batch size:     2 | lm loss: 8.353902E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.258 | TFLOPs: 25.49 |
[default0]:[2023-07-30 22:27:52,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=1615, skipped=0, lr=[3.5258368000000003e-06, 3.5258368000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:52,154] [INFO] [timer.py:215:stop] epoch=0/micro_step=1615/global_step=1615, RunningAvgSamplesPerSec=12.35923704501412, CurrSamplesPerSec=12.389298241727404, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1615/219726562 | consumed samples:         3230 | consumed tokens:      6615040 | elapsed time per iteration (ms): 191.2 | learning rate: 3.526E-06 | global batch size:     2 | lm loss: 8.171336E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.462 | TFLOPs: 25.99 |
[default0]:[2023-07-30 22:27:53,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=0, lr=[3.536759466666667e-06, 3.536759466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:53,078] [INFO] [timer.py:215:stop] epoch=0/micro_step=1620/global_step=1620, RunningAvgSamplesPerSec=12.359350548350186, CurrSamplesPerSec=12.352737270078002, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1620/219726562 | consumed samples:         3240 | consumed tokens:      6635520 | elapsed time per iteration (ms): 184.9 | learning rate: 3.537E-06 | global batch size:     2 | lm loss: 8.208829E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.818 | TFLOPs: 26.88 |
[default0]:[2023-07-30 22:27:54,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=1625, skipped=0, lr=[3.5476821333333335e-06, 3.5476821333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:54,055] [INFO] [timer.py:215:stop] epoch=0/micro_step=1625/global_step=1625, RunningAvgSamplesPerSec=12.359353332587412, CurrSamplesPerSec=12.398325726582637, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1625/219726562 | consumed samples:         3250 | consumed tokens:      6656000 | elapsed time per iteration (ms): 195.5 | learning rate: 3.548E-06 | global batch size:     2 | lm loss: 8.413853E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.231 | TFLOPs: 25.42 |
[default0]:[2023-07-30 22:27:55,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=0, lr=[3.5586048e-06, 3.5586048e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:55,256] [INFO] [timer.py:215:stop] epoch=0/micro_step=1630/global_step=1630, RunningAvgSamplesPerSec=12.359587483039697, CurrSamplesPerSec=12.329352697107632, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1630/219726562 | consumed samples:         3260 | consumed tokens:      6676480 | elapsed time per iteration (ms): 239.9 | learning rate: 3.559E-06 | global batch size:     2 | lm loss: 8.387740E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.335 | TFLOPs: 20.71 |
[default0]:[2023-07-30 22:27:56,231] [INFO] [logging.py:96:log_dist] [Rank 0] step=1635, skipped=0, lr=[3.5695274666666672e-06, 3.5695274666666672e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:56,232] [INFO] [timer.py:215:stop] epoch=0/micro_step=1635/global_step=1635, RunningAvgSamplesPerSec=12.359856240256503, CurrSamplesPerSec=12.414178486018043, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1635/219726562 | consumed samples:         3270 | consumed tokens:      6696960 | elapsed time per iteration (ms): 195.4 | learning rate: 3.570E-06 | global batch size:     2 | lm loss: 8.205858E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.234 | TFLOPs: 25.43 |
[default0]:[2023-07-30 22:27:57,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=0, lr=[3.580450133333334e-06, 3.580450133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:57,294] [INFO] [timer.py:215:stop] epoch=0/micro_step=1640/global_step=1640, RunningAvgSamplesPerSec=12.360122489937892, CurrSamplesPerSec=12.40360547919279, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1640/219726562 | consumed samples:         3280 | consumed tokens:      6717440 | elapsed time per iteration (ms): 212.2 | learning rate: 3.580E-06 | global batch size:     2 | lm loss: 8.463193E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.426 | TFLOPs: 23.42 |
[default0]:[2023-07-30 22:27:58,377] [INFO] [logging.py:96:log_dist] [Rank 0] step=1645, skipped=0, lr=[3.5913728000000005e-06, 3.5913728000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:58,378] [INFO] [timer.py:215:stop] epoch=0/micro_step=1645/global_step=1645, RunningAvgSamplesPerSec=12.36010815994509, CurrSamplesPerSec=12.290064346745742, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1645/219726562 | consumed samples:         3290 | consumed tokens:      6737920 | elapsed time per iteration (ms): 216.8 | learning rate: 3.591E-06 | global batch size:     2 | lm loss: 8.208185E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.226 | TFLOPs: 22.92 |
[default0]:[2023-07-30 22:27:59,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=0, lr=[3.602295466666667e-06, 3.602295466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:59,302] [INFO] [timer.py:215:stop] epoch=0/micro_step=1650/global_step=1650, RunningAvgSamplesPerSec=12.360073727034226, CurrSamplesPerSec=12.297162827380166, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1650/219726562 | consumed samples:         3300 | consumed tokens:      6758400 | elapsed time per iteration (ms): 184.7 | learning rate: 3.602E-06 | global batch size:     2 | lm loss: 8.336576E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.827 | TFLOPs: 26.90 |
[default0]:[2023-07-30 22:28:00,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=1655, skipped=0, lr=[3.6132181333333337e-06, 3.6132181333333337e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:00,288] [INFO] [timer.py:215:stop] epoch=0/micro_step=1655/global_step=1655, RunningAvgSamplesPerSec=12.36013524409188, CurrSamplesPerSec=12.373417665505329, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1655/219726562 | consumed samples:         3310 | consumed tokens:      6778880 | elapsed time per iteration (ms): 197.1 | learning rate: 3.613E-06 | global batch size:     2 | lm loss: 8.116180E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.145 | TFLOPs: 25.21 |
[default0]:[2023-07-30 22:28:01,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=0, lr=[3.6241408000000004e-06, 3.6241408000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:01,218] [INFO] [timer.py:215:stop] epoch=0/micro_step=1660/global_step=1660, RunningAvgSamplesPerSec=12.360193370015551, CurrSamplesPerSec=12.331727051684243, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1660/219726562 | consumed samples:         3320 | consumed tokens:      6799360 | elapsed time per iteration (ms): 186.3 | learning rate: 3.624E-06 | global batch size:     2 | lm loss: 8.321931E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.736 | TFLOPs: 26.68 |
[default0]:[2023-07-30 22:28:02,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=1665, skipped=0, lr=[3.635063466666667e-06, 3.635063466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:02,332] [INFO] [timer.py:215:stop] epoch=0/micro_step=1665/global_step=1665, RunningAvgSamplesPerSec=12.360242911456114, CurrSamplesPerSec=12.33604310537523, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1665/219726562 | consumed samples:         3330 | consumed tokens:      6819840 | elapsed time per iteration (ms): 223.6 | learning rate: 3.635E-06 | global batch size:     2 | lm loss: 8.167561E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.946 | TFLOPs: 22.23 |
[default0]:[2023-07-30 22:28:03,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=0, lr=[3.6459861333333336e-06, 3.6459861333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:03,261] [INFO] [timer.py:215:stop] epoch=0/micro_step=1670/global_step=1670, RunningAvgSamplesPerSec=12.360309364197834, CurrSamplesPerSec=12.302410880714275, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1670/219726562 | consumed samples:         3340 | consumed tokens:      6840320 | elapsed time per iteration (ms): 184.9 | learning rate: 3.646E-06 | global batch size:     2 | lm loss: 8.113722E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.815 | TFLOPs: 26.87 |
[default0]:[2023-07-30 22:28:04,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=1675, skipped=0, lr=[3.6569088000000007e-06, 3.6569088000000007e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:04,289] [INFO] [timer.py:215:stop] epoch=0/micro_step=1675/global_step=1675, RunningAvgSamplesPerSec=12.360352047750295, CurrSamplesPerSec=12.408228952802448, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1675/219726562 | consumed samples:         3350 | consumed tokens:      6860800 | elapsed time per iteration (ms): 206.0 | learning rate: 3.657E-06 | global batch size:     2 | lm loss: 8.194388E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.711 | TFLOPs: 24.13 |
[default0]:[2023-07-30 22:28:05,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=0, lr=[3.6678314666666673e-06, 3.6678314666666673e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:05,283] [INFO] [timer.py:215:stop] epoch=0/micro_step=1680/global_step=1680, RunningAvgSamplesPerSec=12.360420710973486, CurrSamplesPerSec=12.304233417330874, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1680/219726562 | consumed samples:         3360 | consumed tokens:      6881280 | elapsed time per iteration (ms): 198.5 | learning rate: 3.668E-06 | global batch size:     2 | lm loss: 8.119105E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.078 | TFLOPs: 25.04 |
[default0]:[2023-07-30 22:28:06,245] [INFO] [logging.py:96:log_dist] [Rank 0] step=1685, skipped=0, lr=[3.678754133333334e-06, 3.678754133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:06,246] [INFO] [timer.py:215:stop] epoch=0/micro_step=1685/global_step=1685, RunningAvgSamplesPerSec=12.36053439682309, CurrSamplesPerSec=12.428893156010716, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1685/219726562 | consumed samples:         3370 | consumed tokens:      6901760 | elapsed time per iteration (ms): 192.4 | learning rate: 3.679E-06 | global batch size:     2 | lm loss: 8.225238E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.397 | TFLOPs: 25.83 |
[default0]:[2023-07-30 22:28:07,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=0, lr=[3.6896767999999997e-06, 3.6896767999999997e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:07,184] [INFO] [timer.py:215:stop] epoch=0/micro_step=1690/global_step=1690, RunningAvgSamplesPerSec=12.360499709165385, CurrSamplesPerSec=12.296099334962337, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1690/219726562 | consumed samples:         3380 | consumed tokens:      6922240 | elapsed time per iteration (ms): 187.9 | learning rate: 3.690E-06 | global batch size:     2 | lm loss: 8.143133E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.643 | TFLOPs: 26.45 |
[default0]:[2023-07-30 22:28:08,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=1695, skipped=0, lr=[3.7005994666666668e-06, 3.7005994666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:08,167] [INFO] [timer.py:215:stop] epoch=0/micro_step=1695/global_step=1695, RunningAvgSamplesPerSec=12.360439278871455, CurrSamplesPerSec=12.255876199492736, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1695/219726562 | consumed samples:         3390 | consumed tokens:      6942720 | elapsed time per iteration (ms): 196.5 | learning rate: 3.701E-06 | global batch size:     2 | lm loss: 8.130443E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.178 | TFLOPs: 25.29 |
[default0]:[2023-07-30 22:28:09,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=0, lr=[3.7115221333333334e-06, 3.7115221333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:09,146] [INFO] [timer.py:215:stop] epoch=0/micro_step=1700/global_step=1700, RunningAvgSamplesPerSec=12.358938212192948, CurrSamplesPerSec=10.24431431694256, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1700/219726562 | consumed samples:         3400 | consumed tokens:      6963200 | elapsed time per iteration (ms): 195.5 | learning rate: 3.712E-06 | global batch size:     2 | lm loss: 8.169426E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.228 | TFLOPs: 25.41 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1700 | lm loss value: 8.169253E+00 | lm loss PPL: 3.530707E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:28:14,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=1705, skipped=0, lr=[3.7224448e-06, 3.7224448e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:14,404] [INFO] [timer.py:215:stop] epoch=0/micro_step=1705/global_step=1705, RunningAvgSamplesPerSec=12.35848016778875, CurrSamplesPerSec=12.435249628288883, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1705/219726562 | consumed samples:         3410 | consumed tokens:      6983680 | elapsed time per iteration (ms): 1051.8 | learning rate: 3.722E-06 | global batch size:     2 | lm loss: 8.083923E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.902 | TFLOPs: 4.72 |
[default0]:[2023-07-30 22:28:15,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=0, lr=[3.7333674666666666e-06, 3.7333674666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:15,328] [INFO] [timer.py:215:stop] epoch=0/micro_step=1710/global_step=1710, RunningAvgSamplesPerSec=12.358391032220423, CurrSamplesPerSec=12.366449665062742, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1710/219726562 | consumed samples:         3420 | consumed tokens:      7004160 | elapsed time per iteration (ms): 184.8 | learning rate: 3.733E-06 | global batch size:     2 | lm loss: 8.013811E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.823 | TFLOPs: 26.89 |
[default0]:[2023-07-30 22:28:16,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=1715, skipped=0, lr=[3.7442901333333333e-06, 3.7442901333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:16,251] [INFO] [timer.py:215:stop] epoch=0/micro_step=1715/global_step=1715, RunningAvgSamplesPerSec=12.35849026854537, CurrSamplesPerSec=12.373983475999486, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1715/219726562 | consumed samples:         3430 | consumed tokens:      7024640 | elapsed time per iteration (ms): 184.6 | learning rate: 3.744E-06 | global batch size:     2 | lm loss: 8.003168E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.836 | TFLOPs: 26.93 |
[default0]:[2023-07-30 22:28:17,169] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=0, lr=[3.7552128e-06, 3.7552128e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:17,170] [INFO] [timer.py:215:stop] epoch=0/micro_step=1720/global_step=1720, RunningAvgSamplesPerSec=12.358355490657063, CurrSamplesPerSec=12.21577304890178, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1720/219726562 | consumed samples:         3440 | consumed tokens:      7045120 | elapsed time per iteration (ms): 185.5 | learning rate: 3.755E-06 | global batch size:     2 | lm loss: 8.237654E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.782 | TFLOPs: 26.79 |
[default0]:[2023-07-30 22:28:18,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=1725, skipped=0, lr=[3.7661354666666665e-06, 3.7661354666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:18,148] [INFO] [timer.py:215:stop] epoch=0/micro_step=1725/global_step=1725, RunningAvgSamplesPerSec=12.358340891491368, CurrSamplesPerSec=12.254945157689903, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1725/219726562 | consumed samples:         3450 | consumed tokens:      7065600 | elapsed time per iteration (ms): 194.1 | learning rate: 3.766E-06 | global batch size:     2 | lm loss: 8.101315E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.304 | TFLOPs: 25.60 |
[default0]:[2023-07-30 22:28:19,104] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=0, lr=[3.777058133333333e-06, 3.777058133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:19,105] [INFO] [timer.py:215:stop] epoch=0/micro_step=1730/global_step=1730, RunningAvgSamplesPerSec=12.358273211204036, CurrSamplesPerSec=12.25729093759726, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1730/219726562 | consumed samples:         3460 | consumed tokens:      7086080 | elapsed time per iteration (ms): 191.2 | learning rate: 3.777E-06 | global batch size:     2 | lm loss: 8.120950E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.458 | TFLOPs: 25.99 |
[default0]:[2023-07-30 22:28:20,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=1735, skipped=0, lr=[3.7879808e-06, 3.7879808e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:20,048] [INFO] [timer.py:215:stop] epoch=0/micro_step=1735/global_step=1735, RunningAvgSamplesPerSec=12.357273176071494, CurrSamplesPerSec=10.851580337657868, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1735/219726562 | consumed samples:         3470 | consumed tokens:      7106560 | elapsed time per iteration (ms): 188.6 | learning rate: 3.788E-06 | global batch size:     2 | lm loss: 8.123808E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.602 | TFLOPs: 26.34 |
[default0]:[2023-07-30 22:28:21,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=0, lr=[3.798903466666667e-06, 3.798903466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:21,209] [INFO] [timer.py:215:stop] epoch=0/micro_step=1740/global_step=1740, RunningAvgSamplesPerSec=12.357289302186405, CurrSamplesPerSec=12.25890306260385, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1740/219726562 | consumed samples:         3480 | consumed tokens:      7127040 | elapsed time per iteration (ms): 231.9 | learning rate: 3.799E-06 | global batch size:     2 | lm loss: 8.052429E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.623 | TFLOPs: 21.43 |
[default0]:[2023-07-30 22:28:22,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=1745, skipped=0, lr=[3.8098261333333335e-06, 3.8098261333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:22,126] [INFO] [timer.py:215:stop] epoch=0/micro_step=1745/global_step=1745, RunningAvgSamplesPerSec=12.35727675109594, CurrSamplesPerSec=12.284538975103207, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1745/219726562 | consumed samples:         3490 | consumed tokens:      7147520 | elapsed time per iteration (ms): 183.4 | learning rate: 3.810E-06 | global batch size:     2 | lm loss: 8.117279E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.904 | TFLOPs: 27.09 |
[default0]:[2023-07-30 22:28:23,048] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=0, lr=[3.8207488e-06, 3.8207488e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:23,048] [INFO] [timer.py:215:stop] epoch=0/micro_step=1750/global_step=1750, RunningAvgSamplesPerSec=12.357358809927183, CurrSamplesPerSec=12.375772507653155, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1750/219726562 | consumed samples:         3500 | consumed tokens:      7168000 | elapsed time per iteration (ms): 186.5 | learning rate: 3.821E-06 | global batch size:     2 | lm loss: 8.051389E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.722 | TFLOPs: 26.64 |
[default0]:[2023-07-30 22:28:23,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=1755, skipped=0, lr=[3.831671466666666e-06, 3.831671466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:23,980] [INFO] [timer.py:215:stop] epoch=0/micro_step=1755/global_step=1755, RunningAvgSamplesPerSec=12.357388967561775, CurrSamplesPerSec=12.409477147388104, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1755/219726562 | consumed samples:         3510 | consumed tokens:      7188480 | elapsed time per iteration (ms): 184.2 | learning rate: 3.832E-06 | global batch size:     2 | lm loss: 8.028073E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.860 | TFLOPs: 26.98 |
[default0]:[2023-07-30 22:28:24,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=0, lr=[3.842594133333333e-06, 3.842594133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:24,911] [INFO] [timer.py:215:stop] epoch=0/micro_step=1760/global_step=1760, RunningAvgSamplesPerSec=12.357414118051176, CurrSamplesPerSec=12.313571102489691, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1760/219726562 | consumed samples:         3520 | consumed tokens:      7208960 | elapsed time per iteration (ms): 186.4 | learning rate: 3.843E-06 | global batch size:     2 | lm loss: 8.077668E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.727 | TFLOPs: 26.65 |
[default0]:[2023-07-30 22:28:25,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=1765, skipped=0, lr=[3.8535168e-06, 3.8535168e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:25,832] [INFO] [timer.py:215:stop] epoch=0/micro_step=1765/global_step=1765, RunningAvgSamplesPerSec=12.357394519610258, CurrSamplesPerSec=12.29701861423759, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1765/219726562 | consumed samples:         3530 | consumed tokens:      7229440 | elapsed time per iteration (ms): 184.1 | learning rate: 3.854E-06 | global batch size:     2 | lm loss: 7.950287E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.867 | TFLOPs: 27.00 |
[default0]:[2023-07-30 22:28:26,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=0, lr=[3.864439466666667e-06, 3.864439466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:26,756] [INFO] [timer.py:215:stop] epoch=0/micro_step=1770/global_step=1770, RunningAvgSamplesPerSec=12.357317589409522, CurrSamplesPerSec=12.133277020185949, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1770/219726562 | consumed samples:         3540 | consumed tokens:      7249920 | elapsed time per iteration (ms): 184.8 | learning rate: 3.864E-06 | global batch size:     2 | lm loss: 7.923776E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.823 | TFLOPs: 26.89 |
[default0]:[2023-07-30 22:28:27,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=1775, skipped=0, lr=[3.875362133333334e-06, 3.875362133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:27,689] [INFO] [timer.py:215:stop] epoch=0/micro_step=1775/global_step=1775, RunningAvgSamplesPerSec=12.357300098902174, CurrSamplesPerSec=12.32022629505009, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1775/219726562 | consumed samples:         3550 | consumed tokens:      7270400 | elapsed time per iteration (ms): 186.6 | learning rate: 3.875E-06 | global batch size:     2 | lm loss: 7.984994E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.719 | TFLOPs: 26.64 |
[default0]:[2023-07-30 22:28:28,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=0, lr=[3.8862848e-06, 3.8862848e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:28,683] [INFO] [timer.py:215:stop] epoch=0/micro_step=1780/global_step=1780, RunningAvgSamplesPerSec=12.357310452444677, CurrSamplesPerSec=12.237478573564703, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1780/219726562 | consumed samples:         3560 | consumed tokens:      7290880 | elapsed time per iteration (ms): 198.8 | learning rate: 3.886E-06 | global batch size:     2 | lm loss: 8.000900E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.061 | TFLOPs: 25.00 |
[default0]:[2023-07-30 22:28:29,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=1785, skipped=0, lr=[3.897207466666667e-06, 3.897207466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:29,669] [INFO] [timer.py:215:stop] epoch=0/micro_step=1785/global_step=1785, RunningAvgSamplesPerSec=12.357402098315138, CurrSamplesPerSec=12.368254754606427, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1785/219726562 | consumed samples:         3570 | consumed tokens:      7311360 | elapsed time per iteration (ms): 197.1 | learning rate: 3.897E-06 | global batch size:     2 | lm loss: 8.107306E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.147 | TFLOPs: 25.21 |
[default0]:[2023-07-30 22:28:30,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=0, lr=[3.9081301333333336e-06, 3.9081301333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:30,699] [INFO] [timer.py:215:stop] epoch=0/micro_step=1790/global_step=1790, RunningAvgSamplesPerSec=12.357590587355777, CurrSamplesPerSec=12.4069076086635, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1790/219726562 | consumed samples:         3580 | consumed tokens:      7331840 | elapsed time per iteration (ms): 205.9 | learning rate: 3.908E-06 | global batch size:     2 | lm loss: 8.199348E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.711 | TFLOPs: 24.13 |
[default0]:[2023-07-30 22:28:31,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=1795, skipped=0, lr=[3.9190528e-06, 3.9190528e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:31,730] [INFO] [timer.py:215:stop] epoch=0/micro_step=1795/global_step=1795, RunningAvgSamplesPerSec=12.357767735331489, CurrSamplesPerSec=12.424971968818365, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1795/219726562 | consumed samples:         3590 | consumed tokens:      7352320 | elapsed time per iteration (ms): 206.1 | learning rate: 3.919E-06 | global batch size:     2 | lm loss: 8.117126E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.702 | TFLOPs: 24.11 |
[default0]:[2023-07-30 22:28:32,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=0, lr=[3.929975466666667e-06, 3.929975466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:32,683] [INFO] [timer.py:215:stop] epoch=0/micro_step=1800/global_step=1800, RunningAvgSamplesPerSec=12.358004716638145, CurrSamplesPerSec=12.323140204489365, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1800/219726562 | consumed samples:         3600 | consumed tokens:      7372800 | elapsed time per iteration (ms): 190.9 | learning rate: 3.930E-06 | global batch size:     2 | lm loss: 8.061386E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.475 | TFLOPs: 26.03 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1800 | lm loss value: 8.046996E+00 | lm loss PPL: 3.124396E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:28:37,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=1805, skipped=0, lr=[3.9408981333333334e-06, 3.9408981333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:37,764] [INFO] [timer.py:215:stop] epoch=0/micro_step=1805/global_step=1805, RunningAvgSamplesPerSec=12.357951116993132, CurrSamplesPerSec=12.344175643063158, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1805/219726562 | consumed samples:         3610 | consumed tokens:      7393280 | elapsed time per iteration (ms): 1016.4 | learning rate: 3.941E-06 | global batch size:     2 | lm loss: 7.959788E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.968 | TFLOPs: 4.89 |
[default0]:[2023-07-30 22:28:38,701] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=0, lr=[3.9518208e-06, 3.9518208e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:38,701] [INFO] [timer.py:215:stop] epoch=0/micro_step=1810/global_step=1810, RunningAvgSamplesPerSec=12.357915727643798, CurrSamplesPerSec=12.328029980160188, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1810/219726562 | consumed samples:         3620 | consumed tokens:      7413760 | elapsed time per iteration (ms): 187.0 | learning rate: 3.952E-06 | global batch size:     2 | lm loss: 7.878202E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.694 | TFLOPs: 26.57 |
[default0]:[2023-07-30 22:28:39,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=1815, skipped=0, lr=[3.962743466666667e-06, 3.962743466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:39,653] [INFO] [timer.py:215:stop] epoch=0/micro_step=1815/global_step=1815, RunningAvgSamplesPerSec=12.357658961065795, CurrSamplesPerSec=12.264333668623827, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1815/219726562 | consumed samples:         3630 | consumed tokens:      7434240 | elapsed time per iteration (ms): 190.6 | learning rate: 3.963E-06 | global batch size:     2 | lm loss: 7.832294E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.496 | TFLOPs: 26.08 |
[default0]:[2023-07-30 22:28:40,574] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=0, lr=[3.973666133333333e-06, 3.973666133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:40,576] [INFO] [timer.py:215:stop] epoch=0/micro_step=1820/global_step=1820, RunningAvgSamplesPerSec=12.357579570988943, CurrSamplesPerSec=12.319104535915635, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1820/219726562 | consumed samples:         3640 | consumed tokens:      7454720 | elapsed time per iteration (ms): 184.5 | learning rate: 3.974E-06 | global batch size:     2 | lm loss: 7.907404E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.840 | TFLOPs: 26.94 |
[default0]:[2023-07-30 22:28:41,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=1825, skipped=0, lr=[3.9845888e-06, 3.9845888e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:41,577] [INFO] [timer.py:215:stop] epoch=0/micro_step=1825/global_step=1825, RunningAvgSamplesPerSec=12.357276599085186, CurrSamplesPerSec=12.182403308819259, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1825/219726562 | consumed samples:         3650 | consumed tokens:      7475200 | elapsed time per iteration (ms): 200.1 | learning rate: 3.985E-06 | global batch size:     2 | lm loss: 8.072572E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.995 | TFLOPs: 24.84 |
[default0]:[2023-07-30 22:28:42,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=0, lr=[3.995511466666667e-06, 3.995511466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:42,513] [INFO] [timer.py:215:stop] epoch=0/micro_step=1830/global_step=1830, RunningAvgSamplesPerSec=12.357230440314394, CurrSamplesPerSec=12.441409627600486, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1830/219726562 | consumed samples:         3660 | consumed tokens:      7495680 | elapsed time per iteration (ms): 187.1 | learning rate: 3.996E-06 | global batch size:     2 | lm loss: 7.944606E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.687 | TFLOPs: 26.55 |
[default0]:[2023-07-30 22:28:43,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=1835, skipped=0, lr=[4.006434133333333e-06, 4.006434133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:43,472] [INFO] [timer.py:215:stop] epoch=0/micro_step=1835/global_step=1835, RunningAvgSamplesPerSec=12.357150788748854, CurrSamplesPerSec=12.28250645707291, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1835/219726562 | consumed samples:         3670 | consumed tokens:      7516160 | elapsed time per iteration (ms): 191.8 | learning rate: 4.006E-06 | global batch size:     2 | lm loss: 8.066937E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.429 | TFLOPs: 25.91 |
[default0]:[2023-07-30 22:28:44,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=0, lr=[4.0173568e-06, 4.0173568e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:44,446] [INFO] [timer.py:215:stop] epoch=0/micro_step=1840/global_step=1840, RunningAvgSamplesPerSec=12.35711305813226, CurrSamplesPerSec=12.345374646428077, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1840/219726562 | consumed samples:         3680 | consumed tokens:      7536640 | elapsed time per iteration (ms): 194.8 | learning rate: 4.017E-06 | global batch size:     2 | lm loss: 8.051008E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.266 | TFLOPs: 25.51 |
[default0]:[2023-07-30 22:28:45,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=1845, skipped=0, lr=[4.0282794666666665e-06, 4.0282794666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:45,484] [INFO] [timer.py:215:stop] epoch=0/micro_step=1845/global_step=1845, RunningAvgSamplesPerSec=12.356921426712422, CurrSamplesPerSec=12.267884981558558, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1845/219726562 | consumed samples:         3690 | consumed tokens:      7557120 | elapsed time per iteration (ms): 207.7 | learning rate: 4.028E-06 | global batch size:     2 | lm loss: 7.914629E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.629 | TFLOPs: 23.92 |
[default0]:[2023-07-30 22:28:46,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=0, lr=[4.039202133333333e-06, 4.039202133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:46,447] [INFO] [timer.py:215:stop] epoch=0/micro_step=1850/global_step=1850, RunningAvgSamplesPerSec=12.357006749221712, CurrSamplesPerSec=12.345356477972611, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1850/219726562 | consumed samples:         3700 | consumed tokens:      7577600 | elapsed time per iteration (ms): 192.7 | learning rate: 4.039E-06 | global batch size:     2 | lm loss: 7.816264E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.378 | TFLOPs: 25.79 |
[default0]:[2023-07-30 22:28:47,382] [INFO] [logging.py:96:log_dist] [Rank 0] step=1855, skipped=0, lr=[4.050124800000001e-06, 4.050124800000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:47,384] [INFO] [timer.py:215:stop] epoch=0/micro_step=1855/global_step=1855, RunningAvgSamplesPerSec=12.357037642250644, CurrSamplesPerSec=12.253781554341174, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1855/219726562 | consumed samples:         3710 | consumed tokens:      7598080 | elapsed time per iteration (ms): 187.6 | learning rate: 4.050E-06 | global batch size:     2 | lm loss: 8.015207E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.664 | TFLOPs: 26.50 |
[default0]:[2023-07-30 22:28:48,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=0, lr=[4.061047466666667e-06, 4.061047466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:48,318] [INFO] [timer.py:215:stop] epoch=0/micro_step=1860/global_step=1860, RunningAvgSamplesPerSec=12.356949777020013, CurrSamplesPerSec=12.300065836066727, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1860/219726562 | consumed samples:         3720 | consumed tokens:      7618560 | elapsed time per iteration (ms): 186.3 | learning rate: 4.061E-06 | global batch size:     2 | lm loss: 7.873486E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.733 | TFLOPs: 26.67 |
[default0]:[2023-07-30 22:28:49,294] [INFO] [logging.py:96:log_dist] [Rank 0] step=1865, skipped=0, lr=[4.071970133333334e-06, 4.071970133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:49,295] [INFO] [timer.py:215:stop] epoch=0/micro_step=1865/global_step=1865, RunningAvgSamplesPerSec=12.357183547033644, CurrSamplesPerSec=12.487916419794622, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1865/219726562 | consumed samples:         3730 | consumed tokens:      7639040 | elapsed time per iteration (ms): 195.5 | learning rate: 4.072E-06 | global batch size:     2 | lm loss: 8.112474E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.230 | TFLOPs: 25.42 |
[default0]:[2023-07-30 22:28:50,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=0, lr=[4.0828928000000005e-06, 4.0828928000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:50,240] [INFO] [timer.py:215:stop] epoch=0/micro_step=1870/global_step=1870, RunningAvgSamplesPerSec=12.357299825564898, CurrSamplesPerSec=12.425376859486994, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1870/219726562 | consumed samples:         3740 | consumed tokens:      7659520 | elapsed time per iteration (ms): 189.2 | learning rate: 4.083E-06 | global batch size:     2 | lm loss: 7.877310E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.572 | TFLOPs: 26.27 |
[default0]:[2023-07-30 22:28:51,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=1875, skipped=0, lr=[4.093815466666667e-06, 4.093815466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:51,198] [INFO] [timer.py:215:stop] epoch=0/micro_step=1875/global_step=1875, RunningAvgSamplesPerSec=12.357575017396082, CurrSamplesPerSec=12.449977366667408, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1875/219726562 | consumed samples:         3750 | consumed tokens:      7680000 | elapsed time per iteration (ms): 191.2 | learning rate: 4.094E-06 | global batch size:     2 | lm loss: 7.853573E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.458 | TFLOPs: 25.99 |
[default0]:[2023-07-30 22:28:52,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=0, lr=[4.104738133333334e-06, 4.104738133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:52,131] [INFO] [timer.py:215:stop] epoch=0/micro_step=1880/global_step=1880, RunningAvgSamplesPerSec=12.357752970976843, CurrSamplesPerSec=12.382787578549276, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1880/219726562 | consumed samples:         3760 | consumed tokens:      7700480 | elapsed time per iteration (ms): 186.9 | learning rate: 4.105E-06 | global batch size:     2 | lm loss: 7.964075E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.699 | TFLOPs: 26.58 |
[default0]:[2023-07-30 22:28:53,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=1885, skipped=0, lr=[4.1156608e-06, 4.1156608e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:53,096] [INFO] [timer.py:215:stop] epoch=0/micro_step=1885/global_step=1885, RunningAvgSamplesPerSec=12.357935476199618, CurrSamplesPerSec=12.43060599998518, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1885/219726562 | consumed samples:         3770 | consumed tokens:      7720960 | elapsed time per iteration (ms): 192.6 | learning rate: 4.116E-06 | global batch size:     2 | lm loss: 7.956319E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.383 | TFLOPs: 25.80 |
[default0]:[2023-07-30 22:28:54,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=0, lr=[4.126583466666667e-06, 4.126583466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:54,069] [INFO] [timer.py:215:stop] epoch=0/micro_step=1890/global_step=1890, RunningAvgSamplesPerSec=12.35801249074323, CurrSamplesPerSec=12.366012146942627, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1890/219726562 | consumed samples:         3780 | consumed tokens:      7741440 | elapsed time per iteration (ms): 194.5 | learning rate: 4.127E-06 | global batch size:     2 | lm loss: 7.918270E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.282 | TFLOPs: 25.55 |
[default0]:[2023-07-30 22:28:55,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=1895, skipped=0, lr=[4.137506133333334e-06, 4.137506133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:55,114] [INFO] [timer.py:215:stop] epoch=0/micro_step=1895/global_step=1895, RunningAvgSamplesPerSec=12.357156012825108, CurrSamplesPerSec=10.82670652175316, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1895/219726562 | consumed samples:         3790 | consumed tokens:      7761920 | elapsed time per iteration (ms): 209.7 | learning rate: 4.138E-06 | global batch size:     2 | lm loss: 7.750838E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.539 | TFLOPs: 23.70 |
[default0]:[2023-07-30 22:28:56,033] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=0, lr=[4.1484288e-06, 4.1484288e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:56,033] [INFO] [timer.py:215:stop] epoch=0/micro_step=1900/global_step=1900, RunningAvgSamplesPerSec=12.357259447983546, CurrSamplesPerSec=12.393819495950286, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1900/219726562 | consumed samples:         3800 | consumed tokens:      7782400 | elapsed time per iteration (ms): 183.3 | learning rate: 4.148E-06 | global batch size:     2 | lm loss: 8.109443E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.911 | TFLOPs: 27.11 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 1900 | lm loss value: 7.831897E+00 | lm loss PPL: 2.519705E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:29:01,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=1905, skipped=0, lr=[4.159351466666667e-06, 4.159351466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:01,924] [INFO] [timer.py:215:stop] epoch=0/micro_step=1905/global_step=1905, RunningAvgSamplesPerSec=12.35689131844276, CurrSamplesPerSec=12.323574694136589, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1905/219726562 | consumed samples:         3810 | consumed tokens:      7802880 | elapsed time per iteration (ms): 1178.4 | learning rate: 4.159E-06 | global batch size:     2 | lm loss: 7.777160E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.697 | TFLOPs: 4.22 |
[default0]:[2023-07-30 22:29:02,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=0, lr=[4.1702741333333335e-06, 4.1702741333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:02,933] [INFO] [timer.py:215:stop] epoch=0/micro_step=1910/global_step=1910, RunningAvgSamplesPerSec=12.35699364140978, CurrSamplesPerSec=12.349936620620309, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1910/219726562 | consumed samples:         3820 | consumed tokens:      7823360 | elapsed time per iteration (ms): 201.6 | learning rate: 4.170E-06 | global batch size:     2 | lm loss: 8.096292E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.920 | TFLOPs: 24.65 |
[default0]:[2023-07-30 22:29:03,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=1915, skipped=0, lr=[4.1811968e-06, 4.1811968e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:03,926] [INFO] [timer.py:215:stop] epoch=0/micro_step=1915/global_step=1915, RunningAvgSamplesPerSec=12.35709155840149, CurrSamplesPerSec=12.290298429833314, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1915/219726562 | consumed samples:         3830 | consumed tokens:      7843840 | elapsed time per iteration (ms): 198.6 | learning rate: 4.181E-06 | global batch size:     2 | lm loss: 7.820840E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.069 | TFLOPs: 25.02 |
[default0]:[2023-07-30 22:29:05,048] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=0, lr=[4.192119466666667e-06, 4.192119466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:05,049] [INFO] [timer.py:215:stop] epoch=0/micro_step=1920/global_step=1920, RunningAvgSamplesPerSec=12.357078941045728, CurrSamplesPerSec=12.271474254885266, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1920/219726562 | consumed samples:         3840 | consumed tokens:      7864320 | elapsed time per iteration (ms): 224.7 | learning rate: 4.192E-06 | global batch size:     2 | lm loss: 7.788023E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.899 | TFLOPs: 22.11 |
[default0]:[2023-07-30 22:29:06,125] [INFO] [logging.py:96:log_dist] [Rank 0] step=1925, skipped=0, lr=[4.203042133333333e-06, 4.203042133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:06,126] [INFO] [timer.py:215:stop] epoch=0/micro_step=1925/global_step=1925, RunningAvgSamplesPerSec=12.35719872392498, CurrSamplesPerSec=12.288048000187501, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1925/219726562 | consumed samples:         3850 | consumed tokens:      7884800 | elapsed time per iteration (ms): 216.4 | learning rate: 4.203E-06 | global batch size:     2 | lm loss: 7.797594E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.240 | TFLOPs: 22.96 |
[default0]:[2023-07-30 22:29:07,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=0, lr=[4.2139648e-06, 4.2139648e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:07,331] [INFO] [timer.py:215:stop] epoch=0/micro_step=1930/global_step=1930, RunningAvgSamplesPerSec=12.357328018810932, CurrSamplesPerSec=12.187394486722399, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1930/219726562 | consumed samples:         3860 | consumed tokens:      7905280 | elapsed time per iteration (ms): 239.8 | learning rate: 4.214E-06 | global batch size:     2 | lm loss: 7.610844E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.342 | TFLOPs: 20.73 |
[default0]:[2023-07-30 22:29:08,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=1935, skipped=0, lr=[4.224887466666667e-06, 4.224887466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:08,337] [INFO] [timer.py:215:stop] epoch=0/micro_step=1935/global_step=1935, RunningAvgSamplesPerSec=12.357642768701579, CurrSamplesPerSec=12.494277594742893, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1935/219726562 | consumed samples:         3870 | consumed tokens:      7925760 | elapsed time per iteration (ms): 201.5 | learning rate: 4.225E-06 | global batch size:     2 | lm loss: 7.896972E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.924 | TFLOPs: 24.66 |
[default0]:[2023-07-30 22:29:09,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=0, lr=[4.235810133333334e-06, 4.235810133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:09,315] [INFO] [timer.py:215:stop] epoch=0/micro_step=1940/global_step=1940, RunningAvgSamplesPerSec=12.357874859682221, CurrSamplesPerSec=12.462758544868919, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1940/219726562 | consumed samples:         3880 | consumed tokens:      7946240 | elapsed time per iteration (ms): 196.0 | learning rate: 4.236E-06 | global batch size:     2 | lm loss: 7.646974E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.202 | TFLOPs: 25.35 |
[default0]:[2023-07-30 22:29:10,294] [INFO] [logging.py:96:log_dist] [Rank 0] step=1945, skipped=0, lr=[4.246732800000001e-06, 4.246732800000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:10,295] [INFO] [timer.py:215:stop] epoch=0/micro_step=1945/global_step=1945, RunningAvgSamplesPerSec=12.357992369718273, CurrSamplesPerSec=12.328319866438186, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1945/219726562 | consumed samples:         3890 | consumed tokens:      7966720 | elapsed time per iteration (ms): 195.4 | learning rate: 4.247E-06 | global batch size:     2 | lm loss: 7.696741E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.236 | TFLOPs: 25.43 |
[default0]:[2023-07-30 22:29:11,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=0, lr=[4.257655466666667e-06, 4.257655466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:11,281] [INFO] [timer.py:215:stop] epoch=0/micro_step=1950/global_step=1950, RunningAvgSamplesPerSec=12.357675098091331, CurrSamplesPerSec=12.118920933672932, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1950/219726562 | consumed samples:         3900 | consumed tokens:      7987200 | elapsed time per iteration (ms): 197.1 | learning rate: 4.258E-06 | global batch size:     2 | lm loss: 7.844226E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.148 | TFLOPs: 25.22 |
[default0]:[2023-07-30 22:29:12,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=1955, skipped=0, lr=[4.268578133333334e-06, 4.268578133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:12,243] [INFO] [timer.py:215:stop] epoch=0/micro_step=1955/global_step=1955, RunningAvgSamplesPerSec=12.357663262362852, CurrSamplesPerSec=12.187288248865334, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1955/219726562 | consumed samples:         3910 | consumed tokens:      8007680 | elapsed time per iteration (ms): 192.3 | learning rate: 4.269E-06 | global batch size:     2 | lm loss: 7.723076E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.400 | TFLOPs: 25.84 |
[default0]:[2023-07-30 22:29:13,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=0, lr=[4.279500800000001e-06, 4.279500800000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:13,193] [INFO] [timer.py:215:stop] epoch=0/micro_step=1960/global_step=1960, RunningAvgSamplesPerSec=12.357326266784195, CurrSamplesPerSec=12.319213084323513, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1960/219726562 | consumed samples:         3920 | consumed tokens:      8028160 | elapsed time per iteration (ms): 190.1 | learning rate: 4.280E-06 | global batch size:     2 | lm loss: 7.801656E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.522 | TFLOPs: 26.14 |
[default0]:[2023-07-30 22:29:14,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=1965, skipped=0, lr=[4.290423466666667e-06, 4.290423466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:14,141] [INFO] [timer.py:215:stop] epoch=0/micro_step=1965/global_step=1965, RunningAvgSamplesPerSec=12.357282126252029, CurrSamplesPerSec=12.184137368080107, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1965/219726562 | consumed samples:         3930 | consumed tokens:      8048640 | elapsed time per iteration (ms): 189.4 | learning rate: 4.290E-06 | global batch size:     2 | lm loss: 7.671811E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.557 | TFLOPs: 26.23 |
[default0]:[2023-07-30 22:29:15,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=0, lr=[4.301346133333334e-06, 4.301346133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:15,076] [INFO] [timer.py:215:stop] epoch=0/micro_step=1970/global_step=1970, RunningAvgSamplesPerSec=12.356936973302371, CurrSamplesPerSec=12.17079076839596, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1970/219726562 | consumed samples:         3940 | consumed tokens:      8069120 | elapsed time per iteration (ms): 187.7 | learning rate: 4.301E-06 | global batch size:     2 | lm loss: 7.605118E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.655 | TFLOPs: 26.47 |
[default0]:[2023-07-30 22:29:16,069] [INFO] [logging.py:96:log_dist] [Rank 0] step=1975, skipped=0, lr=[4.3122688000000005e-06, 4.3122688000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:16,070] [INFO] [timer.py:215:stop] epoch=0/micro_step=1975/global_step=1975, RunningAvgSamplesPerSec=12.356927083270882, CurrSamplesPerSec=12.443107466065026, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1975/219726562 | consumed samples:         3950 | consumed tokens:      8089600 | elapsed time per iteration (ms): 198.0 | learning rate: 4.312E-06 | global batch size:     2 | lm loss: 7.677164E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.099 | TFLOPs: 25.09 |
[default0]:[2023-07-30 22:29:17,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=0, lr=[4.323191466666667e-06, 4.323191466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:17,001] [INFO] [timer.py:215:stop] epoch=0/micro_step=1980/global_step=1980, RunningAvgSamplesPerSec=12.356880139035114, CurrSamplesPerSec=12.223373686388566, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1980/219726562 | consumed samples:         3960 | consumed tokens:      8110080 | elapsed time per iteration (ms): 186.5 | learning rate: 4.323E-06 | global batch size:     2 | lm loss: 7.725465E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.725 | TFLOPs: 26.65 |
[default0]:[2023-07-30 22:29:18,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=1985, skipped=0, lr=[4.334114133333334e-06, 4.334114133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:18,194] [INFO] [timer.py:215:stop] epoch=0/micro_step=1985/global_step=1985, RunningAvgSamplesPerSec=12.356598432134527, CurrSamplesPerSec=12.275856013533486, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1985/219726562 | consumed samples:         3970 | consumed tokens:      8130560 | elapsed time per iteration (ms): 238.8 | learning rate: 4.334E-06 | global batch size:     2 | lm loss: 7.716080E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.377 | TFLOPs: 20.81 |
[default0]:[2023-07-30 22:29:19,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=0, lr=[4.3450368e-06, 4.3450368e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:19,162] [INFO] [timer.py:215:stop] epoch=0/micro_step=1990/global_step=1990, RunningAvgSamplesPerSec=12.356619087420539, CurrSamplesPerSec=12.323357445483232, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1990/219726562 | consumed samples:         3980 | consumed tokens:      8151040 | elapsed time per iteration (ms): 193.3 | learning rate: 4.345E-06 | global batch size:     2 | lm loss: 7.725760E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.346 | TFLOPs: 25.71 |
[default0]:[2023-07-30 22:29:20,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=1995, skipped=0, lr=[4.355959466666667e-06, 4.355959466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:20,103] [INFO] [timer.py:215:stop] epoch=0/micro_step=1995/global_step=1995, RunningAvgSamplesPerSec=12.356459515706204, CurrSamplesPerSec=12.119953997214404, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     1995/219726562 | consumed samples:         3990 | consumed tokens:      8171520 | elapsed time per iteration (ms): 188.8 | learning rate: 4.356E-06 | global batch size:     2 | lm loss: 7.642078E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.591 | TFLOPs: 26.32 |
[default0]:[2023-07-30 22:29:21,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=0, lr=[4.366882133333334e-06, 4.366882133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:21,046] [INFO] [timer.py:215:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=12.35656158276704, CurrSamplesPerSec=12.461314493175593, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2000/219726562 | consumed samples:         4000 | consumed tokens:      8192000 | elapsed time per iteration (ms): 187.9 | learning rate: 4.367E-06 | global batch size:     2 | lm loss: 7.786336E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.647 | TFLOPs: 26.45 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 2000 | lm loss value: 7.770573E+00 | lm loss PPL: 2.369829E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:29:24,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=2005, skipped=0, lr=[4.3778048e-06, 4.3778048e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:24,651] [INFO] [timer.py:215:stop] epoch=0/micro_step=2005/global_step=2005, RunningAvgSamplesPerSec=12.356511723256856, CurrSamplesPerSec=12.311167597617782, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2005/219726562 | consumed samples:         4010 | consumed tokens:      8212480 | elapsed time per iteration (ms): 721.0 | learning rate: 4.378E-06 | global batch size:     2 | lm loss: 7.639999E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.774 | TFLOPs: 6.89 |
[default0]:[2023-07-30 22:29:25,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=0, lr=[4.388727466666667e-06, 4.388727466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:25,570] [INFO] [timer.py:215:stop] epoch=0/micro_step=2010/global_step=2010, RunningAvgSamplesPerSec=12.35659221383916, CurrSamplesPerSec=12.369768518655055, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2010/219726562 | consumed samples:         4020 | consumed tokens:      8232960 | elapsed time per iteration (ms): 183.8 | learning rate: 4.389E-06 | global batch size:     2 | lm loss: 7.596939E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.883 | TFLOPs: 27.04 |
[default0]:[2023-07-30 22:29:26,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=2015, skipped=0, lr=[4.3996501333333336e-06, 4.3996501333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:26,508] [INFO] [timer.py:215:stop] epoch=0/micro_step=2015/global_step=2015, RunningAvgSamplesPerSec=12.35687811631144, CurrSamplesPerSec=12.441483437029566, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2015/219726562 | consumed samples:         4030 | consumed tokens:      8253440 | elapsed time per iteration (ms): 187.6 | learning rate: 4.400E-06 | global batch size:     2 | lm loss: 7.551207E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.659 | TFLOPs: 26.49 |
[default0]:[2023-07-30 22:29:27,455] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=0, lr=[4.4105728e-06, 4.4105728e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:27,456] [INFO] [timer.py:215:stop] epoch=0/micro_step=2020/global_step=2020, RunningAvgSamplesPerSec=12.357124531084795, CurrSamplesPerSec=12.29869530094286, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2020/219726562 | consumed samples:         4040 | consumed tokens:      8273920 | elapsed time per iteration (ms): 189.6 | learning rate: 4.411E-06 | global batch size:     2 | lm loss: 7.554569E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.549 | TFLOPs: 26.21 |
[default0]:[2023-07-30 22:29:28,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=2025, skipped=0, lr=[4.421495466666667e-06, 4.421495466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:28,417] [INFO] [timer.py:215:stop] epoch=0/micro_step=2025/global_step=2025, RunningAvgSamplesPerSec=12.357236238418283, CurrSamplesPerSec=12.398820512441524, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2025/219726562 | consumed samples:         4050 | consumed tokens:      8294400 | elapsed time per iteration (ms): 192.2 | learning rate: 4.421E-06 | global batch size:     2 | lm loss: 7.746624E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.406 | TFLOPs: 25.86 |
[default0]:[2023-07-30 22:29:29,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=0, lr=[4.4324181333333334e-06, 4.4324181333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:29,334] [INFO] [timer.py:215:stop] epoch=0/micro_step=2030/global_step=2030, RunningAvgSamplesPerSec=12.357337576973237, CurrSamplesPerSec=12.42107164697573, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2030/219726562 | consumed samples:         4060 | consumed tokens:      8314880 | elapsed time per iteration (ms): 183.2 | learning rate: 4.432E-06 | global batch size:     2 | lm loss: 7.870302E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.914 | TFLOPs: 27.12 |
[default0]:[2023-07-30 22:29:30,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=2035, skipped=0, lr=[4.4433408e-06, 4.4433408e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:30,242] [INFO] [timer.py:215:stop] epoch=0/micro_step=2035/global_step=2035, RunningAvgSamplesPerSec=12.357511208306622, CurrSamplesPerSec=12.346373993839034, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2035/219726562 | consumed samples:         4070 | consumed tokens:      8335360 | elapsed time per iteration (ms): 181.9 | learning rate: 4.443E-06 | global batch size:     2 | lm loss: 7.512041E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.998 | TFLOPs: 27.33 |
[default0]:[2023-07-30 22:29:31,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=0, lr=[4.454263466666667e-06, 4.454263466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:31,172] [INFO] [timer.py:215:stop] epoch=0/micro_step=2040/global_step=2040, RunningAvgSamplesPerSec=12.357567422592352, CurrSamplesPerSec=12.304143180041391, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2040/219726562 | consumed samples:         4080 | consumed tokens:      8355840 | elapsed time per iteration (ms): 186.1 | learning rate: 4.454E-06 | global batch size:     2 | lm loss: 7.826755E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.749 | TFLOPs: 26.71 |
[default0]:[2023-07-30 22:29:32,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=2045, skipped=0, lr=[4.465186133333333e-06, 4.465186133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:32,091] [INFO] [timer.py:215:stop] epoch=0/micro_step=2045/global_step=2045, RunningAvgSamplesPerSec=12.357700769933864, CurrSamplesPerSec=12.372906655338932, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2045/219726562 | consumed samples:         4090 | consumed tokens:      8376320 | elapsed time per iteration (ms): 183.7 | learning rate: 4.465E-06 | global batch size:     2 | lm loss: 7.548499E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.890 | TFLOPs: 27.06 |
[default0]:[2023-07-30 22:29:33,027] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=0, lr=[4.4761088e-06, 4.4761088e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:33,027] [INFO] [timer.py:215:stop] epoch=0/micro_step=2050/global_step=2050, RunningAvgSamplesPerSec=12.357774355832692, CurrSamplesPerSec=12.372231456640595, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2050/219726562 | consumed samples:         4100 | consumed tokens:      8396800 | elapsed time per iteration (ms): 187.4 | learning rate: 4.476E-06 | global batch size:     2 | lm loss: 7.771782E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.672 | TFLOPs: 26.52 |
[default0]:[2023-07-30 22:29:33,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=2055, skipped=0, lr=[4.487031466666667e-06, 4.487031466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:33,967] [INFO] [timer.py:215:stop] epoch=0/micro_step=2055/global_step=2055, RunningAvgSamplesPerSec=12.357533784986385, CurrSamplesPerSec=12.036963484210164, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2055/219726562 | consumed samples:         4110 | consumed tokens:      8417280 | elapsed time per iteration (ms): 188.0 | learning rate: 4.487E-06 | global batch size:     2 | lm loss: 7.658900E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.639 | TFLOPs: 26.44 |
[default0]:[2023-07-30 22:29:34,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=0, lr=[4.497954133333333e-06, 4.497954133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:34,907] [INFO] [timer.py:215:stop] epoch=0/micro_step=2060/global_step=2060, RunningAvgSamplesPerSec=12.357673114601885, CurrSamplesPerSec=12.430587579797166, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2060/219726562 | consumed samples:         4120 | consumed tokens:      8437760 | elapsed time per iteration (ms): 187.8 | learning rate: 4.498E-06 | global batch size:     2 | lm loss: 7.423222E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.647 | TFLOPs: 26.46 |
[default0]:[2023-07-30 22:29:35,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=2065, skipped=0, lr=[4.5088768e-06, 4.5088768e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:35,939] [INFO] [timer.py:215:stop] epoch=0/micro_step=2065/global_step=2065, RunningAvgSamplesPerSec=12.35766127941529, CurrSamplesPerSec=12.36745242753372, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2065/219726562 | consumed samples:         4130 | consumed tokens:      8458240 | elapsed time per iteration (ms): 206.3 | learning rate: 4.509E-06 | global batch size:     2 | lm loss: 7.548734E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.693 | TFLOPs: 24.08 |
[default0]:[2023-07-30 22:29:36,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=0, lr=[4.5197994666666665e-06, 4.5197994666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:36,909] [INFO] [timer.py:215:stop] epoch=0/micro_step=2070/global_step=2070, RunningAvgSamplesPerSec=12.357800070039287, CurrSamplesPerSec=12.39550436943846, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2070/219726562 | consumed samples:         4140 | consumed tokens:      8478720 | elapsed time per iteration (ms): 193.8 | learning rate: 4.520E-06 | global batch size:     2 | lm loss: 7.584488E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.318 | TFLOPs: 25.64 |
[default0]:[2023-07-30 22:29:37,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=2075, skipped=0, lr=[4.530722133333333e-06, 4.530722133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:37,908] [INFO] [timer.py:215:stop] epoch=0/micro_step=2075/global_step=2075, RunningAvgSamplesPerSec=12.357837147189112, CurrSamplesPerSec=12.27355699591789, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2075/219726562 | consumed samples:         4150 | consumed tokens:      8499200 | elapsed time per iteration (ms): 200.2 | learning rate: 4.531E-06 | global batch size:     2 | lm loss: 7.596295E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.991 | TFLOPs: 24.83 |
[default0]:[2023-07-30 22:29:38,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=2080, skipped=0, lr=[4.5416448e-06, 4.5416448e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:38,940] [INFO] [timer.py:215:stop] epoch=0/micro_step=2080/global_step=2080, RunningAvgSamplesPerSec=12.357893285248029, CurrSamplesPerSec=12.161597768505658, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2080/219726562 | consumed samples:         4160 | consumed tokens:      8519680 | elapsed time per iteration (ms): 206.8 | learning rate: 4.542E-06 | global batch size:     2 | lm loss: 7.648494E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.669 | TFLOPs: 24.03 |
[default0]:[2023-07-30 22:29:39,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=2085, skipped=0, lr=[4.552567466666666e-06, 4.552567466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:39,905] [INFO] [timer.py:215:stop] epoch=0/micro_step=2085/global_step=2085, RunningAvgSamplesPerSec=12.358008132408449, CurrSamplesPerSec=12.369184854892316, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2085/219726562 | consumed samples:         4170 | consumed tokens:      8540160 | elapsed time per iteration (ms): 192.3 | learning rate: 4.553E-06 | global batch size:     2 | lm loss: 7.641920E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.402 | TFLOPs: 25.85 |
[default0]:[2023-07-30 22:29:40,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=2090, skipped=0, lr=[4.563490133333333e-06, 4.563490133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:40,843] [INFO] [timer.py:215:stop] epoch=0/micro_step=2090/global_step=2090, RunningAvgSamplesPerSec=12.358129302513172, CurrSamplesPerSec=12.348609483657091, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2090/219726562 | consumed samples:         4180 | consumed tokens:      8560640 | elapsed time per iteration (ms): 187.6 | learning rate: 4.563E-06 | global batch size:     2 | lm loss: 7.436276E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.662 | TFLOPs: 26.49 |
[default0]:[2023-07-30 22:29:41,846] [INFO] [logging.py:96:log_dist] [Rank 0] step=2095, skipped=0, lr=[4.5744128000000005e-06, 4.5744128000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:41,846] [INFO] [timer.py:215:stop] epoch=0/micro_step=2095/global_step=2095, RunningAvgSamplesPerSec=12.358099940816905, CurrSamplesPerSec=12.400818382865996, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2095/219726562 | consumed samples:         4190 | consumed tokens:      8581120 | elapsed time per iteration (ms): 200.7 | learning rate: 4.574E-06 | global batch size:     2 | lm loss: 7.614835E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.965 | TFLOPs: 24.76 |
[default0]:[2023-07-30 22:29:42,789] [INFO] [logging.py:96:log_dist] [Rank 0] step=2100, skipped=0, lr=[4.585335466666667e-06, 4.585335466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:42,789] [INFO] [timer.py:215:stop] epoch=0/micro_step=2100/global_step=2100, RunningAvgSamplesPerSec=12.358005246108977, CurrSamplesPerSec=12.175277617719928, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2100/219726562 | consumed samples:         4200 | consumed tokens:      8601600 | elapsed time per iteration (ms): 189.3 | learning rate: 4.585E-06 | global batch size:     2 | lm loss: 7.564011E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.565 | TFLOPs: 26.25 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 2100 | lm loss value: 7.633212E+00 | lm loss PPL: 2.065674E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:29:48,533] [INFO] [logging.py:96:log_dist] [Rank 0] step=2105, skipped=0, lr=[4.596258133333334e-06, 4.596258133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:48,534] [INFO] [timer.py:215:stop] epoch=0/micro_step=2105/global_step=2105, RunningAvgSamplesPerSec=12.357558488074712, CurrSamplesPerSec=12.379754046224434, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2105/219726562 | consumed samples:         4210 | consumed tokens:      8622080 | elapsed time per iteration (ms): 1148.3 | learning rate: 4.596E-06 | global batch size:     2 | lm loss: 7.502300E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.742 | TFLOPs: 4.33 |
[default0]:[2023-07-30 22:29:49,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=2110, skipped=0, lr=[4.6071808e-06, 4.6071808e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:49,526] [INFO] [timer.py:215:stop] epoch=0/micro_step=2110/global_step=2110, RunningAvgSamplesPerSec=12.357616713721464, CurrSamplesPerSec=12.3525008025304, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2110/219726562 | consumed samples:         4220 | consumed tokens:      8642560 | elapsed time per iteration (ms): 198.2 | learning rate: 4.607E-06 | global batch size:     2 | lm loss: 7.534331E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.091 | TFLOPs: 25.07 |
[default0]:[2023-07-30 22:29:50,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=2115, skipped=0, lr=[4.618103466666667e-06, 4.618103466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:50,646] [INFO] [timer.py:215:stop] epoch=0/micro_step=2115/global_step=2115, RunningAvgSamplesPerSec=12.35769595346904, CurrSamplesPerSec=12.417449485604322, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2115/219726562 | consumed samples:         4230 | consumed tokens:      8663040 | elapsed time per iteration (ms): 224.2 | learning rate: 4.618E-06 | global batch size:     2 | lm loss: 7.510568E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.919 | TFLOPs: 22.16 |
[default0]:[2023-07-30 22:29:51,701] [INFO] [logging.py:96:log_dist] [Rank 0] step=2120, skipped=0, lr=[4.629026133333334e-06, 4.629026133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:51,702] [INFO] [timer.py:215:stop] epoch=0/micro_step=2120/global_step=2120, RunningAvgSamplesPerSec=12.357783458455623, CurrSamplesPerSec=12.249397650178588, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2120/219726562 | consumed samples:         4240 | consumed tokens:      8683520 | elapsed time per iteration (ms): 211.2 | learning rate: 4.629E-06 | global batch size:     2 | lm loss: 7.479326E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.472 | TFLOPs: 23.53 |
[default0]:[2023-07-30 22:29:52,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=2125, skipped=0, lr=[4.6399488e-06, 4.6399488e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:52,815] [INFO] [timer.py:215:stop] epoch=0/micro_step=2125/global_step=2125, RunningAvgSamplesPerSec=12.357920992392538, CurrSamplesPerSec=12.458464399117219, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2125/219726562 | consumed samples:         4250 | consumed tokens:      8704000 | elapsed time per iteration (ms): 222.4 | learning rate: 4.640E-06 | global batch size:     2 | lm loss: 7.826080E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.994 | TFLOPs: 22.35 |
[default0]:[2023-07-30 22:29:53,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=2130, skipped=0, lr=[4.650871466666667e-06, 4.650871466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:53,938] [INFO] [timer.py:215:stop] epoch=0/micro_step=2130/global_step=2130, RunningAvgSamplesPerSec=12.357991476618244, CurrSamplesPerSec=12.142163603361578, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2130/219726562 | consumed samples:         4260 | consumed tokens:      8724480 | elapsed time per iteration (ms): 224.9 | learning rate: 4.651E-06 | global batch size:     2 | lm loss: 7.618915E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.893 | TFLOPs: 22.10 |
[default0]:[2023-07-30 22:29:55,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=2135, skipped=0, lr=[4.6617941333333335e-06, 4.6617941333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:55,476] [INFO] [timer.py:215:stop] epoch=0/micro_step=2135/global_step=2135, RunningAvgSamplesPerSec=12.358150621113712, CurrSamplesPerSec=12.367014838457626, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2135/219726562 | consumed samples:         4270 | consumed tokens:      8744960 | elapsed time per iteration (ms): 307.5 | learning rate: 4.662E-06 | global batch size:     2 | lm loss: 7.505127E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 6.504 | TFLOPs: 16.16 |
[default0]:[2023-07-30 22:29:56,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=2140, skipped=0, lr=[4.6727168e-06, 4.6727168e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:56,755] [INFO] [timer.py:215:stop] epoch=0/micro_step=2140/global_step=2140, RunningAvgSamplesPerSec=12.358247005765005, CurrSamplesPerSec=12.280456633571761, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2140/219726562 | consumed samples:         4280 | consumed tokens:      8765440 | elapsed time per iteration (ms): 255.8 | learning rate: 4.673E-06 | global batch size:     2 | lm loss: 7.432839E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 7.819 | TFLOPs: 19.43 |
[default0]:[2023-07-30 22:29:57,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=2145, skipped=0, lr=[4.683639466666667e-06, 4.683639466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:57,912] [INFO] [timer.py:215:stop] epoch=0/micro_step=2145/global_step=2145, RunningAvgSamplesPerSec=12.358315984773016, CurrSamplesPerSec=12.192549247032387, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2145/219726562 | consumed samples:         4290 | consumed tokens:      8785920 | elapsed time per iteration (ms): 231.4 | learning rate: 4.684E-06 | global batch size:     2 | lm loss: 7.366193E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.642 | TFLOPs: 21.47 |
[default0]:[2023-07-30 22:29:59,105] [INFO] [logging.py:96:log_dist] [Rank 0] step=2150, skipped=0, lr=[4.694562133333333e-06, 4.694562133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:59,105] [INFO] [timer.py:215:stop] epoch=0/micro_step=2150/global_step=2150, RunningAvgSamplesPerSec=12.358460853055178, CurrSamplesPerSec=12.45345213829634, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2150/219726562 | consumed samples:         4300 | consumed tokens:      8806400 | elapsed time per iteration (ms): 238.3 | learning rate: 4.695E-06 | global batch size:     2 | lm loss: 7.493090E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.394 | TFLOPs: 20.86 |
[default0]:[2023-07-30 22:30:00,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=2155, skipped=0, lr=[4.7054848e-06, 4.7054848e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:00,250] [INFO] [timer.py:215:stop] epoch=0/micro_step=2155/global_step=2155, RunningAvgSamplesPerSec=12.358619090112004, CurrSamplesPerSec=12.435231194336065, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2155/219726562 | consumed samples:         4310 | consumed tokens:      8826880 | elapsed time per iteration (ms): 229.4 | learning rate: 4.705E-06 | global batch size:     2 | lm loss: 7.633286E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.719 | TFLOPs: 21.67 |
[default0]:[2023-07-30 22:30:01,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=2160, skipped=0, lr=[4.716407466666667e-06, 4.716407466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:01,380] [INFO] [timer.py:215:stop] epoch=0/micro_step=2160/global_step=2160, RunningAvgSamplesPerSec=12.358870431185482, CurrSamplesPerSec=12.532618601551972, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2160/219726562 | consumed samples:         4320 | consumed tokens:      8847360 | elapsed time per iteration (ms): 225.6 | learning rate: 4.716E-06 | global batch size:     2 | lm loss: 7.456359E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.864 | TFLOPs: 22.03 |
[default0]:[2023-07-30 22:30:02,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=2165, skipped=0, lr=[4.727330133333333e-06, 4.727330133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:02,624] [INFO] [timer.py:215:stop] epoch=0/micro_step=2165/global_step=2165, RunningAvgSamplesPerSec=12.359154285617928, CurrSamplesPerSec=12.424603909289784, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2165/219726562 | consumed samples:         4330 | consumed tokens:      8867840 | elapsed time per iteration (ms): 249.2 | learning rate: 4.727E-06 | global batch size:     2 | lm loss: 7.474720E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.027 | TFLOPs: 19.95 |
[default0]:[2023-07-30 22:30:03,725] [INFO] [logging.py:96:log_dist] [Rank 0] step=2170, skipped=0, lr=[4.7382528e-06, 4.7382528e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:03,726] [INFO] [timer.py:215:stop] epoch=0/micro_step=2170/global_step=2170, RunningAvgSamplesPerSec=12.359442807283704, CurrSamplesPerSec=12.43845796838417, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2170/219726562 | consumed samples:         4340 | consumed tokens:      8888320 | elapsed time per iteration (ms): 220.0 | learning rate: 4.738E-06 | global batch size:     2 | lm loss: 7.586591E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.091 | TFLOPs: 22.59 |
[default0]:[2023-07-30 22:30:04,750] [INFO] [logging.py:96:log_dist] [Rank 0] step=2175, skipped=0, lr=[4.7491754666666665e-06, 4.7491754666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:04,750] [INFO] [timer.py:215:stop] epoch=0/micro_step=2175/global_step=2175, RunningAvgSamplesPerSec=12.359508556255816, CurrSamplesPerSec=12.391274740500785, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2175/219726562 | consumed samples:         4350 | consumed tokens:      8908800 | elapsed time per iteration (ms): 205.0 | learning rate: 4.749E-06 | global batch size:     2 | lm loss: 7.508536E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.756 | TFLOPs: 24.24 |
[default0]:[2023-07-30 22:30:05,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=2180, skipped=0, lr=[4.760098133333333e-06, 4.760098133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:05,820] [INFO] [timer.py:215:stop] epoch=0/micro_step=2180/global_step=2180, RunningAvgSamplesPerSec=12.359571554266731, CurrSamplesPerSec=12.229949541264341, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2180/219726562 | consumed samples:         4360 | consumed tokens:      8929280 | elapsed time per iteration (ms): 213.7 | learning rate: 4.760E-06 | global batch size:     2 | lm loss: 7.516414E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.358 | TFLOPs: 23.25 |
[default0]:[2023-07-30 22:30:07,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=2185, skipped=0, lr=[4.771020800000001e-06, 4.771020800000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:07,031] [INFO] [timer.py:215:stop] epoch=0/micro_step=2185/global_step=2185, RunningAvgSamplesPerSec=12.359630118382933, CurrSamplesPerSec=12.34048925875636, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2185/219726562 | consumed samples:         4370 | consumed tokens:      8949760 | elapsed time per iteration (ms): 242.5 | learning rate: 4.771E-06 | global batch size:     2 | lm loss: 7.673821E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.247 | TFLOPs: 20.49 |
[default0]:[2023-07-30 22:30:08,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=2190, skipped=0, lr=[4.781943466666667e-06, 4.781943466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:08,088] [INFO] [timer.py:215:stop] epoch=0/micro_step=2190/global_step=2190, RunningAvgSamplesPerSec=12.359640050856365, CurrSamplesPerSec=12.191769553306058, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2190/219726562 | consumed samples:         4380 | consumed tokens:      8970240 | elapsed time per iteration (ms): 211.5 | learning rate: 4.782E-06 | global batch size:     2 | lm loss: 7.422240E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.458 | TFLOPs: 23.50 |
[default0]:[2023-07-30 22:30:09,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=2195, skipped=0, lr=[4.792866133333334e-06, 4.792866133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:09,073] [INFO] [timer.py:215:stop] epoch=0/micro_step=2195/global_step=2195, RunningAvgSamplesPerSec=12.359700899497646, CurrSamplesPerSec=12.385401994395377, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2195/219726562 | consumed samples:         4390 | consumed tokens:      8990720 | elapsed time per iteration (ms): 196.9 | learning rate: 4.793E-06 | global batch size:     2 | lm loss: 7.402532E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.155 | TFLOPs: 25.23 |
[default0]:[2023-07-30 22:30:10,202] [INFO] [logging.py:96:log_dist] [Rank 0] step=2200, skipped=0, lr=[4.8037888000000005e-06, 4.8037888000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:10,203] [INFO] [timer.py:215:stop] epoch=0/micro_step=2200/global_step=2200, RunningAvgSamplesPerSec=12.359843454418751, CurrSamplesPerSec=12.327885042133387, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2200/219726562 | consumed samples:         4400 | consumed tokens:      9011200 | elapsed time per iteration (ms): 225.6 | learning rate: 4.804E-06 | global batch size:     2 | lm loss: 7.558527E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.863 | TFLOPs: 22.02 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 2200 | lm loss value: 7.551963E+00 | lm loss PPL: 1.904477E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:30:13,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=2205, skipped=0, lr=[4.814711466666667e-06, 4.814711466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:13,186] [INFO] [timer.py:215:stop] epoch=0/micro_step=2205/global_step=2205, RunningAvgSamplesPerSec=12.359840485246227, CurrSamplesPerSec=12.337567139856807, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2205/219726562 | consumed samples:         4410 | consumed tokens:      9031680 | elapsed time per iteration (ms): 597.0 | learning rate: 4.815E-06 | global batch size:     2 | lm loss: 7.495065E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.350 | TFLOPs: 8.32 |
[default0]:[2023-07-30 22:30:14,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=2210, skipped=0, lr=[4.825634133333334e-06, 4.825634133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:14,114] [INFO] [timer.py:215:stop] epoch=0/micro_step=2210/global_step=2210, RunningAvgSamplesPerSec=12.359867477248791, CurrSamplesPerSec=12.202907363254571, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2210/219726562 | consumed samples:         4420 | consumed tokens:      9052160 | elapsed time per iteration (ms): 185.4 | learning rate: 4.826E-06 | global batch size:     2 | lm loss: 7.545853E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.789 | TFLOPs: 26.81 |
[default0]:[2023-07-30 22:30:15,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=2215, skipped=0, lr=[4.8365568e-06, 4.8365568e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:15,030] [INFO] [timer.py:215:stop] epoch=0/micro_step=2215/global_step=2215, RunningAvgSamplesPerSec=12.359889031330189, CurrSamplesPerSec=12.389810606787908, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2215/219726562 | consumed samples:         4430 | consumed tokens:      9072640 | elapsed time per iteration (ms): 183.3 | learning rate: 4.837E-06 | global batch size:     2 | lm loss: 7.396403E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.910 | TFLOPs: 27.11 |
[default0]:[2023-07-30 22:30:15,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=2220, skipped=0, lr=[4.847479466666667e-06, 4.847479466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:15,969] [INFO] [timer.py:215:stop] epoch=0/micro_step=2220/global_step=2220, RunningAvgSamplesPerSec=12.359635395545357, CurrSamplesPerSec=12.227293398789604, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2220/219726562 | consumed samples:         4440 | consumed tokens:      9093120 | elapsed time per iteration (ms): 187.6 | learning rate: 4.847E-06 | global batch size:     2 | lm loss: 7.366402E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.660 | TFLOPs: 26.49 |
[default0]:[2023-07-30 22:30:16,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=2225, skipped=0, lr=[4.858402133333334e-06, 4.858402133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:16,905] [INFO] [timer.py:215:stop] epoch=0/micro_step=2225/global_step=2225, RunningAvgSamplesPerSec=12.359638196696793, CurrSamplesPerSec=12.385054560862935, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2225/219726562 | consumed samples:         4450 | consumed tokens:      9113600 | elapsed time per iteration (ms): 187.5 | learning rate: 4.858E-06 | global batch size:     2 | lm loss: 7.899351E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.669 | TFLOPs: 26.51 |
[default0]:[2023-07-30 22:30:17,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=2230, skipped=0, lr=[4.8693248e-06, 4.8693248e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:17,825] [INFO] [timer.py:215:stop] epoch=0/micro_step=2230/global_step=2230, RunningAvgSamplesPerSec=12.359736574781381, CurrSamplesPerSec=12.391805574431123, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2230/219726562 | consumed samples:         4460 | consumed tokens:      9134080 | elapsed time per iteration (ms): 183.7 | learning rate: 4.869E-06 | global batch size:     2 | lm loss: 7.349802E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.884 | TFLOPs: 27.05 |
[default0]:[2023-07-30 22:30:18,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=2235, skipped=0, lr=[4.880247466666667e-06, 4.880247466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:18,759] [INFO] [timer.py:215:stop] epoch=0/micro_step=2235/global_step=2235, RunningAvgSamplesPerSec=12.35961746098709, CurrSamplesPerSec=12.323791950449841, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2235/219726562 | consumed samples:         4470 | consumed tokens:      9154560 | elapsed time per iteration (ms): 186.8 | learning rate: 4.880E-06 | global batch size:     2 | lm loss: 7.463948E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.709 | TFLOPs: 26.61 |
[default0]:[2023-07-30 22:30:19,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=2240, skipped=0, lr=[4.8911701333333336e-06, 4.8911701333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:19,668] [INFO] [timer.py:215:stop] epoch=0/micro_step=2240/global_step=2240, RunningAvgSamplesPerSec=12.359795514572824, CurrSamplesPerSec=12.475825749677641, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2240/219726562 | consumed samples:         4480 | consumed tokens:      9175040 | elapsed time per iteration (ms): 181.6 | learning rate: 4.891E-06 | global batch size:     2 | lm loss: 7.479540E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 11.012 | TFLOPs: 27.36 |
[default0]:[2023-07-30 22:30:20,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=2245, skipped=0, lr=[4.9020928e-06, 4.9020928e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:20,588] [INFO] [timer.py:215:stop] epoch=0/micro_step=2245/global_step=2245, RunningAvgSamplesPerSec=12.35999562704417, CurrSamplesPerSec=12.425174410854206, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2245/219726562 | consumed samples:         4490 | consumed tokens:      9195520 | elapsed time per iteration (ms): 183.9 | learning rate: 4.902E-06 | global batch size:     2 | lm loss: 7.546375E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.874 | TFLOPs: 27.02 |
[default0]:[2023-07-30 22:30:21,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=2250, skipped=0, lr=[4.913015466666667e-06, 4.913015466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:21,503] [INFO] [timer.py:215:stop] epoch=0/micro_step=2250/global_step=2250, RunningAvgSamplesPerSec=12.360059918974857, CurrSamplesPerSec=12.256216423133173, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2250/219726562 | consumed samples:         4500 | consumed tokens:      9216000 | elapsed time per iteration (ms): 183.3 | learning rate: 4.913E-06 | global batch size:     2 | lm loss: 7.610121E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.913 | TFLOPs: 27.12 |
[default0]:[2023-07-30 22:30:22,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=2255, skipped=0, lr=[4.9239381333333334e-06, 4.9239381333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:22,426] [INFO] [timer.py:215:stop] epoch=0/micro_step=2255/global_step=2255, RunningAvgSamplesPerSec=12.360074884360161, CurrSamplesPerSec=12.332289055666223, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2255/219726562 | consumed samples:         4510 | consumed tokens:      9236480 | elapsed time per iteration (ms): 184.5 | learning rate: 4.924E-06 | global batch size:     2 | lm loss: 7.447829E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.841 | TFLOPs: 26.94 |
[default0]:[2023-07-30 22:30:23,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=2260, skipped=0, lr=[4.9348608e-06, 4.9348608e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:23,339] [INFO] [timer.py:215:stop] epoch=0/micro_step=2260/global_step=2260, RunningAvgSamplesPerSec=12.36009268707239, CurrSamplesPerSec=12.361055771021702, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2260/219726562 | consumed samples:         4520 | consumed tokens:      9256960 | elapsed time per iteration (ms): 182.5 | learning rate: 4.935E-06 | global batch size:     2 | lm loss: 7.557327E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.957 | TFLOPs: 27.23 |
[default0]:[2023-07-30 22:30:24,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=2265, skipped=0, lr=[4.945783466666667e-06, 4.945783466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:24,254] [INFO] [timer.py:215:stop] epoch=0/micro_step=2265/global_step=2265, RunningAvgSamplesPerSec=12.360062793277478, CurrSamplesPerSec=12.319773948239552, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2265/219726562 | consumed samples:         4530 | consumed tokens:      9277440 | elapsed time per iteration (ms): 182.9 | learning rate: 4.946E-06 | global batch size:     2 | lm loss: 7.385040E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.935 | TFLOPs: 27.17 |
[default0]:[2023-07-30 22:30:25,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=2270, skipped=0, lr=[4.956706133333333e-06, 4.956706133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:25,173] [INFO] [timer.py:215:stop] epoch=0/micro_step=2270/global_step=2270, RunningAvgSamplesPerSec=12.360113033140056, CurrSamplesPerSec=12.385639723307028, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2270/219726562 | consumed samples:         4540 | consumed tokens:      9297920 | elapsed time per iteration (ms): 184.0 | learning rate: 4.957E-06 | global batch size:     2 | lm loss: 7.279546E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.870 | TFLOPs: 27.01 |
[default0]:[2023-07-30 22:30:26,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=2275, skipped=0, lr=[4.967628800000001e-06, 4.967628800000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:26,105] [INFO] [timer.py:215:stop] epoch=0/micro_step=2275/global_step=2275, RunningAvgSamplesPerSec=12.360076704055446, CurrSamplesPerSec=12.243962389180322, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2275/219726562 | consumed samples:         4550 | consumed tokens:      9318400 | elapsed time per iteration (ms): 186.5 | learning rate: 4.968E-06 | global batch size:     2 | lm loss: 7.480317E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.725 | TFLOPs: 26.65 |
[default0]:[2023-07-30 22:30:27,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=2280, skipped=0, lr=[4.9785514666666674e-06, 4.9785514666666674e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:27,019] [INFO] [timer.py:215:stop] epoch=0/micro_step=2280/global_step=2280, RunningAvgSamplesPerSec=12.360123159296009, CurrSamplesPerSec=12.378109981304329, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2280/219726562 | consumed samples:         4560 | consumed tokens:      9338880 | elapsed time per iteration (ms): 182.8 | learning rate: 4.979E-06 | global batch size:     2 | lm loss: 7.312150E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.944 | TFLOPs: 27.19 |
[default0]:[2023-07-30 22:30:27,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=2285, skipped=0, lr=[4.989474133333334e-06, 4.989474133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:27,942] [INFO] [timer.py:215:stop] epoch=0/micro_step=2285/global_step=2285, RunningAvgSamplesPerSec=12.360184440551642, CurrSamplesPerSec=12.382568237206124, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2285/219726562 | consumed samples:         4570 | consumed tokens:      9359360 | elapsed time per iteration (ms): 184.5 | learning rate: 4.989E-06 | global batch size:     2 | lm loss: 7.352258E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.842 | TFLOPs: 26.94 |
[default0]:[2023-07-30 22:30:28,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=2290, skipped=0, lr=[5.000396800000001e-06, 5.000396800000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:28,864] [INFO] [timer.py:215:stop] epoch=0/micro_step=2290/global_step=2290, RunningAvgSamplesPerSec=12.360094218094561, CurrSamplesPerSec=12.284574954932, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2290/219726562 | consumed samples:         4580 | consumed tokens:      9379840 | elapsed time per iteration (ms): 184.3 | learning rate: 5.000E-06 | global batch size:     2 | lm loss: 7.210756E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.850 | TFLOPs: 26.96 |
[default0]:[2023-07-30 22:30:29,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=2295, skipped=0, lr=[5.011319466666667e-06, 5.011319466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:29,775] [INFO] [timer.py:215:stop] epoch=0/micro_step=2295/global_step=2295, RunningAvgSamplesPerSec=12.360132841682853, CurrSamplesPerSec=12.380393522754778, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2295/219726562 | consumed samples:         4590 | consumed tokens:      9400320 | elapsed time per iteration (ms): 182.1 | learning rate: 5.011E-06 | global batch size:     2 | lm loss: 7.655469E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.981 | TFLOPs: 27.29 |
[default0]:[2023-07-30 22:30:30,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=2300, skipped=0, lr=[5.022242133333334e-06, 5.022242133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:30,690] [INFO] [timer.py:215:stop] epoch=0/micro_step=2300/global_step=2300, RunningAvgSamplesPerSec=12.360240674838469, CurrSamplesPerSec=12.41783550373115, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2300/219726562 | consumed samples:         4600 | consumed tokens:      9420800 | elapsed time per iteration (ms): 183.2 | learning rate: 5.022E-06 | global batch size:     2 | lm loss: 7.293797E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.915 | TFLOPs: 27.12 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 2300 | lm loss value: 7.556630E+00 | lm loss PPL: 1.913386E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:30:36,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=2305, skipped=0, lr=[5.0331648000000006e-06, 5.0331648000000006e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:36,119] [INFO] [timer.py:215:stop] epoch=0/micro_step=2305/global_step=2305, RunningAvgSamplesPerSec=12.3595217217701, CurrSamplesPerSec=12.21570189323831, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2305/219726562 | consumed samples:         4610 | consumed tokens:      9441280 | elapsed time per iteration (ms): 1085.5 | learning rate: 5.033E-06 | global batch size:     2 | lm loss: 7.454795E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.842 | TFLOPs: 4.58 |
[default0]:[2023-07-30 22:30:37,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=2310, skipped=0, lr=[5.044087466666667e-06, 5.044087466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:37,127] [INFO] [timer.py:215:stop] epoch=0/micro_step=2310/global_step=2310, RunningAvgSamplesPerSec=12.359707756479866, CurrSamplesPerSec=12.425708153298544, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2310/219726562 | consumed samples:         4620 | consumed tokens:      9461760 | elapsed time per iteration (ms): 201.7 | learning rate: 5.044E-06 | global batch size:     2 | lm loss: 7.152604E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.915 | TFLOPs: 24.64 |
[default0]:[2023-07-30 22:30:38,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=2315, skipped=0, lr=[5.055010133333334e-06, 5.055010133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:38,146] [INFO] [timer.py:215:stop] epoch=0/micro_step=2315/global_step=2315, RunningAvgSamplesPerSec=12.359865246553504, CurrSamplesPerSec=12.230555802604856, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2315/219726562 | consumed samples:         4630 | consumed tokens:      9482240 | elapsed time per iteration (ms): 204.0 | learning rate: 5.055E-06 | global batch size:     2 | lm loss: 7.463887E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.805 | TFLOPs: 24.36 |
[default0]:[2023-07-30 22:30:39,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=2320, skipped=0, lr=[5.0659328000000005e-06, 5.0659328000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:39,153] [INFO] [timer.py:215:stop] epoch=0/micro_step=2320/global_step=2320, RunningAvgSamplesPerSec=12.359994186005434, CurrSamplesPerSec=12.308566364305983, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2320/219726562 | consumed samples:         4640 | consumed tokens:      9502720 | elapsed time per iteration (ms): 201.5 | learning rate: 5.066E-06 | global batch size:     2 | lm loss: 7.314507E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.925 | TFLOPs: 24.66 |
[default0]:[2023-07-30 22:30:40,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=2325, skipped=0, lr=[5.076855466666667e-06, 5.076855466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:40,153] [INFO] [timer.py:215:stop] epoch=0/micro_step=2325/global_step=2325, RunningAvgSamplesPerSec=12.36007524422387, CurrSamplesPerSec=12.368765380583271, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2325/219726562 | consumed samples:         4650 | consumed tokens:      9523200 | elapsed time per iteration (ms): 199.7 | learning rate: 5.077E-06 | global batch size:     2 | lm loss: 7.304462E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.016 | TFLOPs: 24.89 |
[default0]:[2023-07-30 22:30:41,200] [INFO] [logging.py:96:log_dist] [Rank 0] step=2330, skipped=0, lr=[5.087778133333334e-06, 5.087778133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:41,201] [INFO] [timer.py:215:stop] epoch=0/micro_step=2330/global_step=2330, RunningAvgSamplesPerSec=12.360015056313832, CurrSamplesPerSec=12.232553323971471, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2330/219726562 | consumed samples:         4660 | consumed tokens:      9543680 | elapsed time per iteration (ms): 209.6 | learning rate: 5.088E-06 | global batch size:     2 | lm loss: 7.248518E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.541 | TFLOPs: 23.71 |
[default0]:[2023-07-30 22:30:42,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=2335, skipped=0, lr=[5.0987008e-06, 5.0987008e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:42,789] [INFO] [timer.py:215:stop] epoch=0/micro_step=2335/global_step=2335, RunningAvgSamplesPerSec=12.360021806275407, CurrSamplesPerSec=12.263795770262012, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2335/219726562 | consumed samples:         4670 | consumed tokens:      9564160 | elapsed time per iteration (ms): 317.7 | learning rate: 5.099E-06 | global batch size:     2 | lm loss: 7.405958E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 6.295 | TFLOPs: 15.64 |
[default0]:[2023-07-30 22:30:43,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=2340, skipped=0, lr=[5.109623466666667e-06, 5.109623466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:43,887] [INFO] [timer.py:215:stop] epoch=0/micro_step=2340/global_step=2340, RunningAvgSamplesPerSec=12.36009512718422, CurrSamplesPerSec=12.375352586420554, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2340/219726562 | consumed samples:         4680 | consumed tokens:      9584640 | elapsed time per iteration (ms): 219.3 | learning rate: 5.110E-06 | global batch size:     2 | lm loss: 7.315314E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.119 | TFLOPs: 22.66 |
[default0]:[2023-07-30 22:30:45,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=2345, skipped=0, lr=[5.120546133333334e-06, 5.120546133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:45,057] [INFO] [timer.py:215:stop] epoch=0/micro_step=2345/global_step=2345, RunningAvgSamplesPerSec=12.360076485701017, CurrSamplesPerSec=12.374841416424243, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2345/219726562 | consumed samples:         4690 | consumed tokens:      9605120 | elapsed time per iteration (ms): 234.2 | learning rate: 5.121E-06 | global batch size:     2 | lm loss: 7.192934E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.541 | TFLOPs: 21.22 |
[default0]:[2023-07-30 22:30:46,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=2350, skipped=0, lr=[5.1314688e-06, 5.1314688e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:46,172] [INFO] [timer.py:215:stop] epoch=0/micro_step=2350/global_step=2350, RunningAvgSamplesPerSec=12.35987418031945, CurrSamplesPerSec=12.263455125703, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2350/219726562 | consumed samples:         4700 | consumed tokens:      9625600 | elapsed time per iteration (ms): 223.1 | learning rate: 5.131E-06 | global batch size:     2 | lm loss: 7.379468E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.965 | TFLOPs: 22.28 |
[default0]:[2023-07-30 22:30:47,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=2355, skipped=0, lr=[5.142391466666667e-06, 5.142391466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:47,324] [INFO] [timer.py:215:stop] epoch=0/micro_step=2355/global_step=2355, RunningAvgSamplesPerSec=12.359937841488469, CurrSamplesPerSec=12.399132064539398, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2355/219726562 | consumed samples:         4710 | consumed tokens:      9646080 | elapsed time per iteration (ms): 230.2 | learning rate: 5.142E-06 | global batch size:     2 | lm loss: 7.245340E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.689 | TFLOPs: 21.59 |
[default0]:[2023-07-30 22:30:48,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=2360, skipped=0, lr=[5.1533141333333335e-06, 5.1533141333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:48,666] [INFO] [timer.py:215:stop] epoch=0/micro_step=2360/global_step=2360, RunningAvgSamplesPerSec=12.35985842330046, CurrSamplesPerSec=12.250596202123107, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2360/219726562 | consumed samples:         4720 | consumed tokens:      9666560 | elapsed time per iteration (ms): 268.6 | learning rate: 5.153E-06 | global batch size:     2 | lm loss: 7.429549E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 7.445 | TFLOPs: 18.50 |
[default0]:[2023-07-30 22:30:49,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=2365, skipped=0, lr=[5.1642368e-06, 5.1642368e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:49,991] [INFO] [timer.py:215:stop] epoch=0/micro_step=2365/global_step=2365, RunningAvgSamplesPerSec=12.359828071801433, CurrSamplesPerSec=12.249612298957661, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2365/219726562 | consumed samples:         4730 | consumed tokens:      9687040 | elapsed time per iteration (ms): 274.3 | learning rate: 5.164E-06 | global batch size:     2 | lm loss: 7.260390E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 7.291 | TFLOPs: 18.12 |
[default0]:[2023-07-30 22:30:51,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=2370, skipped=0, lr=[5.175159466666667e-06, 5.175159466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:51,307] [INFO] [timer.py:215:stop] epoch=0/micro_step=2370/global_step=2370, RunningAvgSamplesPerSec=12.359886120114423, CurrSamplesPerSec=12.411735549173573, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2370/219726562 | consumed samples:         4740 | consumed tokens:      9707520 | elapsed time per iteration (ms): 253.8 | learning rate: 5.175E-06 | global batch size:     2 | lm loss: 7.345985E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 7.881 | TFLOPs: 19.58 |
[default0]:[2023-07-30 22:30:52,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=2375, skipped=0, lr=[5.186082133333333e-06, 5.186082133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:52,547] [INFO] [timer.py:215:stop] epoch=0/micro_step=2375/global_step=2375, RunningAvgSamplesPerSec=12.359991866614802, CurrSamplesPerSec=12.327866925119661, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2375/219726562 | consumed samples:         4750 | consumed tokens:      9728000 | elapsed time per iteration (ms): 249.7 | learning rate: 5.186E-06 | global batch size:     2 | lm loss: 7.690479E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.010 | TFLOPs: 19.90 |
[default0]:[2023-07-30 22:30:53,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=2380, skipped=0, lr=[5.1970048e-06, 5.1970048e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:53,796] [INFO] [timer.py:215:stop] epoch=0/micro_step=2380/global_step=2380, RunningAvgSamplesPerSec=12.360106352738978, CurrSamplesPerSec=12.397684396277418, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2380/219726562 | consumed samples:         4760 | consumed tokens:      9748480 | elapsed time per iteration (ms): 248.1 | learning rate: 5.197E-06 | global batch size:     2 | lm loss: 7.327357E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.061 | TFLOPs: 20.03 |
[default0]:[2023-07-30 22:30:54,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=2385, skipped=0, lr=[5.207927466666667e-06, 5.207927466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:54,956] [INFO] [timer.py:215:stop] epoch=0/micro_step=2385/global_step=2385, RunningAvgSamplesPerSec=12.36020839231507, CurrSamplesPerSec=12.463202935801625, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2385/219726562 | consumed samples:         4770 | consumed tokens:      9768960 | elapsed time per iteration (ms): 231.8 | learning rate: 5.208E-06 | global batch size:     2 | lm loss: 7.360835E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.629 | TFLOPs: 21.44 |
[default0]:[2023-07-30 22:30:56,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=2390, skipped=0, lr=[5.218850133333333e-06, 5.218850133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:56,059] [INFO] [timer.py:215:stop] epoch=0/micro_step=2390/global_step=2390, RunningAvgSamplesPerSec=12.360346065227471, CurrSamplesPerSec=12.218388594666713, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2390/219726562 | consumed samples:         4780 | consumed tokens:      9789440 | elapsed time per iteration (ms): 220.8 | learning rate: 5.219E-06 | global batch size:     2 | lm loss: 7.397519E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.059 | TFLOPs: 22.51 |
[default0]:[2023-07-30 22:30:57,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=2395, skipped=0, lr=[5.2297728e-06, 5.2297728e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:57,349] [INFO] [timer.py:215:stop] epoch=0/micro_step=2395/global_step=2395, RunningAvgSamplesPerSec=12.360530977982268, CurrSamplesPerSec=12.390011904656404, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2395/219726562 | consumed samples:         4790 | consumed tokens:      9809920 | elapsed time per iteration (ms): 257.9 | learning rate: 5.230E-06 | global batch size:     2 | lm loss: 7.348403E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 7.755 | TFLOPs: 19.27 |
[default0]:[2023-07-30 22:30:58,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=2400, skipped=0, lr=[5.2406954666666665e-06, 5.2406954666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:58,509] [INFO] [timer.py:215:stop] epoch=0/micro_step=2400/global_step=2400, RunningAvgSamplesPerSec=12.360476126686976, CurrSamplesPerSec=12.233819947789817, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2400/219726562 | consumed samples:         4800 | consumed tokens:      9830400 | elapsed time per iteration (ms): 232.5 | learning rate: 5.241E-06 | global batch size:     2 | lm loss: 7.401346E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 8.601 | TFLOPs: 21.37 |
[default0]:------------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 2400 | lm loss value: 7.526165E+00 | lm loss PPL: 1.855973E+03 | 
[default0]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:31:02,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=2405, skipped=0, lr=[5.251618133333333e-06, 5.251618133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:02,055] [INFO] [timer.py:215:stop] epoch=0/micro_step=2405/global_step=2405, RunningAvgSamplesPerSec=12.360584405850934, CurrSamplesPerSec=12.393270179710077, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2405/219726562 | consumed samples:         4810 | consumed tokens:      9850880 | elapsed time per iteration (ms): 708.7 | learning rate: 5.252E-06 | global batch size:     2 | lm loss: 7.318053E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.822 | TFLOPs: 7.01 |
[default0]:[2023-07-30 22:31:03,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=2410, skipped=0, lr=[5.2625408e-06, 5.2625408e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:03,087] [INFO] [timer.py:215:stop] epoch=0/micro_step=2410/global_step=2410, RunningAvgSamplesPerSec=12.360593810479068, CurrSamplesPerSec=12.322633338621634, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2410/219726562 | consumed samples:         4820 | consumed tokens:      9871360 | elapsed time per iteration (ms): 206.4 | learning rate: 5.263E-06 | global batch size:     2 | lm loss: 7.403439E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.689 | TFLOPs: 24.08 |
[default0]:[2023-07-30 22:31:04,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=2415, skipped=0, lr=[5.273463466666666e-06, 5.273463466666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:04,129] [INFO] [timer.py:215:stop] epoch=0/micro_step=2415/global_step=2415, RunningAvgSamplesPerSec=12.360655650091271, CurrSamplesPerSec=12.341288091108238, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2415/219726562 | consumed samples:         4830 | consumed tokens:      9891840 | elapsed time per iteration (ms): 208.4 | learning rate: 5.273E-06 | global batch size:     2 | lm loss: 7.347931E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.597 | TFLOPs: 23.85 |
[default0]:[2023-07-30 22:31:05,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=2420, skipped=0, lr=[5.284386133333333e-06, 5.284386133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:05,096] [INFO] [timer.py:215:stop] epoch=0/micro_step=2420/global_step=2420, RunningAvgSamplesPerSec=12.360764983493956, CurrSamplesPerSec=12.433259077118265, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2420/219726562 | consumed samples:         4840 | consumed tokens:      9912320 | elapsed time per iteration (ms): 193.3 | learning rate: 5.284E-06 | global batch size:     2 | lm loss: 7.452756E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.347 | TFLOPs: 25.71 |
[default0]:[2023-07-30 22:31:06,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=2425, skipped=0, lr=[5.2953088000000005e-06, 5.2953088000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:06,046] [INFO] [timer.py:215:stop] epoch=0/micro_step=2425/global_step=2425, RunningAvgSamplesPerSec=12.360837642395927, CurrSamplesPerSec=12.358014670027504, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2425/219726562 | consumed samples:         4850 | consumed tokens:      9932800 | elapsed time per iteration (ms): 190.1 | learning rate: 5.295E-06 | global batch size:     2 | lm loss: 7.415083E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.521 | TFLOPs: 26.14 |
[default0]:[2023-07-30 22:31:07,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=2430, skipped=0, lr=[5.306231466666667e-06, 5.306231466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:07,019] [INFO] [timer.py:215:stop] epoch=0/micro_step=2430/global_step=2430, RunningAvgSamplesPerSec=12.360885697286216, CurrSamplesPerSec=12.321257769996357, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2430/219726562 | consumed samples:         4860 | consumed tokens:      9953280 | elapsed time per iteration (ms): 194.4 | learning rate: 5.306E-06 | global batch size:     2 | lm loss: 7.235751E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.286 | TFLOPs: 25.56 |
[default0]:[2023-07-30 22:31:07,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=2435, skipped=0, lr=[5.317154133333334e-06, 5.317154133333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:07,937] [INFO] [timer.py:215:stop] epoch=0/micro_step=2435/global_step=2435, RunningAvgSamplesPerSec=12.3609689431459, CurrSamplesPerSec=12.370954255267376, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2435/219726562 | consumed samples:         4870 | consumed tokens:      9973760 | elapsed time per iteration (ms): 183.7 | learning rate: 5.317E-06 | global batch size:     2 | lm loss: 7.244566E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.887 | TFLOPs: 27.05 |
[default0]:[2023-07-30 22:31:08,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=2440, skipped=0, lr=[5.3280768e-06, 5.3280768e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:08,901] [INFO] [timer.py:215:stop] epoch=0/micro_step=2440/global_step=2440, RunningAvgSamplesPerSec=12.360917175248332, CurrSamplesPerSec=12.323303134516705, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2440/219726562 | consumed samples:         4880 | consumed tokens:      9994240 | elapsed time per iteration (ms): 192.7 | learning rate: 5.328E-06 | global batch size:     2 | lm loss: 7.291463E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.378 | TFLOPs: 25.79 |
[default0]:[2023-07-30 22:31:09,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=2445, skipped=0, lr=[5.338999466666667e-06, 5.338999466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:09,872] [INFO] [timer.py:215:stop] epoch=0/micro_step=2445/global_step=2445, RunningAvgSamplesPerSec=12.360961171789652, CurrSamplesPerSec=12.427051914883531, MemAllocated=1.82GB, MaxMemAllocated=4.0GB
[default0]: iteration     2445/219726562 | consumed samples:         4890 | consumed tokens:     10014720 | elapsed time per iteration (ms): 194.3 | learning rate: 5.339E-06 | global batch size:     2 | lm loss: 7.356572E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 10.292 | TFLOPs: 25.57 |
slurmstepd: error: *** JOB 4504979 ON n2gpu1227 CANCELLED AT 2023-07-30T22:31:10 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 4504979.0 ON n2gpu1227 CANCELLED AT 2023-07-30T22:31:10 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3528636 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3528636 closing signal SIGTERM
