cpu-bind=MASK - n2gpu1225, task  0  0 [413533]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||--------|--------||--------|--------||B-------|--------||--------|--------|  set
Total estimated parameters in the Dense GPT-2 model: 760300032 (0.76B)
Total Estimated Parameters in the Sparse(MoE) GPT-2 model: 29763125760 (29.76B)
LAUNCHER: python -u -m torch.distributed.run --nproc_per_node 1 --nnodes 1 --rdzv_id=658 --rdzv_endpoint n2gpu1225:6030 --rdzv_backend c10d --max_restarts 0 --tee 3
CMD: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/pretrain_gpt.py --override-opt_param-scheduler --adam-beta1 0.9 --adam-beta2 0.95 --tensor-model-parallel-size 1 --moe-expert-parallel-size 1 --num-experts 128 --moe-loss-coeff 0.01 --moe-train-capacity-factor 1.0 --moe-eval-capacity-factor 1.0 --moe-min-capacity 4 --init-method-std 0.01 --lr-decay-tokens 300000000000 --lr-warmup-tokens 375000000 --micro-batch-size 2 --exit-duration-in-mins 30000000 --global-batch-size 2 --num-layers 24 --hidden-size 1536 --num-attention-heads 16 --seq-length 2048 --max-position-embeddings 2048 --train-tokens 300000000000 --train-iters 219726562 --lr 2.0e-4 --min-lr 2e-06 --lr-decay-style cosine --split 98,2,0 --log-interval 5 --eval-interval 100 --eval-iters 50 --save-interval 1000000 --weight-decay 0.1 --clip-grad 1.0 --hysteresis 2 --num-workers 0 --fp16 --load /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.76B_MoE128_2BATCH_1GPU_1Node/checkpoint/gpt-0.76B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true --save /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.76B_MoE128_2BATCH_1GPU_1Node/checkpoint/gpt-0.76B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true --tensorboard-queue-size 1 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.76B_MoE128_2BATCH_1GPU_1Node/tensorboard/gpt-0.76B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true_n2gpu1225_2023.07.30-22.21.45 --checkpoint-activations --create-moe-param-group --vocab-file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-vocab.json --merge-file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-merges.txt --data-path /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/meg-gpt2-oscar-en-10k_text_document --data-impl mmap --deepspeed --deepspeed_config /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.76B_MoE128_2BATCH_1GPU_1Node/ds_config_gpt_gpt-0.76B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true.json --pipeline-model-parallel-size 1 --no-pipeline-parallel --deepspeed-activation-checkpointing
cpu-bind=MASK - n2gpu1225, task  0  0 [413770]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||--------|--------||--------|--------||B-------|--------||--------|--------|  set
[default0]:[2023-07-30 22:21:48,029] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.10.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:**** Git info for Megatron: git_hash=6e47d55 git_branch=main ****
[default0]:using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
[default0]:using torch.float16 for parameters ...
[default0]:------------------------ arguments ------------------------
[default0]:  accumulate_allreduce_grads_in_fp32 .............. False
[default0]:  adam_beta1 ...................................... 0.9
[default0]:  adam_beta2 ...................................... 0.95
[default0]:  adam_eps ........................................ 1e-08
[default0]:  add_bias_linear ................................. True
[default0]:  add_position_embedding .......................... True
[default0]:  adlr_autoresume ................................. False
[default0]:  adlr_autoresume_interval ........................ 1000
[default0]:  aml_data_download_path .......................... None
[default0]:  apply_layernorm_1p .............................. False
[default0]:  apply_query_key_layer_scaling ................... True
[default0]:  apply_residual_connection_post_layernorm ........ False
[default0]:  async_tensor_model_parallel_allreduce ........... False
[default0]:  attention_dropout ............................... 0.1
[default0]:  attention_softmax_in_fp32 ....................... False
[default0]:  barrier_with_L1_time ............................ True
[default0]:  bert_binary_head ................................ True
[default0]:  bert_embedder_type .............................. megatron
[default0]:  bert_load ....................................... None
[default0]:  bf16 ............................................ False
[default0]:  bias_dropout_fusion ............................. True
[default0]:  bias_gelu_fusion ................................ True
[default0]:  biencoder_projection_dim ........................ 0
[default0]:  biencoder_shared_query_context_model ............ False
[default0]:  block_data_path ................................. None
[default0]:  checkpoint_activations .......................... True
[default0]:  checkpoint_in_cpu ............................... False
[default0]:  checkpoint_num_layers ........................... 1
[default0]:  classes_fraction ................................ 1.0
[default0]:  clip_grad ....................................... 1.0
[default0]:  compression_training ............................ False
[default0]:  consumed_train_samples .......................... 0
[default0]:  consumed_train_tokens ........................... 0
[default0]:  consumed_valid_samples .......................... 0
[default0]:  contigious_checkpointing ........................ False
[default0]:  cpu_optimizer ................................... False
[default0]:  cpu_torch_adam .................................. False
[default0]:  create_moe_param_group .......................... True
[default0]:  curriculum_learning_legacy ...................... False
[default0]:  data_cache_path ................................. None
[default0]:  data_efficiency_curriculum_learning ............. False
[default0]:  data_impl ....................................... mmap
[default0]:  data_parallel_random_init ....................... False
[default0]:  data_parallel_size .............................. 1
[default0]:  data_path ....................................... ['/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/meg-gpt2-oscar-en-10k_text_document']
[default0]:  data_per_class_fraction ......................... 1.0
[default0]:  data_sharding ................................... True
[default0]:  dataloader_type ................................. single
[default0]:  DDP_impl ........................................ local
[default0]:  decoder_num_layers .............................. None
[default0]:  decoder_seq_length .............................. None
[default0]:  deepscale ....................................... False
[default0]:  deepscale_config ................................ None
[default0]:  deepspeed ....................................... True
[default0]:  deepspeed_activation_checkpointing .............. True
[default0]:  deepspeed_config ................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.76B_MoE128_2BATCH_1GPU_1Node/ds_config_gpt_gpt-0.76B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true.json
[default0]:  deepspeed_mpi ................................... False
[default0]:  dino_bottleneck_size ............................ 256
[default0]:  dino_freeze_last_layer .......................... 1
[default0]:  dino_head_hidden_size ........................... 2048
[default0]:  dino_local_crops_number ......................... 10
[default0]:  dino_local_img_size ............................. 96
[default0]:  dino_norm_last_layer ............................ False
[default0]:  dino_teacher_temp ............................... 0.07
[default0]:  dino_warmup_teacher_temp ........................ 0.04
[default0]:  dino_warmup_teacher_temp_epochs ................. 30
[default0]:  distribute_checkpointed_activations ............. False
[default0]:  distribute_saved_activations .................... False
[default0]:  distributed_backend ............................. nccl
[default0]:  distributed_timeout_minutes ..................... 10
[default0]:  ds_inference .................................... False
[default0]:  ds_pipeline_enabled ............................. False
[default0]:  embedding_path .................................. None
[default0]:  embedding_weights_in_fp32 ....................... False
[default0]:  empty_unused_memory_level ....................... 0
[default0]:  enable_expert_tensor_parallelism ................ False
[default0]:  encoder_num_layers .............................. 24
[default0]:  encoder_seq_length .............................. 2048
[default0]:  end_weight_decay ................................ 0.1
[default0]:  eod_mask_loss ................................... False
[default0]:  eval_interval ................................... 100
[default0]:  eval_iters ...................................... 50
[default0]:  evidence_data_path .............................. None
[default0]:  exit_duration_in_mins ........................... 30000000
[default0]:  exit_interval ................................... None
[default0]:  exit_on_missing_checkpoint ...................... False
[default0]:  exit_signal_handler ............................. False
[default0]:  expert_interval ................................. 2
[default0]:  ffn_hidden_size ................................. 6144
[default0]:  finetune ........................................ False
[default0]:  fp16 ............................................ True
[default0]:  fp16_lm_cross_entropy ........................... False
[default0]:  fp32_residual_connection ........................ False
[default0]:  fp8_amax_compute_algo ........................... most_recent
[default0]:  fp8_amax_history_len ............................ 1
[default0]:  fp8_e4m3 ........................................ False
[default0]:  fp8_hybrid ...................................... False
[default0]:  fp8_interval .................................... 1
[default0]:  fp8_margin ...................................... 0
[default0]:  fp8_wgrad ....................................... True
[default0]:  global_batch_size ............................... 2
[default0]:  gradient_accumulation_fusion .................... True
[default0]:  head_lr_mult .................................... 1.0
[default0]:  hidden_dropout .................................. 0.1
[default0]:  hidden_size ..................................... 1536
[default0]:  hidden_size_teacher ............................. None
[default0]:  hysteresis ...................................... 2
[default0]:  ict_head_size ................................... None
[default0]:  ict_load ........................................ None
[default0]:  img_h ........................................... 224
[default0]:  img_w ........................................... 224
[default0]:  indexer_batch_size .............................. 128
[default0]:  indexer_log_interval ............................ 1000
[default0]:  inference ....................................... False
[default0]:  inference_batch_times_seqlen_threshold .......... 512
[default0]:  init_method_std ................................. 0.01
[default0]:  init_method_xavier_uniform ...................... False
[default0]:  initial_loss_scale .............................. 4294967296
[default0]:  iter_per_epoch .................................. 1250
[default0]:  kd .............................................. False
[default0]:  kd_alpha_ce ..................................... 1
[default0]:  kd_beta_ce ...................................... 1
[default0]:  kd_temp ......................................... 1.0
[default0]:  kv_channels ..................................... 96
[default0]:  layernorm_epsilon ............................... 1e-05
[default0]:  lazy_mpu_init ................................... None
[default0]:  load ............................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.76B_MoE128_2BATCH_1GPU_1Node/checkpoint/gpt-0.76B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:  load_teacher .................................... None
[default0]:  local_rank ...................................... None
[default0]:  log_batch_size_to_tensorboard ................... True
[default0]:  log_interval .................................... 5
[default0]:  log_learning_rate_to_tensorboard ................ True
[default0]:  log_loss_scale_to_tensorboard ................... True
[default0]:  log_memory_to_tensorboard ....................... False
[default0]:  log_num_zeros_in_grad ........................... False
[default0]:  log_optimizer_states_to_tensorboard ............. False
[default0]:  log_params_norm ................................. False
[default0]:  log_timers_to_tensorboard ....................... True
[default0]:  log_validation_ppl_to_tensorboard ............... True
[default0]:  log_world_size_to_tensorboard ................... False
[default0]:  loss_scale ...................................... None
[default0]:  loss_scale_window ............................... 1000
[default0]:  lr .............................................. 0.0002
[default0]:  lr_decay_iters .................................. None
[default0]:  lr_decay_samples ................................ None
[default0]:  lr_decay_style .................................. cosine
[default0]:  lr_decay_tokens ................................. 300000000000
[default0]:  lr_warmup_fraction .............................. None
[default0]:  lr_warmup_iters ................................. 0
[default0]:  lr_warmup_samples ............................... 0
[default0]:  lr_warmup_tokens ................................ 375000000
[default0]:  make_vocab_size_divisible_by .................... 128
[default0]:  mask_factor ..................................... 1.0
[default0]:  mask_prob ....................................... 0.15
[default0]:  mask_type ....................................... random
[default0]:  masked_softmax_fusion ........................... True
[default0]:  max_position_embeddings ......................... 2048
[default0]:  max_tokens_to_oom ............................... 12000
[default0]:  memory_centric_tiled_linear ..................... False
[default0]:  merge_file ...................................... /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-merges.txt
[default0]:  micro_batch_size ................................ 2
[default0]:  min_loss_scale .................................. 1.0
[default0]:  min_lr .......................................... 2e-06
[default0]:  mlp_type ........................................ standard
[default0]:  mmap_warmup ..................................... False
[default0]:  moe_eval_capacity_factor ........................ 1.0
[default0]:  moe_expert_parallel_size ........................ 1
[default0]:  moe_loss_coeff .................................. 0.01
[default0]:  moe_min_capacity ................................ 4
[default0]:  moe_token_dropping .............................. True
[default0]:  moe_train_capacity_factor ....................... 1.0
[default0]:  mos ............................................. False
[default0]:  no_load_lr_state ................................ False
[default0]:  no_load_optim ................................... None
[default0]:  no_load_rng ..................................... None
[default0]:  no_persist_layer_norm ........................... False
[default0]:  no_pipeline_parallel ............................ True
[default0]:  no_save_optim ................................... None
[default0]:  no_save_rng ..................................... None
[default0]:  normalization ................................... layernorm
[default0]:  num_attention_heads ............................. 16
[default0]:  num_attention_heads_teacher ..................... None
[default0]:  num_channels .................................... 3
[default0]:  num_classes ..................................... 1000
[default0]:  num_experts ..................................... [128]
[default0]:  num_experts_switch .............................. None
[default0]:  num_experts_teacher ............................. [1]
[default0]:  num_layers ...................................... 24
[default0]:  num_layers_per_virtual_pipeline_stage ........... None
[default0]:  num_layers_teacher .............................. None
[default0]:  num_workers ..................................... 0
[default0]:  onnx_safe ....................................... None
[default0]:  openai_gelu ..................................... False
[default0]:  optimizer ....................................... adam
[default0]:  output_bert_embeddings .......................... False
[default0]:  overlap_p2p_comm ................................ False
[default0]:  override_opt_param_scheduler .................... True
[default0]:  params_dtype .................................... torch.float16
[default0]:  partition_activations ........................... False
[default0]:  patch_dim ....................................... 16
[default0]:  perform_initialization .......................... True
[default0]:  pipeline_model_parallel_size .................... 1
[default0]:  pipeline_model_parallel_split_rank .............. None
[default0]:  profile_backward ................................ False
[default0]:  query_in_block_prob ............................. 0.1
[default0]:  rampup_batch_size ............................... None
[default0]:  random_ltd ...................................... False
[default0]:  rank ............................................ 0
[default0]:  recompute_granularity ........................... None
[default0]:  recompute_method ................................ None
[default0]:  recompute_num_layers ............................ 1
[default0]:  remote_device ................................... none
[default0]:  reset_attention_mask ............................ False
[default0]:  reset_iteration ................................. False
[default0]:  reset_position_ids .............................. False
[default0]:  retriever_report_topk_accuracies ................ []
[default0]:  retriever_score_scaling ......................... False
[default0]:  retriever_seq_length ............................ 256
[default0]:  retro_add_retriever ............................. False
[default0]:  retro_cyclic_train_iters ........................ None
[default0]:  retro_encoder_attention_dropout ................. 0.1
[default0]:  retro_encoder_hidden_dropout .................... 0.1
[default0]:  retro_encoder_layers ............................ 2
[default0]:  retro_num_neighbors ............................. 2
[default0]:  retro_num_retrieved_chunks ...................... 2
[default0]:  retro_return_doc_ids ............................ False
[default0]:  retro_workdir ................................... None
[default0]:  return_data_index ............................... False
[default0]:  rotary_percent .................................. 1.0
[default0]:  sample_rate ..................................... 1.0
[default0]:  save ............................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.76B_MoE128_2BATCH_1GPU_1Node/checkpoint/gpt-0.76B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:  save_interval ................................... 1000000
[default0]:  scatter_gather_tensors_in_pipeline .............. True
[default0]:  scattered_embeddings ............................ False
[default0]:  seed ............................................ 1234
[default0]:  seq_length ...................................... 2048
[default0]:  sequence_parallel ............................... False
[default0]:  sgd_momentum .................................... 0.9
[default0]:  short_seq_prob .................................. 0.1
[default0]:  skip_train ...................................... False
[default0]:  split ........................................... 98,2,0
[default0]:  split_transformers .............................. False
[default0]:  squared_relu .................................... False
[default0]:  standalone_embedding_stage ...................... False
[default0]:  start_weight_decay .............................. 0.1
[default0]:  swiglu .......................................... False
[default0]:  swin_backbone_type .............................. tiny
[default0]:  synchronize_each_layer .......................... False
[default0]:  tensor_model_parallel_size ...................... 1
[default0]:  tensorboard_dir ................................. /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.76B_MoE128_2BATCH_1GPU_1Node/tensorboard/gpt-0.76B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true_n2gpu1225_2023.07.30-22.21.45
[default0]:  tensorboard_log_interval ........................ 1
[default0]:  tensorboard_queue_size .......................... 1
[default0]:  test_data_path .................................. None
[default0]:  tile_factor ..................................... 1
[default0]:  timing_log_level ................................ 0
[default0]:  timing_log_option ............................... minmax
[default0]:  titles_data_path ................................ None
[default0]:  tokenizer_model ................................. None
[default0]:  tokenizer_type .................................. GPT2BPETokenizer
[default0]:  topk ............................................ 1
[default0]:  train_data_exact_num_epochs ..................... None
[default0]:  train_data_path ................................. None
[default0]:  train_desc_path ................................. None
[default0]:  train_doc_idx_path .............................. None
[default0]:  train_idx_path .................................. None
[default0]:  train_iters ..................................... 219726562
[default0]:  train_sample_idx_path ........................... None
[default0]:  train_samples ................................... None
[default0]:  train_shuffle_idx_path .......................... None
[default0]:  train_tokens .................................... 300000000000
[default0]:  transformer_impl ................................ local
[default0]:  transformer_pipeline_model_parallel_size ........ 1
[default0]:  untie_embeddings_and_output_weights ............. False
[default0]:  use_checkpoint_args ............................. False
[default0]:  use_checkpoint_opt_param_scheduler .............. False
[default0]:  use_contiguous_buffers_in_local_ddp ............. True
[default0]:  use_cpu_initialization .......................... None
[default0]:  use_distributed_optimizer ....................... False
[default0]:  use_flash_attn .................................. False
[default0]:  use_one_sent_docs ............................... False
[default0]:  use_pin_memory .................................. False
[default0]:  use_ring_exchange_p2p ........................... False
[default0]:  use_rotary_position_embeddings .................. False
[default0]:  use_tutel ....................................... False
[default0]:  valid_data_path ................................. None
[default0]:  variable_seq_lengths ............................ False
[default0]:  virtual_pipeline_model_parallel_size ............ None
[default0]:  vision_backbone_type ............................ vit
[default0]:  vision_pretraining .............................. False
[default0]:  vision_pretraining_type ......................... classify
[default0]:  vocab_extra_ids ................................. 0
[default0]:  vocab_file ...................................... /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-vocab.json
[default0]:  vocab_size ...................................... None
[default0]:  weight_decay .................................... 0.1
[default0]:  weight_decay_incr_style ......................... constant
[default0]:  world_size ...................................... 1
[default0]:  zero_allgather_bucket_size ...................... 0.0
[default0]:  zero_contigious_gradients ....................... False
[default0]:  zero_reduce_bucket_size ......................... 0.0
[default0]:  zero_reduce_scatter ............................. False
[default0]:  zero_stage ...................................... 1.0
[default0]:-------------------- end of arguments ---------------------
[default0]:setting number of micro-batches to constant 1
[default0]:> building GPT2BPETokenizer tokenizer ...
[default0]: > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
[default0]:> setting tensorboard ...
[default0]:> initializing torch distributed ...
[default0]:[2023-07-30 22:22:05,930] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[default0]:[2023-07-30 22:22:05,930] [INFO] [comm.py:616:init_distributed] cdb=None
[default0]:[2023-07-30 22:22:05,930] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[default0]:> initialized tensor model parallel with size 1
[default0]:> initialized pipeline model parallel with size 1
[default0]:> setting random seeds to 1234 ...
[default0]:> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[default0]:> compiling dataset index builder ...
[default0]:make: Entering directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/data'
[default0]:make: Nothing to be done for 'default'.
[default0]:make: Leaving directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/data'
[default0]:>>> done with dataset index builder. Compilation time: 0.109 seconds
[default0]:> compiling and loading fused kernels ...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_softmax_cuda...
[default0]:n2gpu1225:413790:413790 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.93<0>
[default0]:n2gpu1225:413790:413790 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1225:413790:413790 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.93<0>
[default0]:n2gpu1225:413790:413790 [0] NCCL INFO Using network IB
[default0]:NCCL version 2.12.12+cuda11.7
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Connected all rings
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO Connected all trees
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1225:413790:414016 [0] NCCL INFO comm 0x1468dc0090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:>>> done with compiling and loading fused kernels. Compilation time: 1.840 seconds
[default0]:time to initialize megatron (seconds): 5.404
[default0]:[after megatron is initialized] datetime: 2023-07-30 22:22:08 
[default0]:building GPT model ...
[default0]:[2023-07-30 22:22:08,653] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[default0]:[2023-07-30 22:22:08,653] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.8 GB         CA 0.0 GB         Max_CA 1 GB 
[default0]:[2023-07-30 22:22:08,654] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.05 GB, percent = 4.4%
[default0]:[2023-07-30 22:22:08,810] [INFO] [utils.py:785:see_memory_usage] After Building Model
[default0]:[2023-07-30 22:22:08,811] [INFO] [utils.py:786:see_memory_usage] MA 1.41 GB         Max_MA 1.41 GB         CA 1.47 GB         Max_CA 1 GB 
[default0]:[2023-07-30 22:22:08,811] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.07 GB, percent = 4.4%
[default0]: > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 760372224
[default0]:param_group keyset: dict_keys(['name', 'params', 'wd_mult', 'lr_mult'])
[default0]:> learning rate decay style: cosine
[default0]:DeepSpeed is enabled.
[default0]:[2023-07-30 22:22:08,814] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Connected all rings
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO Connected all trees
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1225:413790:414151 [0] NCCL INFO comm 0x14674c0090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:[2023-07-30 22:22:09,326] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[default0]:[2023-07-30 22:22:09,327] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[default0]:[2023-07-30 22:22:09,327] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[default0]:[2023-07-30 22:22:09,336] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[default0]:[2023-07-30 22:22:09,336] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'apex.optimizers.fused_adam.FusedAdam'>
[default0]:[2023-07-30 22:22:09,336] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer
[default0]:[2023-07-30 22:22:09,336] [INFO] [stage_1_and_2.py:133:__init__] Reduce bucket size 500,000,000
[default0]:[2023-07-30 22:22:09,336] [INFO] [stage_1_and_2.py:134:__init__] Allgather bucket size 500,000,000
[default0]:[2023-07-30 22:22:09,336] [INFO] [stage_1_and_2.py:135:__init__] CPU Offload: False
[default0]:[2023-07-30 22:22:09,336] [INFO] [stage_1_and_2.py:136:__init__] Round robin gradient partitioning: False
[default0]:Rank: 0 partition count [1, 1] and sizes[(759889920, False), (482304, False)] 
[default0]:[2023-07-30 22:22:10,778] [INFO] [utils.py:785:see_memory_usage] Before initializing optimizer states
[default0]:[2023-07-30 22:22:10,779] [INFO] [utils.py:786:see_memory_usage] MA 4.25 GB         Max_MA 4.25 GB         CA 4.27 GB         Max_CA 4 GB 
[default0]:[2023-07-30 22:22:10,779] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.15 GB, percent = 4.4%
[default0]:[2023-07-30 22:22:10,863] [INFO] [utils.py:785:see_memory_usage] After initializing optimizer states
[default0]:[2023-07-30 22:22:10,864] [INFO] [utils.py:786:see_memory_usage] MA 9.91 GB         Max_MA 12.75 GB         CA 12.77 GB         Max_CA 13 GB 
[default0]:[2023-07-30 22:22:10,864] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.15 GB, percent = 4.4%
[default0]:[2023-07-30 22:22:10,864] [INFO] [stage_1_and_2.py:493:__init__] optimizer state initialized
[default0]:[2023-07-30 22:22:10,908] [INFO] [utils.py:785:see_memory_usage] After initializing ZeRO optimizer
[default0]:[2023-07-30 22:22:10,908] [INFO] [utils.py:786:see_memory_usage] MA 9.91 GB         Max_MA 9.91 GB         CA 12.77 GB         Max_CA 13 GB 
[default0]:[2023-07-30 22:22:10,909] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 22.15 GB, percent = 4.4%
[default0]:[2023-07-30 22:22:10,913] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[default0]:[2023-07-30 22:22:10,913] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[default0]:[2023-07-30 22:22:10,914] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x14697073f700>
[default0]:[2023-07-30 22:22:10,914] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:10,914] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[default0]:[2023-07-30 22:22:10,914] [INFO] [config.py:964:print]   activation_checkpointing_config  {
[default0]:    "partition_activations": false, 
[default0]:    "contiguous_memory_optimization": false, 
[default0]:    "cpu_checkpointing": false, 
[default0]:    "number_checkpoints": null, 
[default0]:    "synchronize_checkpoint_boundary": false, 
[default0]:    "profile": false
[default0]:}
[default0]:[2023-07-30 22:22:10,914] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   amp_enabled .................. False
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   amp_params ................... False
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   autotuning_config ............ {
[default0]:    "enabled": false, 
[default0]:    "start_step": null, 
[default0]:    "end_step": null, 
[default0]:    "metric_path": null, 
[default0]:    "arg_mappings": null, 
[default0]:    "metric": "throughput", 
[default0]:    "model_info": null, 
[default0]:    "results_dir": "autotuning_results", 
[default0]:    "exps_dir": "autotuning_exps", 
[default0]:    "overwrite": true, 
[default0]:    "fast": true, 
[default0]:    "start_profile_step": 3, 
[default0]:    "end_profile_step": 5, 
[default0]:    "tuner_type": "gridsearch", 
[default0]:    "tuner_early_stopping": 5, 
[default0]:    "tuner_num_trials": 50, 
[default0]:    "model_info_path": null, 
[default0]:    "mp_size": 1, 
[default0]:    "max_train_batch_size": null, 
[default0]:    "min_train_batch_size": 1, 
[default0]:    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[default0]:    "min_train_micro_batch_size_per_gpu": 1, 
[default0]:    "num_tuning_micro_batch_sizes": 3
[default0]:}
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14695631b730>
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   communication_data_type ...... None
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   curriculum_params_legacy ..... {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 28195488, 'difficulty_step': 8}}
[default0]:[2023-07-30 22:22:10,915] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   disable_allgather ............ False
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   dump_state ................... False
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[default0]:[2023-07-30 22:22:10,916] [INFO] [config.py:964:print]   flops_profiler_config ........ {
[default0]:    "enabled": false, 
[default0]:    "recompute_fwd_factor": 0.0, 
[default0]:    "profile_step": 1, 
[default0]:    "module_depth": -1, 
[default0]:    "top_modules": 1, 
[default0]:    "detailed": true, 
[default0]:    "output_file": null
[default0]:}
[default0]:[2023-07-30 22:22:10,917] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[default0]:[2023-07-30 22:22:10,917] [INFO] [config.py:964:print]   fp16_enabled ................. True
[default0]:[2023-07-30 22:22:10,917] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[default0]:[2023-07-30 22:22:10,917] [INFO] [config.py:964:print]   global_rank .................. 0
[default0]:[2023-07-30 22:22:10,917] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[default0]:[2023-07-30 22:22:10,917] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[default0]:[2023-07-30 22:22:10,917] [INFO] [config.py:964:print]   gradient_clipping ............ 1
[default0]:[2023-07-30 22:22:10,917] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[default0]:[2023-07-30 22:22:10,951] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[default0]:[2023-07-30 22:22:10,951] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[default0]:[2023-07-30 22:22:10,951] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[default0]:[2023-07-30 22:22:10,951] [INFO] [config.py:964:print]   loss_scale ................... 0
[default0]:[2023-07-30 22:22:10,952] [INFO] [config.py:964:print]   memory_breakdown ............. False
[default0]:[2023-07-30 22:22:10,952] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[default0]:[2023-07-30 22:22:10,952] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[default0]:[2023-07-30 22:22:10,952] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[default0]:[2023-07-30 22:22:10,952] [INFO] [config.py:964:print]   nebula_config ................ {
[default0]:    "enabled": false, 
[default0]:    "persistent_storage_path": null, 
[default0]:    "persistent_time_interval": 100, 
[default0]:    "num_of_version_in_retention": 2, 
[default0]:    "enable_nebula_load": true, 
[default0]:    "load_path": null
[default0]:}
[default0]:[2023-07-30 22:22:10,952] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[default0]:[2023-07-30 22:22:10,952] [INFO] [config.py:964:print]   optimizer_name ............... None
[default0]:[2023-07-30 22:22:10,952] [INFO] [config.py:964:print]   optimizer_params ............. None
[default0]:[2023-07-30 22:22:10,952] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[default0]:[2023-07-30 22:22:10,952] [INFO] [config.py:964:print]   pld_enabled .................. False
[default0]:[2023-07-30 22:22:10,952] [INFO] [config.py:964:print]   pld_params ................... False
[default0]:[2023-07-30 22:22:10,952] [INFO] [config.py:964:print]   prescale_gradients ........... False
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   scheduler_name ............... None
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   scheduler_params ............. None
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   sparse_attention ............. None
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   steps_per_print .............. 5
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   train_batch_size ............. 2
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  2
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   world_size ................... 1
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   zero_enabled ................. True
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[default0]:[2023-07-30 22:22:10,953] [INFO] [config.py:964:print]   zero_optimization_stage ...... 2
[default0]:[2023-07-30 22:22:10,954] [INFO] [config.py:950:print_user_config]   json = {
[default0]:    "train_batch_size": 2, 
[default0]:    "train_micro_batch_size_per_gpu": 2, 
[default0]:    "steps_per_print": 5, 
[default0]:    "zero_optimization": {
[default0]:        "stage": 2
[default0]:    }, 
[default0]:    "gradient_clipping": 1, 
[default0]:    "prescale_gradients": false, 
[default0]:    "fp16": {
[default0]:        "enabled": true, 
[default0]:        "loss_scale": 0, 
[default0]:        "loss_scale_window": 500, 
[default0]:        "hysteresis": 2, 
[default0]:        "min_loss_scale": 1, 
[default0]:        "initial_scale_power": 11
[default0]:    }, 
[default0]:    "bf16": {
[default0]:        "enabled": false
[default0]:    }, 
[default0]:    "curriculum_learning": {
[default0]:        "enabled": false, 
[default0]:        "curriculum_type": "seqlen", 
[default0]:        "min_difficulty": 80, 
[default0]:        "max_difficulty": 2.048000e+03, 
[default0]:        "schedule_type": "fixed_linear", 
[default0]:        "schedule_config": {
[default0]:            "total_curriculum_step": 2.819549e+07, 
[default0]:            "difficulty_step": 8
[default0]:        }
[default0]:    }, 
[default0]:    "wall_clock_breakdown": false
[default0]:}
[default0]:[2023-07-30 22:22:10,955] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.76B_MoE128_2BATCH_1GPU_1Node/checkpoint/gpt-0.76B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:WARNING: could not find the metadata file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/scaling_experiments/output/GPT_0.76B_MoE128_2BATCH_1GPU_1Node/checkpoint/gpt-0.76B-lr-2.0e-4-minlr-2e-06-bs-2-gpus-1-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true 
[default0]:    will not load any checkpoints and will start from random
[default0]:(min, max) time across ranks (ms):
[default0]:    load-checkpoint ................................: (1.16, 1.16)
[default0]:[after model, optimizer, and learning rate scheduler are built] datetime: 2023-07-30 22:22:10 
[default0]:> building train, validation, and test datasets ...
[default0]: > datasets target sizes (minimum size):
[default0]:    train:      439453124
[default0]:    validation: 219726600
[default0]:    test:       100
[default0]:> building train, validation, and test datasets for GPT ...
[default0]:Single data path provided for train, valid & test
[default0]: > building dataset index ...
[default0]:    reading sizes...
[default0]:    reading pointers...
[default0]:    reading document index...
[default0]:    creating numpy buffer of mmap...
[default0]:    creating memory view of numpy buffer...
[default0]: > finished creating indexed dataset in 0.003373 seconds
[default0]:    number of documents: 10000
[default0]: > dataset split:
[default0]:    train:
[default0]:     document indices in [0, 9800) total of 9800 documents
[default0]:    validation:
[default0]:     document indices in [9800, 10000) total of 200 documents
[default0]:    test:
[default0]:     document indices in [10000, 10000) total of 0 documents
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Connected all rings
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO Connected all trees
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1225:413790:414160 [0] NCCL INFO comm 0x1468d40090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]: > loading doc-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/71654a473c8694dd2b48f0197bdd21a9_doc_idx.npy
[default0]: > loading sample-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/71654a473c8694dd2b48f0197bdd21a9_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/71654a473c8694dd2b48f0197bdd21a9_shuffle_idx.npy
[default0]:    loaded indexed file in 0.069 seconds
[default0]:    total number of samples: 439466004
[default0]:    total number of epochs: 30909
[default0]: > loading doc-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/de3819301120d9a35080a0a129b3870d_doc_idx.npy
[default0]: > loading sample-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/de3819301120d9a35080a0a129b3870d_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/de3819301120d9a35080a0a129b3870d_shuffle_idx.npy
[default0]:    loaded indexed file in 0.087 seconds
[default0]:    total number of samples: 219726809
[default0]:    total number of epochs: 648115
[default0]:> finished creating GPT datasets ...
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Connected all rings
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO Connected all trees
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1225:413790:414167 [0] NCCL INFO comm 0x1468d42cf4b0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:[after dataloaders are built] datetime: 2023-07-30 22:22:12 
[default0]:done with setup ...
[default0]:(min, max) time across ranks (ms):
[default0]:    model-and-optimizer-setup ......................: (2352.72, 2352.72)
[default0]:    train/valid/test-data-iterators-setup ..........: (1430.44, 1430.44)
[default0]:training ...
[default0]:[before the start of training step] datetime: 2023-07-30 22:22:12 
[default0]:[2023-07-30 22:22:16,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[8.738133333333334e-09, 8.738133333333334e-09], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:16,220] [INFO] [timer.py:215:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=4.06671357279591, CurrSamplesPerSec=4.256303444118756, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration        5/219726562 | consumed samples:           10 | consumed tokens:        20480 | elapsed time per iteration (ms): 730.4 | learning rate: 8.738E-09 | global batch size:     2 | lm loss: 1.091043E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.738 | TFLOPs: 39.86 |
[default0]:[Rank 0] (after 5 iterations) memory (MB) | allocated: 10408.69287109375 | max allocated: 16207.11376953125 | reserved: 17820.0 | max reserved: 17820.0
[default0]:[2023-07-30 22:22:18,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.9660800000000002e-08, 1.9660800000000002e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:18,674] [INFO] [timer.py:215:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=4.171544833552792, CurrSamplesPerSec=4.22795002630937, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       10/219726562 | consumed samples:           20 | consumed tokens:        40960 | elapsed time per iteration (ms): 490.8 | learning rate: 1.966E-08 | global batch size:     2 | lm loss: 1.091168E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.075 | TFLOPs: 59.32 |
[default0]:[2023-07-30 22:22:21,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=15, skipped=0, lr=[3.0583466666666664e-08, 3.0583466666666664e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:21,120] [INFO] [timer.py:215:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=4.190826419999558, CurrSamplesPerSec=4.168200643470268, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       15/219726562 | consumed samples:           30 | consumed tokens:        61440 | elapsed time per iteration (ms): 489.5 | learning rate: 3.058E-08 | global batch size:     2 | lm loss: 1.090179E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.085 | TFLOPs: 59.47 |
[default0]:[2023-07-30 22:22:23,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[4.150613333333333e-08, 4.150613333333333e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:23,599] [INFO] [timer.py:215:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=4.195667591967389, CurrSamplesPerSec=4.199692205099142, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       20/219726562 | consumed samples:           40 | consumed tokens:        81920 | elapsed time per iteration (ms): 495.8 | learning rate: 4.151E-08 | global batch size:     2 | lm loss: 1.088692E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.034 | TFLOPs: 58.72 |
[default0]:[2023-07-30 22:22:26,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=25, skipped=0, lr=[5.24288e-08, 5.24288e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:26,070] [INFO] [timer.py:215:stop] epoch=0/micro_step=25/global_step=25, RunningAvgSamplesPerSec=4.198652434796656, CurrSamplesPerSec=4.246833078346063, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       25/219726562 | consumed samples:           50 | consumed tokens:       102400 | elapsed time per iteration (ms): 494.0 | learning rate: 5.243E-08 | global batch size:     2 | lm loss: 1.085338E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.048 | TFLOPs: 58.93 |
[default0]:[2023-07-30 22:22:28,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[6.335146666666667e-08, 6.335146666666667e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:28,516] [INFO] [timer.py:215:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=4.206348804688855, CurrSamplesPerSec=4.235397948596255, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       30/219726562 | consumed samples:           60 | consumed tokens:       122880 | elapsed time per iteration (ms): 489.2 | learning rate: 6.335E-08 | global batch size:     2 | lm loss: 1.077934E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.088 | TFLOPs: 59.51 |
[default0]:[2023-07-30 22:22:30,971] [INFO] [logging.py:96:log_dist] [Rank 0] step=35, skipped=0, lr=[7.427413333333334e-08, 7.427413333333334e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:30,972] [INFO] [timer.py:215:stop] epoch=0/micro_step=35/global_step=35, RunningAvgSamplesPerSec=4.2101635585443224, CurrSamplesPerSec=4.203749825858704, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       35/219726562 | consumed samples:           70 | consumed tokens:       143360 | elapsed time per iteration (ms): 491.3 | learning rate: 7.427E-08 | global batch size:     2 | lm loss: 1.072745E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.071 | TFLOPs: 59.26 |
[default0]:[2023-07-30 22:22:33,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[8.519680000000001e-08, 8.519680000000001e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:33,441] [INFO] [timer.py:215:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=4.2108174964893905, CurrSamplesPerSec=4.22213207248978, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       40/219726562 | consumed samples:           80 | consumed tokens:       163840 | elapsed time per iteration (ms): 494.0 | learning rate: 8.520E-08 | global batch size:     2 | lm loss: 1.065495E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.049 | TFLOPs: 58.93 |
[default0]:[2023-07-30 22:22:35,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=45, skipped=0, lr=[9.611946666666668e-08, 9.611946666666668e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:35,928] [INFO] [timer.py:215:stop] epoch=0/micro_step=45/global_step=45, RunningAvgSamplesPerSec=4.210265704765702, CurrSamplesPerSec=4.208155142652039, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       45/219726562 | consumed samples:           90 | consumed tokens:       184320 | elapsed time per iteration (ms): 497.2 | learning rate: 9.612E-08 | global batch size:     2 | lm loss: 1.057248E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.022 | TFLOPs: 58.55 |
[default0]:[2023-07-30 22:22:38,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[1.0704213333333333e-07, 1.0704213333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:38,374] [INFO] [timer.py:215:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=4.213912343718488, CurrSamplesPerSec=4.229020025388413, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       50/219726562 | consumed samples:          100 | consumed tokens:       204800 | elapsed time per iteration (ms): 489.3 | learning rate: 1.070E-07 | global batch size:     2 | lm loss: 1.046265E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.087 | TFLOPs: 59.49 |
[default0]:[2023-07-30 22:22:40,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=55, skipped=0, lr=[1.179648e-07, 1.179648e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:40,828] [INFO] [timer.py:215:stop] epoch=0/micro_step=55/global_step=55, RunningAvgSamplesPerSec=4.215533426335023, CurrSamplesPerSec=4.206540827266518, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       55/219726562 | consumed samples:          110 | consumed tokens:       225280 | elapsed time per iteration (ms): 490.9 | learning rate: 1.180E-07 | global batch size:     2 | lm loss: 1.042204E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.074 | TFLOPs: 59.30 |
[default0]:[2023-07-30 22:22:43,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.2888746666666666e-07, 1.2888746666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:43,313] [INFO] [timer.py:215:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=4.213801221927988, CurrSamplesPerSec=4.220825557389859, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       60/219726562 | consumed samples:          120 | consumed tokens:       245760 | elapsed time per iteration (ms): 496.7 | learning rate: 1.289E-07 | global batch size:     2 | lm loss: 1.034768E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.027 | TFLOPs: 58.61 |
[default0]:[2023-07-30 22:22:45,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=65, skipped=0, lr=[1.3981013333333334e-07, 1.3981013333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:45,782] [INFO] [timer.py:215:stop] epoch=0/micro_step=65/global_step=65, RunningAvgSamplesPerSec=4.213773674377164, CurrSamplesPerSec=4.215383022805539, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       65/219726562 | consumed samples:          130 | consumed tokens:       266240 | elapsed time per iteration (ms): 493.7 | learning rate: 1.398E-07 | global batch size:     2 | lm loss: 1.026252E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.051 | TFLOPs: 58.97 |
[default0]:[2023-07-30 22:22:48,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.5073280000000002e-07, 1.5073280000000002e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:48,230] [INFO] [timer.py:215:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=4.216055996906156, CurrSamplesPerSec=4.224760296980936, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       70/219726562 | consumed samples:          140 | consumed tokens:       286720 | elapsed time per iteration (ms): 489.6 | learning rate: 1.507E-07 | global batch size:     2 | lm loss: 1.018211E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.085 | TFLOPs: 59.46 |
[default0]:[2023-07-30 22:22:50,674] [INFO] [logging.py:96:log_dist] [Rank 0] step=75, skipped=0, lr=[1.6165546666666667e-07, 1.6165546666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:50,674] [INFO] [timer.py:215:stop] epoch=0/micro_step=75/global_step=75, RunningAvgSamplesPerSec=4.218058342244074, CurrSamplesPerSec=4.2177886471875405, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       75/219726562 | consumed samples:          150 | consumed tokens:       307200 | elapsed time per iteration (ms): 488.8 | learning rate: 1.617E-07 | global batch size:     2 | lm loss: 1.013236E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.092 | TFLOPs: 59.56 |
[default0]:[2023-07-30 22:22:53,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.7257813333333336e-07, 1.7257813333333336e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:53,149] [INFO] [timer.py:215:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=4.217223256726186, CurrSamplesPerSec=4.222155448415473, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       80/219726562 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 495.1 | learning rate: 1.726E-07 | global batch size:     2 | lm loss: 1.016909E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.040 | TFLOPs: 58.80 |
[default0]:[2023-07-30 22:22:55,616] [INFO] [logging.py:96:log_dist] [Rank 0] step=85, skipped=0, lr=[1.8350080000000004e-07, 1.8350080000000004e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:55,617] [INFO] [timer.py:215:stop] epoch=0/micro_step=85/global_step=85, RunningAvgSamplesPerSec=4.216939464106835, CurrSamplesPerSec=4.181585707071941, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       85/219726562 | consumed samples:          170 | consumed tokens:       348160 | elapsed time per iteration (ms): 493.5 | learning rate: 1.835E-07 | global batch size:     2 | lm loss: 9.998073E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.052 | TFLOPs: 58.98 |
[default0]:[2023-07-30 22:22:58,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1.9442346666666667e-07, 1.9442346666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:22:58,065] [INFO] [timer.py:215:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=4.2182832144126206, CurrSamplesPerSec=4.235038719927624, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       90/219726562 | consumed samples:          180 | consumed tokens:       368640 | elapsed time per iteration (ms): 489.4 | learning rate: 1.944E-07 | global batch size:     2 | lm loss: 1.004064E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.086 | TFLOPs: 59.48 |
[default0]:[2023-07-30 22:23:00,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=95, skipped=0, lr=[2.0534613333333335e-07, 2.0534613333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:00,517] [INFO] [timer.py:215:stop] epoch=0/micro_step=95/global_step=95, RunningAvgSamplesPerSec=4.2193211718708294, CurrSamplesPerSec=4.23371138557695, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration       95/219726562 | consumed samples:          190 | consumed tokens:       389120 | elapsed time per iteration (ms): 490.8 | learning rate: 2.053E-07 | global batch size:     2 | lm loss: 9.890471E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.075 | TFLOPs: 59.31 |
[default0]:[2023-07-30 22:23:02,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[2.162688e-07, 2.162688e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:02,994] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=4.218079290001343, CurrSamplesPerSec=4.173007058477062, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      100/219726562 | consumed samples:          200 | consumed tokens:       409600 | elapsed time per iteration (ms): 496.2 | learning rate: 2.163E-07 | global batch size:     2 | lm loss: 9.885562E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.030 | TFLOPs: 58.67 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 100 | lm loss value: 9.832664E+00 | lm loss PPL: 1.863253E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:23:12,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=105, skipped=0, lr=[2.271914666666667e-07, 2.271914666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:12,709] [INFO] [timer.py:215:stop] epoch=0/micro_step=105/global_step=105, RunningAvgSamplesPerSec=4.21686525261246, CurrSamplesPerSec=4.1460932279454505, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      105/219726562 | consumed samples:          210 | consumed tokens:       430080 | elapsed time per iteration (ms): 1942.1 | learning rate: 2.272E-07 | global batch size:     2 | lm loss: 9.818442E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.030 | TFLOPs: 14.99 |
[default0]:[2023-07-30 22:23:15,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[2.3811413333333334e-07, 2.3811413333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:15,270] [INFO] [timer.py:215:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=4.217208400740716, CurrSamplesPerSec=4.213682726860833, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      110/219726562 | consumed samples:          220 | consumed tokens:       450560 | elapsed time per iteration (ms): 512.5 | learning rate: 2.381E-07 | global batch size:     2 | lm loss: 9.743645E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.903 | TFLOPs: 56.81 |
[default0]:[2023-07-30 22:23:17,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=115, skipped=0, lr=[2.490368e-07, 2.490368e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:17,860] [INFO] [timer.py:215:stop] epoch=0/micro_step=115/global_step=115, RunningAvgSamplesPerSec=4.218210727965393, CurrSamplesPerSec=4.261992972387458, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      115/219726562 | consumed samples:          230 | consumed tokens:       471040 | elapsed time per iteration (ms): 517.4 | learning rate: 2.490E-07 | global batch size:     2 | lm loss: 9.786731E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.865 | TFLOPs: 56.26 |
[default0]:[2023-07-30 22:23:20,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[2.599594666666667e-07, 2.599594666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:20,603] [INFO] [timer.py:215:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=4.2192180535330905, CurrSamplesPerSec=4.228546771757694, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      120/219726562 | consumed samples:          240 | consumed tokens:       491520 | elapsed time per iteration (ms): 548.6 | learning rate: 2.600E-07 | global batch size:     2 | lm loss: 9.722272E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.646 | TFLOPs: 53.07 |
[default0]:[2023-07-30 22:23:23,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=125, skipped=0, lr=[2.7088213333333336e-07, 2.7088213333333336e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:23,250] [INFO] [timer.py:215:stop] epoch=0/micro_step=125/global_step=125, RunningAvgSamplesPerSec=4.218595840968218, CurrSamplesPerSec=4.201006706199742, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      125/219726562 | consumed samples:          250 | consumed tokens:       512000 | elapsed time per iteration (ms): 531.1 | learning rate: 2.709E-07 | global batch size:     2 | lm loss: 9.821014E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.766 | TFLOPs: 54.81 |
[default0]:[2023-07-30 22:23:25,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[2.818048e-07, 2.818048e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:25,995] [INFO] [timer.py:215:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=4.218538183571436, CurrSamplesPerSec=4.191208792306881, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      130/219726562 | consumed samples:          260 | consumed tokens:       532480 | elapsed time per iteration (ms): 547.6 | learning rate: 2.818E-07 | global batch size:     2 | lm loss: 9.706744E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.652 | TFLOPs: 53.16 |
[default0]:[2023-07-30 22:23:28,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=135, skipped=0, lr=[2.9272746666666667e-07, 2.9272746666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:28,577] [INFO] [timer.py:215:stop] epoch=0/micro_step=135/global_step=135, RunningAvgSamplesPerSec=4.219531691661985, CurrSamplesPerSec=4.233617370906235, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      135/219726562 | consumed samples:          270 | consumed tokens:       552960 | elapsed time per iteration (ms): 516.4 | learning rate: 2.927E-07 | global batch size:     2 | lm loss: 9.715108E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.873 | TFLOPs: 56.38 |
[default0]:[2023-07-30 22:23:31,097] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[3.0365013333333335e-07, 3.0365013333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:31,097] [INFO] [timer.py:215:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=4.220104526436396, CurrSamplesPerSec=4.204510449115228, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      140/219726562 | consumed samples:          280 | consumed tokens:       573440 | elapsed time per iteration (ms): 504.2 | learning rate: 3.037E-07 | global batch size:     2 | lm loss: 9.651488E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.967 | TFLOPs: 57.74 |
[default0]:[2023-07-30 22:23:33,761] [INFO] [logging.py:96:log_dist] [Rank 0] step=145, skipped=0, lr=[3.1457280000000004e-07, 3.1457280000000004e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:33,763] [INFO] [timer.py:215:stop] epoch=0/micro_step=145/global_step=145, RunningAvgSamplesPerSec=4.219618182913053, CurrSamplesPerSec=4.204577886064599, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      145/219726562 | consumed samples:          290 | consumed tokens:       593920 | elapsed time per iteration (ms): 532.9 | learning rate: 3.146E-07 | global batch size:     2 | lm loss: 9.636896E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.753 | TFLOPs: 54.63 |
[default0]:[2023-07-30 22:23:36,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[3.2549546666666666e-07, 3.2549546666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:36,440] [INFO] [timer.py:215:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=4.219546094519473, CurrSamplesPerSec=4.24203538211961, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      150/219726562 | consumed samples:          300 | consumed tokens:       614400 | elapsed time per iteration (ms): 535.3 | learning rate: 3.255E-07 | global batch size:     2 | lm loss: 9.683517E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.737 | TFLOPs: 54.39 |
[default0]:[2023-07-30 22:23:39,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=155, skipped=0, lr=[3.3641813333333335e-07, 3.3641813333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:39,068] [INFO] [timer.py:215:stop] epoch=0/micro_step=155/global_step=155, RunningAvgSamplesPerSec=4.220612193019968, CurrSamplesPerSec=4.245317862089802, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      155/219726562 | consumed samples:          310 | consumed tokens:       634880 | elapsed time per iteration (ms): 525.8 | learning rate: 3.364E-07 | global batch size:     2 | lm loss: 9.671202E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.804 | TFLOPs: 55.37 |
[default0]:[2023-07-30 22:23:41,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[3.4734080000000003e-07, 3.4734080000000003e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:41,620] [INFO] [timer.py:215:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=4.220643446329014, CurrSamplesPerSec=4.180433322236818, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      160/219726562 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 510.4 | learning rate: 3.473E-07 | global batch size:     2 | lm loss: 9.643143E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.918 | TFLOPs: 57.03 |
[default0]:[2023-07-30 22:23:44,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=165, skipped=0, lr=[3.582634666666667e-07, 3.582634666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:44,283] [INFO] [timer.py:215:stop] epoch=0/micro_step=165/global_step=165, RunningAvgSamplesPerSec=4.219112393817711, CurrSamplesPerSec=4.227918062556355, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      165/219726562 | consumed samples:          330 | consumed tokens:       675840 | elapsed time per iteration (ms): 532.7 | learning rate: 3.583E-07 | global batch size:     2 | lm loss: 9.619713E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.755 | TFLOPs: 54.65 |
[default0]:[2023-07-30 22:23:46,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[3.691861333333334e-07, 3.691861333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:46,922] [INFO] [timer.py:215:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=4.219245252084659, CurrSamplesPerSec=4.230224955207142, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      170/219726562 | consumed samples:          340 | consumed tokens:       696320 | elapsed time per iteration (ms): 527.7 | learning rate: 3.692E-07 | global batch size:     2 | lm loss: 9.631194E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.790 | TFLOPs: 55.16 |
[default0]:[2023-07-30 22:23:49,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=175, skipped=0, lr=[3.801088000000001e-07, 3.801088000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:49,536] [INFO] [timer.py:215:stop] epoch=0/micro_step=175/global_step=175, RunningAvgSamplesPerSec=4.22002605895109, CurrSamplesPerSec=4.2336964284182175, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      175/219726562 | consumed samples:          350 | consumed tokens:       716800 | elapsed time per iteration (ms): 522.6 | learning rate: 3.801E-07 | global batch size:     2 | lm loss: 9.760855E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.827 | TFLOPs: 55.71 |
[default0]:[2023-07-30 22:23:52,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[3.9103146666666665e-07, 3.9103146666666665e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:52,185] [INFO] [timer.py:215:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=4.220271997463403, CurrSamplesPerSec=4.206030412790861, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      180/219726562 | consumed samples:          360 | consumed tokens:       737280 | elapsed time per iteration (ms): 530.7 | learning rate: 3.910E-07 | global batch size:     2 | lm loss: 9.658356E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.769 | TFLOPs: 54.85 |
[default0]:[2023-07-30 22:23:55,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=185, skipped=0, lr=[4.0195413333333333e-07, 4.0195413333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:55,184] [INFO] [timer.py:215:stop] epoch=0/micro_step=185/global_step=185, RunningAvgSamplesPerSec=4.220208527801725, CurrSamplesPerSec=4.1811834463453685, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      185/219726562 | consumed samples:          370 | consumed tokens:       757760 | elapsed time per iteration (ms): 599.0 | learning rate: 4.020E-07 | global batch size:     2 | lm loss: 9.694608E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.339 | TFLOPs: 48.60 |
[default0]:[2023-07-30 22:23:57,953] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[4.128768e-07, 4.128768e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:23:57,954] [INFO] [timer.py:215:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=4.220200226085189, CurrSamplesPerSec=4.209899572968061, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      190/219726562 | consumed samples:          380 | consumed tokens:       778240 | elapsed time per iteration (ms): 554.0 | learning rate: 4.129E-07 | global batch size:     2 | lm loss: 9.588467E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.610 | TFLOPs: 52.55 |
[default0]:[2023-07-30 22:24:00,832] [INFO] [logging.py:96:log_dist] [Rank 0] step=195, skipped=0, lr=[4.237994666666667e-07, 4.237994666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:00,832] [INFO] [timer.py:215:stop] epoch=0/micro_step=195/global_step=195, RunningAvgSamplesPerSec=4.221092716571312, CurrSamplesPerSec=4.245582141100209, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      195/219726562 | consumed samples:          390 | consumed tokens:       798720 | elapsed time per iteration (ms): 578.2 | learning rate: 4.238E-07 | global batch size:     2 | lm loss: 9.539703E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.459 | TFLOPs: 50.35 |
[default0]:[2023-07-30 22:24:03,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[4.347221333333334e-07, 4.347221333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:03,613] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=4.220903279761322, CurrSamplesPerSec=4.237578148047865, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      200/219726562 | consumed samples:          400 | consumed tokens:       819200 | elapsed time per iteration (ms): 553.6 | learning rate: 4.347E-07 | global batch size:     2 | lm loss: 9.594508E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.613 | TFLOPs: 52.59 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 200 | lm loss value: 9.561610E+00 | lm loss PPL: 1.420871E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:24:12,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=205, skipped=0, lr=[4.456448e-07, 4.456448e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:12,712] [INFO] [timer.py:215:stop] epoch=0/micro_step=205/global_step=205, RunningAvgSamplesPerSec=4.220527635132991, CurrSamplesPerSec=4.158992077763747, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      205/219726562 | consumed samples:          410 | consumed tokens:       839680 | elapsed time per iteration (ms): 1819.8 | learning rate: 4.456E-07 | global batch size:     2 | lm loss: 9.587111E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.099 | TFLOPs: 16.00 |
[default0]:[2023-07-30 22:24:15,479] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[4.565674666666667e-07, 4.565674666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:15,482] [INFO] [timer.py:215:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=4.220431412571485, CurrSamplesPerSec=4.202850496484602, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      210/219726562 | consumed samples:          420 | consumed tokens:       860160 | elapsed time per iteration (ms): 554.1 | learning rate: 4.566E-07 | global batch size:     2 | lm loss: 9.633725E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.609 | TFLOPs: 52.54 |
[default0]:[2023-07-30 22:24:18,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=215, skipped=0, lr=[4.6749013333333337e-07, 4.6749013333333337e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:18,280] [INFO] [timer.py:215:stop] epoch=0/micro_step=215/global_step=215, RunningAvgSamplesPerSec=4.220755404766536, CurrSamplesPerSec=4.242655422522781, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      215/219726562 | consumed samples:          430 | consumed tokens:       880640 | elapsed time per iteration (ms): 559.3 | learning rate: 4.675E-07 | global batch size:     2 | lm loss: 9.544566E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.576 | TFLOPs: 52.05 |
[default0]:[2023-07-30 22:24:21,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[4.784128e-07, 4.784128e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:21,042] [INFO] [timer.py:215:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=4.221394002076426, CurrSamplesPerSec=4.238693719982517, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      220/219726562 | consumed samples:          440 | consumed tokens:       901120 | elapsed time per iteration (ms): 552.9 | learning rate: 4.784E-07 | global batch size:     2 | lm loss: 9.607462E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.618 | TFLOPs: 52.66 |
[default0]:[2023-07-30 22:24:23,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=225, skipped=0, lr=[4.893354666666666e-07, 4.893354666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:23,816] [INFO] [timer.py:215:stop] epoch=0/micro_step=225/global_step=225, RunningAvgSamplesPerSec=4.220867918680262, CurrSamplesPerSec=4.223447902249473, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      225/219726562 | consumed samples:          450 | consumed tokens:       921600 | elapsed time per iteration (ms): 554.4 | learning rate: 4.893E-07 | global batch size:     2 | lm loss: 9.536008E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.607 | TFLOPs: 52.51 |
[default0]:[2023-07-30 22:24:26,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[5.002581333333334e-07, 5.002581333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:26,473] [INFO] [timer.py:215:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=4.220882312775332, CurrSamplesPerSec=4.226237450173511, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      230/219726562 | consumed samples:          460 | consumed tokens:       942080 | elapsed time per iteration (ms): 531.4 | learning rate: 5.003E-07 | global batch size:     2 | lm loss: 9.612906E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.764 | TFLOPs: 54.78 |
[default0]:[2023-07-30 22:24:29,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=235, skipped=0, lr=[5.111808e-07, 5.111808e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:29,156] [INFO] [timer.py:215:stop] epoch=0/micro_step=235/global_step=235, RunningAvgSamplesPerSec=4.221365261299483, CurrSamplesPerSec=4.248958351272417, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      235/219726562 | consumed samples:          470 | consumed tokens:       962560 | elapsed time per iteration (ms): 536.9 | learning rate: 5.112E-07 | global batch size:     2 | lm loss: 9.590596E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.725 | TFLOPs: 54.23 |
[default0]:[2023-07-30 22:24:31,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[5.221034666666667e-07, 5.221034666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:31,744] [INFO] [timer.py:215:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=4.221856500732261, CurrSamplesPerSec=4.206880469162561, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      240/219726562 | consumed samples:          480 | consumed tokens:       983040 | elapsed time per iteration (ms): 517.5 | learning rate: 5.221E-07 | global batch size:     2 | lm loss: 9.510128E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.865 | TFLOPs: 56.26 |
[default0]:[2023-07-30 22:24:34,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=245, skipped=0, lr=[5.330261333333334e-07, 5.330261333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:34,328] [INFO] [timer.py:215:stop] epoch=0/micro_step=245/global_step=245, RunningAvgSamplesPerSec=4.221496124506765, CurrSamplesPerSec=4.210489120177964, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      245/219726562 | consumed samples:          490 | consumed tokens:      1003520 | elapsed time per iteration (ms): 516.8 | learning rate: 5.330E-07 | global batch size:     2 | lm loss: 9.404497E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.870 | TFLOPs: 56.34 |
[default0]:[2023-07-30 22:24:36,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[5.439488000000001e-07, 5.439488000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:36,889] [INFO] [timer.py:215:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=4.221485635342384, CurrSamplesPerSec=4.243428044760166, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      250/219726562 | consumed samples:          500 | consumed tokens:      1024000 | elapsed time per iteration (ms): 512.2 | learning rate: 5.439E-07 | global batch size:     2 | lm loss: 9.441989E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.905 | TFLOPs: 56.84 |
[default0]:[2023-07-30 22:24:39,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=255, skipped=0, lr=[5.548714666666667e-07, 5.548714666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:39,447] [INFO] [timer.py:215:stop] epoch=0/micro_step=255/global_step=255, RunningAvgSamplesPerSec=4.221742094733971, CurrSamplesPerSec=4.208663961436421, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      255/219726562 | consumed samples:          510 | consumed tokens:      1044480 | elapsed time per iteration (ms): 511.8 | learning rate: 5.549E-07 | global batch size:     2 | lm loss: 9.425566E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.908 | TFLOPs: 56.89 |
[default0]:[2023-07-30 22:24:41,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[5.657941333333333e-07, 5.657941333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:41,946] [INFO] [timer.py:215:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=4.221999061974739, CurrSamplesPerSec=4.184096895623127, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      260/219726562 | consumed samples:          520 | consumed tokens:      1064960 | elapsed time per iteration (ms): 499.7 | learning rate: 5.658E-07 | global batch size:     2 | lm loss: 9.463542E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 4.002 | TFLOPs: 58.26 |
[default0]:[2023-07-30 22:24:44,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=265, skipped=0, lr=[5.767168e-07, 5.767168e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:44,598] [INFO] [timer.py:215:stop] epoch=0/micro_step=265/global_step=265, RunningAvgSamplesPerSec=4.2215504726309385, CurrSamplesPerSec=4.189626282255861, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      265/219726562 | consumed samples:          530 | consumed tokens:      1085440 | elapsed time per iteration (ms): 530.4 | learning rate: 5.767E-07 | global batch size:     2 | lm loss: 9.370210E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.771 | TFLOPs: 54.89 |
[default0]:[2023-07-30 22:24:47,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[5.876394666666667e-07, 5.876394666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:47,161] [INFO] [timer.py:215:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=4.221589241799758, CurrSamplesPerSec=4.235098587140632, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      270/219726562 | consumed samples:          540 | consumed tokens:      1105920 | elapsed time per iteration (ms): 512.5 | learning rate: 5.876E-07 | global batch size:     2 | lm loss: 9.415961E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.903 | TFLOPs: 56.81 |
[default0]:[2023-07-30 22:24:49,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=275, skipped=0, lr=[5.985621333333333e-07, 5.985621333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:49,716] [INFO] [timer.py:215:stop] epoch=0/micro_step=275/global_step=275, RunningAvgSamplesPerSec=4.2219200740816465, CurrSamplesPerSec=4.239004300347161, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      275/219726562 | consumed samples:          550 | consumed tokens:      1126400 | elapsed time per iteration (ms): 510.9 | learning rate: 5.986E-07 | global batch size:     2 | lm loss: 9.460936E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.914 | TFLOPs: 56.98 |
[default0]:[2023-07-30 22:24:52,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[6.094848000000001e-07, 6.094848000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:52,257] [INFO] [timer.py:215:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=4.222177677953135, CurrSamplesPerSec=4.214112435503083, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      280/219726562 | consumed samples:          560 | consumed tokens:      1146880 | elapsed time per iteration (ms): 508.5 | learning rate: 6.095E-07 | global batch size:     2 | lm loss: 9.348513E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.933 | TFLOPs: 57.25 |
[default0]:[2023-07-30 22:24:54,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=285, skipped=0, lr=[6.204074666666667e-07, 6.204074666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:54,895] [INFO] [timer.py:215:stop] epoch=0/micro_step=285/global_step=285, RunningAvgSamplesPerSec=4.222019288975118, CurrSamplesPerSec=4.208520381849889, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      285/219726562 | consumed samples:          570 | consumed tokens:      1167360 | elapsed time per iteration (ms): 527.7 | learning rate: 6.204E-07 | global batch size:     2 | lm loss: 9.482132E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.790 | TFLOPs: 55.17 |
[default0]:[2023-07-30 22:24:57,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[6.313301333333333e-07, 6.313301333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:24:57,536] [INFO] [timer.py:215:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=4.2222026216446515, CurrSamplesPerSec=4.25751316926753, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      290/219726562 | consumed samples:          580 | consumed tokens:      1187840 | elapsed time per iteration (ms): 527.7 | learning rate: 6.313E-07 | global batch size:     2 | lm loss: 9.513795E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.790 | TFLOPs: 55.17 |
[default0]:[2023-07-30 22:25:00,109] [INFO] [logging.py:96:log_dist] [Rank 0] step=295, skipped=0, lr=[6.422528000000001e-07, 6.422528000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:00,111] [INFO] [timer.py:215:stop] epoch=0/micro_step=295/global_step=295, RunningAvgSamplesPerSec=4.222580365300867, CurrSamplesPerSec=4.257191228832311, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      295/219726562 | consumed samples:          590 | consumed tokens:      1208320 | elapsed time per iteration (ms): 515.2 | learning rate: 6.423E-07 | global batch size:     2 | lm loss: 9.447090E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.882 | TFLOPs: 56.50 |
[default0]:[2023-07-30 22:25:02,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[6.531754666666667e-07, 6.531754666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:02,686] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=4.222370036959562, CurrSamplesPerSec=4.14791578383152, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      300/219726562 | consumed samples:          600 | consumed tokens:      1228800 | elapsed time per iteration (ms): 514.8 | learning rate: 6.532E-07 | global batch size:     2 | lm loss: 9.414091E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.885 | TFLOPs: 56.55 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 300 | lm loss value: 9.314860E+00 | lm loss PPL: 1.110178E+04 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:25:12,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=305, skipped=0, lr=[6.640981333333333e-07, 6.640981333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:12,631] [INFO] [timer.py:215:stop] epoch=0/micro_step=305/global_step=305, RunningAvgSamplesPerSec=4.222381807813199, CurrSamplesPerSec=4.188047057695803, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      305/219726562 | consumed samples:          610 | consumed tokens:      1249280 | elapsed time per iteration (ms): 1989.1 | learning rate: 6.641E-07 | global batch size:     2 | lm loss: 9.321986E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.005 | TFLOPs: 14.64 |
[default0]:[2023-07-30 22:25:15,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[6.750208e-07, 6.750208e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:15,362] [INFO] [timer.py:215:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=4.2222995458759875, CurrSamplesPerSec=4.212480854888845, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      310/219726562 | consumed samples:          620 | consumed tokens:      1269760 | elapsed time per iteration (ms): 546.4 | learning rate: 6.750E-07 | global batch size:     2 | lm loss: 9.522714E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.660 | TFLOPs: 53.28 |
[default0]:[2023-07-30 22:25:18,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=315, skipped=0, lr=[6.859434666666668e-07, 6.859434666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:18,052] [INFO] [timer.py:215:stop] epoch=0/micro_step=315/global_step=315, RunningAvgSamplesPerSec=4.222345459551866, CurrSamplesPerSec=4.243262765791704, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      315/219726562 | consumed samples:          630 | consumed tokens:      1290240 | elapsed time per iteration (ms): 537.6 | learning rate: 6.859E-07 | global batch size:     2 | lm loss: 9.309065E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.720 | TFLOPs: 54.15 |
[default0]:[2023-07-30 22:25:20,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[6.968661333333334e-07, 6.968661333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:20,687] [INFO] [timer.py:215:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=4.22252611639388, CurrSamplesPerSec=4.204046878641251, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      320/219726562 | consumed samples:          640 | consumed tokens:      1310720 | elapsed time per iteration (ms): 527.0 | learning rate: 6.969E-07 | global batch size:     2 | lm loss: 9.406744E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.795 | TFLOPs: 55.24 |
[default0]:[2023-07-30 22:25:23,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=325, skipped=0, lr=[7.077888e-07, 7.077888e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:23,391] [INFO] [timer.py:215:stop] epoch=0/micro_step=325/global_step=325, RunningAvgSamplesPerSec=4.2225259055314, CurrSamplesPerSec=4.163134611166199, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      325/219726562 | consumed samples:          650 | consumed tokens:      1331200 | elapsed time per iteration (ms): 541.7 | learning rate: 7.078E-07 | global batch size:     2 | lm loss: 9.264904E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.692 | TFLOPs: 53.74 |
[default0]:[2023-07-30 22:25:26,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[7.187114666666667e-07, 7.187114666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:26,103] [INFO] [timer.py:215:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=4.222585208578325, CurrSamplesPerSec=4.224036996417785, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      330/219726562 | consumed samples:          660 | consumed tokens:      1351680 | elapsed time per iteration (ms): 541.8 | learning rate: 7.187E-07 | global batch size:     2 | lm loss: 9.388188E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.692 | TFLOPs: 53.74 |
[default0]:[2023-07-30 22:25:28,769] [INFO] [logging.py:96:log_dist] [Rank 0] step=335, skipped=0, lr=[7.296341333333333e-07, 7.296341333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:28,769] [INFO] [timer.py:215:stop] epoch=0/micro_step=335/global_step=335, RunningAvgSamplesPerSec=4.222983549447527, CurrSamplesPerSec=4.256763490699027, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      335/219726562 | consumed samples:          670 | consumed tokens:      1372160 | elapsed time per iteration (ms): 533.2 | learning rate: 7.296E-07 | global batch size:     2 | lm loss: 9.313893E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.751 | TFLOPs: 54.60 |
[default0]:[2023-07-30 22:25:31,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[7.405567999999999e-07, 7.405567999999999e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:31,519] [INFO] [timer.py:215:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=4.223442561098877, CurrSamplesPerSec=4.256655489611834, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      340/219726562 | consumed samples:          680 | consumed tokens:      1392640 | elapsed time per iteration (ms): 549.9 | learning rate: 7.406E-07 | global batch size:     2 | lm loss: 9.306403E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.637 | TFLOPs: 52.94 |
[default0]:[2023-07-30 22:25:34,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=345, skipped=0, lr=[7.514794666666667e-07, 7.514794666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:34,256] [INFO] [timer.py:215:stop] epoch=0/micro_step=345/global_step=345, RunningAvgSamplesPerSec=4.223149180077711, CurrSamplesPerSec=4.197181167690452, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      345/219726562 | consumed samples:          690 | consumed tokens:      1413120 | elapsed time per iteration (ms): 547.5 | learning rate: 7.515E-07 | global batch size:     2 | lm loss: 9.140872E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.653 | TFLOPs: 53.17 |
[default0]:[2023-07-30 22:25:37,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[7.624021333333333e-07, 7.624021333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:37,024] [INFO] [timer.py:215:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=4.2231909171277, CurrSamplesPerSec=4.235979686142109, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      350/219726562 | consumed samples:          700 | consumed tokens:      1433600 | elapsed time per iteration (ms): 553.4 | learning rate: 7.624E-07 | global batch size:     2 | lm loss: 9.196725E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.614 | TFLOPs: 52.61 |
[default0]:[2023-07-30 22:25:39,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=355, skipped=0, lr=[7.733248e-07, 7.733248e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:39,833] [INFO] [timer.py:215:stop] epoch=0/micro_step=355/global_step=355, RunningAvgSamplesPerSec=4.223519827502743, CurrSamplesPerSec=4.1911313135956085, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      355/219726562 | consumed samples:          710 | consumed tokens:      1454080 | elapsed time per iteration (ms): 561.8 | learning rate: 7.733E-07 | global batch size:     2 | lm loss: 9.164829E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.560 | TFLOPs: 51.82 |
[default0]:[2023-07-30 22:25:42,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[7.842474666666667e-07, 7.842474666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:42,520] [INFO] [timer.py:215:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=4.223603865578753, CurrSamplesPerSec=4.208763205994193, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      360/219726562 | consumed samples:          720 | consumed tokens:      1474560 | elapsed time per iteration (ms): 537.6 | learning rate: 7.842E-07 | global batch size:     2 | lm loss: 9.171452E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.720 | TFLOPs: 54.15 |
[default0]:[2023-07-30 22:25:45,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=365, skipped=0, lr=[7.951701333333334e-07, 7.951701333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:45,235] [INFO] [timer.py:215:stop] epoch=0/micro_step=365/global_step=365, RunningAvgSamplesPerSec=4.22359703649268, CurrSamplesPerSec=4.21864134190143, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      365/219726562 | consumed samples:          730 | consumed tokens:      1495040 | elapsed time per iteration (ms): 542.9 | learning rate: 7.952E-07 | global batch size:     2 | lm loss: 9.192955E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.684 | TFLOPs: 53.63 |
[default0]:[2023-07-30 22:25:47,953] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[8.060928e-07, 8.060928e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:47,953] [INFO] [timer.py:215:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=4.223654658267846, CurrSamplesPerSec=4.247140552037624, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      370/219726562 | consumed samples:          740 | consumed tokens:      1515520 | elapsed time per iteration (ms): 544.2 | learning rate: 8.061E-07 | global batch size:     2 | lm loss: 9.322408E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.675 | TFLOPs: 53.49 |
[default0]:[2023-07-30 22:25:50,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=375, skipped=0, lr=[8.170154666666668e-07, 8.170154666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:50,748] [INFO] [timer.py:215:stop] epoch=0/micro_step=375/global_step=375, RunningAvgSamplesPerSec=4.223880899339973, CurrSamplesPerSec=4.224647530890622, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      375/219726562 | consumed samples:          750 | consumed tokens:      1536000 | elapsed time per iteration (ms): 558.7 | learning rate: 8.170E-07 | global batch size:     2 | lm loss: 9.262402E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.579 | TFLOPs: 52.10 |
[default0]:[2023-07-30 22:25:53,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[8.279381333333334e-07, 8.279381333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:53,518] [INFO] [timer.py:215:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=4.223985437224687, CurrSamplesPerSec=4.193823807174079, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      380/219726562 | consumed samples:          760 | consumed tokens:      1556480 | elapsed time per iteration (ms): 554.2 | learning rate: 8.279E-07 | global batch size:     2 | lm loss: 9.078853E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.608 | TFLOPs: 52.52 |
[default0]:[2023-07-30 22:25:56,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=385, skipped=0, lr=[8.388608e-07, 8.388608e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:56,225] [INFO] [timer.py:215:stop] epoch=0/micro_step=385/global_step=385, RunningAvgSamplesPerSec=4.223942877802107, CurrSamplesPerSec=4.220477289913398, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      385/219726562 | consumed samples:          770 | consumed tokens:      1576960 | elapsed time per iteration (ms): 540.6 | learning rate: 8.389E-07 | global batch size:     2 | lm loss: 9.292293E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.699 | TFLOPs: 53.85 |
[default0]:[2023-07-30 22:25:58,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[8.497834666666668e-07, 8.497834666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:25:58,909] [INFO] [timer.py:215:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=4.224224508811207, CurrSamplesPerSec=4.23937919933695, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      390/219726562 | consumed samples:          780 | consumed tokens:      1597440 | elapsed time per iteration (ms): 537.0 | learning rate: 8.498E-07 | global batch size:     2 | lm loss: 9.069400E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.724 | TFLOPs: 54.21 |
[default0]:[2023-07-30 22:26:01,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=395, skipped=0, lr=[8.607061333333334e-07, 8.607061333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:01,634] [INFO] [timer.py:215:stop] epoch=0/micro_step=395/global_step=395, RunningAvgSamplesPerSec=4.22444335020273, CurrSamplesPerSec=4.216932057420117, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      395/219726562 | consumed samples:          790 | consumed tokens:      1617920 | elapsed time per iteration (ms): 544.9 | learning rate: 8.607E-07 | global batch size:     2 | lm loss: 9.183579E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.670 | TFLOPs: 53.43 |
[default0]:[2023-07-30 22:26:04,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[8.716288000000001e-07, 8.716288000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:04,333] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=4.2244092792639085, CurrSamplesPerSec=4.239518464423072, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      400/219726562 | consumed samples:          800 | consumed tokens:      1638400 | elapsed time per iteration (ms): 540.9 | learning rate: 8.716E-07 | global batch size:     2 | lm loss: 9.151853E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.697 | TFLOPs: 53.82 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 400 | lm loss value: 9.155387E+00 | lm loss PPL: 9.465292E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:26:14,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=405, skipped=0, lr=[8.825514666666668e-07, 8.825514666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:14,283] [INFO] [timer.py:215:stop] epoch=0/micro_step=405/global_step=405, RunningAvgSamplesPerSec=4.224267721502712, CurrSamplesPerSec=4.233431490551421, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      405/219726562 | consumed samples:          810 | consumed tokens:      1658880 | elapsed time per iteration (ms): 1988.6 | learning rate: 8.826E-07 | global batch size:     2 | lm loss: 9.193082E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.006 | TFLOPs: 14.64 |
[default0]:[2023-07-30 22:26:16,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[8.934741333333333e-07, 8.934741333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:16,969] [INFO] [timer.py:215:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=4.224229909701236, CurrSamplesPerSec=4.207099894328601, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      410/219726562 | consumed samples:          820 | consumed tokens:      1679360 | elapsed time per iteration (ms): 537.4 | learning rate: 8.935E-07 | global batch size:     2 | lm loss: 9.086125E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.722 | TFLOPs: 54.18 |
[default0]:[2023-07-30 22:26:19,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=415, skipped=0, lr=[9.043968e-07, 9.043968e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:19,680] [INFO] [timer.py:215:stop] epoch=0/micro_step=415/global_step=415, RunningAvgSamplesPerSec=4.224514098707448, CurrSamplesPerSec=4.2428914716880355, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      415/219726562 | consumed samples:          830 | consumed tokens:      1699840 | elapsed time per iteration (ms): 542.1 | learning rate: 9.044E-07 | global batch size:     2 | lm loss: 9.100069E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.689 | TFLOPs: 53.70 |
[default0]:[2023-07-30 22:26:22,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[9.153194666666666e-07, 9.153194666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:22,301] [INFO] [timer.py:215:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=4.2245254058054424, CurrSamplesPerSec=4.136704526177504, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      420/219726562 | consumed samples:          840 | consumed tokens:      1720320 | elapsed time per iteration (ms): 524.3 | learning rate: 9.153E-07 | global batch size:     2 | lm loss: 9.121086E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.814 | TFLOPs: 55.52 |
[default0]:[2023-07-30 22:26:24,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=425, skipped=0, lr=[9.262421333333334e-07, 9.262421333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:24,979] [INFO] [timer.py:215:stop] epoch=0/micro_step=425/global_step=425, RunningAvgSamplesPerSec=4.224500001956657, CurrSamplesPerSec=4.203960497262211, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      425/219726562 | consumed samples:          850 | consumed tokens:      1740800 | elapsed time per iteration (ms): 535.8 | learning rate: 9.262E-07 | global batch size:     2 | lm loss: 9.147396E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.733 | TFLOPs: 54.34 |
[default0]:[2023-07-30 22:26:27,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[9.371648e-07, 9.371648e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:27,603] [INFO] [timer.py:215:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=4.224471637949727, CurrSamplesPerSec=4.222140572796447, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      430/219726562 | consumed samples:          860 | consumed tokens:      1761280 | elapsed time per iteration (ms): 524.7 | learning rate: 9.372E-07 | global batch size:     2 | lm loss: 9.137100E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.812 | TFLOPs: 55.49 |
[default0]:[2023-07-30 22:26:30,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=435, skipped=0, lr=[9.480874666666667e-07, 9.480874666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:30,206] [INFO] [timer.py:215:stop] epoch=0/micro_step=435/global_step=435, RunningAvgSamplesPerSec=4.224734155192559, CurrSamplesPerSec=4.240099191315807, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      435/219726562 | consumed samples:          870 | consumed tokens:      1781760 | elapsed time per iteration (ms): 520.6 | learning rate: 9.481E-07 | global batch size:     2 | lm loss: 8.939015E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.842 | TFLOPs: 55.92 |
[default0]:[2023-07-30 22:26:32,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[9.590101333333334e-07, 9.590101333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:32,849] [INFO] [timer.py:215:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=4.2247950985872755, CurrSamplesPerSec=4.205109027646538, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      440/219726562 | consumed samples:          880 | consumed tokens:      1802240 | elapsed time per iteration (ms): 528.5 | learning rate: 9.590E-07 | global batch size:     2 | lm loss: 9.055381E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.785 | TFLOPs: 55.09 |
[default0]:[2023-07-30 22:26:35,418] [INFO] [logging.py:96:log_dist] [Rank 0] step=445, skipped=0, lr=[9.699328e-07, 9.699328e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:35,419] [INFO] [timer.py:215:stop] epoch=0/micro_step=445/global_step=445, RunningAvgSamplesPerSec=4.224524229436998, CurrSamplesPerSec=4.159513826356355, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      445/219726562 | consumed samples:          890 | consumed tokens:      1822720 | elapsed time per iteration (ms): 514.1 | learning rate: 9.699E-07 | global batch size:     2 | lm loss: 9.005764E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.890 | TFLOPs: 56.62 |
[default0]:[2023-07-30 22:26:38,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[9.808554666666666e-07, 9.808554666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:38,049] [INFO] [timer.py:215:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=4.224445522528466, CurrSamplesPerSec=4.2395763156870645, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      450/219726562 | consumed samples:          900 | consumed tokens:      1843200 | elapsed time per iteration (ms): 525.9 | learning rate: 9.809E-07 | global batch size:     2 | lm loss: 9.133785E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.803 | TFLOPs: 55.36 |
[default0]:[2023-07-30 22:26:40,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=455, skipped=0, lr=[9.917781333333335e-07, 9.917781333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:40,706] [INFO] [timer.py:215:stop] epoch=0/micro_step=455/global_step=455, RunningAvgSamplesPerSec=4.224646229905006, CurrSamplesPerSec=4.235947600864095, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      455/219726562 | consumed samples:          910 | consumed tokens:      1863680 | elapsed time per iteration (ms): 531.7 | learning rate: 9.918E-07 | global batch size:     2 | lm loss: 9.039713E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.761 | TFLOPs: 54.75 |
[default0]:[2023-07-30 22:26:43,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[1.0027008e-06, 1.0027008e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:43,375] [INFO] [timer.py:215:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=4.224647651671711, CurrSamplesPerSec=4.233880195123921, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      460/219726562 | consumed samples:          920 | consumed tokens:      1884160 | elapsed time per iteration (ms): 533.5 | learning rate: 1.003E-06 | global batch size:     2 | lm loss: 9.009563E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.749 | TFLOPs: 54.56 |
[default0]:[2023-07-30 22:26:46,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=465, skipped=0, lr=[1.0136234666666667e-06, 1.0136234666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:46,167] [INFO] [timer.py:215:stop] epoch=0/micro_step=465/global_step=465, RunningAvgSamplesPerSec=4.2246480317740005, CurrSamplesPerSec=4.215122489978001, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      465/219726562 | consumed samples:          930 | consumed tokens:      1904640 | elapsed time per iteration (ms): 558.6 | learning rate: 1.014E-06 | global batch size:     2 | lm loss: 8.977258E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.580 | TFLOPs: 52.11 |
[default0]:[2023-07-30 22:26:48,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[1.0245461333333334e-06, 1.0245461333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:48,900] [INFO] [timer.py:215:stop] epoch=0/micro_step=470/global_step=470, RunningAvgSamplesPerSec=4.224764702454737, CurrSamplesPerSec=4.244162296579063, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      470/219726562 | consumed samples:          940 | consumed tokens:      1925120 | elapsed time per iteration (ms): 546.3 | learning rate: 1.025E-06 | global batch size:     2 | lm loss: 9.085698E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.661 | TFLOPs: 53.29 |
[default0]:[2023-07-30 22:26:51,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=475, skipped=0, lr=[1.0354688000000002e-06, 1.0354688000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:51,590] [INFO] [timer.py:215:stop] epoch=0/micro_step=475/global_step=475, RunningAvgSamplesPerSec=4.224984376105993, CurrSamplesPerSec=4.242709067754006, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      475/219726562 | consumed samples:          950 | consumed tokens:      1945600 | elapsed time per iteration (ms): 538.0 | learning rate: 1.035E-06 | global batch size:     2 | lm loss: 8.982824E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.718 | TFLOPs: 54.11 |
[default0]:[2023-07-30 22:26:54,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[1.0463914666666668e-06, 1.0463914666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:54,391] [INFO] [timer.py:215:stop] epoch=0/micro_step=480/global_step=480, RunningAvgSamplesPerSec=4.224682636669632, CurrSamplesPerSec=4.196874585683731, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      480/219726562 | consumed samples:          960 | consumed tokens:      1966080 | elapsed time per iteration (ms): 560.2 | learning rate: 1.046E-06 | global batch size:     2 | lm loss: 9.049940E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.570 | TFLOPs: 51.96 |
[default0]:[2023-07-30 22:26:57,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=485, skipped=0, lr=[1.0573141333333335e-06, 1.0573141333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:57,088] [INFO] [timer.py:215:stop] epoch=0/micro_step=485/global_step=485, RunningAvgSamplesPerSec=4.2246437073715315, CurrSamplesPerSec=4.217080451518652, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      485/219726562 | consumed samples:          970 | consumed tokens:      1986560 | elapsed time per iteration (ms): 540.6 | learning rate: 1.057E-06 | global batch size:     2 | lm loss: 9.033333E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.699 | TFLOPs: 53.85 |
[default0]:[2023-07-30 22:26:59,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[1.0682368e-06, 1.0682368e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:26:59,681] [INFO] [timer.py:215:stop] epoch=0/micro_step=490/global_step=490, RunningAvgSamplesPerSec=4.224840720181175, CurrSamplesPerSec=4.23824185329309, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      490/219726562 | consumed samples:          980 | consumed tokens:      2007040 | elapsed time per iteration (ms): 517.0 | learning rate: 1.068E-06 | global batch size:     2 | lm loss: 8.983987E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.869 | TFLOPs: 56.31 |
[default0]:[2023-07-30 22:27:02,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=495, skipped=0, lr=[1.0791594666666667e-06, 1.0791594666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:02,375] [INFO] [timer.py:215:stop] epoch=0/micro_step=495/global_step=495, RunningAvgSamplesPerSec=4.224949033644693, CurrSamplesPerSec=4.187210864795037, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      495/219726562 | consumed samples:          990 | consumed tokens:      2027520 | elapsed time per iteration (ms): 539.9 | learning rate: 1.079E-06 | global batch size:     2 | lm loss: 8.799986E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.704 | TFLOPs: 53.92 |
[default0]:[2023-07-30 22:27:05,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[1.0900821333333333e-06, 1.0900821333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:05,019] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=500, RunningAvgSamplesPerSec=4.224785718674667, CurrSamplesPerSec=4.212785489944386, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      500/219726562 | consumed samples:         1000 | consumed tokens:      2048000 | elapsed time per iteration (ms): 527.7 | learning rate: 1.090E-06 | global batch size:     2 | lm loss: 9.110824E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.790 | TFLOPs: 55.16 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 500 | lm loss value: 9.042959E+00 | lm loss PPL: 8.458771E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:27:15,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=505, skipped=0, lr=[1.1010048e-06, 1.1010048e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:15,114] [INFO] [timer.py:215:stop] epoch=0/micro_step=505/global_step=505, RunningAvgSamplesPerSec=4.224641888302815, CurrSamplesPerSec=4.193565932395899, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      505/219726562 | consumed samples:         1010 | consumed tokens:      2068480 | elapsed time per iteration (ms): 2019.8 | learning rate: 1.101E-06 | global batch size:     2 | lm loss: 8.822285E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.990 | TFLOPs: 14.41 |
[default0]:[2023-07-30 22:27:17,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[1.1119274666666666e-06, 1.1119274666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:17,791] [INFO] [timer.py:215:stop] epoch=0/micro_step=510/global_step=510, RunningAvgSamplesPerSec=4.224673539768959, CurrSamplesPerSec=4.228904899890908, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      510/219726562 | consumed samples:         1020 | consumed tokens:      2088960 | elapsed time per iteration (ms): 534.8 | learning rate: 1.112E-06 | global batch size:     2 | lm loss: 9.141898E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.740 | TFLOPs: 54.43 |
[default0]:[2023-07-30 22:27:20,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=515, skipped=0, lr=[1.1228501333333334e-06, 1.1228501333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:20,563] [INFO] [timer.py:215:stop] epoch=0/micro_step=515/global_step=515, RunningAvgSamplesPerSec=4.224844573749405, CurrSamplesPerSec=4.191231827070757, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      515/219726562 | consumed samples:         1030 | consumed tokens:      2109440 | elapsed time per iteration (ms): 554.3 | learning rate: 1.123E-06 | global batch size:     2 | lm loss: 8.833755E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.608 | TFLOPs: 52.52 |
[default0]:[2023-07-30 22:27:23,376] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[1.1337728e-06, 1.1337728e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:23,378] [INFO] [timer.py:215:stop] epoch=0/micro_step=520/global_step=520, RunningAvgSamplesPerSec=4.224814624339432, CurrSamplesPerSec=4.177429274445876, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      520/219726562 | consumed samples:         1040 | consumed tokens:      2129920 | elapsed time per iteration (ms): 563.4 | learning rate: 1.134E-06 | global batch size:     2 | lm loss: 8.874435E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.550 | TFLOPs: 51.67 |
[default0]:[2023-07-30 22:27:26,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=525, skipped=0, lr=[1.1446954666666667e-06, 1.1446954666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:26,340] [INFO] [timer.py:215:stop] epoch=0/micro_step=525/global_step=525, RunningAvgSamplesPerSec=4.22479306329206, CurrSamplesPerSec=4.216315273887329, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      525/219726562 | consumed samples:         1050 | consumed tokens:      2150400 | elapsed time per iteration (ms): 592.0 | learning rate: 1.145E-06 | global batch size:     2 | lm loss: 8.903413E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.379 | TFLOPs: 49.18 |
[default0]:[2023-07-30 22:27:29,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[1.1556181333333333e-06, 1.1556181333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:29,167] [INFO] [timer.py:215:stop] epoch=0/micro_step=530/global_step=530, RunningAvgSamplesPerSec=4.224917794346527, CurrSamplesPerSec=4.248205227932001, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      530/219726562 | consumed samples:         1060 | consumed tokens:      2170880 | elapsed time per iteration (ms): 565.3 | learning rate: 1.156E-06 | global batch size:     2 | lm loss: 8.905594E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.538 | TFLOPs: 51.49 |
[default0]:[2023-07-30 22:27:31,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=535, skipped=0, lr=[1.1665408000000002e-06, 1.1665408000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:31,870] [INFO] [timer.py:215:stop] epoch=0/micro_step=535/global_step=535, RunningAvgSamplesPerSec=4.225113627660934, CurrSamplesPerSec=4.235316689159475, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      535/219726562 | consumed samples:         1070 | consumed tokens:      2191360 | elapsed time per iteration (ms): 541.0 | learning rate: 1.167E-06 | global batch size:     2 | lm loss: 8.971586E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.697 | TFLOPs: 53.81 |
[default0]:[2023-07-30 22:27:34,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[1.1774634666666668e-06, 1.1774634666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:34,629] [INFO] [timer.py:215:stop] epoch=0/micro_step=540/global_step=540, RunningAvgSamplesPerSec=4.224975551627517, CurrSamplesPerSec=4.226784727236448, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      540/219726562 | consumed samples:         1080 | consumed tokens:      2211840 | elapsed time per iteration (ms): 551.8 | learning rate: 1.177E-06 | global batch size:     2 | lm loss: 8.985469E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.624 | TFLOPs: 52.76 |
[default0]:[2023-07-30 22:27:37,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=545, skipped=0, lr=[1.1883861333333334e-06, 1.1883861333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:37,399] [INFO] [timer.py:215:stop] epoch=0/micro_step=545/global_step=545, RunningAvgSamplesPerSec=4.224946541328422, CurrSamplesPerSec=4.216497534774285, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      545/219726562 | consumed samples:         1090 | consumed tokens:      2232320 | elapsed time per iteration (ms): 553.5 | learning rate: 1.188E-06 | global batch size:     2 | lm loss: 9.058882E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.613 | TFLOPs: 52.59 |
[default0]:[2023-07-30 22:27:40,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[1.1993088e-06, 1.1993088e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:40,216] [INFO] [timer.py:215:stop] epoch=0/micro_step=550/global_step=550, RunningAvgSamplesPerSec=4.225108487265401, CurrSamplesPerSec=4.235455687459355, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      550/219726562 | consumed samples:         1100 | consumed tokens:      2252800 | elapsed time per iteration (ms): 563.3 | learning rate: 1.199E-06 | global batch size:     2 | lm loss: 8.949502E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.550 | TFLOPs: 51.68 |
[default0]:[2023-07-30 22:27:42,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=555, skipped=0, lr=[1.2102314666666667e-06, 1.2102314666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:42,975] [INFO] [timer.py:215:stop] epoch=0/micro_step=555/global_step=555, RunningAvgSamplesPerSec=4.2251384838049795, CurrSamplesPerSec=4.18501744628669, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      555/219726562 | consumed samples:         1110 | consumed tokens:      2273280 | elapsed time per iteration (ms): 552.3 | learning rate: 1.210E-06 | global batch size:     2 | lm loss: 8.893269E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.622 | TFLOPs: 52.71 |
[default0]:[2023-07-30 22:27:45,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[1.2211541333333335e-06, 1.2211541333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:45,771] [INFO] [timer.py:215:stop] epoch=0/micro_step=560/global_step=560, RunningAvgSamplesPerSec=4.225063190706837, CurrSamplesPerSec=4.21189286460258, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      560/219726562 | consumed samples:         1120 | consumed tokens:      2293760 | elapsed time per iteration (ms): 559.0 | learning rate: 1.221E-06 | global batch size:     2 | lm loss: 8.673621E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.578 | TFLOPs: 52.08 |
[default0]:[2023-07-30 22:27:48,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=565, skipped=0, lr=[1.2320768000000001e-06, 1.2320768000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:48,502] [INFO] [timer.py:215:stop] epoch=0/micro_step=565/global_step=565, RunningAvgSamplesPerSec=4.2251105077199, CurrSamplesPerSec=4.23999417725409, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      565/219726562 | consumed samples:         1130 | consumed tokens:      2314240 | elapsed time per iteration (ms): 545.9 | learning rate: 1.232E-06 | global batch size:     2 | lm loss: 8.817577E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.664 | TFLOPs: 53.33 |
[default0]:[2023-07-30 22:27:51,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[1.2429994666666668e-06, 1.2429994666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:51,278] [INFO] [timer.py:215:stop] epoch=0/micro_step=570/global_step=570, RunningAvgSamplesPerSec=4.225308015847765, CurrSamplesPerSec=4.250723857764103, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      570/219726562 | consumed samples:         1140 | consumed tokens:      2334720 | elapsed time per iteration (ms): 555.3 | learning rate: 1.243E-06 | global batch size:     2 | lm loss: 8.803727E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.602 | TFLOPs: 52.43 |
[default0]:[2023-07-30 22:27:54,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=575, skipped=0, lr=[1.2539221333333334e-06, 1.2539221333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:54,025] [INFO] [timer.py:215:stop] epoch=0/micro_step=575/global_step=575, RunningAvgSamplesPerSec=4.224972919896879, CurrSamplesPerSec=4.12719376577167, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      575/219726562 | consumed samples:         1150 | consumed tokens:      2355200 | elapsed time per iteration (ms): 549.9 | learning rate: 1.254E-06 | global batch size:     2 | lm loss: 8.833868E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.637 | TFLOPs: 52.94 |
[default0]:[2023-07-30 22:27:56,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[1.2648448e-06, 1.2648448e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:27:56,920] [INFO] [timer.py:215:stop] epoch=0/micro_step=580/global_step=580, RunningAvgSamplesPerSec=4.22481686634727, CurrSamplesPerSec=4.172347023787319, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      580/219726562 | consumed samples:         1160 | consumed tokens:      2375680 | elapsed time per iteration (ms): 578.6 | learning rate: 1.265E-06 | global batch size:     2 | lm loss: 9.005896E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.457 | TFLOPs: 50.31 |
[default0]:[2023-07-30 22:27:59,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=585, skipped=0, lr=[1.2757674666666667e-06, 1.2757674666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:00,000] [INFO] [timer.py:215:stop] epoch=0/micro_step=585/global_step=585, RunningAvgSamplesPerSec=4.224883109710667, CurrSamplesPerSec=4.246037723624642, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      585/219726562 | consumed samples:         1170 | consumed tokens:      2396160 | elapsed time per iteration (ms): 615.8 | learning rate: 1.276E-06 | global batch size:     2 | lm loss: 8.848138E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.248 | TFLOPs: 47.28 |
[default0]:[2023-07-30 22:28:02,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[1.2866901333333333e-06, 1.2866901333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:02,755] [INFO] [timer.py:215:stop] epoch=0/micro_step=590/global_step=590, RunningAvgSamplesPerSec=4.224984218693004, CurrSamplesPerSec=4.191116655783277, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      590/219726562 | consumed samples:         1180 | consumed tokens:      2416640 | elapsed time per iteration (ms): 551.1 | learning rate: 1.287E-06 | global batch size:     2 | lm loss: 8.765118E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.629 | TFLOPs: 52.82 |
[default0]:[2023-07-30 22:28:05,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=595, skipped=0, lr=[1.2976128000000001e-06, 1.2976128000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:05,509] [INFO] [timer.py:215:stop] epoch=0/micro_step=595/global_step=595, RunningAvgSamplesPerSec=4.2246240125516, CurrSamplesPerSec=4.20373297305857, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      595/219726562 | consumed samples:         1190 | consumed tokens:      2437120 | elapsed time per iteration (ms): 550.8 | learning rate: 1.298E-06 | global batch size:     2 | lm loss: 8.865231E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.631 | TFLOPs: 52.86 |
[default0]:[2023-07-30 22:28:08,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[1.3085354666666668e-06, 1.3085354666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:08,327] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=600, RunningAvgSamplesPerSec=4.224610230527876, CurrSamplesPerSec=4.24858390800872, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      600/219726562 | consumed samples:         1200 | consumed tokens:      2457600 | elapsed time per iteration (ms): 563.5 | learning rate: 1.309E-06 | global batch size:     2 | lm loss: 8.821783E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.549 | TFLOPs: 51.66 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 600 | lm loss value: 8.893450E+00 | lm loss PPL: 7.284104E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:28:18,109] [INFO] [logging.py:96:log_dist] [Rank 0] step=605, skipped=0, lr=[1.3194581333333334e-06, 1.3194581333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:18,110] [INFO] [timer.py:215:stop] epoch=0/micro_step=605/global_step=605, RunningAvgSamplesPerSec=4.224367040811163, CurrSamplesPerSec=4.224917754045816, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      605/219726562 | consumed samples:         1210 | consumed tokens:      2478080 | elapsed time per iteration (ms): 1956.5 | learning rate: 1.319E-06 | global batch size:     2 | lm loss: 8.671643E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.022 | TFLOPs: 14.88 |
[default0]:[2023-07-30 22:28:20,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[1.3303808e-06, 1.3303808e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:20,849] [INFO] [timer.py:215:stop] epoch=0/micro_step=610/global_step=610, RunningAvgSamplesPerSec=4.224511197083621, CurrSamplesPerSec=4.242215582792054, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      610/219726562 | consumed samples:         1220 | consumed tokens:      2498560 | elapsed time per iteration (ms): 548.6 | learning rate: 1.330E-06 | global batch size:     2 | lm loss: 8.856162E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.646 | TFLOPs: 53.07 |
[default0]:[2023-07-30 22:28:23,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=615, skipped=0, lr=[1.3413034666666667e-06, 1.3413034666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:23,457] [INFO] [timer.py:215:stop] epoch=0/micro_step=615/global_step=615, RunningAvgSamplesPerSec=4.224437220809698, CurrSamplesPerSec=4.166103644430848, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      615/219726562 | consumed samples:         1230 | consumed tokens:      2519040 | elapsed time per iteration (ms): 520.9 | learning rate: 1.341E-06 | global batch size:     2 | lm loss: 8.838493E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.840 | TFLOPs: 55.89 |
[default0]:[2023-07-30 22:28:26,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[1.3522261333333335e-06, 1.3522261333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:26,106] [INFO] [timer.py:215:stop] epoch=0/micro_step=620/global_step=620, RunningAvgSamplesPerSec=4.22412304147082, CurrSamplesPerSec=4.196356018092847, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      620/219726562 | consumed samples:         1240 | consumed tokens:      2539520 | elapsed time per iteration (ms): 531.8 | learning rate: 1.352E-06 | global batch size:     2 | lm loss: 8.813525E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.761 | TFLOPs: 54.74 |
[default0]:[2023-07-30 22:28:28,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=625, skipped=0, lr=[1.3631488000000001e-06, 1.3631488000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:28,753] [INFO] [timer.py:215:stop] epoch=0/micro_step=625/global_step=625, RunningAvgSamplesPerSec=4.224077542774552, CurrSamplesPerSec=4.251350750039403, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      625/219726562 | consumed samples:         1250 | consumed tokens:      2560000 | elapsed time per iteration (ms): 528.1 | learning rate: 1.363E-06 | global batch size:     2 | lm loss: 8.737071E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.787 | TFLOPs: 55.12 |
[default0]:[2023-07-30 22:28:31,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[1.3740714666666665e-06, 1.3740714666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:31,445] [INFO] [timer.py:215:stop] epoch=0/micro_step=630/global_step=630, RunningAvgSamplesPerSec=4.224266307235088, CurrSamplesPerSec=4.249571806124552, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      630/219726562 | consumed samples:         1260 | consumed tokens:      2580480 | elapsed time per iteration (ms): 537.6 | learning rate: 1.374E-06 | global batch size:     2 | lm loss: 8.774738E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.720 | TFLOPs: 54.15 |
[default0]:[2023-07-30 22:28:34,027] [INFO] [logging.py:96:log_dist] [Rank 0] step=635, skipped=0, lr=[1.3849941333333334e-06, 1.3849941333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:34,029] [INFO] [timer.py:215:stop] epoch=0/micro_step=635/global_step=635, RunningAvgSamplesPerSec=4.224121382162812, CurrSamplesPerSec=4.148933339466267, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      635/219726562 | consumed samples:         1270 | consumed tokens:      2600960 | elapsed time per iteration (ms): 516.9 | learning rate: 1.385E-06 | global batch size:     2 | lm loss: 8.708630E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.869 | TFLOPs: 56.32 |
[default0]:[2023-07-30 22:28:36,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[1.3959168e-06, 1.3959168e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:36,860] [INFO] [timer.py:215:stop] epoch=0/micro_step=640/global_step=640, RunningAvgSamplesPerSec=4.224009639097966, CurrSamplesPerSec=4.220008069149188, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      640/219726562 | consumed samples:         1280 | consumed tokens:      2621440 | elapsed time per iteration (ms): 566.1 | learning rate: 1.396E-06 | global batch size:     2 | lm loss: 8.891296E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.533 | TFLOPs: 51.42 |
[default0]:[2023-07-30 22:28:39,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=645, skipped=0, lr=[1.4068394666666666e-06, 1.4068394666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:39,521] [INFO] [timer.py:215:stop] epoch=0/micro_step=645/global_step=645, RunningAvgSamplesPerSec=4.22409351292932, CurrSamplesPerSec=4.244419989020358, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      645/219726562 | consumed samples:         1290 | consumed tokens:      2641920 | elapsed time per iteration (ms): 532.5 | learning rate: 1.407E-06 | global batch size:     2 | lm loss: 8.663414E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.756 | TFLOPs: 54.67 |
[default0]:[2023-07-30 22:28:42,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=0, lr=[1.4177621333333333e-06, 1.4177621333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:42,210] [INFO] [timer.py:215:stop] epoch=0/micro_step=650/global_step=650, RunningAvgSamplesPerSec=4.224219731834553, CurrSamplesPerSec=4.24843328852915, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      650/219726562 | consumed samples:         1300 | consumed tokens:      2662400 | elapsed time per iteration (ms): 537.3 | learning rate: 1.418E-06 | global batch size:     2 | lm loss: 8.711404E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.722 | TFLOPs: 54.18 |
[default0]:[2023-07-30 22:28:44,882] [INFO] [logging.py:96:log_dist] [Rank 0] step=655, skipped=0, lr=[1.4286848e-06, 1.4286848e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:44,884] [INFO] [timer.py:215:stop] epoch=0/micro_step=655/global_step=655, RunningAvgSamplesPerSec=4.224045022320179, CurrSamplesPerSec=4.236373305106358, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      655/219726562 | consumed samples:         1310 | consumed tokens:      2682880 | elapsed time per iteration (ms): 535.0 | learning rate: 1.429E-06 | global batch size:     2 | lm loss: 8.743120E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.738 | TFLOPs: 54.41 |
[default0]:[2023-07-30 22:28:47,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[1.4396074666666667e-06, 1.4396074666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:47,612] [INFO] [timer.py:215:stop] epoch=0/micro_step=660/global_step=660, RunningAvgSamplesPerSec=4.2239505447558745, CurrSamplesPerSec=4.203977351886691, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      660/219726562 | consumed samples:         1320 | consumed tokens:      2703360 | elapsed time per iteration (ms): 545.8 | learning rate: 1.440E-06 | global batch size:     2 | lm loss: 8.768779E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.665 | TFLOPs: 53.34 |
[default0]:[2023-07-30 22:28:50,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=665, skipped=0, lr=[1.4505301333333334e-06, 1.4505301333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:50,329] [INFO] [timer.py:215:stop] epoch=0/micro_step=665/global_step=665, RunningAvgSamplesPerSec=4.224122437071912, CurrSamplesPerSec=4.251258104830413, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      665/219726562 | consumed samples:         1330 | consumed tokens:      2723840 | elapsed time per iteration (ms): 543.1 | learning rate: 1.451E-06 | global batch size:     2 | lm loss: 8.635604E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.682 | TFLOPs: 53.60 |
[default0]:[2023-07-30 22:28:52,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[1.4614528e-06, 1.4614528e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:52,941] [INFO] [timer.py:215:stop] epoch=0/micro_step=670/global_step=670, RunningAvgSamplesPerSec=4.224326105401607, CurrSamplesPerSec=4.211687741151604, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      670/219726562 | consumed samples:         1340 | consumed tokens:      2744320 | elapsed time per iteration (ms): 522.6 | learning rate: 1.461E-06 | global batch size:     2 | lm loss: 8.599948E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.827 | TFLOPs: 55.70 |
[default0]:[2023-07-30 22:28:55,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=675, skipped=0, lr=[1.4723754666666666e-06, 1.4723754666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:55,626] [INFO] [timer.py:215:stop] epoch=0/micro_step=675/global_step=675, RunningAvgSamplesPerSec=4.224130696689274, CurrSamplesPerSec=4.138220763107979, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      675/219726562 | consumed samples:         1350 | consumed tokens:      2764800 | elapsed time per iteration (ms): 537.2 | learning rate: 1.472E-06 | global batch size:     2 | lm loss: 8.697060E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.723 | TFLOPs: 54.19 |
[default0]:[2023-07-30 22:28:58,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[1.4832981333333335e-06, 1.4832981333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:28:58,415] [INFO] [timer.py:215:stop] epoch=0/micro_step=680/global_step=680, RunningAvgSamplesPerSec=4.224024334902364, CurrSamplesPerSec=4.237524632489072, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      680/219726562 | consumed samples:         1360 | consumed tokens:      2785280 | elapsed time per iteration (ms): 557.4 | learning rate: 1.483E-06 | global batch size:     2 | lm loss: 8.633748E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.588 | TFLOPs: 52.23 |
[default0]: iteration      685/219726562 | consumed samples:         1370 | consumed tokens:      2805760 | elapsed time per iteration (ms): 582.8 | learning rate: 1.494E-06 | global batch size:     2 | lm loss: 8.462216E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.432 | TFLOPs: 49.95 |
[default0]:[2023-07-30 22:29:04,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[1.5051434666666667e-06, 1.5051434666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:04,165] [INFO] [timer.py:215:stop] epoch=0/micro_step=690/global_step=690, RunningAvgSamplesPerSec=4.223367694094028, CurrSamplesPerSec=4.175385329073966, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      690/219726562 | consumed samples:         1380 | consumed tokens:      2826240 | elapsed time per iteration (ms): 567.1 | learning rate: 1.505E-06 | global batch size:     2 | lm loss: 8.690472E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.527 | TFLOPs: 51.34 |
[default0]:[2023-07-30 22:29:06,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=695, skipped=0, lr=[1.5160661333333333e-06, 1.5160661333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:06,921] [INFO] [timer.py:215:stop] epoch=0/micro_step=695/global_step=695, RunningAvgSamplesPerSec=4.223251561766553, CurrSamplesPerSec=4.221144145046277, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      695/219726562 | consumed samples:         1390 | consumed tokens:      2846720 | elapsed time per iteration (ms): 551.2 | learning rate: 1.516E-06 | global batch size:     2 | lm loss: 8.794385E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.628 | TFLOPs: 52.81 |
[default0]:[2023-07-30 22:29:09,674] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[1.5269888e-06, 1.5269888e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:09,675] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=700, RunningAvgSamplesPerSec=4.223328145007867, CurrSamplesPerSec=4.248041727857396, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      700/219726562 | consumed samples:         1400 | consumed tokens:      2867200 | elapsed time per iteration (ms): 550.6 | learning rate: 1.527E-06 | global batch size:     2 | lm loss: 8.510785E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.633 | TFLOPs: 52.88 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 700 | lm loss value: 8.558323E+00 | lm loss PPL: 5.209936E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:29:19,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=705, skipped=0, lr=[1.5379114666666668e-06, 1.5379114666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:19,691] [INFO] [timer.py:215:stop] epoch=0/micro_step=705/global_step=705, RunningAvgSamplesPerSec=4.223380212520813, CurrSamplesPerSec=4.2508638692117975, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      705/219726562 | consumed samples:         1410 | consumed tokens:      2887680 | elapsed time per iteration (ms): 2003.1 | learning rate: 1.538E-06 | global batch size:     2 | lm loss: 8.672952E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.998 | TFLOPs: 14.53 |
[default0]:[2023-07-30 22:29:22,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[1.5488341333333334e-06, 1.5488341333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:22,426] [INFO] [timer.py:215:stop] epoch=0/micro_step=710/global_step=710, RunningAvgSamplesPerSec=4.223501450294023, CurrSamplesPerSec=4.218946868659842, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      710/219726562 | consumed samples:         1420 | consumed tokens:      2908160 | elapsed time per iteration (ms): 546.9 | learning rate: 1.549E-06 | global batch size:     2 | lm loss: 8.573030E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.657 | TFLOPs: 53.23 |
[default0]:[2023-07-30 22:29:25,107] [INFO] [logging.py:96:log_dist] [Rank 0] step=715, skipped=0, lr=[1.5597568e-06, 1.5597568e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:25,108] [INFO] [timer.py:215:stop] epoch=0/micro_step=715/global_step=715, RunningAvgSamplesPerSec=4.223340824613889, CurrSamplesPerSec=4.218089808429736, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      715/219726562 | consumed samples:         1430 | consumed tokens:      2928640 | elapsed time per iteration (ms): 536.8 | learning rate: 1.560E-06 | global batch size:     2 | lm loss: 8.719740E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.726 | TFLOPs: 54.23 |
[default0]:[2023-07-30 22:29:28,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[1.5706794666666667e-06, 1.5706794666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:28,127] [INFO] [timer.py:215:stop] epoch=0/micro_step=720/global_step=720, RunningAvgSamplesPerSec=4.223229033878496, CurrSamplesPerSec=4.193373071178199, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      720/219726562 | consumed samples:         1440 | consumed tokens:      2949120 | elapsed time per iteration (ms): 603.5 | learning rate: 1.571E-06 | global batch size:     2 | lm loss: 8.563004E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.314 | TFLOPs: 48.24 |
[default0]:[2023-07-30 22:29:30,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=725, skipped=0, lr=[1.5816021333333335e-06, 1.5816021333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:30,849] [INFO] [timer.py:215:stop] epoch=0/micro_step=725/global_step=725, RunningAvgSamplesPerSec=4.223380223318354, CurrSamplesPerSec=4.2468072783826445, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      725/219726562 | consumed samples:         1450 | consumed tokens:      2969600 | elapsed time per iteration (ms): 544.4 | learning rate: 1.582E-06 | global batch size:     2 | lm loss: 8.431514E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.674 | TFLOPs: 53.48 |
[default0]:[2023-07-30 22:29:33,631] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[1.5925248000000002e-06, 1.5925248000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:33,632] [INFO] [timer.py:215:stop] epoch=0/micro_step=730/global_step=730, RunningAvgSamplesPerSec=4.223427438696884, CurrSamplesPerSec=4.2119753425496675, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      730/219726562 | consumed samples:         1460 | consumed tokens:      2990080 | elapsed time per iteration (ms): 557.0 | learning rate: 1.593E-06 | global batch size:     2 | lm loss: 8.613840E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.591 | TFLOPs: 52.27 |
[default0]:[2023-07-30 22:29:36,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=735, skipped=0, lr=[1.6034474666666668e-06, 1.6034474666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:36,502] [INFO] [timer.py:215:stop] epoch=0/micro_step=735/global_step=735, RunningAvgSamplesPerSec=4.223391424387807, CurrSamplesPerSec=4.217459963931969, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      735/219726562 | consumed samples:         1470 | consumed tokens:      3010560 | elapsed time per iteration (ms): 574.0 | learning rate: 1.603E-06 | global batch size:     2 | lm loss: 8.375887E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.485 | TFLOPs: 50.72 |
[default0]:[2023-07-30 22:29:39,191] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[1.6143701333333334e-06, 1.6143701333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:39,191] [INFO] [timer.py:215:stop] epoch=0/micro_step=740/global_step=740, RunningAvgSamplesPerSec=4.223447109892958, CurrSamplesPerSec=4.244334088063043, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      740/219726562 | consumed samples:         1480 | consumed tokens:      3031040 | elapsed time per iteration (ms): 537.5 | learning rate: 1.614E-06 | global batch size:     2 | lm loss: 8.773919E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.721 | TFLOPs: 54.16 |
[default0]:[2023-07-30 22:29:41,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=745, skipped=0, lr=[1.6252928000000003e-06, 1.6252928000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:41,911] [INFO] [timer.py:215:stop] epoch=0/micro_step=745/global_step=745, RunningAvgSamplesPerSec=4.223553887061698, CurrSamplesPerSec=4.243694235358217, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      745/219726562 | consumed samples:         1490 | consumed tokens:      3051520 | elapsed time per iteration (ms): 543.9 | learning rate: 1.625E-06 | global batch size:     2 | lm loss: 8.425952E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.677 | TFLOPs: 53.52 |
[default0]:[2023-07-30 22:29:44,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[1.636215466666667e-06, 1.636215466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:44,660] [INFO] [timer.py:215:stop] epoch=0/micro_step=750/global_step=750, RunningAvgSamplesPerSec=4.223464006642272, CurrSamplesPerSec=4.179262833063803, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      750/219726562 | consumed samples:         1500 | consumed tokens:      3072000 | elapsed time per iteration (ms): 550.2 | learning rate: 1.636E-06 | global batch size:     2 | lm loss: 8.543611E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.635 | TFLOPs: 52.91 |
[default0]:[2023-07-30 22:29:47,376] [INFO] [logging.py:96:log_dist] [Rank 0] step=755, skipped=0, lr=[1.6471381333333335e-06, 1.6471381333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:47,377] [INFO] [timer.py:215:stop] epoch=0/micro_step=755/global_step=755, RunningAvgSamplesPerSec=4.223449294432929, CurrSamplesPerSec=4.219904047906626, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      755/219726562 | consumed samples:         1510 | consumed tokens:      3092480 | elapsed time per iteration (ms): 543.7 | learning rate: 1.647E-06 | global batch size:     2 | lm loss: 8.474832E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.679 | TFLOPs: 53.55 |
[default0]:[2023-07-30 22:29:50,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[1.6580608000000002e-06, 1.6580608000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:50,125] [INFO] [timer.py:215:stop] epoch=0/micro_step=760/global_step=760, RunningAvgSamplesPerSec=4.223552981985467, CurrSamplesPerSec=4.239392054192363, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      760/219726562 | consumed samples:         1520 | consumed tokens:      3112960 | elapsed time per iteration (ms): 551.5 | learning rate: 1.658E-06 | global batch size:     2 | lm loss: 8.463177E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.627 | TFLOPs: 52.79 |
[default0]:[2023-07-30 22:29:52,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=765, skipped=0, lr=[1.6689834666666668e-06, 1.6689834666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:52,844] [INFO] [timer.py:215:stop] epoch=0/micro_step=765/global_step=765, RunningAvgSamplesPerSec=4.22368886772889, CurrSamplesPerSec=4.236495256257064, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      765/219726562 | consumed samples:         1530 | consumed tokens:      3133440 | elapsed time per iteration (ms): 541.6 | learning rate: 1.669E-06 | global batch size:     2 | lm loss: 8.853246E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.693 | TFLOPs: 53.75 |
[default0]:[2023-07-30 22:29:55,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[1.6799061333333336e-06, 1.6799061333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:55,725] [INFO] [timer.py:215:stop] epoch=0/micro_step=770/global_step=770, RunningAvgSamplesPerSec=4.223581002847118, CurrSamplesPerSec=4.191895755888242, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      770/219726562 | consumed samples:         1540 | consumed tokens:      3153920 | elapsed time per iteration (ms): 576.6 | learning rate: 1.680E-06 | global batch size:     2 | lm loss: 8.558568E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.469 | TFLOPs: 50.49 |
[default0]:[2023-07-30 22:29:58,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=775, skipped=0, lr=[1.6908288000000003e-06, 1.6908288000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:29:58,598] [INFO] [timer.py:215:stop] epoch=0/micro_step=775/global_step=775, RunningAvgSamplesPerSec=4.223582059339326, CurrSamplesPerSec=4.222374344643188, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      775/219726562 | consumed samples:         1550 | consumed tokens:      3174400 | elapsed time per iteration (ms): 573.9 | learning rate: 1.691E-06 | global batch size:     2 | lm loss: 8.570475E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.485 | TFLOPs: 50.73 |
[default0]:[2023-07-30 22:30:01,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=0, lr=[1.7017514666666667e-06, 1.7017514666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:01,397] [INFO] [timer.py:215:stop] epoch=0/micro_step=780/global_step=780, RunningAvgSamplesPerSec=4.223723277702114, CurrSamplesPerSec=4.240973796139619, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      780/219726562 | consumed samples:         1560 | consumed tokens:      3194880 | elapsed time per iteration (ms): 559.9 | learning rate: 1.702E-06 | global batch size:     2 | lm loss: 8.358476E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.572 | TFLOPs: 52.00 |
[default0]:[2023-07-30 22:30:04,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=785, skipped=0, lr=[1.7126741333333333e-06, 1.7126741333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:04,081] [INFO] [timer.py:215:stop] epoch=0/micro_step=785/global_step=785, RunningAvgSamplesPerSec=4.223751795229376, CurrSamplesPerSec=4.2094284765161225, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      785/219726562 | consumed samples:         1570 | consumed tokens:      3215360 | elapsed time per iteration (ms): 536.9 | learning rate: 1.713E-06 | global batch size:     2 | lm loss: 8.437643E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.725 | TFLOPs: 54.22 |
[default0]:[2023-07-30 22:30:06,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=0, lr=[1.7235968e-06, 1.7235968e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:06,822] [INFO] [timer.py:215:stop] epoch=0/micro_step=790/global_step=790, RunningAvgSamplesPerSec=4.2237433922827226, CurrSamplesPerSec=4.207049255720205, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      790/219726562 | consumed samples:         1580 | consumed tokens:      3235840 | elapsed time per iteration (ms): 548.0 | learning rate: 1.724E-06 | global batch size:     2 | lm loss: 8.286955E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.649 | TFLOPs: 53.12 |
[default0]:[2023-07-30 22:30:09,593] [INFO] [logging.py:96:log_dist] [Rank 0] step=795, skipped=0, lr=[1.7345194666666666e-06, 1.7345194666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:09,594] [INFO] [timer.py:215:stop] epoch=0/micro_step=795/global_step=795, RunningAvgSamplesPerSec=4.2238243161350475, CurrSamplesPerSec=4.234568393570263, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      795/219726562 | consumed samples:         1590 | consumed tokens:      3256320 | elapsed time per iteration (ms): 554.2 | learning rate: 1.735E-06 | global batch size:     2 | lm loss: 8.532963E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.609 | TFLOPs: 52.53 |
[default0]:[2023-07-30 22:30:12,407] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=0, lr=[1.7454421333333334e-06, 1.7454421333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:12,407] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=800, RunningAvgSamplesPerSec=4.223899484835838, CurrSamplesPerSec=4.231725145461314, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      800/219726562 | consumed samples:         1600 | consumed tokens:      3276800 | elapsed time per iteration (ms): 562.8 | learning rate: 1.745E-06 | global batch size:     2 | lm loss: 8.354410E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.554 | TFLOPs: 51.73 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 800 | lm loss value: 8.408654E+00 | lm loss PPL: 4.485720E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:30:22,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=805, skipped=0, lr=[1.7563648e-06, 1.7563648e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:22,397] [INFO] [timer.py:215:stop] epoch=0/micro_step=805/global_step=805, RunningAvgSamplesPerSec=4.2239970687092265, CurrSamplesPerSec=4.234799268000252, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      805/219726562 | consumed samples:         1610 | consumed tokens:      3297280 | elapsed time per iteration (ms): 1998.0 | learning rate: 1.756E-06 | global batch size:     2 | lm loss: 8.239452E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.001 | TFLOPs: 14.57 |
[default0]:[2023-07-30 22:30:25,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=0, lr=[1.7672874666666667e-06, 1.7672874666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:25,068] [INFO] [timer.py:215:stop] epoch=0/micro_step=810/global_step=810, RunningAvgSamplesPerSec=4.223968683750993, CurrSamplesPerSec=4.230284686400177, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      810/219726562 | consumed samples:         1620 | consumed tokens:      3317760 | elapsed time per iteration (ms): 534.9 | learning rate: 1.767E-06 | global batch size:     2 | lm loss: 8.511914E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.739 | TFLOPs: 54.43 |
[default0]:[2023-07-30 22:30:27,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=815, skipped=0, lr=[1.7782101333333333e-06, 1.7782101333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:27,804] [INFO] [timer.py:215:stop] epoch=0/micro_step=815/global_step=815, RunningAvgSamplesPerSec=4.223914751641231, CurrSamplesPerSec=4.220239481735002, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      815/219726562 | consumed samples:         1630 | consumed tokens:      3338240 | elapsed time per iteration (ms): 546.5 | learning rate: 1.778E-06 | global batch size:     2 | lm loss: 8.435202E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.660 | TFLOPs: 53.27 |
[default0]:[2023-07-30 22:30:30,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=0, lr=[1.7891328e-06, 1.7891328e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:30,423] [INFO] [timer.py:215:stop] epoch=0/micro_step=820/global_step=820, RunningAvgSamplesPerSec=4.223980088490675, CurrSamplesPerSec=4.223539339306681, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      820/219726562 | consumed samples:         1640 | consumed tokens:      3358720 | elapsed time per iteration (ms): 523.8 | learning rate: 1.789E-06 | global batch size:     2 | lm loss: 8.221787E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.818 | TFLOPs: 55.57 |
[default0]:[2023-07-30 22:30:32,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=825, skipped=0, lr=[1.8000554666666668e-06, 1.8000554666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:32,995] [INFO] [timer.py:215:stop] epoch=0/micro_step=825/global_step=825, RunningAvgSamplesPerSec=4.224093932298397, CurrSamplesPerSec=4.24339799308095, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      825/219726562 | consumed samples:         1650 | consumed tokens:      3379200 | elapsed time per iteration (ms): 514.4 | learning rate: 1.800E-06 | global batch size:     2 | lm loss: 8.587384E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.888 | TFLOPs: 56.60 |
[default0]:[2023-07-30 22:30:35,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=0, lr=[1.8109781333333334e-06, 1.8109781333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:35,682] [INFO] [timer.py:215:stop] epoch=0/micro_step=830/global_step=830, RunningAvgSamplesPerSec=4.224073384193541, CurrSamplesPerSec=4.212449124604233, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      830/219726562 | consumed samples:         1660 | consumed tokens:      3399680 | elapsed time per iteration (ms): 537.5 | learning rate: 1.811E-06 | global batch size:     2 | lm loss: 8.328186E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.721 | TFLOPs: 54.17 |
[default0]:[2023-07-30 22:30:38,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=835, skipped=0, lr=[1.8219008e-06, 1.8219008e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:38,401] [INFO] [timer.py:215:stop] epoch=0/micro_step=835/global_step=835, RunningAvgSamplesPerSec=4.223858668697691, CurrSamplesPerSec=4.14059019371207, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      835/219726562 | consumed samples:         1670 | consumed tokens:      3420160 | elapsed time per iteration (ms): 544.1 | learning rate: 1.822E-06 | global batch size:     2 | lm loss: 8.247008E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.676 | TFLOPs: 53.50 |
[default0]:[2023-07-30 22:30:41,120] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=0, lr=[1.8328234666666666e-06, 1.8328234666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:41,121] [INFO] [timer.py:215:stop] epoch=0/micro_step=840/global_step=840, RunningAvgSamplesPerSec=4.224029326067824, CurrSamplesPerSec=4.239674880736199, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      840/219726562 | consumed samples:         1680 | consumed tokens:      3440640 | elapsed time per iteration (ms): 543.7 | learning rate: 1.833E-06 | global batch size:     2 | lm loss: 8.452134E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.678 | TFLOPs: 53.54 |
[default0]:[2023-07-30 22:30:43,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=845, skipped=0, lr=[1.8437461333333335e-06, 1.8437461333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:43,936] [INFO] [timer.py:215:stop] epoch=0/micro_step=845/global_step=845, RunningAvgSamplesPerSec=4.224047616257681, CurrSamplesPerSec=4.193243109493298, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      845/219726562 | consumed samples:         1690 | consumed tokens:      3461120 | elapsed time per iteration (ms): 562.7 | learning rate: 1.844E-06 | global batch size:     2 | lm loss: 8.412621E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.554 | TFLOPs: 51.74 |
[default0]:[2023-07-30 22:30:46,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=0, lr=[1.8546688000000001e-06, 1.8546688000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:46,770] [INFO] [timer.py:215:stop] epoch=0/micro_step=850/global_step=850, RunningAvgSamplesPerSec=4.223984925927173, CurrSamplesPerSec=4.164186520986962, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      850/219726562 | consumed samples:         1700 | consumed tokens:      3481600 | elapsed time per iteration (ms): 566.8 | learning rate: 1.855E-06 | global batch size:     2 | lm loss: 8.385265E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.528 | TFLOPs: 51.36 |
[default0]:[2023-07-30 22:30:49,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=855, skipped=0, lr=[1.8655914666666667e-06, 1.8655914666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:49,638] [INFO] [timer.py:215:stop] epoch=0/micro_step=855/global_step=855, RunningAvgSamplesPerSec=4.223929407855653, CurrSamplesPerSec=4.2437865511868385, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      855/219726562 | consumed samples:         1710 | consumed tokens:      3502080 | elapsed time per iteration (ms): 574.2 | learning rate: 1.866E-06 | global batch size:     2 | lm loss: 8.367136E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.483 | TFLOPs: 50.70 |
[default0]:[2023-07-30 22:30:52,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[1.8765141333333334e-06, 1.8765141333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:52,369] [INFO] [timer.py:215:stop] epoch=0/micro_step=860/global_step=860, RunningAvgSamplesPerSec=4.2240594960303115, CurrSamplesPerSec=4.239471327523435, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      860/219726562 | consumed samples:         1720 | consumed tokens:      3522560 | elapsed time per iteration (ms): 545.6 | learning rate: 1.877E-06 | global batch size:     2 | lm loss: 8.215950E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.666 | TFLOPs: 53.36 |
[default0]:[2023-07-30 22:30:55,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=865, skipped=0, lr=[1.8874368e-06, 1.8874368e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:55,166] [INFO] [timer.py:215:stop] epoch=0/micro_step=865/global_step=865, RunningAvgSamplesPerSec=4.224048474310174, CurrSamplesPerSec=4.240515012347052, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      865/219726562 | consumed samples:         1730 | consumed tokens:      3543040 | elapsed time per iteration (ms): 559.6 | learning rate: 1.887E-06 | global batch size:     2 | lm loss: 8.324267E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.574 | TFLOPs: 52.02 |
[default0]:[2023-07-30 22:30:58,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[1.8983594666666668e-06, 1.8983594666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:30:58,029] [INFO] [timer.py:215:stop] epoch=0/micro_step=870/global_step=870, RunningAvgSamplesPerSec=4.223952094941346, CurrSamplesPerSec=4.187357174447591, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      870/219726562 | consumed samples:         1740 | consumed tokens:      3563520 | elapsed time per iteration (ms): 572.5 | learning rate: 1.898E-06 | global batch size:     2 | lm loss: 8.326194E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.493 | TFLOPs: 50.85 |
[default0]:[2023-07-30 22:31:00,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=875, skipped=0, lr=[1.9092821333333333e-06, 1.9092821333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:00,763] [INFO] [timer.py:215:stop] epoch=0/micro_step=875/global_step=875, RunningAvgSamplesPerSec=4.224023883672415, CurrSamplesPerSec=4.233454991763832, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      875/219726562 | consumed samples:         1750 | consumed tokens:      3584000 | elapsed time per iteration (ms): 547.0 | learning rate: 1.909E-06 | global batch size:     2 | lm loss: 8.340976E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.656 | TFLOPs: 53.22 |
[default0]:[2023-07-30 22:31:03,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[1.9202048000000003e-06, 1.9202048000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:03,613] [INFO] [timer.py:215:stop] epoch=0/micro_step=880/global_step=880, RunningAvgSamplesPerSec=4.224088560817531, CurrSamplesPerSec=4.201562198698252, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      880/219726562 | consumed samples:         1760 | consumed tokens:      3604480 | elapsed time per iteration (ms): 570.2 | learning rate: 1.920E-06 | global batch size:     2 | lm loss: 8.258403E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.507 | TFLOPs: 51.05 |
[default0]:[2023-07-30 22:31:06,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=885, skipped=0, lr=[1.931127466666667e-06, 1.931127466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:06,456] [INFO] [timer.py:215:stop] epoch=0/micro_step=885/global_step=885, RunningAvgSamplesPerSec=4.223843567183208, CurrSamplesPerSec=4.164994930672855, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      885/219726562 | consumed samples:         1770 | consumed tokens:      3624960 | elapsed time per iteration (ms): 569.0 | learning rate: 1.931E-06 | global batch size:     2 | lm loss: 8.292513E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.515 | TFLOPs: 51.16 |
[default0]:[2023-07-30 22:31:09,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[1.9420501333333336e-06, 1.9420501333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:09,237] [INFO] [timer.py:215:stop] epoch=0/micro_step=890/global_step=890, RunningAvgSamplesPerSec=4.223626701984807, CurrSamplesPerSec=4.255292986732341, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      890/219726562 | consumed samples:         1780 | consumed tokens:      3645440 | elapsed time per iteration (ms): 555.2 | learning rate: 1.942E-06 | global batch size:     2 | lm loss: 8.327016E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.602 | TFLOPs: 52.44 |
[default0]:[2023-07-30 22:31:12,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=895, skipped=0, lr=[1.9529728e-06, 1.9529728e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:12,051] [INFO] [timer.py:215:stop] epoch=0/micro_step=895/global_step=895, RunningAvgSamplesPerSec=4.223726597725179, CurrSamplesPerSec=4.241883081584594, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      895/219726562 | consumed samples:         1790 | consumed tokens:      3665920 | elapsed time per iteration (ms): 563.0 | learning rate: 1.953E-06 | global batch size:     2 | lm loss: 8.215377E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.553 | TFLOPs: 51.71 |
[default0]:[2023-07-30 22:31:14,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[1.963895466666667e-06, 1.963895466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:14,824] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=900, RunningAvgSamplesPerSec=4.22364927787255, CurrSamplesPerSec=4.174230127317729, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      900/219726562 | consumed samples:         1800 | consumed tokens:      3686400 | elapsed time per iteration (ms): 554.5 | learning rate: 1.964E-06 | global batch size:     2 | lm loss: 8.330154E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.607 | TFLOPs: 52.50 |
[default0]:-----------------------------------------------------------------------------------------------
[default0]: validation loss at iteration 900 | lm loss value: 8.244835E+00 | lm loss PPL: 3.807907E+03 | 
[default0]:-----------------------------------------------------------------------------------------------
[default0]:[2023-07-30 22:31:25,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=905, skipped=0, lr=[1.9748181333333335e-06, 1.9748181333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:25,090] [INFO] [timer.py:215:stop] epoch=0/micro_step=905/global_step=905, RunningAvgSamplesPerSec=4.223616944637399, CurrSamplesPerSec=4.2014843367314745, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      905/219726562 | consumed samples:         1810 | consumed tokens:      3706880 | elapsed time per iteration (ms): 2053.5 | learning rate: 1.975E-06 | global batch size:     2 | lm loss: 8.543422E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.974 | TFLOPs: 14.18 |
[default0]:[2023-07-30 22:31:27,837] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[1.9857408e-06, 1.9857408e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:27,838] [INFO] [timer.py:215:stop] epoch=0/micro_step=910/global_step=910, RunningAvgSamplesPerSec=4.2235567892369685, CurrSamplesPerSec=4.218168287084179, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      910/219726562 | consumed samples:         1820 | consumed tokens:      3727360 | elapsed time per iteration (ms): 549.1 | learning rate: 1.986E-06 | global batch size:     2 | lm loss: 8.348958E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.642 | TFLOPs: 53.02 |
[default0]:[2023-07-30 22:31:30,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=915, skipped=0, lr=[1.9966634666666667e-06, 1.9966634666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:30,614] [INFO] [timer.py:215:stop] epoch=0/micro_step=915/global_step=915, RunningAvgSamplesPerSec=4.223662658320371, CurrSamplesPerSec=4.245036430379465, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      915/219726562 | consumed samples:         1830 | consumed tokens:      3747840 | elapsed time per iteration (ms): 555.3 | learning rate: 1.997E-06 | global batch size:     2 | lm loss: 8.276424E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.602 | TFLOPs: 52.43 |
[default0]:[2023-07-30 22:31:33,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[2.0075861333333333e-06, 2.0075861333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:33,401] [INFO] [timer.py:215:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=4.223725811457387, CurrSamplesPerSec=4.188475736013338, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      920/219726562 | consumed samples:         1840 | consumed tokens:      3768320 | elapsed time per iteration (ms): 557.3 | learning rate: 2.008E-06 | global batch size:     2 | lm loss: 8.216442E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.589 | TFLOPs: 52.24 |
[default0]:[2023-07-30 22:31:36,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=925, skipped=0, lr=[2.0185088000000004e-06, 2.0185088000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:36,147] [INFO] [timer.py:215:stop] epoch=0/micro_step=925/global_step=925, RunningAvgSamplesPerSec=4.223718935063847, CurrSamplesPerSec=4.218399497932189, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      925/219726562 | consumed samples:         1850 | consumed tokens:      3788800 | elapsed time per iteration (ms): 549.3 | learning rate: 2.019E-06 | global batch size:     2 | lm loss: 8.113512E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.641 | TFLOPs: 53.00 |
[default0]:[2023-07-30 22:31:38,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=0, lr=[2.029431466666667e-06, 2.029431466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:38,873] [INFO] [timer.py:215:stop] epoch=0/micro_step=930/global_step=930, RunningAvgSamplesPerSec=4.223514685466104, CurrSamplesPerSec=4.170046837545995, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      930/219726562 | consumed samples:         1860 | consumed tokens:      3809280 | elapsed time per iteration (ms): 545.2 | learning rate: 2.029E-06 | global batch size:     2 | lm loss: 8.237319E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.668 | TFLOPs: 53.40 |
[default0]:[2023-07-30 22:31:41,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=935, skipped=0, lr=[2.0403541333333337e-06, 2.0403541333333337e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:41,589] [INFO] [timer.py:215:stop] epoch=0/micro_step=935/global_step=935, RunningAvgSamplesPerSec=4.223636446655674, CurrSamplesPerSec=4.237826477928324, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      935/219726562 | consumed samples:         1870 | consumed tokens:      3829760 | elapsed time per iteration (ms): 543.1 | learning rate: 2.040E-06 | global batch size:     2 | lm loss: 8.390208E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.683 | TFLOPs: 53.60 |
[default0]:[2023-07-30 22:31:44,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=0, lr=[2.0512768000000003e-06, 2.0512768000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:44,459] [INFO] [timer.py:215:stop] epoch=0/micro_step=940/global_step=940, RunningAvgSamplesPerSec=4.223636305360941, CurrSamplesPerSec=4.198187323274029, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      940/219726562 | consumed samples:         1880 | consumed tokens:      3850240 | elapsed time per iteration (ms): 574.2 | learning rate: 2.051E-06 | global batch size:     2 | lm loss: 7.919267E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.483 | TFLOPs: 50.70 |
[default0]:[2023-07-30 22:31:47,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=945, skipped=0, lr=[2.0621994666666665e-06, 2.0621994666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:47,271] [INFO] [timer.py:215:stop] epoch=0/micro_step=945/global_step=945, RunningAvgSamplesPerSec=4.223567739291219, CurrSamplesPerSec=4.210947771065135, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      945/219726562 | consumed samples:         1890 | consumed tokens:      3870720 | elapsed time per iteration (ms): 562.3 | learning rate: 2.062E-06 | global batch size:     2 | lm loss: 8.166138E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.557 | TFLOPs: 51.77 |
[default0]:[2023-07-30 22:31:50,109] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=0, lr=[2.073122133333333e-06, 2.073122133333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:50,110] [INFO] [timer.py:215:stop] epoch=0/micro_step=950/global_step=950, RunningAvgSamplesPerSec=4.223350825927187, CurrSamplesPerSec=4.088105245972743, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      950/219726562 | consumed samples:         1900 | consumed tokens:      3891200 | elapsed time per iteration (ms): 567.8 | learning rate: 2.073E-06 | global batch size:     2 | lm loss: 8.231776E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.522 | TFLOPs: 51.27 |
[default0]:[2023-07-30 22:31:52,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=955, skipped=0, lr=[2.0840448e-06, 2.0840448e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:52,848] [INFO] [timer.py:215:stop] epoch=0/micro_step=955/global_step=955, RunningAvgSamplesPerSec=4.223507750132662, CurrSamplesPerSec=4.248723778517137, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      955/219726562 | consumed samples:         1910 | consumed tokens:      3911680 | elapsed time per iteration (ms): 547.5 | learning rate: 2.084E-06 | global batch size:     2 | lm loss: 7.942197E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.653 | TFLOPs: 53.18 |
[default0]:[2023-07-30 22:31:55,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=0, lr=[2.094967466666667e-06, 2.094967466666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:55,628] [INFO] [timer.py:215:stop] epoch=0/micro_step=960/global_step=960, RunningAvgSamplesPerSec=4.223105999496437, CurrSamplesPerSec=4.185779659722974, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      960/219726562 | consumed samples:         1920 | consumed tokens:      3932160 | elapsed time per iteration (ms): 556.1 | learning rate: 2.095E-06 | global batch size:     2 | lm loss: 8.331533E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.596 | TFLOPs: 52.35 |
[default0]:[2023-07-30 22:31:58,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=965, skipped=0, lr=[2.1058901333333334e-06, 2.1058901333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:31:58,401] [INFO] [timer.py:215:stop] epoch=0/micro_step=965/global_step=965, RunningAvgSamplesPerSec=4.22307388599558, CurrSamplesPerSec=4.212159343456914, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      965/219726562 | consumed samples:         1930 | consumed tokens:      3952640 | elapsed time per iteration (ms): 554.8 | learning rate: 2.106E-06 | global batch size:     2 | lm loss: 8.089795E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.605 | TFLOPs: 52.48 |
[default0]:[2023-07-30 22:32:01,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=0, lr=[2.1168128e-06, 2.1168128e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:01,140] [INFO] [timer.py:215:stop] epoch=0/micro_step=970/global_step=970, RunningAvgSamplesPerSec=4.223155560451168, CurrSamplesPerSec=4.260981976458613, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      970/219726562 | consumed samples:         1940 | consumed tokens:      3973120 | elapsed time per iteration (ms): 547.6 | learning rate: 2.117E-06 | global batch size:     2 | lm loss: 8.001939E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.652 | TFLOPs: 53.16 |
[default0]:[2023-07-30 22:32:03,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=975, skipped=0, lr=[2.1277354666666667e-06, 2.1277354666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:03,902] [INFO] [timer.py:215:stop] epoch=0/micro_step=975/global_step=975, RunningAvgSamplesPerSec=4.223272273081942, CurrSamplesPerSec=4.21996773377294, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      975/219726562 | consumed samples:         1950 | consumed tokens:      3993600 | elapsed time per iteration (ms): 552.3 | learning rate: 2.128E-06 | global batch size:     2 | lm loss: 8.130194E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.621 | TFLOPs: 52.71 |
[default0]:[2023-07-30 22:32:06,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=0, lr=[2.1386581333333333e-06, 2.1386581333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:06,673] [INFO] [timer.py:215:stop] epoch=0/micro_step=980/global_step=980, RunningAvgSamplesPerSec=4.223183017176249, CurrSamplesPerSec=4.176724167976985, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      980/219726562 | consumed samples:         1960 | consumed tokens:      4014080 | elapsed time per iteration (ms): 554.6 | learning rate: 2.139E-06 | global batch size:     2 | lm loss: 8.070793E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.606 | TFLOPs: 52.49 |
[default0]:[2023-07-30 22:32:09,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=985, skipped=0, lr=[2.1495808e-06, 2.1495808e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-30 22:32:09,526] [INFO] [timer.py:215:stop] epoch=0/micro_step=985/global_step=985, RunningAvgSamplesPerSec=4.223140721854507, CurrSamplesPerSec=4.2460828574046205, MemAllocated=10.16GB, MaxMemAllocated=15.83GB
[default0]: iteration      985/219726562 | consumed samples:         1970 | consumed tokens:      4034560 | elapsed time per iteration (ms): 570.6 | learning rate: 2.150E-06 | global batch size:     2 | lm loss: 8.084612E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.505 | TFLOPs: 51.02 |
slurmstepd: error: *** JOB 4505003 ON n2gpu1225 CANCELLED AT 2023-07-30T22:32:10 DUE TO TIME LIMIT ***
slurmstepd: error: *** STEP 4505003.0 ON n2gpu1225 CANCELLED AT 2023-07-30T22:32:10 DUE TO TIME LIMIT ***
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 413790 closing signal SIGTERM
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 413790 closing signal SIGTERM
