from __future__ import annotations
import sys
from importlib.machinery import ModuleSpec, PathFinder
from importlib.machinery import all_suffixes as module_suffixes
from importlib.util import spec_from_file_location
from itertools import chain
from pathlib import Path

MAPPING: dict[str, str] = {'lm_eval': '/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval'}
NAMESPACES: dict[str, list[str]] = {'lm_eval.caching': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/caching'], 'lm_eval.tasks.aclue': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/aclue'], 'lm_eval.tasks.eus_trivia': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/eus_trivia'], 'lm_eval.tasks.kmmlu': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/kmmlu'], 'lm_eval.tasks.triviaqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/triviaqa'], 'lm_eval.tasks.pile': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/pile'], 'lm_eval.tasks.siqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/siqa'], 'lm_eval.tasks.belebele': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/belebele'], 'lm_eval.tasks.kormedmcqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/kormedmcqa'], 'lm_eval.tasks.csatqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/csatqa'], 'lm_eval.tasks.pubmedqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/pubmedqa'], 'lm_eval.tasks.winogrande': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/winogrande'], 'lm_eval.tasks.storycloze': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/storycloze'], 'lm_eval.tasks.pile_10k': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/pile_10k'], 'lm_eval.tasks.piqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/piqa'], 'lm_eval.tasks.scrolls': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/scrolls'], 'lm_eval.tasks.okapi': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/okapi'], 'lm_eval.tasks.gsm8k': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/gsm8k'], 'lm_eval.tasks.eus_proficiency': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/eus_proficiency'], 'lm_eval.tasks.webqs': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/webqs'], 'lm_eval.tasks.squad_completion': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/squad_completion'], 'lm_eval.tasks.minerva_math': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/minerva_math'], 'lm_eval.tasks.swde': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/swde'], 'lm_eval.tasks.polemo2': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/polemo2'], 'lm_eval.tasks.medmcqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/medmcqa'], 'lm_eval.tasks.model_written_evals': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/model_written_evals'], 'lm_eval.tasks.squadv2': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/squadv2'], 'lm_eval.tasks.openbookqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/openbookqa'], 'lm_eval.tasks.logiqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/logiqa'], 'lm_eval.tasks.cmmlu': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/cmmlu'], 'lm_eval.tasks.sciq': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/sciq'], 'lm_eval.tasks.glue': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/glue'], 'lm_eval.tasks.hellaswag': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/hellaswag'], 'lm_eval.tasks.truthfulqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/truthfulqa'], 'lm_eval.tasks.mutual': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mutual'], 'lm_eval.tasks.bigbench': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/bigbench'], 'lm_eval.tasks.basqueglue': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/basqueglue'], 'lm_eval.tasks.benchmarks': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/benchmarks'], 'lm_eval.tasks.code_x_glue': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/code_x_glue'], 'lm_eval.tasks.lambada_cloze': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/lambada_cloze'], 'lm_eval.tasks.eus_exams': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/eus_exams'], 'lm_eval.tasks.nq_open': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/nq_open'], 'lm_eval.tasks.hendrycks_ethics': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/hendrycks_ethics'], 'lm_eval.tasks.qa4mre': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/qa4mre'], 'lm_eval.tasks.asdiv': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/asdiv'], 'lm_eval.tasks.wikitext': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/wikitext'], 'lm_eval.tasks.xstorycloze': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/xstorycloze'], 'lm_eval.tasks.realtoxicityprompts': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/realtoxicityprompts'], 'lm_eval.tasks.hendrycks_math': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/hendrycks_math'], 'lm_eval.tasks.wmdp': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/wmdp'], 'lm_eval.tasks.crows_pairs': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/crows_pairs'], 'lm_eval.tasks.ceval': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/ceval'], 'lm_eval.tasks.unscramble': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/unscramble'], 'lm_eval.tasks.eq_bench': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/eq_bench'], 'lm_eval.tasks.prost': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/prost'], 'lm_eval.tasks.gpqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/gpqa'], 'lm_eval.tasks.paws-x': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/paws-x'], 'lm_eval.tasks.babi': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/babi'], 'lm_eval.tasks.xcopa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/xcopa'], 'lm_eval.tasks.kobest': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/kobest'], 'lm_eval.tasks.lambada_multilingual': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/lambada_multilingual'], 'lm_eval.tasks.mgsm': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mgsm'], 'lm_eval.tasks.qasper': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/qasper'], 'lm_eval.tasks.swag': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/swag'], 'lm_eval.tasks.race': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/race'], 'lm_eval.tasks.translation': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/translation'], 'lm_eval.tasks.wsc273': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/wsc273'], 'lm_eval.tasks.mc_taco': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mc_taco'], 'lm_eval.tasks.tmmluplus': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/tmmluplus'], 'lm_eval.tasks.wmt2016': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/wmt2016'], 'lm_eval.tasks.arc': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/arc'], 'lm_eval.tasks.fda': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/fda'], 'lm_eval.tasks.arithmetic': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/arithmetic'], 'lm_eval.tasks.fld': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/fld'], 'lm_eval.tasks.xnli_eu': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/xnli_eu'], 'lm_eval.tasks.medqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/medqa'], 'lm_eval.tasks.bbh': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/bbh'], 'lm_eval.tasks.mathqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mathqa'], 'lm_eval.tasks.xnli': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/xnli'], 'lm_eval.tasks.french_bench': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/french_bench'], 'lm_eval.tasks.haerae': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/haerae'], 'lm_eval.tasks.anli': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/anli'], 'lm_eval.tasks.eus_reading': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/eus_reading'], 'lm_eval.tasks.aexams': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/aexams'], 'lm_eval.tasks.xwinograd': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/xwinograd'], 'lm_eval.tasks.mmlu': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mmlu'], 'lm_eval.tasks.lambada': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/lambada'], 'lm_eval.tasks.agieval': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/agieval'], 'lm_eval.tasks.super_glue': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/super_glue'], 'lm_eval.tasks.ifeval': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/ifeval'], 'lm_eval.tasks.headqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/headqa'], 'lm_eval.tasks.coqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/coqa'], 'lm_eval.tasks.toxigen': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/toxigen'], 'lm_eval.tasks.logiqa2': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/logiqa2'], 'lm_eval.tasks.ammlu': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/ammlu'], 'lm_eval.tasks.drop': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/drop'], 'lm_eval.tasks.blimp': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/blimp'], 'lm_eval.tasks.kmmlu.direct_hard': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/kmmlu/direct_hard'], 'lm_eval.tasks.kmmlu.cot_hard': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/kmmlu/cot_hard'], 'lm_eval.tasks.kmmlu.direct': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/kmmlu/direct'], 'lm_eval.tasks.kmmlu.hard': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/kmmlu/hard'], 'lm_eval.tasks.okapi.truthfulqa_multilingual': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/okapi/truthfulqa_multilingual'], 'lm_eval.tasks.okapi.hellaswag_multilingual': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/okapi/hellaswag_multilingual'], 'lm_eval.tasks.okapi.mmlu_multilingual': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/okapi/mmlu_multilingual'], 'lm_eval.tasks.okapi.arc_multilingual': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/okapi/arc_multilingual'], 'lm_eval.tasks.model_written_evals.sycophancy': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/model_written_evals/sycophancy'], 'lm_eval.tasks.model_written_evals.advanced_ai_risk': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/model_written_evals/advanced_ai_risk'], 'lm_eval.tasks.model_written_evals.winogenerated': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/model_written_evals/winogenerated'], 'lm_eval.tasks.model_written_evals.persona': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/model_written_evals/persona'], 'lm_eval.tasks.glue.qqp': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/glue/qqp'], 'lm_eval.tasks.glue.qnli': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/glue/qnli'], 'lm_eval.tasks.glue.cola': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/glue/cola'], 'lm_eval.tasks.glue.mrpc': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/glue/mrpc'], 'lm_eval.tasks.glue.mnli': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/glue/mnli'], 'lm_eval.tasks.glue.rte': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/glue/rte'], 'lm_eval.tasks.glue.wnli': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/glue/wnli'], 'lm_eval.tasks.glue.sst2': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/glue/sst2'], 'lm_eval.tasks.bigbench.generate_until': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/bigbench/generate_until'], 'lm_eval.tasks.bigbench.multiple_choice': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/bigbench/multiple_choice'], 'lm_eval.tasks.benchmarks.multimedqa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/benchmarks/multimedqa'], 'lm_eval.tasks.benchmarks.flan': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/benchmarks/flan'], 'lm_eval.tasks.code_x_glue.code-text': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/code_x_glue/code-text'], 'lm_eval.tasks.gpqa.n_shot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/gpqa/n_shot'], 'lm_eval.tasks.gpqa.zeroshot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/gpqa/zeroshot'], 'lm_eval.tasks.gpqa.cot_zeroshot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/gpqa/cot_zeroshot'], 'lm_eval.tasks.gpqa.cot_n_shot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/gpqa/cot_n_shot'], 'lm_eval.tasks.gpqa.generative': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/gpqa/generative'], 'lm_eval.tasks.mgsm.en_cot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mgsm/en_cot'], 'lm_eval.tasks.mgsm.direct': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mgsm/direct'], 'lm_eval.tasks.mgsm.native_cot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mgsm/native_cot'], 'lm_eval.tasks.tmmluplus.default': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/tmmluplus/default'], 'lm_eval.tasks.bbh.fewshot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/bbh/fewshot'], 'lm_eval.tasks.bbh.zeroshot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/bbh/zeroshot'], 'lm_eval.tasks.bbh.cot_zeroshot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/bbh/cot_zeroshot'], 'lm_eval.tasks.bbh.cot_fewshot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/bbh/cot_fewshot'], 'lm_eval.tasks.mmlu.flan_cot_fewshot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mmlu/flan_cot_fewshot'], 'lm_eval.tasks.mmlu.flan_n_shot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mmlu/flan_n_shot'], 'lm_eval.tasks.mmlu.default': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mmlu/default'], 'lm_eval.tasks.mmlu.flan_cot_zeroshot': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mmlu/flan_cot_zeroshot'], 'lm_eval.tasks.mmlu.flan_n_shot.generative': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mmlu/flan_n_shot/generative'], 'lm_eval.tasks.mmlu.flan_n_shot.loglikelihood': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/mmlu/flan_n_shot/loglikelihood'], 'lm_eval.tasks.super_glue.record': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/super_glue/record'], 'lm_eval.tasks.super_glue.wic': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/super_glue/wic'], 'lm_eval.tasks.super_glue.cb': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/super_glue/cb'], 'lm_eval.tasks.super_glue.boolq': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/super_glue/boolq'], 'lm_eval.tasks.super_glue.wsc': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/super_glue/wsc'], 'lm_eval.tasks.super_glue.multirc': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/super_glue/multirc'], 'lm_eval.tasks.super_glue.rte': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/super_glue/rte'], 'lm_eval.tasks.super_glue.copa': ['/data/tatiana/evaluation/test/LOLA-Megatron-DeepSpeed/lola_ws/evaluate/tasks/lm-eval-harness/lm-evaluation-harness/lm_eval/tasks/super_glue/copa']}
PATH_PLACEHOLDER = '__editable__.lm_eval-0.4.2.finder' + ".__path_hook__"


class _EditableFinder:  # MetaPathFinder
    @classmethod
    def find_spec(cls, fullname: str, _path=None, _target=None) -> ModuleSpec | None:
        # Top-level packages and modules (we know these exist in the FS)
        if fullname in MAPPING:
            pkg_path = MAPPING[fullname]
            return cls._find_spec(fullname, Path(pkg_path))

        # Handle immediate children modules (required for namespaces to work)
        # To avoid problems with case sensitivity in the file system we delegate
        # to the importlib.machinery implementation.
        parent, _, child = fullname.rpartition(".")
        if parent and parent in MAPPING:
            return PathFinder.find_spec(fullname, path=[MAPPING[parent]])

        # Other levels of nesting should be handled automatically by importlib
        # using the parent path.
        return None

    @classmethod
    def _find_spec(cls, fullname: str, candidate_path: Path) -> ModuleSpec | None:
        init = candidate_path / "__init__.py"
        candidates = (candidate_path.with_suffix(x) for x in module_suffixes())
        for candidate in chain([init], candidates):
            if candidate.exists():
                return spec_from_file_location(fullname, candidate)
        return None


class _EditableNamespaceFinder:  # PathEntryFinder
    @classmethod
    def _path_hook(cls, path) -> type[_EditableNamespaceFinder]:
        if path == PATH_PLACEHOLDER:
            return cls
        raise ImportError

    @classmethod
    def _paths(cls, fullname: str) -> list[str]:
        paths = NAMESPACES[fullname]
        if not paths and fullname in MAPPING:
            paths = [MAPPING[fullname]]
        # Always add placeholder, for 2 reasons:
        # 1. __path__ cannot be empty for the spec to be considered namespace.
        # 2. In the case of nested namespaces, we need to force
        #    import machinery to query _EditableNamespaceFinder again.
        return [*paths, PATH_PLACEHOLDER]

    @classmethod
    def find_spec(cls, fullname: str, _target=None) -> ModuleSpec | None:
        if fullname in NAMESPACES:
            spec = ModuleSpec(fullname, None, is_package=True)
            spec.submodule_search_locations = cls._paths(fullname)
            return spec
        return None

    @classmethod
    def find_module(cls, _fullname) -> None:
        return None


def install():
    if not any(finder == _EditableFinder for finder in sys.meta_path):
        sys.meta_path.append(_EditableFinder)

    if not NAMESPACES:
        return

    if not any(hook == _EditableNamespaceFinder._path_hook for hook in sys.path_hooks):
        # PathEntryFinder is needed to create NamespaceSpec without private APIS
        sys.path_hooks.append(_EditableNamespaceFinder._path_hook)
    if PATH_PLACEHOLDER not in sys.path:
        sys.path.append(PATH_PLACEHOLDER)  # Used just to trigger the path hook
