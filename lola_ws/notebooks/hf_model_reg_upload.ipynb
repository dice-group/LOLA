{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lola_hf_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configuration_lola_gpt2 import LOLAConfig\n",
    "from modeling_lola_gpt2 import LOLAModel, LOLALMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoConfig.register(\"lola_v1\", LOLAConfig)\n",
    "AutoModel.register(LOLAConfig, LOLAModel)\n",
    "AutoModelForCausalLM.register(LOLAConfig, LOLALMHeadModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOLAConfig.register_for_auto_class()\n",
    "LOLAModel.register_for_auto_class(\"AutoModel\")\n",
    "LOLALMHeadModel.register_for_auto_class(\"AutoModelForCausalLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LOLALMHeadModel.from_pretrained(\"/data/nikit_ws/lola_converted_model/lola_hf_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def generate_hf_model_text(inp_text, max_length, tokenizer, model):\n",
    "    inputs = tokenizer(inp_text, return_tensors=\"pt\").to(model.device)\n",
    "    output_sequences = model.generate(input_ids=inputs['input_ids'], max_length=max_length)\n",
    "\n",
    "    # Decode the generated indices to text\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('ai-forever/mGPT')\n",
    "generated_text = generate_hf_model_text(\"The quick brown fox\", 100, tokenizer, model)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"lola_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lola-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
