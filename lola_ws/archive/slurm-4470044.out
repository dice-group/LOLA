cpu-bind=MASK - n2gpu1211, task  0  0 [3554803]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
LAUNCHER: python -u -m torch.distributed.run --nproc_per_node 2 --nnodes 2 --rdzv_id=32567 --rdzv_endpoint n2gpu1211:6005 --rdzv_backend c10d --max_restarts 0 --tee 3
CMD: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/pretrain_gpt.py --override-opt_param-scheduler --adam-beta1 0.9 --adam-beta2 0.95 --tensor-model-parallel-size 1 --moe-expert-parallel-size 4 --num-experts 128 --moe-loss-coeff 0.01 --moe-train-capacity-factor 1.0 --moe-eval-capacity-factor 1.0 --moe-min-capacity 4 --init-method-std 0.014 --lr-decay-tokens 300000000000 --lr-warmup-tokens 375000000 --micro-batch-size 1 --exit-duration-in-mins 30000000 --global-batch-size 4 --num-layers 24 --hidden-size 2048 --num-attention-heads 16 --seq-length 2048 --max-position-embeddings 2048 --train-tokens 300000000000 --train-iters 109863281 --lr 2.0e-4 --min-lr 2e-06 --lr-decay-style cosine --split 98,2,0 --log-interval 5 --eval-interval 100 --eval-iters 50 --save-interval 50 --weight-decay 0.1 --clip-grad 1.0 --hysteresis 2 --num-workers 0 --fp16 --load /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true --save /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true --tensorboard-queue-size 1 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/tensorboard/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true_n2gpu1211_2023.07.25-15.03.26 --checkpoint-activations --create-moe-param-group --vocab-file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-vocab.json --merge-file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-merges.txt --data-path /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/meg-gpt2-oscar-en-10k_text_document --data-impl mmap --deepspeed --deepspeed_config ds_config_gpt_gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true.json --pipeline-model-parallel-size 1 --no-pipeline-parallel --deepspeed-activation-checkpointing
cpu-bind=MASK - n2gpu1211, task  0  0 [3554990]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||B-------|--------||--------|--------||--------|--------||--------|--------|  set
cpu-bind=MASK - n2gpu1215, task  1  0 [144510]: mask |--------|--------||--------|--------||--------|--------||--------|--------||||--B-----|--------||--------|--------||--------|--------||--------|--------|  set
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:__main__:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[default0]:[2023-07-25 15:03:31,799] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default0]:[2023-07-25 15:03:31,798] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:[2023-07-25 15:03:31,799] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:[2023-07-25 15:03:31,798] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default1]:--------------------------------------------------
[default1]:DeepSpeed C++/CUDA extension op report
[default1]:--------------------------------------------------
[default1]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default1]:      runtime if needed. Op compatibility means that your system
[default1]:      meet the required dependencies to JIT install the op.
[default1]:--------------------------------------------------
[default1]:JIT compiled ops requires ninja
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default0]:--------------------------------------------------
[default0]:DeepSpeed C++/CUDA extension op report
[default0]:--------------------------------------------------
[default0]:NOTE: Ops not installed will be just-in-time (JIT) compiled at
[default0]:      runtime if needed. Op compatibility means that your system
[default0]:      meet the required dependencies to JIT install the op.
[default0]:--------------------------------------------------
[default0]:JIT compiled ops requires ninja
[default0]:ninja .................. [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:op name ................ installed .. compatible
[default0]:--------------------------------------------------
[default1]:ninja .................. [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:op name ................ installed .. compatible
[default1]:--------------------------------------------------
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.10.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default1]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default1]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default1]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default1]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default1]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default1]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default1]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.10.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[default0]:[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[default0]:[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[default0]:async_io ............... [93m[NO][0m ....... [93m[NO][0m
[default0]:cpu_adagrad ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:cpu_adam ............... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_adam ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:fused_lamb ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:quantizer .............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:random_ltd ............. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:[93m [WARNING] [0m please install triton==1.0.0 if you want to use sparse attention
[default0]:sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
[default0]:spatial_inference ...... [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer ............ [93m[NO][0m ....... [92m[OKAY][0m
[default0]:stochastic_transformer . [93m[NO][0m ....... [92m[OKAY][0m
[default0]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default0]:--------------------------------------------------
[default0]:DeepSpeed general environment info:
[default0]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default0]:torch version .................... 1.12.0
[default0]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default0]:deepspeed info ................... 0.10.0, unknown, unknown
[default0]:torch cuda version ............... 11.7
[default0]:torch hip version ................ None
[default0]:nvcc version ..................... 11.7
[default0]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default1]:transformer_inference .. [93m[NO][0m ....... [92m[OKAY][0m
[default1]:--------------------------------------------------
[default1]:DeepSpeed general environment info:
[default1]:torch install path ............... ['/opt/software/pc2/EB-SW/software/PyTorch/1.12.0-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch']
[default1]:torch version .................... 1.12.0
[default1]:deepspeed install path ........... ['/scratch/hpc-prf-lola/lib_repo/custom-venvs/lola1/lib/python3.10/site-packages/deepspeed']
[default1]:deepspeed info ................... 0.10.0, unknown, unknown
[default1]:torch cuda version ............... 11.7
[default1]:torch hip version ................ None
[default1]:nvcc version ..................... 11.7
[default1]:deepspeed wheel compiled w. ...... torch 1.12, cuda 11.7
[default0]:**** Git info for Megatron: git_hash=aad39cb git_branch=main ****
[default1]:**** Git info for Megatron: git_hash=aad39cb git_branch=main ****
[default0]:**** Git info for Megatron: git_hash=aad39cb git_branch=main ****
[default0]:using world size: 4, data-parallel-size: 4, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 
[default0]:using torch.float16 for parameters ...
[default0]:------------------------ arguments ------------------------
[default0]:  accumulate_allreduce_grads_in_fp32 .............. False
[default0]:  adam_beta1 ...................................... 0.9
[default0]:  adam_beta2 ...................................... 0.95
[default0]:  adam_eps ........................................ 1e-08
[default0]:  add_bias_linear ................................. True
[default0]:  add_position_embedding .......................... True
[default0]:  adlr_autoresume ................................. False
[default0]:  adlr_autoresume_interval ........................ 1000
[default0]:  aml_data_download_path .......................... None
[default0]:  apply_layernorm_1p .............................. False
[default0]:  apply_query_key_layer_scaling ................... True
[default0]:  apply_residual_connection_post_layernorm ........ False
[default0]:  async_tensor_model_parallel_allreduce ........... False
[default0]:  attention_dropout ............................... 0.1
[default0]:  attention_softmax_in_fp32 ....................... False
[default0]:  barrier_with_L1_time ............................ True
[default0]:  bert_binary_head ................................ True
[default0]:  bert_embedder_type .............................. megatron
[default0]:  bert_load ....................................... None
[default0]:  bf16 ............................................ False
[default0]:  bias_dropout_fusion ............................. True
[default0]:  bias_gelu_fusion ................................ True
[default0]:  biencoder_projection_dim ........................ 0
[default0]:  biencoder_shared_query_context_model ............ False
[default0]:  block_data_path ................................. None
[default0]:  checkpoint_activations .......................... True
[default0]:  checkpoint_in_cpu ............................... False
[default0]:  checkpoint_num_layers ........................... 1
[default0]:  classes_fraction ................................ 1.0
[default0]:  clip_grad ....................................... 1.0
[default0]:  compression_training ............................ False
[default0]:  consumed_train_samples .......................... 0
[default0]:  consumed_train_tokens ........................... 0
[default0]:  consumed_valid_samples .......................... 0
[default0]:  contigious_checkpointing ........................ False
[default0]:  cpu_optimizer ................................... False
[default0]:  cpu_torch_adam .................................. False
[default0]:  create_moe_param_group .......................... True
[default0]:  curriculum_learning_legacy ...................... False
[default0]:  data_cache_path ................................. None
[default0]:  data_efficiency_curriculum_learning ............. False
[default0]:  data_impl ....................................... mmap
[default0]:  data_parallel_random_init ....................... False
[default0]:  data_parallel_size .............................. 4
[default0]:  data_path ....................................... ['/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/meg-gpt2-oscar-en-10k_text_document']
[default0]:  data_per_class_fraction ......................... 1.0
[default0]:  data_sharding ................................... True
[default0]:  dataloader_type ................................. single
[default0]:  DDP_impl ........................................ local
[default0]:  decoder_num_layers .............................. None
[default0]:  decoder_seq_length .............................. None
[default0]:  deepscale ....................................... False
[default0]:  deepscale_config ................................ None
[default0]:  deepspeed ....................................... True
[default0]:  deepspeed_activation_checkpointing .............. True
[default0]:  deepspeed_config ................................ ds_config_gpt_gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true.json
[default0]:  deepspeed_mpi ................................... False
[default0]:  dino_bottleneck_size ............................ 256
[default0]:  dino_freeze_last_layer .......................... 1
[default0]:  dino_head_hidden_size ........................... 2048
[default0]:  dino_local_crops_number ......................... 10
[default0]:  dino_local_img_size ............................. 96
[default0]:  dino_norm_last_layer ............................ False
[default0]:  dino_teacher_temp ............................... 0.07
[default0]:  dino_warmup_teacher_temp ........................ 0.04
[default0]:  dino_warmup_teacher_temp_epochs ................. 30
[default0]:  distribute_checkpointed_activations ............. False
[default0]:  distribute_saved_activations .................... False
[default0]:  distributed_backend ............................. nccl
[default0]:  distributed_timeout_minutes ..................... 10
[default0]:  ds_inference .................................... False
[default0]:  ds_pipeline_enabled ............................. False
[default0]:  embedding_path .................................. None
[default0]:  embedding_weights_in_fp32 ....................... False
[default0]:  empty_unused_memory_level ....................... 0
[default0]:  enable_expert_tensor_parallelism ................ False
[default0]:  encoder_num_layers .............................. 24
[default0]:  encoder_seq_length .............................. 2048
[default0]:  end_weight_decay ................................ 0.1
[default0]:  eod_mask_loss ................................... False
[default0]:  eval_interval ................................... 100
[default0]:  eval_iters ...................................... 50
[default0]:  evidence_data_path .............................. None
[default0]:  exit_duration_in_mins ........................... 30000000
[default0]:  exit_interval ................................... None
[default0]:  exit_on_missing_checkpoint ...................... False
[default0]:  exit_signal_handler ............................. False
[default0]:  expert_interval ................................. 2
[default0]:  ffn_hidden_size ................................. 8192
[default0]:  finetune ........................................ False
[default0]:  fp16 ............................................ True
[default0]:  fp16_lm_cross_entropy ........................... False
[default0]:  fp32_residual_connection ........................ False
[default0]:  fp8_amax_compute_algo ........................... most_recent
[default0]:  fp8_amax_history_len ............................ 1
[default0]:  fp8_e4m3 ........................................ False
[default0]:  fp8_hybrid ...................................... False
[default0]:  fp8_interval .................................... 1
[default0]:  fp8_margin ...................................... 0
[default0]:  fp8_wgrad ....................................... True
[default0]:  global_batch_size ............................... 4
[default0]:  gradient_accumulation_fusion .................... True
[default0]:  head_lr_mult .................................... 1.0
[default0]:  hidden_dropout .................................. 0.1
[default0]:  hidden_size ..................................... 2048
[default0]:  hidden_size_teacher ............................. None
[default0]:  hysteresis ...................................... 2
[default0]:  ict_head_size ................................... None
[default0]:  ict_load ........................................ None
[default0]:  img_h ........................................... 224
[default0]:  img_w ........................................... 224
[default0]:  indexer_batch_size .............................. 128
[default0]:  indexer_log_interval ............................ 1000
[default0]:  inference ....................................... False
[default0]:  inference_batch_times_seqlen_threshold .......... 512
[default0]:  init_method_std ................................. 0.014
[default0]:  init_method_xavier_uniform ...................... False
[default0]:  initial_loss_scale .............................. 4294967296
[default0]:  iter_per_epoch .................................. 1250
[default0]:  kd .............................................. False
[default0]:  kd_alpha_ce ..................................... 1
[default0]:  kd_beta_ce ...................................... 1
[default0]:  kd_temp ......................................... 1.0
[default0]:  kv_channels ..................................... 128
[default0]:  layernorm_epsilon ............................... 1e-05
[default0]:  lazy_mpu_init ................................... None
[default0]:  load ............................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:  load_teacher .................................... None
[default0]:  local_rank ...................................... None
[default0]:  log_batch_size_to_tensorboard ................... True
[default0]:  log_interval .................................... 5
[default0]:  log_learning_rate_to_tensorboard ................ True
[default0]:  log_loss_scale_to_tensorboard ................... True
[default0]:  log_memory_to_tensorboard ....................... False
[default0]:  log_num_zeros_in_grad ........................... False
[default0]:  log_optimizer_states_to_tensorboard ............. False
[default0]:  log_params_norm ................................. False
[default0]:  log_timers_to_tensorboard ....................... True
[default0]:  log_validation_ppl_to_tensorboard ............... True
[default0]:  log_world_size_to_tensorboard ................... False
[default0]:  loss_scale ...................................... None
[default0]:  loss_scale_window ............................... 1000
[default0]:  lr .............................................. 0.0002
[default0]:  lr_decay_iters .................................. None
[default0]:  lr_decay_samples ................................ None
[default0]:  lr_decay_style .................................. cosine
[default0]:  lr_decay_tokens ................................. 300000000000
[default0]:  lr_warmup_fraction .............................. None
[default0]:  lr_warmup_iters ................................. 0
[default0]:  lr_warmup_samples ............................... 0
[default0]:  lr_warmup_tokens ................................ 375000000
[default0]:  make_vocab_size_divisible_by .................... 128
[default0]:  mask_factor ..................................... 1.0
[default0]:  mask_prob ....................................... 0.15
[default0]:  mask_type ....................................... random
[default0]:  masked_softmax_fusion ........................... True
[default0]:  max_position_embeddings ......................... 2048
[default0]:  max_tokens_to_oom ............................... 12000
[default0]:  memory_centric_tiled_linear ..................... False
[default0]:  merge_file ...................................... /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-merges.txt
[default0]:  micro_batch_size ................................ 1
[default0]:  min_loss_scale .................................. 1.0
[default0]:  min_lr .......................................... 2e-06
[default0]:  mlp_type ........................................ standard
[default0]:  mmap_warmup ..................................... False
[default0]:  moe_eval_capacity_factor ........................ 1.0
[default0]:  moe_expert_parallel_size ........................ 4
[default0]:  moe_loss_coeff .................................. 0.01
[default0]:  moe_min_capacity ................................ 4
[default0]:  moe_token_dropping .............................. True
[default0]:  moe_train_capacity_factor ....................... 1.0
[default0]:  mos ............................................. False
[default0]:  no_load_lr_state ................................ False
[default0]:  no_load_optim ................................... None
[default0]:  no_load_rng ..................................... None
[default0]:  no_persist_layer_norm ........................... False
[default0]:  no_pipeline_parallel ............................ True
[default0]:  no_save_optim ................................... None
[default0]:  no_save_rng ..................................... None
[default0]:  normalization ................................... layernorm
[default0]:  num_attention_heads ............................. 16
[default0]:  num_attention_heads_teacher ..................... None
[default0]:  num_channels .................................... 3
[default0]:  num_classes ..................................... 1000
[default0]:  num_experts ..................................... [128]
[default0]:  num_experts_switch .............................. None
[default0]:  num_experts_teacher ............................. [1]
[default0]:  num_layers ...................................... 24
[default0]:  num_layers_per_virtual_pipeline_stage ........... None
[default0]:  num_layers_teacher .............................. None
[default0]:  num_workers ..................................... 0
[default0]:  onnx_safe ....................................... None
[default0]:  openai_gelu ..................................... False
[default0]:  optimizer ....................................... adam
[default0]:  output_bert_embeddings .......................... False
[default0]:  overlap_p2p_comm ................................ False
[default0]:  override_opt_param_scheduler .................... True
[default0]:  params_dtype .................................... torch.float16
[default0]:  partition_activations ........................... False
[default0]:  patch_dim ....................................... 16
[default0]:  perform_initialization .......................... True
[default0]:  pipeline_model_parallel_size .................... 1
[default0]:  pipeline_model_parallel_split_rank .............. None
[default0]:  profile_backward ................................ False
[default0]:  query_in_block_prob ............................. 0.1
[default0]:  rampup_batch_size ............................... None
[default0]:  random_ltd ...................................... False
[default0]:  rank ............................................ 0
[default0]:  recompute_granularity ........................... None
[default0]:  recompute_method ................................ None
[default0]:  recompute_num_layers ............................ 1
[default0]:  remote_device ................................... none
[default0]:  reset_attention_mask ............................ False
[default0]:  reset_iteration ................................. False
[default0]:  reset_position_ids .............................. False
[default0]:  retriever_report_topk_accuracies ................ []
[default0]:  retriever_score_scaling ......................... False
[default0]:  retriever_seq_length ............................ 256
[default0]:  retro_add_retriever ............................. False
[default0]:  retro_cyclic_train_iters ........................ None
[default0]:  retro_encoder_attention_dropout ................. 0.1
[default0]:  retro_encoder_hidden_dropout .................... 0.1
[default0]:  retro_encoder_layers ............................ 2
[default0]:  retro_num_neighbors ............................. 2
[default0]:  retro_num_retrieved_chunks ...................... 2
[default0]:  retro_return_doc_ids ............................ False
[default0]:  retro_workdir ................................... None
[default0]:  return_data_index ............................... False
[default0]:  rotary_percent .................................. 1.0
[default0]:  sample_rate ..................................... 1.0
[default0]:  save ............................................ /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:  save_interval ................................... 50
[default0]:  scatter_gather_tensors_in_pipeline .............. True
[default0]:  scattered_embeddings ............................ False
[default0]:  seed ............................................ 1234
[default0]:  seq_length ...................................... 2048
[default0]:  sequence_parallel ............................... False
[default0]:  sgd_momentum .................................... 0.9
[default0]:  short_seq_prob .................................. 0.1
[default0]:  skip_train ...................................... False
[default0]:  split ........................................... 98,2,0
[default0]:  split_transformers .............................. False
[default0]:  squared_relu .................................... False
[default0]:  standalone_embedding_stage ...................... False
[default0]:  start_weight_decay .............................. 0.1
[default0]:  swiglu .......................................... False
[default0]:  swin_backbone_type .............................. tiny
[default0]:  synchronize_each_layer .......................... False
[default0]:  tensor_model_parallel_size ...................... 1
[default0]:  tensorboard_dir ................................. /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/tensorboard/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true_n2gpu1211_2023.07.25-15.03.26
[default0]:  tensorboard_log_interval ........................ 1
[default0]:  tensorboard_queue_size .......................... 1
[default0]:  test_data_path .................................. None
[default0]:  tile_factor ..................................... 1
[default0]:  timing_log_level ................................ 0
[default0]:  timing_log_option ............................... minmax
[default0]:  titles_data_path ................................ None
[default0]:  tokenizer_model ................................. None
[default0]:  tokenizer_type .................................. GPT2BPETokenizer
[default0]:  topk ............................................ 1
[default0]:  train_data_exact_num_epochs ..................... None
[default0]:  train_data_path ................................. None
[default0]:  train_desc_path ................................. None
[default0]:  train_doc_idx_path .............................. None
[default0]:  train_idx_path .................................. None
[default0]:  train_iters ..................................... 109863281
[default0]:  train_sample_idx_path ........................... None
[default0]:  train_samples ................................... None
[default0]:  train_shuffle_idx_path .......................... None
[default0]:  train_tokens .................................... 300000000000
[default0]:  transformer_impl ................................ local
[default0]:  transformer_pipeline_model_parallel_size ........ 1
[default0]:  untie_embeddings_and_output_weights ............. False
[default0]:  use_checkpoint_args ............................. False
[default0]:  use_checkpoint_opt_param_scheduler .............. False
[default0]:  use_contiguous_buffers_in_local_ddp ............. True
[default0]:  use_cpu_initialization .......................... None
[default0]:  use_distributed_optimizer ....................... False
[default0]:  use_flash_attn .................................. False
[default0]:  use_one_sent_docs ............................... False
[default0]:  use_pin_memory .................................. False
[default0]:  use_ring_exchange_p2p ........................... False
[default0]:  use_rotary_position_embeddings .................. False
[default0]:  use_tutel ....................................... False
[default0]:  valid_data_path ................................. None
[default0]:  variable_seq_lengths ............................ False
[default0]:  virtual_pipeline_model_parallel_size ............ None
[default0]:  vision_backbone_type ............................ vit
[default0]:  vision_pretraining .............................. False
[default0]:  vision_pretraining_type ......................... classify
[default0]:  vocab_extra_ids ................................. 0
[default0]:  vocab_file ...................................... /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/gpt2-vocab.json
[default0]:  vocab_size ...................................... None
[default0]:  weight_decay .................................... 0.1
[default0]:  weight_decay_incr_style ......................... constant
[default0]:  world_size ...................................... 4
[default0]:  zero_allgather_bucket_size ...................... 0.0
[default0]:  zero_contigious_gradients ....................... False
[default0]:  zero_reduce_bucket_size ......................... 0.0
[default0]:  zero_reduce_scatter ............................. False
[default0]:  zero_stage ...................................... 1.0
[default0]:-------------------- end of arguments ---------------------
[default0]:setting number of micro-batches to constant 1
[default0]:> building GPT2BPETokenizer tokenizer ...
[default1]:**** Git info for Megatron: git_hash=aad39cb git_branch=main ****
[default0]:[2023-07-25 15:03:55,489] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[default0]:[2023-07-25 15:03:55,510] [INFO] [comm.py:616:init_distributed] cdb=None
[default0]: > padded vocab (size: 50257) with 47 dummy tokens (new size: 50304)
[default0]:> initializing torch distributed ...
[default0]:[2023-07-25 15:03:55,511] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[default0]:[2023-07-25 15:03:55,512] [INFO] [comm.py:616:init_distributed] cdb=None
[default0]:[2023-07-25 15:03:55,512] [INFO] [comm.py:643:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[default1]:[2023-07-25 15:03:55,503] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[default1]:[2023-07-25 15:03:55,503] [INFO] [comm.py:616:init_distributed] cdb=None
[default1]:> setting tensorboard ...
[default1]:[2023-07-25 15:03:58,074] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented
[default1]:[2023-07-25 15:03:58,074] [INFO] [comm.py:616:init_distributed] cdb=None
[default0]:> initialized tensor model parallel with size 1
[default0]:> initialized pipeline model parallel with size 1
[default0]:> setting random seeds to 1234 ...
[default0]:> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
[default0]:> compiling dataset index builder ...
[default0]:make: Entering directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/data'
[default0]:make: Nothing to be done for 'default'.
[default0]:make: Leaving directory '/scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/data'
[default0]:>>> done with dataset index builder. Compilation time: 0.252 seconds
[default0]:> compiling and loading fused kernels ...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_upper_triang_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_masked_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_masked_softmax_cuda...
[default0]:Detected CUDA files, patching ldflags
[default0]:Emitting ninja build file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/megatron/fused_kernels/build/build.ninja...
[default0]:Building extension module scaled_softmax_cuda...
[default0]:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
[default0]:ninja: no work to do.
[default0]:Loading extension module scaled_softmax_cuda...
[default0]:n2gpu1211:3555029:3555029 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.79<0>
[default0]:n2gpu1211:3555029:3555029 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1211:3555029:3555029 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.79<0>
[default0]:n2gpu1211:3555029:3555029 [0] NCCL INFO Using network IB
[default0]:NCCL version 2.12.12+cuda11.7
[default0]:n2gpu1215:144522:144522 [0] NCCL INFO Bootstrap : Using ib1:10.10.103.83<0>
[default0]:n2gpu1215:144522:144522 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1215:144523:144523 [1] NCCL INFO Bootstrap : Using ib1:10.10.103.83<0>
[default1]:n2gpu1215:144523:144523 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default0]:n2gpu1215:144522:144522 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.83<0>
[default0]:n2gpu1215:144522:144522 [0] NCCL INFO Using network IB
[default1]:n2gpu1215:144523:144523 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.83<0>
[default1]:n2gpu1215:144523:144523 [1] NCCL INFO Using network IB
[default1]:n2gpu1211:3555030:3555030 [1] NCCL INFO Bootstrap : Using ib1:10.10.103.79<0>
[default1]:n2gpu1211:3555030:3555030 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[default1]:n2gpu1211:3555030:3555030 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib1:10.10.103.79<0>
[default1]:n2gpu1211:3555030:3555030 [1] NCCL INFO Using network IB
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
[default1]:n2gpu1211:3555030:3555354 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Channel 00/02 :    0   1   2   3
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Channel 01/02 :    0   1   2   3
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Channel 00/0 : 3[c4000] -> 0[3000] [receive] via NET/IB/1
[default1]:n2gpu1215:144523:144616 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO Channel 00/0 : 1[44000] -> 2[84000] [receive] via NET/IB/0
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO Channel 01/0 : 1[44000] -> 2[84000] [receive] via NET/IB/0
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO Channel 00 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1211:3555030:3555354 [1] NCCL INFO Channel 00/0 : 1[44000] -> 2[84000] [send] via NET/IB/1
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Channel 01/0 : 3[c4000] -> 0[3000] [receive] via NET/IB/1
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Channel 00 : 0[3000] -> 1[44000] via P2P/IPC/read
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Channel 01 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1215:144523:144616 [1] NCCL INFO Channel 00/0 : 3[c4000] -> 0[3000] [send] via NET/IB/0
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO Channel 01 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1211:3555030:3555354 [1] NCCL INFO Channel 01/0 : 1[44000] -> 2[84000] [send] via NET/IB/1
[default1]:n2gpu1215:144523:144616 [1] NCCL INFO Channel 01/0 : 3[c4000] -> 0[3000] [send] via NET/IB/0
[default1]:n2gpu1215:144523:144616 [1] NCCL INFO Connected all rings
[default1]:n2gpu1215:144523:144616 [1] NCCL INFO Channel 00 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1211:3555030:3555354 [1] NCCL INFO Connected all rings
[default1]:n2gpu1211:3555030:3555354 [1] NCCL INFO Channel 00 : 1[44000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1211:3555030:3555354 [1] NCCL INFO Channel 01 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Connected all rings
[default1]:n2gpu1215:144523:144616 [1] NCCL INFO Channel 01 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO Connected all rings
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Channel 00/0 : 2[84000] -> 0[3000] [receive] via NET/IB/1
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO Channel 00/0 : 0[3000] -> 2[84000] [receive] via NET/IB/0
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Channel 01/0 : 2[84000] -> 0[3000] [receive] via NET/IB/1
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO Channel 01/0 : 0[3000] -> 2[84000] [receive] via NET/IB/0
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO Channel 00/0 : 2[84000] -> 0[3000] [send] via NET/IB/0
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Channel 00/0 : 0[3000] -> 2[84000] [send] via NET/IB/1
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO Channel 01/0 : 2[84000] -> 0[3000] [send] via NET/IB/0
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Channel 01/0 : 0[3000] -> 2[84000] [send] via NET/IB/1
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO Connected all trees
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1211:3555030:3555354 [1] NCCL INFO Connected all trees
[default1]:n2gpu1211:3555030:3555354 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
[default1]:n2gpu1211:3555030:3555354 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1211:3555030:3555354 [1] NCCL INFO comm 0x14b2840090d0 rank 1 nranks 4 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1215:144523:144616 [1] NCCL INFO Connected all trees
[default1]:n2gpu1215:144523:144616 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
[default1]:n2gpu1215:144523:144616 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO Connected all trees
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1211:3555029:3555351 [0] NCCL INFO comm 0x14e52c0090d0 rank 0 nranks 4 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1211:3555029:3555029 [0] NCCL INFO Launch mode Parallel
[default1]:n2gpu1215:144523:144616 [1] NCCL INFO comm 0x1496c80090d0 rank 3 nranks 4 cudaDev 1 busId c4000 - Init COMPLETE
[default0]:n2gpu1215:144522:144619 [0] NCCL INFO comm 0x14b8900090d0 rank 2 nranks 4 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:>>> done with compiling and loading fused kernels. Compilation time: 6.063 seconds
[default0]:time to initialize megatron (seconds): 11.717
[default0]:[after megatron is initialized] datetime: 2023-07-25 15:04:06 
[default0]:building GPT model ...
[default0]:[2023-07-25 15:04:06,228] [INFO] [utils.py:785:see_memory_usage] Before Building Model
[default0]:[2023-07-25 15:04:06,229] [INFO] [utils.py:786:see_memory_usage] MA 0.0 GB         Max_MA 0.53 GB         CA 0.0 GB         Max_CA 1 GB 
[default0]:[2023-07-25 15:04:06,229] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 18.42 GB, percent = 3.7%
[default1]:param_group keyset: dict_keys(['name', 'params', 'wd_mult', 'lr_mult'])
[default1]:param_group keyset: dict_keys(['name', 'params', 'wd_mult', 'lr_mult'])
[default0]:param_group keyset: dict_keys(['name', 'params', 'wd_mult', 'lr_mult'])
[default0]:[2023-07-25 15:04:06,990] [INFO] [utils.py:785:see_memory_usage] After Building Model
[default0]:[2023-07-25 15:04:06,991] [INFO] [utils.py:786:see_memory_usage] MA 2.44 GB         Max_MA 2.44 GB         CA 2.49 GB         Max_CA 2 GB 
[default0]:[2023-07-25 15:04:06,991] [INFO] [utils.py:793:see_memory_usage] CPU Virtual Memory:  used = 18.45 GB, percent = 3.7%
[default0]: > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 1315819520
[default0]:param_group keyset: dict_keys(['name', 'params', 'wd_mult', 'lr_mult'])
[default0]:> learning rate decay style: cosine
[default0]:DeepSpeed is enabled.
[default0]:[2023-07-25 15:04:06,994] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.0, git-hash=unknown, git-branch=unknown
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO Trees [0] 3/-1/-1->2->0 [1] 3/0/-1->2->-1
[default1]:n2gpu1211:3555030:3555395 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Channel 00/02 :    0   1   2   3
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Channel 01/02 :    0   1   2   3
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Trees [0] 1/2/-1->0->-1 [1] 1/-1/-1->0->2
[default1]:n2gpu1215:144523:144641 [1] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO Channel 00/0 : 1[44000] -> 2[84000] [receive] via NET/IB/0
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Channel 00/0 : 3[c4000] -> 0[3000] [receive] via NET/IB/1
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Channel 01/0 : 3[c4000] -> 0[3000] [receive] via NET/IB/1
[default1]:n2gpu1211:3555030:3555395 [1] NCCL INFO Channel 00/0 : 1[44000] -> 2[84000] [send] via NET/IB/1
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Channel 00 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1215:144523:144641 [1] NCCL INFO Channel 00/0 : 3[c4000] -> 0[3000] [send] via NET/IB/0
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO Channel 01/0 : 1[44000] -> 2[84000] [receive] via NET/IB/0
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO Channel 00 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO Channel 01 : 2[84000] -> 3[c4000] via P2P/IPC/read
[default1]:n2gpu1211:3555030:3555395 [1] NCCL INFO Channel 01/0 : 1[44000] -> 2[84000] [send] via NET/IB/1
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Channel 01 : 0[3000] -> 1[44000] via P2P/IPC/read
[default1]:n2gpu1215:144523:144641 [1] NCCL INFO Channel 01/0 : 3[c4000] -> 0[3000] [send] via NET/IB/0
[default1]:n2gpu1211:3555030:3555395 [1] NCCL INFO Connected all rings
[default1]:n2gpu1211:3555030:3555395 [1] NCCL INFO Channel 00 : 1[44000] -> 0[3000] via P2P/IPC/read
[default1]:n2gpu1215:144523:144641 [1] NCCL INFO Connected all rings
[default1]:n2gpu1215:144523:144641 [1] NCCL INFO Channel 00 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default1]:n2gpu1215:144523:144641 [1] NCCL INFO Channel 01 : 3[c4000] -> 2[84000] via P2P/IPC/read
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO Connected all rings
[default1]:n2gpu1211:3555030:3555395 [1] NCCL INFO Channel 01 : 1[44000] -> 0[3000] via P2P/IPC/read
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO Channel 00/0 : 0[3000] -> 2[84000] [receive] via NET/IB/0
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Connected all rings
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Channel 00/0 : 2[84000] -> 0[3000] [receive] via NET/IB/1
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO Channel 01/0 : 0[3000] -> 2[84000] [receive] via NET/IB/0
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Channel 01/0 : 2[84000] -> 0[3000] [receive] via NET/IB/1
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO Channel 00/0 : 2[84000] -> 0[3000] [send] via NET/IB/0
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Channel 00/0 : 0[3000] -> 2[84000] [send] via NET/IB/1
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO Channel 01/0 : 2[84000] -> 0[3000] [send] via NET/IB/0
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Channel 01/0 : 0[3000] -> 2[84000] [send] via NET/IB/1
[default1]:n2gpu1211:3555030:3555395 [1] NCCL INFO Connected all trees
[default1]:n2gpu1211:3555030:3555395 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
[default1]:n2gpu1211:3555030:3555395 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO Connected all trees
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1211:3555029:3555394 [0] NCCL INFO comm 0x14e4740090d0 rank 0 nranks 4 cudaDev 0 busId 3000 - Init COMPLETE
[default0]:n2gpu1211:3555029:3555029 [0] NCCL INFO Launch mode Parallel
[default1]:n2gpu1211:3555030:3555395 [1] NCCL INFO comm 0x14b1cc0090d0 rank 1 nranks 4 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1215:144523:144641 [1] NCCL INFO Connected all trees
[default1]:n2gpu1215:144523:144641 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
[default1]:n2gpu1215:144523:144641 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO Connected all trees
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 8/8/512
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[default1]:n2gpu1215:144523:144641 [1] NCCL INFO comm 0x1496100090d0 rank 3 nranks 4 cudaDev 1 busId c4000 - Init COMPLETE
[default0]:n2gpu1215:144522:144642 [0] NCCL INFO comm 0x14b7d80090d0 rank 2 nranks 4 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:[2023-07-25 15:04:16,588] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[default0]:[2023-07-25 15:04:16,607] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[default0]:[2023-07-25 15:04:16,607] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[default0]:[2023-07-25 15:04:16,618] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[default0]:[2023-07-25 15:04:16,618] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
[default0]:[2023-07-25 15:04:16,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
[default0]:[2023-07-25 15:04:16,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[default0]:[2023-07-25 15:04:16,950] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x14e5a76a3fa0>
[default0]:[2023-07-25 15:04:16,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:04:16,951] [INFO] [config.py:960:print] DeepSpeedEngine configuration:
[default0]:[2023-07-25 15:04:16,951] [INFO] [config.py:964:print]   activation_checkpointing_config  {
[default0]:    "partition_activations": false, 
[default0]:    "contiguous_memory_optimization": false, 
[default0]:    "cpu_checkpointing": false, 
[default0]:    "number_checkpoints": null, 
[default0]:    "synchronize_checkpoint_boundary": false, 
[default0]:    "profile": false
[default0]:}
[default0]:[2023-07-25 15:04:16,951] [INFO] [config.py:964:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[default0]:[2023-07-25 15:04:16,951] [INFO] [config.py:964:print]   amp_enabled .................. False
[default0]:[2023-07-25 15:04:16,951] [INFO] [config.py:964:print]   amp_params ................... False
[default0]:[2023-07-25 15:04:16,951] [INFO] [config.py:964:print]   autotuning_config ............ {
[default0]:    "enabled": false, 
[default0]:    "start_step": null, 
[default0]:    "end_step": null, 
[default0]:    "metric_path": null, 
[default0]:    "arg_mappings": null, 
[default0]:    "metric": "throughput", 
[default0]:    "model_info": null, 
[default0]:    "results_dir": "autotuning_results", 
[default0]:    "exps_dir": "autotuning_exps", 
[default0]:    "overwrite": true, 
[default0]:    "fast": true, 
[default0]:    "start_profile_step": 3, 
[default0]:    "end_profile_step": 5, 
[default0]:    "tuner_type": "gridsearch", 
[default0]:    "tuner_early_stopping": 5, 
[default0]:    "tuner_num_trials": 50, 
[default0]:    "model_info_path": null, 
[default0]:    "mp_size": 1, 
[default0]:    "max_train_batch_size": null, 
[default0]:    "min_train_batch_size": 1, 
[default0]:    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
[default0]:    "min_train_micro_batch_size_per_gpu": 1, 
[default0]:    "num_tuning_micro_batch_sizes": 3
[default0]:}
[default0]:[2023-07-25 15:04:16,951] [INFO] [config.py:964:print]   bfloat16_enabled ............. False
[default0]:[2023-07-25 15:04:16,951] [INFO] [config.py:964:print]   checkpoint_parallel_write_pipeline  False
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   checkpoint_tag_validation_enabled  True
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   checkpoint_tag_validation_fail  False
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x14e5a76a39d0>
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   communication_data_type ...... None
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   curriculum_enabled_legacy .... False
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   curriculum_params_legacy ..... {'curriculum_type': 'seqlen', 'min_difficulty': 80, 'max_difficulty': 2048, 'schedule_type': 'fixed_linear', 'schedule_config': {'total_curriculum_step': 14097744, 'difficulty_step': 8}}
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   data_efficiency_enabled ...... False
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   dataloader_drop_last ......... False
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   disable_allgather ............ False
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   dump_state ................... False
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   eigenvalue_enabled ........... False
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   eigenvalue_gas_boundary_resolution  1
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   eigenvalue_layer_name ........ bert.encoder.layer
[default0]:[2023-07-25 15:04:16,952] [INFO] [config.py:964:print]   eigenvalue_layer_num ......... 0
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   eigenvalue_max_iter .......... 100
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   eigenvalue_stability ......... 1e-06
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   eigenvalue_tol ............... 0.01
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   eigenvalue_verbose ........... False
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   elasticity_enabled ........... False
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   flops_profiler_config ........ {
[default0]:    "enabled": false, 
[default0]:    "recompute_fwd_factor": 0.0, 
[default0]:    "profile_step": 1, 
[default0]:    "module_depth": -1, 
[default0]:    "top_modules": 1, 
[default0]:    "detailed": true, 
[default0]:    "output_file": null
[default0]:}
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   fp16_auto_cast ............... False
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   fp16_enabled ................. True
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   fp16_master_weights_and_gradients  False
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   global_rank .................. 0
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   grad_accum_dtype ............. None
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   gradient_accumulation_steps .. 1
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   gradient_clipping ............ 1
[default0]:[2023-07-25 15:04:16,953] [INFO] [config.py:964:print]   gradient_predivide_factor .... 1.0
[default0]:[2023-07-25 15:04:17,035] [INFO] [config.py:964:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[default0]:[2023-07-25 15:04:17,035] [INFO] [config.py:964:print]   initial_dynamic_scale ........ 2048
[default0]:[2023-07-25 15:04:17,035] [INFO] [config.py:964:print]   load_universal_checkpoint .... False
[default0]:[2023-07-25 15:04:17,035] [INFO] [config.py:964:print]   loss_scale ................... 0
[default0]:[2023-07-25 15:04:17,035] [INFO] [config.py:964:print]   memory_breakdown ............. False
[default0]:[2023-07-25 15:04:17,035] [INFO] [config.py:964:print]   mics_hierarchial_params_gather  False
[default0]:[2023-07-25 15:04:17,035] [INFO] [config.py:964:print]   mics_shard_size .............. -1
[default0]:[2023-07-25 15:04:17,035] [INFO] [config.py:964:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   nebula_config ................ {
[default0]:    "enabled": false, 
[default0]:    "persistent_storage_path": null, 
[default0]:    "persistent_time_interval": 100, 
[default0]:    "num_of_version_in_retention": 2, 
[default0]:    "enable_nebula_load": true, 
[default0]:    "load_path": null
[default0]:}
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   optimizer_legacy_fusion ...... False
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   optimizer_name ............... None
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   optimizer_params ............. None
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   pld_enabled .................. False
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   pld_params ................... False
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   prescale_gradients ........... True
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   scheduler_name ............... None
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   scheduler_params ............. None
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   sparse_attention ............. None
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   sparse_gradients_enabled ..... False
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   steps_per_print .............. 5
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   train_batch_size ............. 4
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   train_micro_batch_size_per_gpu  1
[default0]:[2023-07-25 15:04:17,036] [INFO] [config.py:964:print]   use_node_local_storage ....... False
[default0]:[2023-07-25 15:04:17,037] [INFO] [config.py:964:print]   wall_clock_breakdown ......... False
[default0]:[2023-07-25 15:04:17,037] [INFO] [config.py:964:print]   world_size ................... 4
[default0]:[2023-07-25 15:04:17,037] [INFO] [config.py:964:print]   zero_allow_untested_optimizer  False
[default0]:[2023-07-25 15:04:17,037] [INFO] [config.py:964:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=True offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True
[default0]:[2023-07-25 15:04:17,037] [INFO] [config.py:964:print]   zero_enabled ................. False
[default0]:[2023-07-25 15:04:17,037] [INFO] [config.py:964:print]   zero_force_ds_cpu_optimizer .. True
[default0]:[2023-07-25 15:04:17,037] [INFO] [config.py:964:print]   zero_optimization_stage ...... 0
[default0]:[2023-07-25 15:04:17,037] [INFO] [config.py:950:print_user_config]   json = {
[default0]:    "train_batch_size": 4, 
[default0]:    "train_micro_batch_size_per_gpu": 1, 
[default0]:    "steps_per_print": 5, 
[default0]:    "zero_optimization": {
[default0]:        "stage": 0, 
[default0]:        "elastic_checkpoint": true
[default0]:    }, 
[default0]:    "gradient_clipping": 1, 
[default0]:    "prescale_gradients": true, 
[default0]:    "fp16": {
[default0]:        "enabled": true, 
[default0]:        "loss_scale": 0, 
[default0]:        "loss_scale_window": 500, 
[default0]:        "hysteresis": 2, 
[default0]:        "min_loss_scale": 1, 
[default0]:        "initial_scale_power": 11
[default0]:    }, 
[default0]:    "bf16": {
[default0]:        "enabled": false
[default0]:    }, 
[default0]:    "curriculum_learning": {
[default0]:        "enabled": false, 
[default0]:        "curriculum_type": "seqlen", 
[default0]:        "min_difficulty": 80, 
[default0]:        "max_difficulty": 2.048000e+03, 
[default0]:        "schedule_type": "fixed_linear", 
[default0]:        "schedule_config": {
[default0]:            "total_curriculum_step": 1.409774e+07, 
[default0]:            "difficulty_step": 8
[default0]:        }
[default0]:    }, 
[default0]:    "wall_clock_breakdown": false
[default0]:}
[default0]:[2023-07-25 15:04:17,051] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:WARNING: could not find the metadata file /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true 
[default0]:    will not load any checkpoints and will start from random
[default1]:[2023-07-25 15:04:17,073] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:[2023-07-25 15:04:17,062] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default1]:(min, max) time across ranks (ms):
[default1]:    load-checkpoint ................................: (29.64, 44.25)
[default0]:[2023-07-25 15:04:17,051] [WARNING] [engine.py:2635:load_checkpoint] Unable to find latest file at /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
[default0]:[after model, optimizer, and learning rate scheduler are built] datetime: 2023-07-25 15:04:17 
[default0]:> building train, validation, and test datasets ...
[default0]: > datasets target sizes (minimum size):
[default0]:    train:      439453124
[default0]:    validation: 219726600
[default0]:    test:       200
[default0]:> building train, validation, and test datasets for GPT ...
[default0]:Single data path provided for train, valid & test
[default0]: > building dataset index ...
[default0]:    reading sizes...
[default0]:    reading pointers...
[default0]:    reading document index...
[default0]:    creating numpy buffer of mmap...
[default0]:    creating memory view of numpy buffer...
[default0]: > finished creating indexed dataset in 0.027597 seconds
[default0]:    number of documents: 10000
[default0]: > dataset split:
[default0]:    train:
[default0]:     document indices in [0, 9800) total of 9800 documents
[default0]:    validation:
[default0]:     document indices in [9800, 10000) total of 200 documents
[default0]:    test:
[default0]:     document indices in [10000, 10000) total of 0 documents
[default1]:NCCL version 2.12.12+cuda11.7
[default1]:NCCL version 2.12.12+cuda11.7
[default0]:NCCL version 2.12.12+cuda11.7
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Connected all rings
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO Connected all trees
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Connected all rings
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO Connected all trees
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1211:3555029:3555413 [0] NCCL INFO comm 0x14dd900090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default0]: > loading doc-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/71654a473c8694dd2b48f0197bdd21a9_doc_idx.npy
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Connected all rings
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO Connected all trees
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1211:3555030:3555415 [1] NCCL INFO comm 0x14aadc0090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Connected all rings
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO Connected all trees
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1215:144523:144655 [1] NCCL INFO comm 0x148f240090d0 rank 0 nranks 1 cudaDev 1 busId c4000 - Init COMPLETE
[default0]:n2gpu1215:144522:144653 [0] NCCL INFO comm 0x14b0e80090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]: > loading sample-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/71654a473c8694dd2b48f0197bdd21a9_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/71654a473c8694dd2b48f0197bdd21a9_shuffle_idx.npy
[default0]:    loaded indexed file in 0.126 seconds
[default0]:    total number of samples: 439466004
[default0]:    total number of epochs: 30909
[default0]: > loading doc-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/de3819301120d9a35080a0a129b3870d_doc_idx.npy
[default0]: > loading sample-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/de3819301120d9a35080a0a129b3870d_sample_idx.npy
[default0]: > loading shuffle-idx mapping from /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/data/index-cache/de3819301120d9a35080a0a129b3870d_shuffle_idx.npy
[default0]:    loaded indexed file in 0.061 seconds
[default0]:    total number of samples: 219726809
[default0]:    total number of epochs: 648115
[default0]:> finished creating GPT datasets ...
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Connected all rings
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO Connected all trees
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Connected all rings
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO Connected all trees
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1211:3555029:3555421 [0] NCCL INFO comm 0x14da000090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Connected all rings
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO Connected all trees
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1215:144523:144663 [1] NCCL INFO comm 0x148b940090d0 rank 0 nranks 1 cudaDev 1 busId c4000 - Init COMPLETE
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Connected all rings
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO Connected all trees
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1215:144522:144661 [0] NCCL INFO comm 0x14b0ec0090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:[after dataloaders are built] datetime: 2023-07-25 15:04:18 
[default0]:done with setup ...
[default1]:n2gpu1211:3555030:3555423 [1] NCCL INFO comm 0x14a7580090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:(min, max) time across ranks (ms):
[default1]:    model-and-optimizer-setup ......................: (11032.77, 11040.42)
[default1]:    train/valid/test-data-iterators-setup ..........: (1254.12, 1306.73)
[default0]:training ...
[default0]:[before the start of training step] datetime: 2023-07-25 15:04:18 
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Connected all rings
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO Connected all trees
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Connected all rings
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO Connected all trees
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 00/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 01/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 02/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 03/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 04/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 05/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 06/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 07/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 08/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 09/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 10/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 11/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 12/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 13/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 14/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 15/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 16/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 17/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 18/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 19/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 20/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 21/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 22/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 23/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 24/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 25/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 26/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 27/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 28/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 29/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 30/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Channel 31/32 :    0
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Connected all rings
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO Connected all trees
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 00/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 01/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 02/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 03/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 04/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 05/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 06/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 07/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 08/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 09/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 10/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 11/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 12/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 13/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 14/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 15/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 16/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 17/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 18/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 19/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 20/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 21/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 22/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 23/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 24/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 25/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 26/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 27/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 28/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 29/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 30/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Channel 31/32 :    0
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Connected all rings
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO Connected all trees
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer
[default0]:n2gpu1211:3555029:3555447 [0] NCCL INFO comm 0x14d9ec0090d0 rank 0 nranks 1 cudaDev 0 busId 3000 - Init COMPLETE
[default1]:n2gpu1211:3555030:3555446 [1] NCCL INFO comm 0x14a7480090d0 rank 0 nranks 1 cudaDev 1 busId 44000 - Init COMPLETE
[default1]:n2gpu1215:144523:144677 [1] NCCL INFO comm 0x148b800090d0 rank 0 nranks 1 cudaDev 1 busId c4000 - Init COMPLETE
[default0]:n2gpu1215:144522:144675 [0] NCCL INFO comm 0x14ad540090d0 rank 0 nranks 1 cudaDev 0 busId 84000 - Init COMPLETE
[default0]:[2023-07-25 15:04:35,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[1.7476266666666667e-08, 1.7476266666666667e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:04:35,168] [INFO] [timer.py:215:stop] epoch=0/micro_step=5/global_step=5, RunningAvgSamplesPerSec=1.5386412057245116, CurrSamplesPerSec=1.617987601149444, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default0]:[Rank 0] (after 5 iterations) memory (MB) | allocated: 17699.36279296875 | max allocated: 30245.05126953125 | reserved: 35234.0 | max reserved: 35234.0
[default1]: iteration        5/109863281 | consumed samples:           20 | consumed tokens:        40960 | elapsed time per iteration (ms): 3278.4 | learning rate: 1.748E-08 | global batch size:     4 | lm loss: 1.102513E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.220 | TFLOPs: 7.43 |
[default0]:[2023-07-25 15:04:49,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3.9321600000000005e-08, 3.9321600000000005e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:04:49,185] [INFO] [timer.py:215:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=1.5287166168033357, CurrSamplesPerSec=1.4021363311603299, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       10/109863281 | consumed samples:           40 | consumed tokens:        81920 | elapsed time per iteration (ms): 2797.8 | learning rate: 3.932E-08 | global batch size:     4 | lm loss: 1.102962E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.430 | TFLOPs: 8.71 |
[default0]:[2023-07-25 15:05:02,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=15, skipped=0, lr=[6.116693333333333e-08, 6.116693333333333e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:05:02,817] [INFO] [timer.py:215:stop] epoch=0/micro_step=15/global_step=15, RunningAvgSamplesPerSec=1.549051804373784, CurrSamplesPerSec=1.705154292777636, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       15/109863281 | consumed samples:           60 | consumed tokens:       122880 | elapsed time per iteration (ms): 2720.4 | learning rate: 6.117E-08 | global batch size:     4 | lm loss: 1.099091E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.470 | TFLOPs: 8.95 |
[default0]:[2023-07-25 15:05:16,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[8.301226666666667e-08, 8.301226666666667e-08], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:05:16,461] [INFO] [timer.py:215:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=1.5589857062500319, CurrSamplesPerSec=1.6423631634539988, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       20/109863281 | consumed samples:           80 | consumed tokens:       163840 | elapsed time per iteration (ms): 2724.0 | learning rate: 8.301E-08 | global batch size:     4 | lm loss: 1.090087E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.468 | TFLOPs: 8.94 |
[default0]:[2023-07-25 15:05:30,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=25, skipped=0, lr=[1.048576e-07, 1.048576e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:05:30,547] [INFO] [timer.py:215:stop] epoch=0/micro_step=25/global_step=25, RunningAvgSamplesPerSec=1.5511455034586408, CurrSamplesPerSec=1.4739583543818706, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       25/109863281 | consumed samples:          100 | consumed tokens:       204800 | elapsed time per iteration (ms): 2809.5 | learning rate: 1.049E-07 | global batch size:     4 | lm loss: 1.074090E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.424 | TFLOPs: 8.67 |
[default0]:[2023-07-25 15:05:44,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[1.2670293333333334e-07, 1.2670293333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:05:44,346] [INFO] [timer.py:215:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=1.5525123033485828, CurrSamplesPerSec=1.5253688291069756, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       30/109863281 | consumed samples:          120 | consumed tokens:       245760 | elapsed time per iteration (ms): 2752.9 | learning rate: 1.267E-07 | global batch size:     4 | lm loss: 1.052876E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.453 | TFLOPs: 8.85 |
[default0]:[2023-07-25 15:05:58,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=35, skipped=0, lr=[1.4854826666666668e-07, 1.4854826666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:05:58,197] [INFO] [timer.py:215:stop] epoch=0/micro_step=35/global_step=35, RunningAvgSamplesPerSec=1.550735271998458, CurrSamplesPerSec=1.4894266451507345, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       35/109863281 | consumed samples:          140 | consumed tokens:       286720 | elapsed time per iteration (ms): 2775.4 | learning rate: 1.485E-07 | global batch size:     4 | lm loss: 1.034714E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.441 | TFLOPs: 8.78 |
[default0]:[2023-07-25 15:06:11,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[1.7039360000000002e-07, 1.7039360000000002e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:06:11,763] [INFO] [timer.py:215:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=1.5554757497666052, CurrSamplesPerSec=1.5631547087694022, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       40/109863281 | consumed samples:          160 | consumed tokens:       327680 | elapsed time per iteration (ms): 2702.4 | learning rate: 1.704E-07 | global batch size:     4 | lm loss: 1.023813E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.480 | TFLOPs: 9.01 |
[default0]:[2023-07-25 15:06:25,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=45, skipped=0, lr=[1.9223893333333335e-07, 1.9223893333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:06:25,865] [INFO] [timer.py:215:stop] epoch=0/micro_step=45/global_step=45, RunningAvgSamplesPerSec=1.5515536385462496, CurrSamplesPerSec=1.5392301379527686, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       45/109863281 | consumed samples:          180 | consumed tokens:       368640 | elapsed time per iteration (ms): 2808.3 | learning rate: 1.922E-07 | global batch size:     4 | lm loss: 1.009111E+01 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.424 | TFLOPs: 8.67 |
[default0]:[2023-07-25 15:06:40,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[2.1408426666666666e-07, 2.1408426666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:06:40,221] [INFO] [timer.py:215:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=1.5458570085166468, CurrSamplesPerSec=1.4900057576342385, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       50/109863281 | consumed samples:          200 | consumed tokens:       409600 | elapsed time per iteration (ms): 2865.1 | learning rate: 2.141E-07 | global batch size:     4 | lm loss: 9.815231E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.396 | TFLOPs: 8.50 |
[default0]:saving checkpoint at iteration      50 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:06:40,372] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step50 is about to be saved!
[default1]:[2023-07-25 15:06:40,462] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50 is ready now!
[default0]:[2023-07-25 15:06:40,461] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step50/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:06:40,462] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step50/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 15:06:40,464] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50 is ready now!
[default0]:[2023-07-25 15:06:40,477] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50 is ready now!
[default0]:[2023-07-25 15:07:53,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step50/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:07:53,803] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step50 is ready now!
[default0]:  successfully saved checkpoint at iteration      50 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.25, Latency(second): 73.612
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (73601.27, 73612.50)
[default0]:[2023-07-25 15:08:07,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=55, skipped=0, lr=[2.359296e-07, 2.359296e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:08:07,458] [INFO] [timer.py:215:stop] epoch=0/micro_step=55/global_step=55, RunningAvgSamplesPerSec=1.5476610048620083, CurrSamplesPerSec=1.5310085700516605, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       55/109863281 | consumed samples:          220 | consumed tokens:       450560 | elapsed time per iteration (ms): 17453.9 | learning rate: 2.359E-07 | global batch size:     4 | lm loss: 9.661499E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.229 | TFLOPs: 1.40 |
[default0]:[2023-07-25 15:08:21,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[2.577749333333333e-07, 2.577749333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:08:21,283] [INFO] [timer.py:215:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=1.5480385142090698, CurrSamplesPerSec=1.4612928734821655, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       60/109863281 | consumed samples:          240 | consumed tokens:       491520 | elapsed time per iteration (ms): 2754.9 | learning rate: 2.578E-07 | global batch size:     4 | lm loss: 9.566370E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.452 | TFLOPs: 8.84 |
[default0]:[2023-07-25 15:08:35,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=65, skipped=0, lr=[2.796202666666667e-07, 2.796202666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:08:35,029] [INFO] [timer.py:215:stop] epoch=0/micro_step=65/global_step=65, RunningAvgSamplesPerSec=1.548771190899539, CurrSamplesPerSec=1.538565574999498, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       65/109863281 | consumed samples:          260 | consumed tokens:       532480 | elapsed time per iteration (ms): 2743.4 | learning rate: 2.796E-07 | global batch size:     4 | lm loss: 9.532632E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.458 | TFLOPs: 8.88 |
[default0]:[2023-07-25 15:08:48,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[3.0146560000000004e-07, 3.0146560000000004e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:08:48,191] [INFO] [timer.py:215:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=1.5547684550771448, CurrSamplesPerSec=1.8378179662385674, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       70/109863281 | consumed samples:          280 | consumed tokens:       573440 | elapsed time per iteration (ms): 2625.8 | learning rate: 3.015E-07 | global batch size:     4 | lm loss: 9.371884E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.523 | TFLOPs: 9.28 |
[default0]:[2023-07-25 15:09:01,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=75, skipped=0, lr=[3.2331093333333335e-07, 3.2331093333333335e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:09:01,677] [INFO] [timer.py:215:stop] epoch=0/micro_step=75/global_step=75, RunningAvgSamplesPerSec=1.558159352036768, CurrSamplesPerSec=1.7611770898133114, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       75/109863281 | consumed samples:          300 | consumed tokens:       614400 | elapsed time per iteration (ms): 2687.5 | learning rate: 3.233E-07 | global batch size:     4 | lm loss: 9.319179E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.488 | TFLOPs: 9.06 |
[default0]:[2023-07-25 15:09:15,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[3.451562666666667e-07, 3.451562666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:09:15,111] [INFO] [timer.py:215:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=1.5604755738519216, CurrSamplesPerSec=1.5752114095777667, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       80/109863281 | consumed samples:          320 | consumed tokens:       655360 | elapsed time per iteration (ms): 2678.0 | learning rate: 3.452E-07 | global batch size:     4 | lm loss: 9.280448E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.494 | TFLOPs: 9.09 |
[default0]:[2023-07-25 15:09:29,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=85, skipped=0, lr=[3.670016000000001e-07, 3.670016000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:09:29,226] [INFO] [timer.py:215:stop] epoch=0/micro_step=85/global_step=85, RunningAvgSamplesPerSec=1.5575975349969564, CurrSamplesPerSec=1.4928536236581997, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       85/109863281 | consumed samples:          340 | consumed tokens:       696320 | elapsed time per iteration (ms): 2812.4 | learning rate: 3.670E-07 | global batch size:     4 | lm loss: 9.207076E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.422 | TFLOPs: 8.66 |
[default0]:[2023-07-25 15:09:43,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[3.8884693333333334e-07, 3.8884693333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:09:43,492] [INFO] [timer.py:215:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=1.5558449064987372, CurrSamplesPerSec=1.4867028780538294, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       90/109863281 | consumed samples:          360 | consumed tokens:       737280 | elapsed time per iteration (ms): 2844.7 | learning rate: 3.888E-07 | global batch size:     4 | lm loss: 9.310506E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.406 | TFLOPs: 8.56 |
[default0]:[2023-07-25 15:09:56,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=95, skipped=0, lr=[4.106922666666667e-07, 4.106922666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:09:56,728] [INFO] [timer.py:215:stop] epoch=0/micro_step=95/global_step=95, RunningAvgSamplesPerSec=1.5583811657843976, CurrSamplesPerSec=1.6122980905416202, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration       95/109863281 | consumed samples:          380 | consumed tokens:       778240 | elapsed time per iteration (ms): 2641.3 | learning rate: 4.107E-07 | global batch size:     4 | lm loss: 9.192912E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.514 | TFLOPs: 9.22 |
[default0]:[2023-07-25 15:10:10,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[4.325376e-07, 4.325376e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:10:10,239] [INFO] [timer.py:215:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=1.5592454823369295, CurrSamplesPerSec=1.5387200897191489, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      100/109863281 | consumed samples:          400 | consumed tokens:       819200 | elapsed time per iteration (ms): 2700.2 | learning rate: 4.325E-07 | global batch size:     4 | lm loss: 9.115784E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.481 | TFLOPs: 9.02 |
[default1]:-----------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 100 | lm loss value: 9.140559E+00 | lm loss PPL: 9.325979E+03 | 
[default1]:-----------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration     100 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:10:21,959] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default1]:[2023-07-25 15:10:21,973] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default0]:[2023-07-25 15:10:21,890] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
[default0]:[2023-07-25 15:10:21,963] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step100/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:10:21,963] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step100/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 15:10:21,962] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default0]:[2023-07-25 15:11:33,055] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step100/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:11:33,076] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
[default0]:  successfully saved checkpoint at iteration     100 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.305
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71303.79, 71313.76)
[default0]:[2023-07-25 15:11:46,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=105, skipped=0, lr=[4.543829333333334e-07, 4.543829333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:11:46,705] [INFO] [timer.py:215:stop] epoch=0/micro_step=105/global_step=105, RunningAvgSamplesPerSec=1.5591879523317624, CurrSamplesPerSec=1.5061523245591237, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      105/109863281 | consumed samples:          420 | consumed tokens:       860160 | elapsed time per iteration (ms): 19285.1 | learning rate: 4.544E-07 | global batch size:     4 | lm loss: 9.184834E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.207 | TFLOPs: 1.26 |
[default0]:[2023-07-25 15:11:59,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[4.762282666666667e-07, 4.762282666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:11:59,977] [INFO] [timer.py:215:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=1.561480550605682, CurrSamplesPerSec=1.5962299127560313, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      110/109863281 | consumed samples:          440 | consumed tokens:       901120 | elapsed time per iteration (ms): 2653.5 | learning rate: 4.762E-07 | global batch size:     4 | lm loss: 9.142636E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.507 | TFLOPs: 9.18 |
[default0]:[2023-07-25 15:12:13,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=115, skipped=0, lr=[4.980736e-07, 4.980736e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:12:13,968] [INFO] [timer.py:215:stop] epoch=0/micro_step=115/global_step=115, RunningAvgSamplesPerSec=1.561066417979279, CurrSamplesPerSec=1.5813865368118425, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      115/109863281 | consumed samples:          460 | consumed tokens:       942080 | elapsed time per iteration (ms): 2784.3 | learning rate: 4.981E-07 | global batch size:     4 | lm loss: 9.133720E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.437 | TFLOPs: 8.75 |
[default0]:[2023-07-25 15:12:27,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[5.199189333333334e-07, 5.199189333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:12:27,679] [INFO] [timer.py:215:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=1.5619846885345217, CurrSamplesPerSec=1.6643840583484248, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      120/109863281 | consumed samples:          480 | consumed tokens:       983040 | elapsed time per iteration (ms): 2725.7 | learning rate: 5.199E-07 | global batch size:     4 | lm loss: 9.155885E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.468 | TFLOPs: 8.94 |
[default0]:[2023-07-25 15:12:41,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=125, skipped=0, lr=[5.417642666666667e-07, 5.417642666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:12:41,554] [INFO] [timer.py:215:stop] epoch=0/micro_step=125/global_step=125, RunningAvgSamplesPerSec=1.5618947855403724, CurrSamplesPerSec=1.6680980089666695, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      125/109863281 | consumed samples:          500 | consumed tokens:      1024000 | elapsed time per iteration (ms): 2773.4 | learning rate: 5.418E-07 | global batch size:     4 | lm loss: 8.985991E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.442 | TFLOPs: 8.78 |
[default0]:[2023-07-25 15:12:55,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[5.636096e-07, 5.636096e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:12:55,112] [INFO] [timer.py:215:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=1.562657770877352, CurrSamplesPerSec=1.5715702048288747, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      130/109863281 | consumed samples:          520 | consumed tokens:      1064960 | elapsed time per iteration (ms): 2703.0 | learning rate: 5.636E-07 | global batch size:     4 | lm loss: 9.040876E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.480 | TFLOPs: 9.01 |
[default0]:[2023-07-25 15:13:08,231] [INFO] [logging.py:96:log_dist] [Rank 0] step=135, skipped=0, lr=[5.854549333333333e-07, 5.854549333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:13:08,250] [INFO] [timer.py:215:stop] epoch=0/micro_step=135/global_step=135, RunningAvgSamplesPerSec=1.5647328448156719, CurrSamplesPerSec=1.5742963144656048, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      135/109863281 | consumed samples:          540 | consumed tokens:      1105920 | elapsed time per iteration (ms): 2623.8 | learning rate: 5.855E-07 | global batch size:     4 | lm loss: 9.007990E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.525 | TFLOPs: 9.28 |
[default0]:[2023-07-25 15:13:22,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[6.073002666666667e-07, 6.073002666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:13:22,261] [INFO] [timer.py:215:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=1.5637475546610697, CurrSamplesPerSec=1.5287808865211228, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      140/109863281 | consumed samples:          560 | consumed tokens:      1146880 | elapsed time per iteration (ms): 2798.3 | learning rate: 6.073E-07 | global batch size:     4 | lm loss: 9.041928E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.429 | TFLOPs: 8.70 |
[default0]:[2023-07-25 15:13:35,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=145, skipped=0, lr=[6.291456000000001e-07, 6.291456000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:13:35,606] [INFO] [timer.py:215:stop] epoch=0/micro_step=145/global_step=145, RunningAvgSamplesPerSec=1.5654090496775732, CurrSamplesPerSec=1.6590849330386832, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      145/109863281 | consumed samples:          580 | consumed tokens:      1187840 | elapsed time per iteration (ms): 2652.4 | learning rate: 6.291E-07 | global batch size:     4 | lm loss: 9.182176E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.508 | TFLOPs: 9.18 |
[default0]:[2023-07-25 15:13:49,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[6.509909333333333e-07, 6.509909333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:13:49,153] [INFO] [timer.py:215:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=1.5659138576550173, CurrSamplesPerSec=1.5825239238466504, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      150/109863281 | consumed samples:          600 | consumed tokens:      1228800 | elapsed time per iteration (ms): 2714.6 | learning rate: 6.510E-07 | global batch size:     4 | lm loss: 9.083900E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.474 | TFLOPs: 8.97 |
[default0]:saving checkpoint at iteration     150 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:13:49,381] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step150 is about to be saved!
[default1]:[2023-07-25 15:13:49,440] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step150 is ready now!
[default0]:[2023-07-25 15:13:49,451] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step150 is ready now!
[default0]:[2023-07-25 15:13:49,440] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step150/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:13:49,440] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step150/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 15:13:49,442] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step150 is ready now!
[default0]:[2023-07-25 15:15:00,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step150/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:15:00,815] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step150 is ready now!
[default0]:  successfully saved checkpoint at iteration     150 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.593
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71592.48, 71606.32)
[default0]:[2023-07-25 15:15:14,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=155, skipped=0, lr=[6.728362666666667e-07, 6.728362666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:15:14,467] [INFO] [timer.py:215:stop] epoch=0/micro_step=155/global_step=155, RunningAvgSamplesPerSec=1.5655831529019655, CurrSamplesPerSec=1.6680805945874628, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      155/109863281 | consumed samples:          620 | consumed tokens:      1269760 | elapsed time per iteration (ms): 17050.6 | learning rate: 6.728E-07 | global batch size:     4 | lm loss: 9.116666E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.235 | TFLOPs: 1.43 |
[default0]:[2023-07-25 15:15:27,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[6.946816000000001e-07, 6.946816000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:15:27,854] [INFO] [timer.py:215:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=1.5670909886716626, CurrSamplesPerSec=1.4861044775484809, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      160/109863281 | consumed samples:          640 | consumed tokens:      1310720 | elapsed time per iteration (ms): 2664.6 | learning rate: 6.947E-07 | global batch size:     4 | lm loss: 9.016553E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.501 | TFLOPs: 9.14 |
[default0]:[2023-07-25 15:15:41,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=165, skipped=0, lr=[7.165269333333334e-07, 7.165269333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:15:41,239] [INFO] [timer.py:215:stop] epoch=0/micro_step=165/global_step=165, RunningAvgSamplesPerSec=1.5680879390399016, CurrSamplesPerSec=1.5807172502864937, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      165/109863281 | consumed samples:          660 | consumed tokens:      1351680 | elapsed time per iteration (ms): 2676.3 | learning rate: 7.165E-07 | global batch size:     4 | lm loss: 9.006120E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.495 | TFLOPs: 9.10 |
[default0]:[2023-07-25 15:15:54,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[7.383722666666668e-07, 7.383722666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:15:54,852] [INFO] [timer.py:215:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=1.5683886022482678, CurrSamplesPerSec=1.6916791370561088, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      170/109863281 | consumed samples:          680 | consumed tokens:      1392640 | elapsed time per iteration (ms): 2706.5 | learning rate: 7.384E-07 | global batch size:     4 | lm loss: 8.994482E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.478 | TFLOPs: 9.00 |
[default0]:[2023-07-25 15:16:08,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=175, skipped=0, lr=[7.602176000000001e-07, 7.602176000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:16:08,667] [INFO] [timer.py:215:stop] epoch=0/micro_step=175/global_step=175, RunningAvgSamplesPerSec=1.5671168869898318, CurrSamplesPerSec=1.4432763646386444, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      175/109863281 | consumed samples:          700 | consumed tokens:      1433600 | elapsed time per iteration (ms): 2762.4 | learning rate: 7.602E-07 | global batch size:     4 | lm loss: 8.832294E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.448 | TFLOPs: 8.82 |
[default0]:[2023-07-25 15:16:22,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[7.820629333333333e-07, 7.820629333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:16:22,239] [INFO] [timer.py:215:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=1.5670107860968627, CurrSamplesPerSec=1.5282556039036923, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      180/109863281 | consumed samples:          720 | consumed tokens:      1474560 | elapsed time per iteration (ms): 2711.0 | learning rate: 7.821E-07 | global batch size:     4 | lm loss: 8.844942E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.475 | TFLOPs: 8.98 |
[default0]:[2023-07-25 15:16:35,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=185, skipped=0, lr=[8.039082666666667e-07, 8.039082666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:16:35,810] [INFO] [timer.py:215:stop] epoch=0/micro_step=185/global_step=185, RunningAvgSamplesPerSec=1.5672492020734168, CurrSamplesPerSec=1.4870804183980777, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      185/109863281 | consumed samples:          740 | consumed tokens:      1515520 | elapsed time per iteration (ms): 2710.4 | learning rate: 8.039E-07 | global batch size:     4 | lm loss: 8.959084E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.476 | TFLOPs: 8.99 |
[default0]:[2023-07-25 15:16:48,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[8.257536e-07, 8.257536e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:16:48,925] [INFO] [timer.py:215:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=1.568820921871077, CurrSamplesPerSec=1.6195771603657105, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      190/109863281 | consumed samples:          760 | consumed tokens:      1556480 | elapsed time per iteration (ms): 2612.9 | learning rate: 8.258E-07 | global batch size:     4 | lm loss: 8.856349E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.531 | TFLOPs: 9.32 |
[default0]:[2023-07-25 15:17:02,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=195, skipped=0, lr=[8.475989333333334e-07, 8.475989333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:17:02,453] [INFO] [timer.py:215:stop] epoch=0/micro_step=195/global_step=195, RunningAvgSamplesPerSec=1.5688917054105558, CurrSamplesPerSec=1.575457399812266, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      195/109863281 | consumed samples:          780 | consumed tokens:      1597440 | elapsed time per iteration (ms): 2691.6 | learning rate: 8.476E-07 | global batch size:     4 | lm loss: 8.873594E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.486 | TFLOPs: 9.05 |
[default0]:[2023-07-25 15:17:15,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[8.694442666666668e-07, 8.694442666666668e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:17:15,878] [INFO] [timer.py:215:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=1.5692738743751857, CurrSamplesPerSec=1.638752075641251, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      200/109863281 | consumed samples:          800 | consumed tokens:      1638400 | elapsed time per iteration (ms): 2678.8 | learning rate: 8.694E-07 | global batch size:     4 | lm loss: 8.875732E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.493 | TFLOPs: 9.09 |
[default1]:-----------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 200 | lm loss value: 8.848180E+00 | lm loss PPL: 6.961706E+03 | 
[default1]:-----------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration     200 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:17:26,898] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[default0]:[2023-07-25 15:17:26,993] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[default1]:[2023-07-25 15:17:26,971] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[default1]:[2023-07-25 15:17:26,974] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[default0]:[2023-07-25 15:17:26,972] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step200/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:17:26,972] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step200/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 15:18:37,982] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step200/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:18:37,998] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
[default0]:  successfully saved checkpoint at iteration     200 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.294
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71293.93, 71307.00)
[default0]:[2023-07-25 15:18:52,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=205, skipped=0, lr=[8.912896e-07, 8.912896e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:18:52,145] [INFO] [timer.py:215:stop] epoch=0/micro_step=205/global_step=205, RunningAvgSamplesPerSec=1.5682291934945618, CurrSamplesPerSec=1.4756399407499976, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      205/109863281 | consumed samples:          820 | consumed tokens:      1679360 | elapsed time per iteration (ms): 19244.9 | learning rate: 8.913E-07 | global batch size:     4 | lm loss: 8.882362E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.208 | TFLOPs: 1.27 |
[default0]:[2023-07-25 15:19:05,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[9.131349333333334e-07, 9.131349333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:19:05,320] [INFO] [timer.py:215:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=1.5693928675025313, CurrSamplesPerSec=1.6416894711493204, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      210/109863281 | consumed samples:          840 | consumed tokens:      1720320 | elapsed time per iteration (ms): 2632.3 | learning rate: 9.131E-07 | global batch size:     4 | lm loss: 8.799990E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.520 | TFLOPs: 9.25 |
[default0]:[2023-07-25 15:19:18,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=215, skipped=0, lr=[9.349802666666667e-07, 9.349802666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:19:18,496] [INFO] [timer.py:215:stop] epoch=0/micro_step=215/global_step=215, RunningAvgSamplesPerSec=1.5707686982604399, CurrSamplesPerSec=1.6848446175897844, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      215/109863281 | consumed samples:          860 | consumed tokens:      1761280 | elapsed time per iteration (ms): 2630.9 | learning rate: 9.350E-07 | global batch size:     4 | lm loss: 8.831788E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.520 | TFLOPs: 9.26 |
[default0]:[2023-07-25 15:19:32,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[9.568256e-07, 9.568256e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:19:32,117] [INFO] [timer.py:215:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=1.5709361001978461, CurrSamplesPerSec=1.4695718798146211, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      220/109863281 | consumed samples:          880 | consumed tokens:      1802240 | elapsed time per iteration (ms): 2714.8 | learning rate: 9.568E-07 | global batch size:     4 | lm loss: 8.683180E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.473 | TFLOPs: 8.97 |
[default0]:[2023-07-25 15:19:45,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=225, skipped=0, lr=[9.786709333333333e-07, 9.786709333333333e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:19:45,746] [INFO] [timer.py:215:stop] epoch=0/micro_step=225/global_step=225, RunningAvgSamplesPerSec=1.5710337017062985, CurrSamplesPerSec=1.6117150975518424, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      225/109863281 | consumed samples:          900 | consumed tokens:      1843200 | elapsed time per iteration (ms): 2718.2 | learning rate: 9.787E-07 | global batch size:     4 | lm loss: 8.764318E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.472 | TFLOPs: 8.96 |
[default0]:[2023-07-25 15:19:59,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[1.0005162666666667e-06, 1.0005162666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:19:59,763] [INFO] [timer.py:215:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=1.5699435602616874, CurrSamplesPerSec=1.5258219092181757, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      230/109863281 | consumed samples:          920 | consumed tokens:      1884160 | elapsed time per iteration (ms): 2790.9 | learning rate: 1.001E-06 | global batch size:     4 | lm loss: 8.715433E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.433 | TFLOPs: 8.73 |
[default0]:[2023-07-25 15:20:13,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=235, skipped=0, lr=[1.0223616e-06, 1.0223616e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:20:13,271] [INFO] [timer.py:215:stop] epoch=0/micro_step=235/global_step=235, RunningAvgSamplesPerSec=1.5701751992207134, CurrSamplesPerSec=1.5673710013846163, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      235/109863281 | consumed samples:          940 | consumed tokens:      1925120 | elapsed time per iteration (ms): 2705.2 | learning rate: 1.022E-06 | global batch size:     4 | lm loss: 8.740506E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.479 | TFLOPs: 9.00 |
[default0]:[2023-07-25 15:20:26,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[1.0442069333333335e-06, 1.0442069333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:20:26,718] [INFO] [timer.py:215:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=1.5703305496001294, CurrSamplesPerSec=1.6356312468717504, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      240/109863281 | consumed samples:          960 | consumed tokens:      1966080 | elapsed time per iteration (ms): 2678.3 | learning rate: 1.044E-06 | global batch size:     4 | lm loss: 8.716071E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.493 | TFLOPs: 9.09 |
[default0]:[2023-07-25 15:20:39,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=245, skipped=0, lr=[1.0660522666666667e-06, 1.0660522666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:20:39,745] [INFO] [timer.py:215:stop] epoch=0/micro_step=245/global_step=245, RunningAvgSamplesPerSec=1.5715552291771535, CurrSamplesPerSec=1.6154372846829497, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      245/109863281 | consumed samples:          980 | consumed tokens:      2007040 | elapsed time per iteration (ms): 2596.4 | learning rate: 1.066E-06 | global batch size:     4 | lm loss: 8.737852E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.541 | TFLOPs: 9.38 |
[default0]:[2023-07-25 15:20:53,359] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[1.0878976000000002e-06, 1.0878976000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:20:53,385] [INFO] [timer.py:215:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=1.5713195526648713, CurrSamplesPerSec=1.6394956121556623, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default0]:saving checkpoint at iteration     250 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]: iteration      250/109863281 | consumed samples:         1000 | consumed tokens:      2048000 | elapsed time per iteration (ms): 2711.9 | learning rate: 1.088E-06 | global batch size:     4 | lm loss: 8.653653E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.475 | TFLOPs: 8.98 |
[default0]:[2023-07-25 15:20:53,567] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step250 is about to be saved!
[default0]:[2023-07-25 15:20:53,660] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step250 is ready now!
[default1]:[2023-07-25 15:20:53,649] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step250 is ready now!
[default1]:[2023-07-25 15:20:53,648] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step250 is ready now!
[default0]:[2023-07-25 15:20:53,651] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step250/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:20:53,651] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step250/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 15:22:05,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step250/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:22:05,293] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step250 is ready now!
[default0]:  successfully saved checkpoint at iteration     250 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.886
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71886.01, 71897.04)
[default0]:[2023-07-25 15:22:18,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=255, skipped=0, lr=[1.1097429333333334e-06, 1.1097429333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:22:18,912] [INFO] [timer.py:215:stop] epoch=0/micro_step=255/global_step=255, RunningAvgSamplesPerSec=1.5712135618369494, CurrSamplesPerSec=1.5434692550715472, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      255/109863281 | consumed samples:         1020 | consumed tokens:      2088960 | elapsed time per iteration (ms): 17109.8 | learning rate: 1.110E-06 | global batch size:     4 | lm loss: 8.690569E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.234 | TFLOPs: 1.42 |
[default0]:[2023-07-25 15:22:32,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[1.1315882666666667e-06, 1.1315882666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:22:32,300] [INFO] [timer.py:215:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=1.5717009766862131, CurrSamplesPerSec=1.5006462436384134, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      260/109863281 | consumed samples:         1040 | consumed tokens:      2129920 | elapsed time per iteration (ms): 2662.4 | learning rate: 1.132E-06 | global batch size:     4 | lm loss: 8.571402E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.502 | TFLOPs: 9.15 |
[default0]:[2023-07-25 15:22:45,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=265, skipped=0, lr=[1.1534336e-06, 1.1534336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:22:45,462] [INFO] [timer.py:215:stop] epoch=0/micro_step=265/global_step=265, RunningAvgSamplesPerSec=1.5724979459834318, CurrSamplesPerSec=1.666307725342946, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      265/109863281 | consumed samples:         1060 | consumed tokens:      2170880 | elapsed time per iteration (ms): 2626.5 | learning rate: 1.153E-06 | global batch size:     4 | lm loss: 8.610149E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.523 | TFLOPs: 9.27 |
[default0]:[2023-07-25 15:22:58,295] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[1.1752789333333334e-06, 1.1752789333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:22:58,331] [INFO] [timer.py:215:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=1.5740317509566863, CurrSamplesPerSec=1.5383756843492165, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      270/109863281 | consumed samples:         1080 | consumed tokens:      2211840 | elapsed time per iteration (ms): 2574.2 | learning rate: 1.175E-06 | global batch size:     4 | lm loss: 8.703918E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.554 | TFLOPs: 9.46 |
[default0]:[2023-07-25 15:23:11,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=275, skipped=0, lr=[1.1971242666666667e-06, 1.1971242666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:23:11,660] [INFO] [timer.py:215:stop] epoch=0/micro_step=275/global_step=275, RunningAvgSamplesPerSec=1.5744632502328029, CurrSamplesPerSec=1.5905122242330763, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      275/109863281 | consumed samples:         1100 | consumed tokens:      2252800 | elapsed time per iteration (ms): 2655.6 | learning rate: 1.197E-06 | global batch size:     4 | lm loss: 8.740765E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.506 | TFLOPs: 9.17 |
[default0]:[2023-07-25 15:23:25,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[1.2189696000000002e-06, 1.2189696000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:23:25,035] [INFO] [timer.py:215:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=1.574745617088528, CurrSamplesPerSec=1.6502834915838789, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      280/109863281 | consumed samples:         1120 | consumed tokens:      2293760 | elapsed time per iteration (ms): 2665.0 | learning rate: 1.219E-06 | global batch size:     4 | lm loss: 8.488102E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.501 | TFLOPs: 9.14 |
[default0]:[2023-07-25 15:23:38,377] [INFO] [logging.py:96:log_dist] [Rank 0] step=285, skipped=0, lr=[1.2408149333333334e-06, 1.2408149333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:23:38,395] [INFO] [timer.py:215:stop] epoch=0/micro_step=285/global_step=285, RunningAvgSamplesPerSec=1.5749507347673204, CurrSamplesPerSec=1.6692044841197788, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      285/109863281 | consumed samples:         1140 | consumed tokens:      2334720 | elapsed time per iteration (ms): 2676.5 | learning rate: 1.241E-06 | global batch size:     4 | lm loss: 8.501391E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.494 | TFLOPs: 9.10 |
[default0]:[2023-07-25 15:23:52,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[1.2626602666666667e-06, 1.2626602666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:23:52,044] [INFO] [timer.py:215:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=1.5746630237998966, CurrSamplesPerSec=1.5666833850869761, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      290/109863281 | consumed samples:         1160 | consumed tokens:      2375680 | elapsed time per iteration (ms): 2720.5 | learning rate: 1.263E-06 | global batch size:     4 | lm loss: 8.663208E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.470 | TFLOPs: 8.95 |
[default0]:[2023-07-25 15:24:05,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=295, skipped=0, lr=[1.2845056000000001e-06, 1.2845056000000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:24:05,725] [INFO] [timer.py:215:stop] epoch=0/micro_step=295/global_step=295, RunningAvgSamplesPerSec=1.5745845796050353, CurrSamplesPerSec=1.587433464335673, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      295/109863281 | consumed samples:         1180 | consumed tokens:      2416640 | elapsed time per iteration (ms): 2710.5 | learning rate: 1.285E-06 | global batch size:     4 | lm loss: 8.510603E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.476 | TFLOPs: 8.99 |
[default0]:[2023-07-25 15:24:18,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[1.3063509333333334e-06, 1.3063509333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:24:18,506] [INFO] [timer.py:215:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=1.5761574994391185, CurrSamplesPerSec=1.6759477768729576, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      300/109863281 | consumed samples:         1200 | consumed tokens:      2457600 | elapsed time per iteration (ms): 2556.2 | learning rate: 1.306E-06 | global batch size:     4 | lm loss: 8.565429E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.565 | TFLOPs: 9.53 |
[default1]:-----------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 300 | lm loss value: 8.615333E+00 | lm loss PPL: 5.515583E+03 | 
[default1]:-----------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration     300 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:24:29,885] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is about to be saved!
[default1]:[2023-07-25 15:24:29,961] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
[default1]:[2023-07-25 15:24:29,961] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
[default0]:[2023-07-25 15:24:29,972] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
[default0]:[2023-07-25 15:24:29,963] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step300/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:24:29,963] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step300/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 15:25:40,897] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step300/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:25:40,908] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
[default0]:  successfully saved checkpoint at iteration     300 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.155
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71155.29, 71166.01)
[default0]:[2023-07-25 15:25:54,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=305, skipped=0, lr=[1.3281962666666667e-06, 1.3281962666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:25:54,184] [INFO] [timer.py:215:stop] epoch=0/micro_step=305/global_step=305, RunningAvgSamplesPerSec=1.5767995012854668, CurrSamplesPerSec=1.6360917365899887, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      305/109863281 | consumed samples:         1220 | consumed tokens:      2498560 | elapsed time per iteration (ms): 19131.2 | learning rate: 1.328E-06 | global batch size:     4 | lm loss: 8.463441E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.209 | TFLOPs: 1.27 |
[default0]:[2023-07-25 15:26:07,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[1.3500416e-06, 1.3500416e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:26:07,674] [INFO] [timer.py:215:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=1.5767126190444976, CurrSamplesPerSec=1.6385134478549346, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      310/109863281 | consumed samples:         1240 | consumed tokens:      2539520 | elapsed time per iteration (ms): 2684.6 | learning rate: 1.350E-06 | global batch size:     4 | lm loss: 8.549349E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.490 | TFLOPs: 9.07 |
[default0]:[2023-07-25 15:26:21,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=315, skipped=0, lr=[1.3718869333333336e-06, 1.3718869333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:26:21,575] [INFO] [timer.py:215:stop] epoch=0/micro_step=315/global_step=315, RunningAvgSamplesPerSec=1.5761592673363551, CurrSamplesPerSec=1.533351916962253, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      315/109863281 | consumed samples:         1260 | consumed tokens:      2580480 | elapsed time per iteration (ms): 2772.5 | learning rate: 1.372E-06 | global batch size:     4 | lm loss: 8.463371E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.443 | TFLOPs: 8.78 |
[default0]:[2023-07-25 15:26:34,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[1.3937322666666669e-06, 1.3937322666666669e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:26:34,987] [INFO] [timer.py:215:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=1.576275569648174, CurrSamplesPerSec=1.531124819700213, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      320/109863281 | consumed samples:         1280 | consumed tokens:      2621440 | elapsed time per iteration (ms): 2677.1 | learning rate: 1.394E-06 | global batch size:     4 | lm loss: 8.546327E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.494 | TFLOPs: 9.10 |
[default0]:[2023-07-25 15:26:48,954] [INFO] [logging.py:96:log_dist] [Rank 0] step=325, skipped=0, lr=[1.4155776e-06, 1.4155776e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:26:48,970] [INFO] [timer.py:215:stop] epoch=0/micro_step=325/global_step=325, RunningAvgSamplesPerSec=1.5755103146799858, CurrSamplesPerSec=1.4978783826264415, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      325/109863281 | consumed samples:         1300 | consumed tokens:      2662400 | elapsed time per iteration (ms): 2783.8 | learning rate: 1.416E-06 | global batch size:     4 | lm loss: 8.387753E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.437 | TFLOPs: 8.75 |
[default0]:[2023-07-25 15:27:02,846] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[1.4374229333333334e-06, 1.4374229333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:27:02,882] [INFO] [timer.py:215:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=1.5747474782517008, CurrSamplesPerSec=1.5336096787377267, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      330/109863281 | consumed samples:         1320 | consumed tokens:      2703360 | elapsed time per iteration (ms): 2774.4 | learning rate: 1.437E-06 | global batch size:     4 | lm loss: 8.482412E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.442 | TFLOPs: 8.78 |
[default0]:[2023-07-25 15:27:16,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=335, skipped=0, lr=[1.4592682666666666e-06, 1.4592682666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:27:16,581] [INFO] [timer.py:215:stop] epoch=0/micro_step=335/global_step=335, RunningAvgSamplesPerSec=1.5743175635947748, CurrSamplesPerSec=1.538500109629445, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      335/109863281 | consumed samples:         1340 | consumed tokens:      2744320 | elapsed time per iteration (ms): 2727.2 | learning rate: 1.459E-06 | global batch size:     4 | lm loss: 8.322694E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.467 | TFLOPs: 8.93 |
[default0]:[2023-07-25 15:27:30,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[1.4811135999999999e-06, 1.4811135999999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:27:30,264] [INFO] [timer.py:215:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=1.5742605051566068, CurrSamplesPerSec=1.510989363319411, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      340/109863281 | consumed samples:         1360 | consumed tokens:      2785280 | elapsed time per iteration (ms): 2734.7 | learning rate: 1.481E-06 | global batch size:     4 | lm loss: 8.371793E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.463 | TFLOPs: 8.91 |
[default0]:[2023-07-25 15:27:43,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=345, skipped=0, lr=[1.5029589333333334e-06, 1.5029589333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:27:43,832] [INFO] [timer.py:215:stop] epoch=0/micro_step=345/global_step=345, RunningAvgSamplesPerSec=1.5745387257814, CurrSamplesPerSec=1.6042917239330334, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      345/109863281 | consumed samples:         1380 | consumed tokens:      2826240 | elapsed time per iteration (ms): 2707.7 | learning rate: 1.503E-06 | global batch size:     4 | lm loss: 8.292764E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.477 | TFLOPs: 9.00 |
[default0]:[2023-07-25 15:27:57,037] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[1.5248042666666666e-06, 1.5248042666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:27:57,045] [INFO] [timer.py:215:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=1.5752072366134862, CurrSamplesPerSec=1.5898478477298454, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      350/109863281 | consumed samples:         1400 | consumed tokens:      2867200 | elapsed time per iteration (ms): 2626.9 | learning rate: 1.525E-06 | global batch size:     4 | lm loss: 8.383180E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.523 | TFLOPs: 9.27 |
[default0]:saving checkpoint at iteration     350 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:27:57,261] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step350 is about to be saved!
[default1]:[2023-07-25 15:27:57,319] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step350 is ready now!
[default1]:[2023-07-25 15:27:57,318] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step350 is ready now!
[default0]:[2023-07-25 15:27:57,320] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step350/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:27:57,321] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step350/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 15:27:57,330] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step350 is ready now!
[default0]:[2023-07-25 15:29:08,341] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step350/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:29:08,367] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step350 is ready now!
[default0]:  successfully saved checkpoint at iteration     350 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.274
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71273.57, 71292.05)
[default0]:[2023-07-25 15:29:21,740] [INFO] [logging.py:96:log_dist] [Rank 0] step=355, skipped=0, lr=[1.5466496e-06, 1.5466496e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:29:21,746] [INFO] [timer.py:215:stop] epoch=0/micro_step=355/global_step=355, RunningAvgSamplesPerSec=1.5757054930387162, CurrSamplesPerSec=1.496670826757921, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      355/109863281 | consumed samples:         1420 | consumed tokens:      2908160 | elapsed time per iteration (ms): 16922.8 | learning rate: 1.547E-06 | global batch size:     4 | lm loss: 8.341888E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.236 | TFLOPs: 1.44 |
[default0]:[2023-07-25 15:29:35,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[1.5684949333333333e-06, 1.5684949333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:29:35,342] [INFO] [timer.py:215:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=1.575611550486494, CurrSamplesPerSec=1.5618210466756643, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      360/109863281 | consumed samples:         1440 | consumed tokens:      2949120 | elapsed time per iteration (ms): 2713.3 | learning rate: 1.568E-06 | global batch size:     4 | lm loss: 8.384749E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.474 | TFLOPs: 8.98 |
[default0]:[2023-07-25 15:29:48,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=365, skipped=0, lr=[1.5903402666666668e-06, 1.5903402666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:29:48,766] [INFO] [timer.py:215:stop] epoch=0/micro_step=365/global_step=365, RunningAvgSamplesPerSec=1.575680530576174, CurrSamplesPerSec=1.526560926439582, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      365/109863281 | consumed samples:         1460 | consumed tokens:      2990080 | elapsed time per iteration (ms): 2675.0 | learning rate: 1.590E-06 | global batch size:     4 | lm loss: 8.224710E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.495 | TFLOPs: 9.10 |
[default0]:[2023-07-25 15:30:02,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[1.6121856e-06, 1.6121856e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:30:02,097] [INFO] [timer.py:215:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=1.5762212763624772, CurrSamplesPerSec=1.493201601248303, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      370/109863281 | consumed samples:         1480 | consumed tokens:      3031040 | elapsed time per iteration (ms): 2661.1 | learning rate: 1.612E-06 | global batch size:     4 | lm loss: 8.324988E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.503 | TFLOPs: 9.15 |
[default0]:[2023-07-25 15:30:15,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=375, skipped=0, lr=[1.6340309333333335e-06, 1.6340309333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:30:15,368] [INFO] [timer.py:215:stop] epoch=0/micro_step=375/global_step=375, RunningAvgSamplesPerSec=1.576512101306276, CurrSamplesPerSec=1.5996983906060955, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      375/109863281 | consumed samples:         1500 | consumed tokens:      3072000 | elapsed time per iteration (ms): 2649.3 | learning rate: 1.634E-06 | global batch size:     4 | lm loss: 8.199683E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.510 | TFLOPs: 9.19 |
[default0]:[2023-07-25 15:30:28,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[1.6558762666666668e-06, 1.6558762666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:30:28,318] [INFO] [timer.py:215:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=1.5773118778818123, CurrSamplesPerSec=1.5975788949756566, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      380/109863281 | consumed samples:         1520 | consumed tokens:      3112960 | elapsed time per iteration (ms): 2589.0 | learning rate: 1.656E-06 | global batch size:     4 | lm loss: 8.191470E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.545 | TFLOPs: 9.41 |
[default0]:[2023-07-25 15:30:41,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=385, skipped=0, lr=[1.6777216e-06, 1.6777216e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:30:42,006] [INFO] [timer.py:215:stop] epoch=0/micro_step=385/global_step=385, RunningAvgSamplesPerSec=1.576945805715558, CurrSamplesPerSec=1.5303813781336977, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      385/109863281 | consumed samples:         1540 | consumed tokens:      3153920 | elapsed time per iteration (ms): 2730.8 | learning rate: 1.678E-06 | global batch size:     4 | lm loss: 8.494484E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.465 | TFLOPs: 8.92 |
[default0]:[2023-07-25 15:30:55,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[1.6995669333333335e-06, 1.6995669333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:30:55,427] [INFO] [timer.py:215:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=1.5772051705975718, CurrSamplesPerSec=1.6320026770063443, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      390/109863281 | consumed samples:         1560 | consumed tokens:      3194880 | elapsed time per iteration (ms): 2675.6 | learning rate: 1.700E-06 | global batch size:     4 | lm loss: 8.228085E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.495 | TFLOPs: 9.10 |
[default0]:[2023-07-25 15:31:08,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=395, skipped=0, lr=[1.7214122666666668e-06, 1.7214122666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:31:08,669] [INFO] [timer.py:215:stop] epoch=0/micro_step=395/global_step=395, RunningAvgSamplesPerSec=1.5776262881215892, CurrSamplesPerSec=1.6467369273228782, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      395/109863281 | consumed samples:         1580 | consumed tokens:      3235840 | elapsed time per iteration (ms): 2642.7 | learning rate: 1.721E-06 | global batch size:     4 | lm loss: 8.083930E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.514 | TFLOPs: 9.22 |
[default0]:[2023-07-25 15:31:21,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[1.7432576000000002e-06, 1.7432576000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:31:21,652] [INFO] [timer.py:215:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=1.5785342758272336, CurrSamplesPerSec=1.6964299996794652, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      400/109863281 | consumed samples:         1600 | consumed tokens:      3276800 | elapsed time per iteration (ms): 2593.2 | learning rate: 1.743E-06 | global batch size:     4 | lm loss: 8.217487E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.543 | TFLOPs: 9.39 |
[default1]:-----------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 400 | lm loss value: 8.166674E+00 | lm loss PPL: 3.521610E+03 | 
[default1]:-----------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration     400 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:31:33,342] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[default1]:[2023-07-25 15:31:33,385] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[default1]:[2023-07-25 15:31:33,387] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[default0]:[2023-07-25 15:31:33,385] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[default0]:[2023-07-25 15:31:33,387] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step400/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:31:33,387] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step400/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 15:32:44,452] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step400/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:32:44,469] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
[default0]:  successfully saved checkpoint at iteration     400 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71330.76, 71356.96)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.331
[default0]:[2023-07-25 15:32:57,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=405, skipped=0, lr=[1.7651029333333335e-06, 1.7651029333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:32:57,919] [INFO] [timer.py:215:stop] epoch=0/micro_step=405/global_step=405, RunningAvgSamplesPerSec=1.5787595543605653, CurrSamplesPerSec=1.5942845582172447, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      405/109863281 | consumed samples:         1620 | consumed tokens:      3317760 | elapsed time per iteration (ms): 19249.0 | learning rate: 1.765E-06 | global batch size:     4 | lm loss: 8.132127E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.208 | TFLOPs: 1.27 |
[default0]:[2023-07-25 15:33:11,295] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[1.7869482666666666e-06, 1.7869482666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:33:11,313] [INFO] [timer.py:215:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=1.5790125566146456, CurrSamplesPerSec=1.5495004386977604, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      410/109863281 | consumed samples:         1640 | consumed tokens:      3358720 | elapsed time per iteration (ms): 2669.2 | learning rate: 1.787E-06 | global batch size:     4 | lm loss: 8.096426E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.499 | TFLOPs: 9.12 |
[default0]:[2023-07-25 15:33:24,562] [INFO] [logging.py:96:log_dist] [Rank 0] step=415, skipped=0, lr=[1.8087936e-06, 1.8087936e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:33:24,572] [INFO] [timer.py:215:stop] epoch=0/micro_step=415/global_step=415, RunningAvgSamplesPerSec=1.5793559540017401, CurrSamplesPerSec=1.497459920457224, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      415/109863281 | consumed samples:         1660 | consumed tokens:      3399680 | elapsed time per iteration (ms): 2648.7 | learning rate: 1.809E-06 | global batch size:     4 | lm loss: 8.274477E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.510 | TFLOPs: 9.20 |
[default0]:[2023-07-25 15:33:37,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[1.8306389333333333e-06, 1.8306389333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:33:37,712] [INFO] [timer.py:215:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=1.5798932945305597, CurrSamplesPerSec=1.6095133225157625, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      420/109863281 | consumed samples:         1680 | consumed tokens:      3440640 | elapsed time per iteration (ms): 2611.9 | learning rate: 1.831E-06 | global batch size:     4 | lm loss: 8.134057E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.531 | TFLOPs: 9.32 |
[default0]:[2023-07-25 15:33:51,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=425, skipped=0, lr=[1.8524842666666667e-06, 1.8524842666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:33:51,123] [INFO] [timer.py:215:stop] epoch=0/micro_step=425/global_step=425, RunningAvgSamplesPerSec=1.579981901808893, CurrSamplesPerSec=1.5553629107133802, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      425/109863281 | consumed samples:         1700 | consumed tokens:      3481600 | elapsed time per iteration (ms): 2679.7 | learning rate: 1.852E-06 | global batch size:     4 | lm loss: 8.192428E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.493 | TFLOPs: 9.09 |
[default0]:[2023-07-25 15:34:04,105] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[1.8743296e-06, 1.8743296e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:34:04,127] [INFO] [timer.py:215:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=1.5806004244252645, CurrSamplesPerSec=1.525094975187187, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      430/109863281 | consumed samples:         1720 | consumed tokens:      3522560 | elapsed time per iteration (ms): 2597.5 | learning rate: 1.874E-06 | global batch size:     4 | lm loss: 8.092651E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.540 | TFLOPs: 9.38 |
[default0]:[2023-07-25 15:34:17,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=435, skipped=0, lr=[1.8961749333333335e-06, 1.8961749333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:34:17,661] [INFO] [timer.py:215:stop] epoch=0/micro_step=435/global_step=435, RunningAvgSamplesPerSec=1.5805377654529116, CurrSamplesPerSec=1.5636238139347964, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      435/109863281 | consumed samples:         1740 | consumed tokens:      3563520 | elapsed time per iteration (ms): 2702.4 | learning rate: 1.896E-06 | global batch size:     4 | lm loss: 8.145007E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.480 | TFLOPs: 9.01 |
[default0]:[2023-07-25 15:34:30,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[1.9180202666666667e-06, 1.9180202666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:34:30,946] [INFO] [timer.py:215:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=1.5809888767888558, CurrSamplesPerSec=1.6788114844156827, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      440/109863281 | consumed samples:         1760 | consumed tokens:      3604480 | elapsed time per iteration (ms): 2652.7 | learning rate: 1.918E-06 | global batch size:     4 | lm loss: 8.111496E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.508 | TFLOPs: 9.18 |
[default0]:[2023-07-25 15:34:44,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=445, skipped=0, lr=[1.9398656e-06, 1.9398656e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:34:44,573] [INFO] [timer.py:215:stop] epoch=0/micro_step=445/global_step=445, RunningAvgSamplesPerSec=1.581060324572849, CurrSamplesPerSec=1.5584518753711576, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      445/109863281 | consumed samples:         1780 | consumed tokens:      3645440 | elapsed time per iteration (ms): 2712.3 | learning rate: 1.940E-06 | global batch size:     4 | lm loss: 8.125900E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.475 | TFLOPs: 8.98 |
[default0]:[2023-07-25 15:34:57,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[1.9617109333333333e-06, 1.9617109333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:34:57,847] [INFO] [timer.py:215:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=1.5814379725834085, CurrSamplesPerSec=1.6564293600767652, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      450/109863281 | consumed samples:         1800 | consumed tokens:      3686400 | elapsed time per iteration (ms): 2641.4 | learning rate: 1.962E-06 | global batch size:     4 | lm loss: 8.087775E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.514 | TFLOPs: 9.22 |
[default0]:saving checkpoint at iteration     450 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:34:58,010] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step450 is about to be saved!
[default0]:[2023-07-25 15:34:58,046] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step450 is ready now!
[default1]:[2023-07-25 15:34:58,048] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step450 is ready now!
[default1]:[2023-07-25 15:34:58,049] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step450 is ready now!
[default0]:[2023-07-25 15:34:58,047] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step450/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:34:58,047] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step450/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 15:36:09,215] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step450/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:36:09,236] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step450 is ready now!
[default0]:  successfully saved checkpoint at iteration     450 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71374.02, 71389.36)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.385
[default0]:[2023-07-25 15:36:23,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=455, skipped=0, lr=[1.983556266666667e-06, 1.983556266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:36:23,022] [INFO] [timer.py:215:stop] epoch=0/micro_step=455/global_step=455, RunningAvgSamplesPerSec=1.5810442493253647, CurrSamplesPerSec=1.5984059780776254, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      455/109863281 | consumed samples:         1820 | consumed tokens:      3727360 | elapsed time per iteration (ms): 17038.9 | learning rate: 1.984E-06 | global batch size:     4 | lm loss: 8.302111E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.235 | TFLOPs: 1.43 |
[default0]:[2023-07-25 15:36:36,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[2.0054016e-06, 2.0054016e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:36:36,622] [INFO] [timer.py:215:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=1.5808856337525141, CurrSamplesPerSec=1.6025532010819699, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      460/109863281 | consumed samples:         1840 | consumed tokens:      3768320 | elapsed time per iteration (ms): 2706.7 | learning rate: 2.005E-06 | global batch size:     4 | lm loss: 8.099329E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.478 | TFLOPs: 9.00 |
[default0]:[2023-07-25 15:36:49,757] [INFO] [logging.py:96:log_dist] [Rank 0] step=465, skipped=0, lr=[2.0272469333333334e-06, 2.0272469333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:36:49,774] [INFO] [timer.py:215:stop] epoch=0/micro_step=465/global_step=465, RunningAvgSamplesPerSec=1.5813954443030123, CurrSamplesPerSec=1.5493064087286783, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      465/109863281 | consumed samples:         1860 | consumed tokens:      3809280 | elapsed time per iteration (ms): 2631.2 | learning rate: 2.027E-06 | global batch size:     4 | lm loss: 8.011696E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.520 | TFLOPs: 9.26 |
[default0]:[2023-07-25 15:37:03,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[2.0490922666666667e-06, 2.0490922666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:37:03,248] [INFO] [timer.py:215:stop] epoch=0/micro_step=470/global_step=470, RunningAvgSamplesPerSec=1.5815747249298981, CurrSamplesPerSec=1.5668497451812309, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      470/109863281 | consumed samples:         1880 | consumed tokens:      3850240 | elapsed time per iteration (ms): 2676.0 | learning rate: 2.049E-06 | global batch size:     4 | lm loss: 7.981376E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.495 | TFLOPs: 9.10 |
[default0]:[2023-07-25 15:37:16,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=475, skipped=0, lr=[2.0709376000000004e-06, 2.0709376000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:37:16,493] [INFO] [timer.py:215:stop] epoch=0/micro_step=475/global_step=475, RunningAvgSamplesPerSec=1.5819163265914318, CurrSamplesPerSec=1.527427888979263, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      475/109863281 | consumed samples:         1900 | consumed tokens:      3891200 | elapsed time per iteration (ms): 2637.0 | learning rate: 2.071E-06 | global batch size:     4 | lm loss: 7.988554E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.517 | TFLOPs: 9.24 |
[default0]:[2023-07-25 15:37:29,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[2.0927829333333336e-06, 2.0927829333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:37:29,733] [INFO] [timer.py:215:stop] epoch=0/micro_step=480/global_step=480, RunningAvgSamplesPerSec=1.5822405027614925, CurrSamplesPerSec=1.6039478578819346, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      480/109863281 | consumed samples:         1920 | consumed tokens:      3932160 | elapsed time per iteration (ms): 2637.7 | learning rate: 2.093E-06 | global batch size:     4 | lm loss: 7.985062E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.516 | TFLOPs: 9.23 |
[default0]:[2023-07-25 15:37:42,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=485, skipped=0, lr=[2.114628266666667e-06, 2.114628266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:37:42,993] [INFO] [timer.py:215:stop] epoch=0/micro_step=485/global_step=485, RunningAvgSamplesPerSec=1.5825442783661188, CurrSamplesPerSec=1.6589279371852161, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      485/109863281 | consumed samples:         1940 | consumed tokens:      3973120 | elapsed time per iteration (ms): 2651.5 | learning rate: 2.115E-06 | global batch size:     4 | lm loss: 7.884650E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.509 | TFLOPs: 9.19 |
[default1]: iteration      490/109863281 | consumed samples:         1960 | consumed tokens:      4014080 | elapsed time per iteration (ms): 2709.0 | learning rate: 2.136E-06 | global batch size:     4 | lm loss: 7.975401E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.477 | TFLOPs: 8.99 |
[default0]:[2023-07-25 15:38:10,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=495, skipped=0, lr=[2.1583189333333334e-06, 2.1583189333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:38:10,242] [INFO] [timer.py:215:stop] epoch=0/micro_step=495/global_step=495, RunningAvgSamplesPerSec=1.5823974938328014, CurrSamplesPerSec=1.4633659932670504, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      495/109863281 | consumed samples:         1980 | consumed tokens:      4055040 | elapsed time per iteration (ms): 2727.5 | learning rate: 2.158E-06 | global batch size:     4 | lm loss: 7.956098E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.467 | TFLOPs: 8.93 |
[default0]:[2023-07-25 15:38:23,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[2.1801642666666667e-06, 2.1801642666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:38:23,817] [INFO] [timer.py:215:stop] epoch=0/micro_step=500/global_step=500, RunningAvgSamplesPerSec=1.5822261842401597, CurrSamplesPerSec=1.6166009881788357, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      500/109863281 | consumed samples:         2000 | consumed tokens:      4096000 | elapsed time per iteration (ms): 2707.9 | learning rate: 2.180E-06 | global batch size:     4 | lm loss: 7.938962E+00 | loss scale: 2048.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.477 | TFLOPs: 8.99 |
[default1]:-----------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 500 | lm loss value: 8.006701E+00 | lm loss PPL: 3.001002E+03 | 
[default1]:-----------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration     500 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:38:35,134] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!
[default0]:[2023-07-25 15:38:35,215] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step500/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:38:35,215] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step500/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 15:38:35,217] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
[default0]:[2023-07-25 15:38:35,236] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
[default1]:[2023-07-25 15:38:35,214] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
[default0]:[2023-07-25 15:39:46,201] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step500/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:39:46,225] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
[default0]:  successfully saved checkpoint at iteration     500 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.238
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71238.01, 71260.50)
[default1]:[2023-07-25 15:39:49,076] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default1]:[2023-07-25 15:39:49,076] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[default0]:[2023-07-25 15:39:49,095] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default0]:[2023-07-25 15:39:49,096] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[default1]:[2023-07-25 15:39:49,080] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default1]:[2023-07-25 15:39:49,098] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[default0]:[2023-07-25 15:39:49,089] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default0]:[2023-07-25 15:39:49,101] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 2048 to 4096
[default0]:[2023-07-25 15:40:00,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=505, skipped=0, lr=[2.2020096e-06, 2.2020096e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:40:00,170] [INFO] [timer.py:215:stop] epoch=0/micro_step=505/global_step=505, RunningAvgSamplesPerSec=1.5817230323994818, CurrSamplesPerSec=1.5598314135607732, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      505/109863281 | consumed samples:         2020 | consumed tokens:      4136960 | elapsed time per iteration (ms): 19260.8 | learning rate: 2.202E-06 | global batch size:     4 | lm loss: 7.808897E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.208 | TFLOPs: 1.26 |
[default0]:[2023-07-25 15:40:13,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[2.223854933333333e-06, 2.223854933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:40:13,854] [INFO] [timer.py:215:stop] epoch=0/micro_step=510/global_step=510, RunningAvgSamplesPerSec=1.581510065326649, CurrSamplesPerSec=1.5046070767906516, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      510/109863281 | consumed samples:         2040 | consumed tokens:      4177920 | elapsed time per iteration (ms): 2721.2 | learning rate: 2.224E-06 | global batch size:     4 | lm loss: 8.226898E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.470 | TFLOPs: 8.95 |
[default0]:[2023-07-25 15:40:27,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=515, skipped=0, lr=[2.245700266666667e-06, 2.245700266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:40:27,237] [INFO] [timer.py:215:stop] epoch=0/micro_step=515/global_step=515, RunningAvgSamplesPerSec=1.5816048013665207, CurrSamplesPerSec=1.5433140690625304, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      515/109863281 | consumed samples:         2060 | consumed tokens:      4218880 | elapsed time per iteration (ms): 2668.2 | learning rate: 2.246E-06 | global batch size:     4 | lm loss: 8.002334E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.499 | TFLOPs: 9.13 |
[default0]:[2023-07-25 15:40:40,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[2.2675456e-06, 2.2675456e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:40:40,959] [INFO] [timer.py:215:stop] epoch=0/micro_step=520/global_step=520, RunningAvgSamplesPerSec=1.5813240723392055, CurrSamplesPerSec=1.4610365797850073, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      520/109863281 | consumed samples:         2080 | consumed tokens:      4259840 | elapsed time per iteration (ms): 2732.6 | learning rate: 2.268E-06 | global batch size:     4 | lm loss: 7.970177E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.464 | TFLOPs: 8.91 |
[default0]:[2023-07-25 15:40:54,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=525, skipped=0, lr=[2.2893909333333334e-06, 2.2893909333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:40:54,469] [INFO] [timer.py:215:stop] epoch=0/micro_step=525/global_step=525, RunningAvgSamplesPerSec=1.5812930745135403, CurrSamplesPerSec=1.5116289504617, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      525/109863281 | consumed samples:         2100 | consumed tokens:      4300800 | elapsed time per iteration (ms): 2699.5 | learning rate: 2.289E-06 | global batch size:     4 | lm loss: 7.722168E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.482 | TFLOPs: 9.02 |
[default0]:[2023-07-25 15:41:08,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[2.3112362666666666e-06, 2.3112362666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:41:08,094] [INFO] [timer.py:215:stop] epoch=0/micro_step=530/global_step=530, RunningAvgSamplesPerSec=1.5811418333678566, CurrSamplesPerSec=1.5971196051737364, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      530/109863281 | consumed samples:         2120 | consumed tokens:      4341760 | elapsed time per iteration (ms): 2709.4 | learning rate: 2.311E-06 | global batch size:     4 | lm loss: 7.678732E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.476 | TFLOPs: 8.99 |
[default0]:[2023-07-25 15:41:21,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=535, skipped=0, lr=[2.3330816000000003e-06, 2.3330816000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:41:21,529] [INFO] [timer.py:215:stop] epoch=0/micro_step=535/global_step=535, RunningAvgSamplesPerSec=1.581109930471648, CurrSamplesPerSec=1.4976633734921905, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      535/109863281 | consumed samples:         2140 | consumed tokens:      4382720 | elapsed time per iteration (ms): 2682.9 | learning rate: 2.333E-06 | global batch size:     4 | lm loss: 7.901146E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.491 | TFLOPs: 9.08 |
[default0]:[2023-07-25 15:41:35,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[2.3549269333333336e-06, 2.3549269333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:41:35,088] [INFO] [timer.py:215:stop] epoch=0/micro_step=540/global_step=540, RunningAvgSamplesPerSec=1.5810368181019137, CurrSamplesPerSec=1.5839766355832838, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      540/109863281 | consumed samples:         2160 | consumed tokens:      4423680 | elapsed time per iteration (ms): 2704.2 | learning rate: 2.355E-06 | global batch size:     4 | lm loss: 7.810256E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.479 | TFLOPs: 9.01 |
[default0]:[2023-07-25 15:41:48,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=545, skipped=0, lr=[2.376772266666667e-06, 2.376772266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:41:48,548] [INFO] [timer.py:215:stop] epoch=0/micro_step=545/global_step=545, RunningAvgSamplesPerSec=1.581099923843183, CurrSamplesPerSec=1.641140416013028, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      545/109863281 | consumed samples:         2180 | consumed tokens:      4464640 | elapsed time per iteration (ms): 2685.3 | learning rate: 2.377E-06 | global batch size:     4 | lm loss: 7.922793E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.490 | TFLOPs: 9.07 |
[default0]:[2023-07-25 15:42:01,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[2.3986176e-06, 2.3986176e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:42:01,662] [INFO] [timer.py:215:stop] epoch=0/micro_step=550/global_step=550, RunningAvgSamplesPerSec=1.5814052207295026, CurrSamplesPerSec=1.7194165926638008, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      550/109863281 | consumed samples:         2200 | consumed tokens:      4505600 | elapsed time per iteration (ms): 2626.5 | learning rate: 2.399E-06 | global batch size:     4 | lm loss: 7.639676E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.523 | TFLOPs: 9.27 |
[default0]:saving checkpoint at iteration     550 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:42:01,897] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step550 is about to be saved!
[default1]:[2023-07-25 15:42:01,997] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step550 is ready now!
[default1]:[2023-07-25 15:42:01,989] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step550 is ready now!
[default0]:[2023-07-25 15:42:01,986] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step550/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:42:01,986] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step550/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 15:42:01,987] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step550 is ready now!
[default0]:[2023-07-25 15:43:12,995] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step550/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:43:13,023] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step550 is ready now!
[default0]:  successfully saved checkpoint at iteration     550 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71262.61, 71275.23)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.263
[default0]:[2023-07-25 15:43:26,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=555, skipped=0, lr=[2.4204629333333334e-06, 2.4204629333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:43:26,428] [INFO] [timer.py:215:stop] epoch=0/micro_step=555/global_step=555, RunningAvgSamplesPerSec=1.581468257331152, CurrSamplesPerSec=1.5086436787231916, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      555/109863281 | consumed samples:         2220 | consumed tokens:      4546560 | elapsed time per iteration (ms): 16937.0 | learning rate: 2.420E-06 | global batch size:     4 | lm loss: 7.851619E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.236 | TFLOPs: 1.44 |
[default0]:[2023-07-25 15:43:39,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[2.442308266666667e-06, 2.442308266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:43:39,978] [INFO] [timer.py:215:stop] epoch=0/micro_step=560/global_step=560, RunningAvgSamplesPerSec=1.5812806299654347, CurrSamplesPerSec=1.5135426356232486, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      560/109863281 | consumed samples:         2240 | consumed tokens:      4587520 | elapsed time per iteration (ms): 2703.6 | learning rate: 2.442E-06 | global batch size:     4 | lm loss: 7.769733E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.480 | TFLOPs: 9.01 |
[default0]:[2023-07-25 15:43:53,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=565, skipped=0, lr=[2.4641536000000003e-06, 2.4641536000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:43:53,276] [INFO] [timer.py:215:stop] epoch=0/micro_step=565/global_step=565, RunningAvgSamplesPerSec=1.5815889651010977, CurrSamplesPerSec=1.5426997366768578, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      565/109863281 | consumed samples:         2260 | consumed tokens:      4628480 | elapsed time per iteration (ms): 2650.5 | learning rate: 2.464E-06 | global batch size:     4 | lm loss: 7.855324E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.509 | TFLOPs: 9.19 |
[default0]:[2023-07-25 15:44:06,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[2.4859989333333336e-06, 2.4859989333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:44:06,821] [INFO] [timer.py:215:stop] epoch=0/micro_step=570/global_step=570, RunningAvgSamplesPerSec=1.581528070366257, CurrSamplesPerSec=1.625664096599463, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      570/109863281 | consumed samples:         2280 | consumed tokens:      4669440 | elapsed time per iteration (ms): 2706.0 | learning rate: 2.486E-06 | global batch size:     4 | lm loss: 7.703564E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.478 | TFLOPs: 9.00 |
[default0]:[2023-07-25 15:44:20,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=575, skipped=0, lr=[2.507844266666667e-06, 2.507844266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:44:20,464] [INFO] [timer.py:215:stop] epoch=0/micro_step=575/global_step=575, RunningAvgSamplesPerSec=1.5812902546512175, CurrSamplesPerSec=1.4826678911753277, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      575/109863281 | consumed samples:         2300 | consumed tokens:      4710400 | elapsed time per iteration (ms): 2730.7 | learning rate: 2.508E-06 | global batch size:     4 | lm loss: 7.914349E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.465 | TFLOPs: 8.92 |
[default0]:[2023-07-25 15:44:33,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[2.5296896e-06, 2.5296896e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:44:33,697] [INFO] [timer.py:215:stop] epoch=0/micro_step=580/global_step=580, RunningAvgSamplesPerSec=1.581635348332668, CurrSamplesPerSec=1.6077717903842168, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      580/109863281 | consumed samples:         2320 | consumed tokens:      4751360 | elapsed time per iteration (ms): 2631.4 | learning rate: 2.530E-06 | global batch size:     4 | lm loss: 7.867453E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.520 | TFLOPs: 9.26 |
[default0]:[2023-07-25 15:44:47,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=585, skipped=0, lr=[2.5515349333333333e-06, 2.5515349333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:44:47,112] [INFO] [timer.py:215:stop] epoch=0/micro_step=585/global_step=585, RunningAvgSamplesPerSec=1.581675961287573, CurrSamplesPerSec=1.6542718009401334, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      585/109863281 | consumed samples:         2340 | consumed tokens:      4792320 | elapsed time per iteration (ms): 2677.0 | learning rate: 2.552E-06 | global batch size:     4 | lm loss: 7.693039E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.494 | TFLOPs: 9.10 |
[default0]:[2023-07-25 15:45:00,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[2.5733802666666666e-06, 2.5733802666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:45:00,561] [INFO] [timer.py:215:stop] epoch=0/micro_step=590/global_step=590, RunningAvgSamplesPerSec=1.582009545728187, CurrSamplesPerSec=1.5898981689824783, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      590/109863281 | consumed samples:         2360 | consumed tokens:      4833280 | elapsed time per iteration (ms): 2687.1 | learning rate: 2.573E-06 | global batch size:     4 | lm loss: 7.587669E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.489 | TFLOPs: 9.06 |
[default0]:[2023-07-25 15:45:14,498] [INFO] [logging.py:96:log_dist] [Rank 0] step=595, skipped=0, lr=[2.5952256000000003e-06, 2.5952256000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:45:14,514] [INFO] [timer.py:215:stop] epoch=0/micro_step=595/global_step=595, RunningAvgSamplesPerSec=1.5818547541718466, CurrSamplesPerSec=1.5716011202747893, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      595/109863281 | consumed samples:         2380 | consumed tokens:      4874240 | elapsed time per iteration (ms): 2772.8 | learning rate: 2.595E-06 | global batch size:     4 | lm loss: 7.606310E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.443 | TFLOPs: 8.78 |
[default0]:[2023-07-25 15:45:28,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[2.6170709333333335e-06, 2.6170709333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:45:28,050] [INFO] [timer.py:215:stop] epoch=0/micro_step=600/global_step=600, RunningAvgSamplesPerSec=1.5817955219515873, CurrSamplesPerSec=1.4762800825481337, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      600/109863281 | consumed samples:         2400 | consumed tokens:      4915200 | elapsed time per iteration (ms): 2702.0 | learning rate: 2.617E-06 | global batch size:     4 | lm loss: 7.708986E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.480 | TFLOPs: 9.01 |
[default1]:-----------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 600 | lm loss value: 7.801069E+00 | lm loss PPL: 2.443212E+03 | 
[default1]:-----------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration     600 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:45:40,256] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[default0]:[2023-07-25 15:45:40,331] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step600/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:45:40,331] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step600/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 15:45:40,330] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[default1]:[2023-07-25 15:45:40,329] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[default1]:[2023-07-25 15:45:40,331] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[default0]:[2023-07-25 15:46:51,069] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step600/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:46:51,094] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
[default0]:  successfully saved checkpoint at iteration     600 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71015.69, 71028.48)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.028
[default0]:[2023-07-25 15:47:04,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=605, skipped=0, lr=[2.6389162666666668e-06, 2.6389162666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:47:04,729] [INFO] [timer.py:215:stop] epoch=0/micro_step=605/global_step=605, RunningAvgSamplesPerSec=1.5817201717395306, CurrSamplesPerSec=1.5155385187262846, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      605/109863281 | consumed samples:         2420 | consumed tokens:      4956160 | elapsed time per iteration (ms): 19332.0 | learning rate: 2.639E-06 | global batch size:     4 | lm loss: 7.669357E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.207 | TFLOPs: 1.26 |
[default0]:[2023-07-25 15:47:17,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[2.6607616e-06, 2.6607616e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:47:18,008] [INFO] [timer.py:215:stop] epoch=0/micro_step=610/global_step=610, RunningAvgSamplesPerSec=1.5819306683750005, CurrSamplesPerSec=1.5382123538215104, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      610/109863281 | consumed samples:         2440 | consumed tokens:      4997120 | elapsed time per iteration (ms): 2656.9 | learning rate: 2.661E-06 | global batch size:     4 | lm loss: 7.865068E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.506 | TFLOPs: 9.17 |
[default0]:[2023-07-25 15:47:31,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=615, skipped=0, lr=[2.6826069333333333e-06, 2.6826069333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:47:31,566] [INFO] [timer.py:215:stop] epoch=0/micro_step=615/global_step=615, RunningAvgSamplesPerSec=1.58201563770584, CurrSamplesPerSec=1.6157831386335064, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      615/109863281 | consumed samples:         2460 | consumed tokens:      5038080 | elapsed time per iteration (ms): 2700.2 | learning rate: 2.683E-06 | global batch size:     4 | lm loss: 7.679237E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.481 | TFLOPs: 9.02 |
[default0]:[2023-07-25 15:47:44,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[2.704452266666667e-06, 2.704452266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:47:44,897] [INFO] [timer.py:215:stop] epoch=0/micro_step=620/global_step=620, RunningAvgSamplesPerSec=1.5820527963974569, CurrSamplesPerSec=1.5810528656862135, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      620/109863281 | consumed samples:         2480 | consumed tokens:      5079040 | elapsed time per iteration (ms): 2667.8 | learning rate: 2.704E-06 | global batch size:     4 | lm loss: 7.734558E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.499 | TFLOPs: 9.13 |
[default0]:[2023-07-25 15:47:58,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=625, skipped=0, lr=[2.7262976000000002e-06, 2.7262976000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:47:58,280] [INFO] [timer.py:215:stop] epoch=0/micro_step=625/global_step=625, RunningAvgSamplesPerSec=1.582144101144172, CurrSamplesPerSec=1.504773335727498, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      625/109863281 | consumed samples:         2500 | consumed tokens:      5120000 | elapsed time per iteration (ms): 2667.2 | learning rate: 2.726E-06 | global batch size:     4 | lm loss: 7.767413E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.500 | TFLOPs: 9.13 |
[default0]:[2023-07-25 15:48:11,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[2.748142933333333e-06, 2.748142933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:48:11,352] [INFO] [timer.py:215:stop] epoch=0/micro_step=630/global_step=630, RunningAvgSamplesPerSec=1.582558534737756, CurrSamplesPerSec=1.6144983297562443, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      630/109863281 | consumed samples:         2520 | consumed tokens:      5160960 | elapsed time per iteration (ms): 2599.7 | learning rate: 2.748E-06 | global batch size:     4 | lm loss: 7.701632E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.539 | TFLOPs: 9.37 |
[default0]:[2023-07-25 15:48:24,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=635, skipped=0, lr=[2.7699882666666668e-06, 2.7699882666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:48:24,644] [INFO] [timer.py:215:stop] epoch=0/micro_step=635/global_step=635, RunningAvgSamplesPerSec=1.5827941384132973, CurrSamplesPerSec=1.6581827353742644, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      635/109863281 | consumed samples:         2540 | consumed tokens:      5201920 | elapsed time per iteration (ms): 2664.1 | learning rate: 2.770E-06 | global batch size:     4 | lm loss: 7.342303E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.501 | TFLOPs: 9.14 |
[default0]:[2023-07-25 15:48:38,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[2.7918336e-06, 2.7918336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:48:38,342] [INFO] [timer.py:215:stop] epoch=0/micro_step=640/global_step=640, RunningAvgSamplesPerSec=1.5826823817895863, CurrSamplesPerSec=1.5386873497627847, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      640/109863281 | consumed samples:         2560 | consumed tokens:      5242880 | elapsed time per iteration (ms): 2728.7 | learning rate: 2.792E-06 | global batch size:     4 | lm loss: 7.561472E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.466 | TFLOPs: 8.93 |
[default0]:[2023-07-25 15:48:51,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=645, skipped=0, lr=[2.8136789333333333e-06, 2.8136789333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:48:51,269] [INFO] [timer.py:215:stop] epoch=0/micro_step=645/global_step=645, RunningAvgSamplesPerSec=1.583125796561561, CurrSamplesPerSec=1.6291012365065847, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      645/109863281 | consumed samples:         2580 | consumed tokens:      5283840 | elapsed time per iteration (ms): 2572.4 | learning rate: 2.814E-06 | global batch size:     4 | lm loss: 7.689921E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.555 | TFLOPs: 9.47 |
[default0]:[2023-07-25 15:49:04,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=0, lr=[2.8355242666666665e-06, 2.8355242666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:49:04,601] [INFO] [timer.py:215:stop] epoch=0/micro_step=650/global_step=650, RunningAvgSamplesPerSec=1.5831876261430633, CurrSamplesPerSec=1.622344022434997, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      650/109863281 | consumed samples:         2600 | consumed tokens:      5324800 | elapsed time per iteration (ms): 2664.1 | learning rate: 2.836E-06 | global batch size:     4 | lm loss: 7.751645E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.501 | TFLOPs: 9.14 |
[default0]:saving checkpoint at iteration     650 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:49:04,854] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step650 is about to be saved!
[default0]:[2023-07-25 15:49:04,921] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step650/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:49:04,921] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step650/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 15:49:04,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step650 is ready now!
[default0]:[2023-07-25 15:49:04,930] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step650 is ready now!
[default1]:[2023-07-25 15:49:04,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step650 is ready now!
[default0]:[2023-07-25 15:50:15,873] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step650/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:50:15,897] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step650 is ready now!
[default0]:  successfully saved checkpoint at iteration     650 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71194.75, 71206.03)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.195
[default0]:[2023-07-25 15:50:29,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=655, skipped=0, lr=[2.8573696e-06, 2.8573696e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:50:29,441] [INFO] [timer.py:215:stop] epoch=0/micro_step=655/global_step=655, RunningAvgSamplesPerSec=1.5832851293796584, CurrSamplesPerSec=1.664416751811048, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      655/109863281 | consumed samples:         2620 | consumed tokens:      5365760 | elapsed time per iteration (ms): 16957.9 | learning rate: 2.857E-06 | global batch size:     4 | lm loss: 7.421024E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.236 | TFLOPs: 1.44 |
[default0]:[2023-07-25 15:50:42,913] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[2.8792149333333335e-06, 2.8792149333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:50:42,923] [INFO] [timer.py:215:stop] epoch=0/micro_step=660/global_step=660, RunningAvgSamplesPerSec=1.5834300589042447, CurrSamplesPerSec=1.5351131217496385, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      660/109863281 | consumed samples:         2640 | consumed tokens:      5406720 | elapsed time per iteration (ms): 2692.1 | learning rate: 2.879E-06 | global batch size:     4 | lm loss: 7.556982E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.486 | TFLOPs: 9.05 |
[default0]:[2023-07-25 15:50:56,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=665, skipped=0, lr=[2.9010602666666667e-06, 2.9010602666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:50:56,374] [INFO] [timer.py:215:stop] epoch=0/micro_step=665/global_step=665, RunningAvgSamplesPerSec=1.5834660647943086, CurrSamplesPerSec=1.6284548165175239, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      665/109863281 | consumed samples:         2660 | consumed tokens:      5447680 | elapsed time per iteration (ms): 2689.8 | learning rate: 2.901E-06 | global batch size:     4 | lm loss: 7.407135E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.487 | TFLOPs: 9.05 |
[default0]:[2023-07-25 15:51:09,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[2.9229056e-06, 2.9229056e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:51:09,425] [INFO] [timer.py:215:stop] epoch=0/micro_step=670/global_step=670, RunningAvgSamplesPerSec=1.5838198117014861, CurrSamplesPerSec=1.5056326106294122, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      670/109863281 | consumed samples:         2680 | consumed tokens:      5488640 | elapsed time per iteration (ms): 2598.6 | learning rate: 2.923E-06 | global batch size:     4 | lm loss: 7.613167E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.539 | TFLOPs: 9.37 |
[default0]:[2023-07-25 15:51:23,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=675, skipped=0, lr=[2.9447509333333332e-06, 2.9447509333333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:51:23,134] [INFO] [timer.py:215:stop] epoch=0/micro_step=675/global_step=675, RunningAvgSamplesPerSec=1.5836327830772157, CurrSamplesPerSec=1.5582233232109408, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      675/109863281 | consumed samples:         2700 | consumed tokens:      5529600 | elapsed time per iteration (ms): 2725.9 | learning rate: 2.945E-06 | global batch size:     4 | lm loss: 7.525732E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.467 | TFLOPs: 8.94 |
[default0]:[2023-07-25 15:51:36,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[2.966596266666667e-06, 2.966596266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:51:36,941] [INFO] [timer.py:215:stop] epoch=0/micro_step=680/global_step=680, RunningAvgSamplesPerSec=1.5832809017985585, CurrSamplesPerSec=1.5957647197579514, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      680/109863281 | consumed samples:         2720 | consumed tokens:      5570560 | elapsed time per iteration (ms): 2759.1 | learning rate: 2.967E-06 | global batch size:     4 | lm loss: 7.530209E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.450 | TFLOPs: 8.83 |
[default0]:[2023-07-25 15:51:50,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=685, skipped=0, lr=[2.9884416e-06, 2.9884416e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:51:50,432] [INFO] [timer.py:215:stop] epoch=0/micro_step=685/global_step=685, RunningAvgSamplesPerSec=1.5833916618855814, CurrSamplesPerSec=1.5589442356370045, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      685/109863281 | consumed samples:         2740 | consumed tokens:      5611520 | elapsed time per iteration (ms): 2690.1 | learning rate: 2.988E-06 | global batch size:     4 | lm loss: 7.480449E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.487 | TFLOPs: 9.05 |
[default0]:[2023-07-25 15:52:04,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[3.0102869333333334e-06, 3.0102869333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:52:04,161] [INFO] [timer.py:215:stop] epoch=0/micro_step=690/global_step=690, RunningAvgSamplesPerSec=1.583239401650913, CurrSamplesPerSec=1.4926731218735854, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      690/109863281 | consumed samples:         2760 | consumed tokens:      5652480 | elapsed time per iteration (ms): 2744.0 | learning rate: 3.010E-06 | global batch size:     4 | lm loss: 7.513615E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.458 | TFLOPs: 8.88 |
[default0]:[2023-07-25 15:52:17,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=695, skipped=0, lr=[3.0321322666666667e-06, 3.0321322666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:52:17,471] [INFO] [timer.py:215:stop] epoch=0/micro_step=695/global_step=695, RunningAvgSamplesPerSec=1.5833458094479629, CurrSamplesPerSec=1.638991733636646, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      695/109863281 | consumed samples:         2780 | consumed tokens:      5693440 | elapsed time per iteration (ms): 2661.9 | learning rate: 3.032E-06 | global batch size:     4 | lm loss: 7.718474E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.503 | TFLOPs: 9.15 |
[default0]:[2023-07-25 15:52:30,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[3.0539776e-06, 3.0539776e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:52:30,814] [INFO] [timer.py:215:stop] epoch=0/micro_step=700/global_step=700, RunningAvgSamplesPerSec=1.5834924009954816, CurrSamplesPerSec=1.483895601677558, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      700/109863281 | consumed samples:         2800 | consumed tokens:      5734400 | elapsed time per iteration (ms): 2654.9 | learning rate: 3.054E-06 | global batch size:     4 | lm loss: 7.399146E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.507 | TFLOPs: 9.17 |
[default1]:-----------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 700 | lm loss value: 7.607667E+00 | lm loss PPL: 2.013575E+03 | 
[default1]:-----------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration     700 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:[2023-07-25 15:52:42,510] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step700 is ready now!
[default0]:[2023-07-25 15:52:42,449] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step700 is about to be saved!
[default0]:[2023-07-25 15:52:42,509] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step700/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:52:42,509] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step700/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 15:52:42,507] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step700 is ready now!
[default1]:[2023-07-25 15:52:42,511] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step700 is ready now!
[default0]:[2023-07-25 15:53:53,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step700/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:53:53,437] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step700 is ready now!
[default0]:  successfully saved checkpoint at iteration     700 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.152
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71147.36, 71168.45)
[default0]:[2023-07-25 15:54:07,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=705, skipped=0, lr=[3.0758229333333336e-06, 3.0758229333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:54:07,311] [INFO] [timer.py:215:stop] epoch=0/micro_step=705/global_step=705, RunningAvgSamplesPerSec=1.5833867986591657, CurrSamplesPerSec=1.6867109258794746, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      705/109863281 | consumed samples:         2820 | consumed tokens:      5775360 | elapsed time per iteration (ms): 19297.0 | learning rate: 3.076E-06 | global batch size:     4 | lm loss: 7.399541E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.207 | TFLOPs: 1.26 |
[default0]:[2023-07-25 15:54:20,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[3.097668266666667e-06, 3.097668266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:54:20,999] [INFO] [timer.py:215:stop] epoch=0/micro_step=710/global_step=710, RunningAvgSamplesPerSec=1.5832826541846041, CurrSamplesPerSec=1.5936248985652812, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      710/109863281 | consumed samples:         2840 | consumed tokens:      5816320 | elapsed time per iteration (ms): 2726.9 | learning rate: 3.098E-06 | global batch size:     4 | lm loss: 7.605296E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.467 | TFLOPs: 8.93 |
[default0]:[2023-07-25 15:54:34,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=715, skipped=0, lr=[3.1195136e-06, 3.1195136e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:54:34,300] [INFO] [timer.py:215:stop] epoch=0/micro_step=715/global_step=715, RunningAvgSamplesPerSec=1.5834089245102905, CurrSamplesPerSec=1.5892129286779666, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      715/109863281 | consumed samples:         2860 | consumed tokens:      5857280 | elapsed time per iteration (ms): 2659.0 | learning rate: 3.120E-06 | global batch size:     4 | lm loss: 7.401215E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.504 | TFLOPs: 9.16 |
[default0]:[2023-07-25 15:54:47,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[3.1413589333333334e-06, 3.1413589333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:54:47,873] [INFO] [timer.py:215:stop] epoch=0/micro_step=720/global_step=720, RunningAvgSamplesPerSec=1.5834401155491753, CurrSamplesPerSec=1.507147607383446, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      720/109863281 | consumed samples:         2880 | consumed tokens:      5898240 | elapsed time per iteration (ms): 2695.5 | learning rate: 3.141E-06 | global batch size:     4 | lm loss: 7.331997E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.484 | TFLOPs: 9.04 |
[default0]:[2023-07-25 15:55:01,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=725, skipped=0, lr=[3.163204266666667e-06, 3.163204266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:55:01,478] [INFO] [timer.py:215:stop] epoch=0/micro_step=725/global_step=725, RunningAvgSamplesPerSec=1.583503506882321, CurrSamplesPerSec=1.6323099218200066, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      725/109863281 | consumed samples:         2900 | consumed tokens:      5939200 | elapsed time per iteration (ms): 2714.3 | learning rate: 3.163E-06 | global batch size:     4 | lm loss: 7.411868E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.474 | TFLOPs: 8.97 |
[default0]:[2023-07-25 15:55:14,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[3.1850496000000004e-06, 3.1850496000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:55:14,994] [INFO] [timer.py:215:stop] epoch=0/micro_step=730/global_step=730, RunningAvgSamplesPerSec=1.5834711724319177, CurrSamplesPerSec=1.5619055244120819, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      730/109863281 | consumed samples:         2920 | consumed tokens:      5980160 | elapsed time per iteration (ms): 2698.8 | learning rate: 3.185E-06 | global batch size:     4 | lm loss: 7.523129E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.482 | TFLOPs: 9.02 |
[default0]:[2023-07-25 15:55:28,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=735, skipped=0, lr=[3.2068949333333336e-06, 3.2068949333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:55:28,310] [INFO] [timer.py:215:stop] epoch=0/micro_step=735/global_step=735, RunningAvgSamplesPerSec=1.5835254470634637, CurrSamplesPerSec=1.6047821644107192, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      735/109863281 | consumed samples:         2940 | consumed tokens:      6021120 | elapsed time per iteration (ms): 2655.6 | learning rate: 3.207E-06 | global batch size:     4 | lm loss: 7.469881E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.506 | TFLOPs: 9.17 |
[default0]:[2023-07-25 15:55:41,109] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[3.228740266666667e-06, 3.228740266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:55:41,138] [INFO] [timer.py:215:stop] epoch=0/micro_step=740/global_step=740, RunningAvgSamplesPerSec=1.584128117045206, CurrSamplesPerSec=1.7033787423059839, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      740/109863281 | consumed samples:         2960 | consumed tokens:      6062080 | elapsed time per iteration (ms): 2550.1 | learning rate: 3.229E-06 | global batch size:     4 | lm loss: 7.528914E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.569 | TFLOPs: 9.55 |
[default0]:[2023-07-25 15:55:54,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=745, skipped=0, lr=[3.2505856000000005e-06, 3.2505856000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:55:54,599] [INFO] [timer.py:215:stop] epoch=0/micro_step=745/global_step=745, RunningAvgSamplesPerSec=1.5840973964276566, CurrSamplesPerSec=1.570480109349015, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      745/109863281 | consumed samples:         2980 | consumed tokens:      6103040 | elapsed time per iteration (ms): 2692.4 | learning rate: 3.251E-06 | global batch size:     4 | lm loss: 7.432571E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.486 | TFLOPs: 9.05 |
[default0]:[2023-07-25 15:56:07,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[3.272430933333334e-06, 3.272430933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:56:07,988] [INFO] [timer.py:215:stop] epoch=0/micro_step=750/global_step=750, RunningAvgSamplesPerSec=1.5843644074435894, CurrSamplesPerSec=1.5410214405613996, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      750/109863281 | consumed samples:         3000 | consumed tokens:      6144000 | elapsed time per iteration (ms): 2672.0 | learning rate: 3.272E-06 | global batch size:     4 | lm loss: 7.388612E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.497 | TFLOPs: 9.12 |
[default0]:saving checkpoint at iteration     750 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:56:08,206] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step750 is about to be saved!
[default0]:[2023-07-25 15:56:08,265] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step750/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:56:08,265] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step750/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 15:56:08,263] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!
[default0]:[2023-07-25 15:56:08,263] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!
[default1]:[2023-07-25 15:56:08,265] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!
[default0]:[2023-07-25 15:57:19,284] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step750/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 15:57:19,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!
[default0]:  successfully saved checkpoint at iteration     750 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.273
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71272.49, 71283.82)
[default0]:[2023-07-25 15:57:33,107] [INFO] [logging.py:96:log_dist] [Rank 0] step=755, skipped=0, lr=[3.294276266666667e-06, 3.294276266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:57:33,135] [INFO] [timer.py:215:stop] epoch=0/micro_step=755/global_step=755, RunningAvgSamplesPerSec=1.5840548006757569, CurrSamplesPerSec=1.423380572297334, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      755/109863281 | consumed samples:         3020 | consumed tokens:      6184960 | elapsed time per iteration (ms): 17028.4 | learning rate: 3.294E-06 | global batch size:     4 | lm loss: 7.323531E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.235 | TFLOPs: 1.43 |
[default0]:[2023-07-25 15:57:46,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[3.3161216000000003e-06, 3.3161216000000003e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:57:46,752] [INFO] [timer.py:215:stop] epoch=0/micro_step=760/global_step=760, RunningAvgSamplesPerSec=1.5841478940703206, CurrSamplesPerSec=1.6103378191275186, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      760/109863281 | consumed samples:         3040 | consumed tokens:      6225920 | elapsed time per iteration (ms): 2704.8 | learning rate: 3.316E-06 | global batch size:     4 | lm loss: 7.426372E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.479 | TFLOPs: 9.00 |
[default0]:[2023-07-25 15:57:59,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=765, skipped=0, lr=[3.3379669333333336e-06, 3.3379669333333336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:57:59,992] [INFO] [timer.py:215:stop] epoch=0/micro_step=765/global_step=765, RunningAvgSamplesPerSec=1.5843617228863125, CurrSamplesPerSec=1.7125448290631355, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      765/109863281 | consumed samples:         3060 | consumed tokens:      6266880 | elapsed time per iteration (ms): 2646.6 | learning rate: 3.338E-06 | global batch size:     4 | lm loss: 7.498050E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.511 | TFLOPs: 9.20 |
[default0]:[2023-07-25 15:58:13,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[3.3598122666666673e-06, 3.3598122666666673e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:58:13,840] [INFO] [timer.py:215:stop] epoch=0/micro_step=770/global_step=770, RunningAvgSamplesPerSec=1.5841117240496738, CurrSamplesPerSec=1.572749975626767, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      770/109863281 | consumed samples:         3080 | consumed tokens:      6307840 | elapsed time per iteration (ms): 2766.8 | learning rate: 3.360E-06 | global batch size:     4 | lm loss: 7.395645E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.446 | TFLOPs: 8.80 |
[default0]:[2023-07-25 15:58:27,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=775, skipped=0, lr=[3.3816576000000005e-06, 3.3816576000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:58:27,139] [INFO] [timer.py:215:stop] epoch=0/micro_step=775/global_step=775, RunningAvgSamplesPerSec=1.5842448769685713, CurrSamplesPerSec=1.694439447925063, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      775/109863281 | consumed samples:         3100 | consumed tokens:      6348800 | elapsed time per iteration (ms): 2641.6 | learning rate: 3.382E-06 | global batch size:     4 | lm loss: 7.216048E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.514 | TFLOPs: 9.22 |
[default0]:[2023-07-25 15:58:40,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=0, lr=[3.4035029333333334e-06, 3.4035029333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:58:40,768] [INFO] [timer.py:215:stop] epoch=0/micro_step=780/global_step=780, RunningAvgSamplesPerSec=1.5841643681232647, CurrSamplesPerSec=1.5894027789606593, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      780/109863281 | consumed samples:         3120 | consumed tokens:      6389760 | elapsed time per iteration (ms): 2720.8 | learning rate: 3.404E-06 | global batch size:     4 | lm loss: 7.277871E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.470 | TFLOPs: 8.95 |
[default0]:[2023-07-25 15:58:54,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=785, skipped=0, lr=[3.4253482666666666e-06, 3.4253482666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:58:54,628] [INFO] [timer.py:215:stop] epoch=0/micro_step=785/global_step=785, RunningAvgSamplesPerSec=1.5838250222556087, CurrSamplesPerSec=1.5599768847075977, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      785/109863281 | consumed samples:         3140 | consumed tokens:      6430720 | elapsed time per iteration (ms): 2771.7 | learning rate: 3.425E-06 | global batch size:     4 | lm loss: 7.223077E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.443 | TFLOPs: 8.79 |
[default0]:[2023-07-25 15:59:08,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=0, lr=[3.4471936e-06, 3.4471936e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:59:08,207] [INFO] [timer.py:215:stop] epoch=0/micro_step=790/global_step=790, RunningAvgSamplesPerSec=1.583706263794761, CurrSamplesPerSec=1.5592597993388497, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      790/109863281 | consumed samples:         3160 | consumed tokens:      6471680 | elapsed time per iteration (ms): 2699.7 | learning rate: 3.447E-06 | global batch size:     4 | lm loss: 7.263928E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.482 | TFLOPs: 9.02 |
[default0]:[2023-07-25 15:59:21,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=795, skipped=0, lr=[3.469038933333333e-06, 3.469038933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:59:21,550] [INFO] [timer.py:215:stop] epoch=0/micro_step=795/global_step=795, RunningAvgSamplesPerSec=1.583909033427359, CurrSamplesPerSec=1.8224504054810329, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      795/109863281 | consumed samples:         3180 | consumed tokens:      6512640 | elapsed time per iteration (ms): 2668.5 | learning rate: 3.469E-06 | global batch size:     4 | lm loss: 7.335680E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.499 | TFLOPs: 9.13 |
[default0]:[2023-07-25 15:59:34,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=0, lr=[3.490884266666667e-06, 3.490884266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 15:59:34,921] [INFO] [timer.py:215:stop] epoch=0/micro_step=800/global_step=800, RunningAvgSamplesPerSec=1.5840481790537568, CurrSamplesPerSec=1.6400724254166212, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      800/109863281 | consumed samples:         3200 | consumed tokens:      6553600 | elapsed time per iteration (ms): 2658.4 | learning rate: 3.491E-06 | global batch size:     4 | lm loss: 7.425410E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.505 | TFLOPs: 9.16 |
[default1]:-----------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 800 | lm loss value: 7.359217E+00 | lm loss PPL: 1.570607E+03 | 
[default1]:-----------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration     800 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 15:59:45,886] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!
[default0]:[2023-07-25 15:59:45,960] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
[default1]:[2023-07-25 15:59:45,939] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
[default1]:[2023-07-25 15:59:45,941] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
[default0]:[2023-07-25 15:59:45,938] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step800/mp_rank_00_model_states.pt
[default0]:[2023-07-25 15:59:45,938] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step800/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:00:57,900] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step800/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:00:57,926] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
[default0]:  successfully saved checkpoint at iteration     800 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 72.209
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (72198.13, 72216.03)
[default0]:[2023-07-25 16:01:11,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=805, skipped=0, lr=[3.5127296e-06, 3.5127296e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:01:11,427] [INFO] [timer.py:215:stop] epoch=0/micro_step=805/global_step=805, RunningAvgSamplesPerSec=1.5840895367008778, CurrSamplesPerSec=1.5131288869423989, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      805/109863281 | consumed samples:         3220 | consumed tokens:      6594560 | elapsed time per iteration (ms): 19295.3 | learning rate: 3.513E-06 | global batch size:     4 | lm loss: 7.271053E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.207 | TFLOPs: 1.26 |
[default0]:[2023-07-25 16:01:24,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=0, lr=[3.5345749333333333e-06, 3.5345749333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:01:24,867] [INFO] [timer.py:215:stop] epoch=0/micro_step=810/global_step=810, RunningAvgSamplesPerSec=1.584111150077411, CurrSamplesPerSec=1.5532539264822358, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      810/109863281 | consumed samples:         3240 | consumed tokens:      6635520 | elapsed time per iteration (ms): 2679.1 | learning rate: 3.535E-06 | global batch size:     4 | lm loss: 7.120748E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.493 | TFLOPs: 9.09 |
[default0]:[2023-07-25 16:01:38,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=815, skipped=0, lr=[3.5564202666666666e-06, 3.5564202666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:01:38,268] [INFO] [timer.py:215:stop] epoch=0/micro_step=815/global_step=815, RunningAvgSamplesPerSec=1.5841724384864324, CurrSamplesPerSec=1.4961627046406665, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      815/109863281 | consumed samples:         3260 | consumed tokens:      6676480 | elapsed time per iteration (ms): 2684.0 | learning rate: 3.556E-06 | global batch size:     4 | lm loss: 7.509349E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.490 | TFLOPs: 9.07 |
[default0]:[2023-07-25 16:01:51,455] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=0, lr=[3.5782656e-06, 3.5782656e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:01:51,471] [INFO] [timer.py:215:stop] epoch=0/micro_step=820/global_step=820, RunningAvgSamplesPerSec=1.5843066428041965, CurrSamplesPerSec=1.6433373943627896, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      820/109863281 | consumed samples:         3280 | consumed tokens:      6717440 | elapsed time per iteration (ms): 2627.7 | learning rate: 3.578E-06 | global batch size:     4 | lm loss: 7.468822E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.522 | TFLOPs: 9.27 |
[default0]:[2023-07-25 16:02:04,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=825, skipped=0, lr=[3.6001109333333335e-06, 3.6001109333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:02:04,785] [INFO] [timer.py:215:stop] epoch=0/micro_step=825/global_step=825, RunningAvgSamplesPerSec=1.5843889855952058, CurrSamplesPerSec=1.6051245455179506, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      825/109863281 | consumed samples:         3300 | consumed tokens:      6758400 | elapsed time per iteration (ms): 2655.6 | learning rate: 3.600E-06 | global batch size:     4 | lm loss: 7.344398E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.506 | TFLOPs: 9.17 |
[default0]:[2023-07-25 16:02:18,048] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=0, lr=[3.6219562666666668e-06, 3.6219562666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:02:18,085] [INFO] [timer.py:215:stop] epoch=0/micro_step=830/global_step=830, RunningAvgSamplesPerSec=1.5845032504712877, CurrSamplesPerSec=1.5120947546750905, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      830/109863281 | consumed samples:         3320 | consumed tokens:      6799360 | elapsed time per iteration (ms): 2651.9 | learning rate: 3.622E-06 | global batch size:     4 | lm loss: 7.309839E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.508 | TFLOPs: 9.18 |
[default0]:[2023-07-25 16:02:31,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=835, skipped=0, lr=[3.6438016e-06, 3.6438016e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:02:31,798] [INFO] [timer.py:215:stop] epoch=0/micro_step=835/global_step=835, RunningAvgSamplesPerSec=1.5844784725794248, CurrSamplesPerSec=1.5776534106430287, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      835/109863281 | consumed samples:         3340 | consumed tokens:      6840320 | elapsed time per iteration (ms): 2734.7 | learning rate: 3.644E-06 | global batch size:     4 | lm loss: 7.172771E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.463 | TFLOPs: 8.91 |
[default0]:[2023-07-25 16:02:45,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=0, lr=[3.6656469333333333e-06, 3.6656469333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:02:45,684] [INFO] [timer.py:215:stop] epoch=0/micro_step=840/global_step=840, RunningAvgSamplesPerSec=1.5841039035065043, CurrSamplesPerSec=1.5638414169449362, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      840/109863281 | consumed samples:         3360 | consumed tokens:      6881280 | elapsed time per iteration (ms): 2787.2 | learning rate: 3.666E-06 | global batch size:     4 | lm loss: 7.231503E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.435 | TFLOPs: 8.74 |
[default0]:[2023-07-25 16:02:59,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=845, skipped=0, lr=[3.687492266666667e-06, 3.687492266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:02:59,171] [INFO] [timer.py:215:stop] epoch=0/micro_step=845/global_step=845, RunningAvgSamplesPerSec=1.584144102973971, CurrSamplesPerSec=1.4185564444095713, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      845/109863281 | consumed samples:         3380 | consumed tokens:      6922240 | elapsed time per iteration (ms): 2682.9 | learning rate: 3.687E-06 | global batch size:     4 | lm loss: 7.306146E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.491 | TFLOPs: 9.08 |
[default0]:[2023-07-25 16:03:12,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=0, lr=[3.7093376000000002e-06, 3.7093376000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:03:12,200] [INFO] [timer.py:215:stop] epoch=0/micro_step=850/global_step=850, RunningAvgSamplesPerSec=1.584493935456659, CurrSamplesPerSec=1.653868352906535, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      850/109863281 | consumed samples:         3400 | consumed tokens:      6963200 | elapsed time per iteration (ms): 2600.1 | learning rate: 3.709E-06 | global batch size:     4 | lm loss: 7.294572E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.538 | TFLOPs: 9.37 |
[default0]:saving checkpoint at iteration     850 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:03:12,452] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step850 is about to be saved!
[default0]:[2023-07-25 16:03:12,515] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step850/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:03:12,515] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step850/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:03:12,527] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step850 is ready now!
[default1]:[2023-07-25 16:03:12,516] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step850 is ready now!
[default1]:[2023-07-25 16:03:12,518] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step850 is ready now!
[default0]:[2023-07-25 16:04:23,999] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step850/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:04:24,024] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step850 is ready now!
[default0]:  successfully saved checkpoint at iteration     850 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.738
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71737.56, 71756.18)
[default0]:[2023-07-25 16:04:37,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=855, skipped=0, lr=[3.7311829333333335e-06, 3.7311829333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:04:37,506] [INFO] [timer.py:215:stop] epoch=0/micro_step=855/global_step=855, RunningAvgSamplesPerSec=1.5845864054873062, CurrSamplesPerSec=1.5978731617242636, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      855/109863281 | consumed samples:         3420 | consumed tokens:      7004160 | elapsed time per iteration (ms): 17045.5 | learning rate: 3.731E-06 | global batch size:     4 | lm loss: 7.104645E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.235 | TFLOPs: 1.43 |
[default0]:[2023-07-25 16:04:51,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[3.7530282666666668e-06, 3.7530282666666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:04:51,108] [INFO] [timer.py:215:stop] epoch=0/micro_step=860/global_step=860, RunningAvgSamplesPerSec=1.5845237446375748, CurrSamplesPerSec=1.5427918056988306, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      860/109863281 | consumed samples:         3440 | consumed tokens:      7045120 | elapsed time per iteration (ms): 2727.0 | learning rate: 3.753E-06 | global batch size:     4 | lm loss: 7.198293E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.467 | TFLOPs: 8.93 |
[default0]:[2023-07-25 16:05:05,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=865, skipped=0, lr=[3.7748736e-06, 3.7748736e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:05:05,242] [INFO] [timer.py:215:stop] epoch=0/micro_step=865/global_step=865, RunningAvgSamplesPerSec=1.584239295064475, CurrSamplesPerSec=1.4599273082687714, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      865/109863281 | consumed samples:         3460 | consumed tokens:      7086080 | elapsed time per iteration (ms): 2805.8 | learning rate: 3.775E-06 | global batch size:     4 | lm loss: 7.249299E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.426 | TFLOPs: 8.68 |
[default0]:[2023-07-25 16:05:18,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[3.7967189333333337e-06, 3.7967189333333337e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:05:18,658] [INFO] [timer.py:215:stop] epoch=0/micro_step=870/global_step=870, RunningAvgSamplesPerSec=1.5842350421929143, CurrSamplesPerSec=1.5488844611877282, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      870/109863281 | consumed samples:         3480 | consumed tokens:      7127040 | elapsed time per iteration (ms): 2685.4 | learning rate: 3.797E-06 | global batch size:     4 | lm loss: 7.286263E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.490 | TFLOPs: 9.07 |
[default0]:[2023-07-25 16:05:31,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=875, skipped=0, lr=[3.8185642666666665e-06, 3.8185642666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:05:31,360] [INFO] [timer.py:215:stop] epoch=0/micro_step=875/global_step=875, RunningAvgSamplesPerSec=1.5847885897660083, CurrSamplesPerSec=1.7284746276440495, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      875/109863281 | consumed samples:         3500 | consumed tokens:      7168000 | elapsed time per iteration (ms): 2524.2 | learning rate: 3.819E-06 | global batch size:     4 | lm loss: 7.237087E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.585 | TFLOPs: 9.65 |
[default0]:[2023-07-25 16:05:44,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[3.840409600000001e-06, 3.840409600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:05:44,347] [INFO] [timer.py:215:stop] epoch=0/micro_step=880/global_step=880, RunningAvgSamplesPerSec=1.5851381958072936, CurrSamplesPerSec=1.6811191417855487, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      880/109863281 | consumed samples:         3520 | consumed tokens:      7208960 | elapsed time per iteration (ms): 2592.6 | learning rate: 3.840E-06 | global batch size:     4 | lm loss: 7.264532E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.543 | TFLOPs: 9.39 |
[default0]:[2023-07-25 16:05:58,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=885, skipped=0, lr=[3.862254933333334e-06, 3.862254933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:05:58,063] [INFO] [timer.py:215:stop] epoch=0/micro_step=885/global_step=885, RunningAvgSamplesPerSec=1.5850244399438889, CurrSamplesPerSec=1.6140381122433274, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      885/109863281 | consumed samples:         3540 | consumed tokens:      7249920 | elapsed time per iteration (ms): 2730.7 | learning rate: 3.862E-06 | global batch size:     4 | lm loss: 7.069861E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.465 | TFLOPs: 8.92 |
[default0]:[2023-07-25 16:06:11,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[3.884100266666667e-06, 3.884100266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:06:11,282] [INFO] [timer.py:215:stop] epoch=0/micro_step=890/global_step=890, RunningAvgSamplesPerSec=1.5852057714941494, CurrSamplesPerSec=1.507252949400992, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      890/109863281 | consumed samples:         3560 | consumed tokens:      7290880 | elapsed time per iteration (ms): 2636.3 | learning rate: 3.884E-06 | global batch size:     4 | lm loss: 7.179913E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.517 | TFLOPs: 9.24 |
[default0]:[2023-07-25 16:06:24,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=895, skipped=0, lr=[3.9059456e-06, 3.9059456e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:06:24,356] [INFO] [timer.py:215:stop] epoch=0/micro_step=895/global_step=895, RunningAvgSamplesPerSec=1.585438445309524, CurrSamplesPerSec=1.682359360047844, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      895/109863281 | consumed samples:         3580 | consumed tokens:      7331840 | elapsed time per iteration (ms): 2602.5 | learning rate: 3.906E-06 | global batch size:     4 | lm loss: 7.436545E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.537 | TFLOPs: 9.36 |
[default0]:[2023-07-25 16:06:37,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[3.927790933333334e-06, 3.927790933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:06:37,827] [INFO] [timer.py:215:stop] epoch=0/micro_step=900/global_step=900, RunningAvgSamplesPerSec=1.5854089628115764, CurrSamplesPerSec=1.557712182281454, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      900/109863281 | consumed samples:         3600 | consumed tokens:      7372800 | elapsed time per iteration (ms): 2688.8 | learning rate: 3.928E-06 | global batch size:     4 | lm loss: 7.391148E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.488 | TFLOPs: 9.06 |
[default1]:-----------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 900 | lm loss value: 7.300933E+00 | lm loss PPL: 1.481682E+03 | 
[default1]:-----------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration     900 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:06:48,835] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step900 is about to be saved!
[default1]:[2023-07-25 16:06:48,936] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step900 is ready now!
[default0]:[2023-07-25 16:06:48,934] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step900 is ready now!
[default1]:[2023-07-25 16:06:48,934] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step900 is ready now!
[default0]:[2023-07-25 16:06:48,932] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step900/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:06:48,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step900/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:07:59,999] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step900/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:08:00,021] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step900 is ready now!
[default0]:  successfully saved checkpoint at iteration     900 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.389
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71389.33, 71406.27)
[default0]:[2023-07-25 16:08:13,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=905, skipped=0, lr=[3.949636266666667e-06, 3.949636266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:08:13,879] [INFO] [timer.py:215:stop] epoch=0/micro_step=905/global_step=905, RunningAvgSamplesPerSec=1.5851892952381708, CurrSamplesPerSec=1.4571299087935445, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      905/109863281 | consumed samples:         3620 | consumed tokens:      7413760 | elapsed time per iteration (ms): 19194.0 | learning rate: 3.950E-06 | global batch size:     4 | lm loss: 7.128750E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.208 | TFLOPs: 1.27 |
[default0]:[2023-07-25 16:08:27,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[3.9714816e-06, 3.9714816e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:08:27,418] [INFO] [timer.py:215:stop] epoch=0/micro_step=910/global_step=910, RunningAvgSamplesPerSec=1.5851017981261883, CurrSamplesPerSec=1.579590934806959, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      910/109863281 | consumed samples:         3640 | consumed tokens:      7454720 | elapsed time per iteration (ms): 2706.3 | learning rate: 3.971E-06 | global batch size:     4 | lm loss: 7.001832E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.478 | TFLOPs: 9.00 |
[default0]:[2023-07-25 16:08:40,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=915, skipped=0, lr=[3.9933269333333334e-06, 3.9933269333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:08:40,894] [INFO] [timer.py:215:stop] epoch=0/micro_step=915/global_step=915, RunningAvgSamplesPerSec=1.585147345628672, CurrSamplesPerSec=1.7096611476277521, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      915/109863281 | consumed samples:         3660 | consumed tokens:      7495680 | elapsed time per iteration (ms): 2690.3 | learning rate: 3.993E-06 | global batch size:     4 | lm loss: 7.325234E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.487 | TFLOPs: 9.05 |
[default0]:[2023-07-25 16:08:53,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[4.015172266666667e-06, 4.015172266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:08:53,852] [INFO] [timer.py:215:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=1.5855132483354297, CurrSamplesPerSec=1.5898473957569805, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      920/109863281 | consumed samples:         3680 | consumed tokens:      7536640 | elapsed time per iteration (ms): 2580.6 | learning rate: 4.015E-06 | global batch size:     4 | lm loss: 7.310937E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.550 | TFLOPs: 9.44 |
[default0]:[2023-07-25 16:09:06,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=925, skipped=0, lr=[4.037017600000001e-06, 4.037017600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:09:06,646] [INFO] [timer.py:215:stop] epoch=0/micro_step=925/global_step=925, RunningAvgSamplesPerSec=1.5860538057026454, CurrSamplesPerSec=1.7555662148734317, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      925/109863281 | consumed samples:         3700 | consumed tokens:      7577600 | elapsed time per iteration (ms): 2556.7 | learning rate: 4.037E-06 | global batch size:     4 | lm loss: 7.082939E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.565 | TFLOPs: 9.53 |
[default0]:[2023-07-25 16:09:20,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=0, lr=[4.058862933333334e-06, 4.058862933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:09:20,227] [INFO] [timer.py:215:stop] epoch=0/micro_step=930/global_step=930, RunningAvgSamplesPerSec=1.5860151158364617, CurrSamplesPerSec=1.7081799865216691, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      930/109863281 | consumed samples:         3720 | consumed tokens:      7618560 | elapsed time per iteration (ms): 2703.7 | learning rate: 4.059E-06 | global batch size:     4 | lm loss: 7.270890E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.479 | TFLOPs: 9.01 |
[default0]:[2023-07-25 16:09:33,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=935, skipped=0, lr=[4.080708266666667e-06, 4.080708266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:09:33,522] [INFO] [timer.py:215:stop] epoch=0/micro_step=935/global_step=935, RunningAvgSamplesPerSec=1.5861205721358864, CurrSamplesPerSec=1.6565631472023579, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      935/109863281 | consumed samples:         3740 | consumed tokens:      7659520 | elapsed time per iteration (ms): 2651.5 | learning rate: 4.081E-06 | global batch size:     4 | lm loss: 7.286295E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.509 | TFLOPs: 9.19 |
[default0]:[2023-07-25 16:09:46,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=0, lr=[4.1025536000000006e-06, 4.1025536000000006e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:09:46,842] [INFO] [timer.py:215:stop] epoch=0/micro_step=940/global_step=940, RunningAvgSamplesPerSec=1.5861932054804686, CurrSamplesPerSec=1.6075280825130274, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      940/109863281 | consumed samples:         3760 | consumed tokens:      7700480 | elapsed time per iteration (ms): 2654.9 | learning rate: 4.103E-06 | global batch size:     4 | lm loss: 7.174368E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.507 | TFLOPs: 9.17 |
[default0]:[2023-07-25 16:10:00,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=945, skipped=0, lr=[4.124398933333333e-06, 4.124398933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:10:00,151] [INFO] [timer.py:215:stop] epoch=0/micro_step=945/global_step=945, RunningAvgSamplesPerSec=1.5863343611717768, CurrSamplesPerSec=1.582706356233681, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      945/109863281 | consumed samples:         3780 | consumed tokens:      7741440 | elapsed time per iteration (ms): 2656.6 | learning rate: 4.124E-06 | global batch size:     4 | lm loss: 7.271044E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.506 | TFLOPs: 9.17 |
[default0]:[2023-07-25 16:10:13,734] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=0, lr=[4.146244266666666e-06, 4.146244266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:10:13,740] [INFO] [timer.py:215:stop] epoch=0/micro_step=950/global_step=950, RunningAvgSamplesPerSec=1.586199716389463, CurrSamplesPerSec=1.660690389708346, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default0]:saving checkpoint at iteration     950 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:10:13,973] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step950 is about to be saved!
[default1]: iteration      950/109863281 | consumed samples:         3800 | consumed tokens:      7782400 | elapsed time per iteration (ms): 2713.8 | learning rate: 4.146E-06 | global batch size:     4 | lm loss: 7.259521E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.474 | TFLOPs: 8.97 |
[default1]:[2023-07-25 16:10:14,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step950 is ready now!
[default0]:[2023-07-25 16:10:14,006] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step950/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:10:14,007] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step950/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:10:14,007] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step950 is ready now!
[default1]:[2023-07-25 16:10:14,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step950 is ready now!
[default0]:[2023-07-25 16:11:25,781] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step950/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:11:25,802] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step950 is ready now!
[default0]:  successfully saved checkpoint at iteration     950 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.977
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71975.62, 71981.61)
[default0]:[2023-07-25 16:11:39,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=955, skipped=0, lr=[4.1680896e-06, 4.1680896e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:11:39,410] [INFO] [timer.py:215:stop] epoch=0/micro_step=955/global_step=955, RunningAvgSamplesPerSec=1.5862261434466527, CurrSamplesPerSec=1.6780946404385695, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      955/109863281 | consumed samples:         3820 | consumed tokens:      7823360 | elapsed time per iteration (ms): 17118.2 | learning rate: 4.168E-06 | global batch size:     4 | lm loss: 7.176408E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.234 | TFLOPs: 1.42 |
[default0]:[2023-07-25 16:11:52,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=0, lr=[4.189934933333334e-06, 4.189934933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:11:52,756] [INFO] [timer.py:215:stop] epoch=0/micro_step=960/global_step=960, RunningAvgSamplesPerSec=1.5862933077665655, CurrSamplesPerSec=1.6591287396915209, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      960/109863281 | consumed samples:         3840 | consumed tokens:      7864320 | elapsed time per iteration (ms): 2669.4 | learning rate: 4.190E-06 | global batch size:     4 | lm loss: 7.031975E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.498 | TFLOPs: 9.12 |
[default0]:[2023-07-25 16:12:05,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=965, skipped=0, lr=[4.211780266666667e-06, 4.211780266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:12:05,804] [INFO] [timer.py:215:stop] epoch=0/micro_step=965/global_step=965, RunningAvgSamplesPerSec=1.5866194037435135, CurrSamplesPerSec=1.5644327342215278, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      965/109863281 | consumed samples:         3860 | consumed tokens:      7905280 | elapsed time per iteration (ms): 2597.8 | learning rate: 4.212E-06 | global batch size:     4 | lm loss: 6.996799E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.540 | TFLOPs: 9.38 |
[default0]:[2023-07-25 16:12:19,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=0, lr=[4.2336256e-06, 4.2336256e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:12:19,199] [INFO] [timer.py:215:stop] epoch=0/micro_step=970/global_step=970, RunningAvgSamplesPerSec=1.5865946112931744, CurrSamplesPerSec=1.624512952557125, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      970/109863281 | consumed samples:         3880 | consumed tokens:      7946240 | elapsed time per iteration (ms): 2682.0 | learning rate: 4.234E-06 | global batch size:     4 | lm loss: 7.128423E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.491 | TFLOPs: 9.08 |
[default0]:[2023-07-25 16:12:32,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=975, skipped=0, lr=[4.255470933333333e-06, 4.255470933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:12:32,769] [INFO] [timer.py:215:stop] epoch=0/micro_step=975/global_step=975, RunningAvgSamplesPerSec=1.5866135288382597, CurrSamplesPerSec=1.623176223855923, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      975/109863281 | consumed samples:         3900 | consumed tokens:      7987200 | elapsed time per iteration (ms): 2708.0 | learning rate: 4.255E-06 | global batch size:     4 | lm loss: 7.116461E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.477 | TFLOPs: 8.99 |
[default0]:[2023-07-25 16:12:45,877] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=0, lr=[4.277316266666667e-06, 4.277316266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:12:45,894] [INFO] [timer.py:215:stop] epoch=0/micro_step=980/global_step=980, RunningAvgSamplesPerSec=1.5869068731260438, CurrSamplesPerSec=1.6600641615393703, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      980/109863281 | consumed samples:         3920 | consumed tokens:      8028160 | elapsed time per iteration (ms): 2605.5 | learning rate: 4.277E-06 | global batch size:     4 | lm loss: 7.127385E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.535 | TFLOPs: 9.35 |
[default0]:[2023-07-25 16:12:59,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=985, skipped=0, lr=[4.2991616e-06, 4.2991616e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:12:59,202] [INFO] [timer.py:215:stop] epoch=0/micro_step=985/global_step=985, RunningAvgSamplesPerSec=1.5869823102906042, CurrSamplesPerSec=1.607290915718666, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      985/109863281 | consumed samples:         3940 | consumed tokens:      8069120 | elapsed time per iteration (ms): 2659.7 | learning rate: 4.299E-06 | global batch size:     4 | lm loss: 6.957414E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.504 | TFLOPs: 9.16 |
[default0]:[2023-07-25 16:13:12,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=0, lr=[4.321006933333333e-06, 4.321006933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:13:12,556] [INFO] [timer.py:215:stop] epoch=0/micro_step=990/global_step=990, RunningAvgSamplesPerSec=1.5869750318204403, CurrSamplesPerSec=1.6045675981748506, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      990/109863281 | consumed samples:         3960 | consumed tokens:      8110080 | elapsed time per iteration (ms): 2658.2 | learning rate: 4.321E-06 | global batch size:     4 | lm loss: 7.054211E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.505 | TFLOPs: 9.16 |
[default0]:[2023-07-25 16:13:26,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=995, skipped=0, lr=[4.342852266666666e-06, 4.342852266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:13:26,300] [INFO] [timer.py:215:stop] epoch=0/micro_step=995/global_step=995, RunningAvgSamplesPerSec=1.586763613614345, CurrSamplesPerSec=1.531519529520921, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration      995/109863281 | consumed samples:         3980 | consumed tokens:      8151040 | elapsed time per iteration (ms): 2740.6 | learning rate: 4.343E-06 | global batch size:     4 | lm loss: 7.123143E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.460 | TFLOPs: 8.89 |
[default0]:[2023-07-25 16:13:40,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[4.3646976000000005e-06, 4.3646976000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:13:40,156] [INFO] [timer.py:215:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=1.5865710489760914, CurrSamplesPerSec=1.4469230491261869, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1000/109863281 | consumed samples:         4000 | consumed tokens:      8192000 | elapsed time per iteration (ms): 2763.6 | learning rate: 4.365E-06 | global batch size:     4 | lm loss: 7.122652E+00 | loss scale: 4096.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.447 | TFLOPs: 8.81 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 1000 | lm loss value: 7.202723E+00 | lm loss PPL: 1.343083E+03 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    1000 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:13:51,171] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
[default1]:[2023-07-25 16:13:51,236] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
[default0]:[2023-07-25 16:13:51,234] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
[default1]:[2023-07-25 16:13:51,236] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
[default0]:[2023-07-25 16:13:51,234] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1000/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:13:51,234] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1000/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:15:02,405] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1000/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:15:02,426] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
[default0]:  successfully saved checkpoint at iteration    1000 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.382
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71381.77, 71393.99)
[default1]:[2023-07-25 16:15:05,078] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default1]:[2023-07-25 16:15:05,078] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[default1]:[2023-07-25 16:15:05,084] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default1]:[2023-07-25 16:15:05,111] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[default0]:[2023-07-25 16:15:05,077] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default0]:[2023-07-25 16:15:05,077] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[default0]:[2023-07-25 16:15:05,075] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default0]:[2023-07-25 16:15:05,103] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 4096 to 8192
[default0]:[2023-07-25 16:15:15,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=1005, skipped=0, lr=[4.386542933333334e-06, 4.386542933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:15:15,826] [INFO] [timer.py:215:stop] epoch=0/micro_step=1005/global_step=1005, RunningAvgSamplesPerSec=1.5865344437818585, CurrSamplesPerSec=1.632620777544489, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1005/109863281 | consumed samples:         4020 | consumed tokens:      8232960 | elapsed time per iteration (ms): 19123.9 | learning rate: 4.387E-06 | global batch size:     4 | lm loss: 6.993419E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.209 | TFLOPs: 1.27 |
[default0]:[2023-07-25 16:15:29,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=0, lr=[4.408388266666667e-06, 4.408388266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:15:29,189] [INFO] [timer.py:215:stop] epoch=0/micro_step=1010/global_step=1010, RunningAvgSamplesPerSec=1.586551463526397, CurrSamplesPerSec=1.4778466790586793, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1010/109863281 | consumed samples:         4040 | consumed tokens:      8273920 | elapsed time per iteration (ms): 2658.6 | learning rate: 4.408E-06 | global batch size:     4 | lm loss: 6.914793E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.505 | TFLOPs: 9.16 |
[default0]:[2023-07-25 16:15:42,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=1015, skipped=0, lr=[4.4302336e-06, 4.4302336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:15:42,758] [INFO] [timer.py:215:stop] epoch=0/micro_step=1015/global_step=1015, RunningAvgSamplesPerSec=1.5863662528084908, CurrSamplesPerSec=1.5674013125380588, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1015/109863281 | consumed samples:         4060 | consumed tokens:      8314880 | elapsed time per iteration (ms): 2720.4 | learning rate: 4.430E-06 | global batch size:     4 | lm loss: 7.266855E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.470 | TFLOPs: 8.95 |
[default0]:[2023-07-25 16:15:55,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=0, lr=[4.4520789333333335e-06, 4.4520789333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:15:55,931] [INFO] [timer.py:215:stop] epoch=0/micro_step=1020/global_step=1020, RunningAvgSamplesPerSec=1.5865698278527796, CurrSamplesPerSec=1.5467349580187222, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1020/109863281 | consumed samples:         4080 | consumed tokens:      8355840 | elapsed time per iteration (ms): 2610.1 | learning rate: 4.452E-06 | global batch size:     4 | lm loss: 7.126395E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.533 | TFLOPs: 9.33 |
[default0]:[2023-07-25 16:16:09,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=1025, skipped=0, lr=[4.473924266666667e-06, 4.473924266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:16:09,171] [INFO] [timer.py:215:stop] epoch=0/micro_step=1025/global_step=1025, RunningAvgSamplesPerSec=1.5866986405058898, CurrSamplesPerSec=1.618369516357119, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1025/109863281 | consumed samples:         4100 | consumed tokens:      8396800 | elapsed time per iteration (ms): 2645.7 | learning rate: 4.474E-06 | global batch size:     4 | lm loss: 6.974550E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.512 | TFLOPs: 9.21 |
[default0]:[2023-07-25 16:16:22,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=0, lr=[4.4957696e-06, 4.4957696e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:16:22,521] [INFO] [timer.py:215:stop] epoch=0/micro_step=1030/global_step=1030, RunningAvgSamplesPerSec=1.5867197722606026, CurrSamplesPerSec=1.6955532245971205, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1030/109863281 | consumed samples:         4120 | consumed tokens:      8437760 | elapsed time per iteration (ms): 2666.0 | learning rate: 4.496E-06 | global batch size:     4 | lm loss: 6.946922E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.500 | TFLOPs: 9.14 |
[default0]:[2023-07-25 16:16:35,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=1035, skipped=0, lr=[4.517614933333333e-06, 4.517614933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:16:35,655] [INFO] [timer.py:215:stop] epoch=0/micro_step=1035/global_step=1035, RunningAvgSamplesPerSec=1.5868252034310475, CurrSamplesPerSec=1.6133499067313626, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1035/109863281 | consumed samples:         4140 | consumed tokens:      8478720 | elapsed time per iteration (ms): 2626.4 | learning rate: 4.518E-06 | global batch size:     4 | lm loss: 6.987513E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.523 | TFLOPs: 9.27 |
[default0]:[2023-07-25 16:16:48,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=0, lr=[4.5394602666666666e-06, 4.5394602666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:16:48,529] [INFO] [timer.py:215:stop] epoch=0/micro_step=1040/global_step=1040, RunningAvgSamplesPerSec=1.5870816965009662, CurrSamplesPerSec=1.5701583717881633, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1040/109863281 | consumed samples:         4160 | consumed tokens:      8519680 | elapsed time per iteration (ms): 2561.6 | learning rate: 4.539E-06 | global batch size:     4 | lm loss: 7.079152E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.562 | TFLOPs: 9.51 |
[default0]:[2023-07-25 16:17:01,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=1045, skipped=0, lr=[4.561305600000001e-06, 4.561305600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:17:02,002] [INFO] [timer.py:215:stop] epoch=0/micro_step=1045/global_step=1045, RunningAvgSamplesPerSec=1.5870244892893186, CurrSamplesPerSec=1.549078957597868, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1045/109863281 | consumed samples:         4180 | consumed tokens:      8560640 | elapsed time per iteration (ms): 2703.5 | learning rate: 4.561E-06 | global batch size:     4 | lm loss: 6.991143E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.480 | TFLOPs: 9.01 |
[default0]:[2023-07-25 16:17:15,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=0, lr=[4.583150933333334e-06, 4.583150933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:17:15,721] [INFO] [timer.py:215:stop] epoch=0/micro_step=1050/global_step=1050, RunningAvgSamplesPerSec=1.5868141606649862, CurrSamplesPerSec=1.5042900451209473, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default0]:saving checkpoint at iteration    1050 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]: iteration     1050/109863281 | consumed samples:         4200 | consumed tokens:      8601600 | elapsed time per iteration (ms): 2733.2 | learning rate: 4.583E-06 | global batch size:     4 | lm loss: 7.040047E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.463 | TFLOPs: 8.91 |
[default0]:[2023-07-25 16:17:15,962] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1050 is about to be saved!
[default0]:[2023-07-25 16:17:16,021] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1050/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:17:16,022] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1050/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:17:16,031] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1050 is ready now!
[default1]:[2023-07-25 16:17:16,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1050 is ready now!
[default1]:[2023-07-25 16:17:16,019] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1050 is ready now!
[default0]:[2023-07-25 16:18:26,929] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1050/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:18:26,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1050 is ready now!
[default0]:  successfully saved checkpoint at iteration    1050 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.133
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71133.20, 71162.06)
[default0]:[2023-07-25 16:18:40,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=1055, skipped=0, lr=[4.604996266666667e-06, 4.604996266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:18:40,736] [INFO] [timer.py:215:stop] epoch=0/micro_step=1055/global_step=1055, RunningAvgSamplesPerSec=1.5866286267156875, CurrSamplesPerSec=1.5146706380854997, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1055/109863281 | consumed samples:         4220 | consumed tokens:      8642560 | elapsed time per iteration (ms): 16983.9 | learning rate: 4.605E-06 | global batch size:     4 | lm loss: 6.941460E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.236 | TFLOPs: 1.43 |
[default0]:[2023-07-25 16:18:53,569] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=0, lr=[4.6268416000000005e-06, 4.6268416000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:18:53,583] [INFO] [timer.py:215:stop] epoch=0/micro_step=1060/global_step=1060, RunningAvgSamplesPerSec=1.5869493866901307, CurrSamplesPerSec=1.6378278798509545, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1060/109863281 | consumed samples:         4240 | consumed tokens:      8683520 | elapsed time per iteration (ms): 2560.4 | learning rate: 4.627E-06 | global batch size:     4 | lm loss: 6.967740E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.562 | TFLOPs: 9.51 |
[default0]:[2023-07-25 16:19:06,512] [INFO] [logging.py:96:log_dist] [Rank 0] step=1065, skipped=0, lr=[4.648686933333334e-06, 4.648686933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:19:06,546] [INFO] [timer.py:215:stop] epoch=0/micro_step=1065/global_step=1065, RunningAvgSamplesPerSec=1.5872518803588722, CurrSamplesPerSec=1.7043922711372093, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1065/109863281 | consumed samples:         4260 | consumed tokens:      8724480 | elapsed time per iteration (ms): 2602.7 | learning rate: 4.649E-06 | global batch size:     4 | lm loss: 7.278052E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.537 | TFLOPs: 9.36 |
[default0]:[2023-07-25 16:19:19,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=0, lr=[4.670532266666667e-06, 4.670532266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:19:19,890] [INFO] [timer.py:215:stop] epoch=0/micro_step=1070/global_step=1070, RunningAvgSamplesPerSec=1.5873062262242208, CurrSamplesPerSec=1.620048833554751, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1070/109863281 | consumed samples:         4280 | consumed tokens:      8765440 | elapsed time per iteration (ms): 2658.9 | learning rate: 4.671E-06 | global batch size:     4 | lm loss: 6.931283E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.504 | TFLOPs: 9.16 |
[default0]:[2023-07-25 16:19:33,097] [INFO] [logging.py:96:log_dist] [Rank 0] step=1075, skipped=0, lr=[4.6923776e-06, 4.6923776e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:19:33,115] [INFO] [timer.py:215:stop] epoch=0/micro_step=1075/global_step=1075, RunningAvgSamplesPerSec=1.5874481549972446, CurrSamplesPerSec=1.5778077153305505, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1075/109863281 | consumed samples:         4300 | consumed tokens:      8806400 | elapsed time per iteration (ms): 2628.1 | learning rate: 4.692E-06 | global batch size:     4 | lm loss: 6.906799E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.522 | TFLOPs: 9.27 |
[default0]:[2023-07-25 16:19:46,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=0, lr=[4.7142229333333335e-06, 4.7142229333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:19:46,579] [INFO] [timer.py:215:stop] epoch=0/micro_step=1080/global_step=1080, RunningAvgSamplesPerSec=1.5873961101734704, CurrSamplesPerSec=1.5385183095719055, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1080/109863281 | consumed samples:         4320 | consumed tokens:      8847360 | elapsed time per iteration (ms): 2690.6 | learning rate: 4.714E-06 | global batch size:     4 | lm loss: 7.080130E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.487 | TFLOPs: 9.05 |
[default0]:[2023-07-25 16:19:59,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=1085, skipped=0, lr=[4.736068266666667e-06, 4.736068266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:19:59,765] [INFO] [timer.py:215:stop] epoch=0/micro_step=1085/global_step=1085, RunningAvgSamplesPerSec=1.5874949240697696, CurrSamplesPerSec=1.5282427966419354, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1085/109863281 | consumed samples:         4340 | consumed tokens:      8888320 | elapsed time per iteration (ms): 2632.0 | learning rate: 4.736E-06 | global batch size:     4 | lm loss: 7.048683E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.520 | TFLOPs: 9.25 |
[default0]:[2023-07-25 16:20:13,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=0, lr=[4.757913600000001e-06, 4.757913600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:20:13,100] [INFO] [timer.py:215:stop] epoch=0/micro_step=1090/global_step=1090, RunningAvgSamplesPerSec=1.5875374897794476, CurrSamplesPerSec=1.5734445518694975, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1090/109863281 | consumed samples:         4360 | consumed tokens:      8929280 | elapsed time per iteration (ms): 2654.3 | learning rate: 4.758E-06 | global batch size:     4 | lm loss: 7.034698E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.507 | TFLOPs: 9.18 |
[default0]:[2023-07-25 16:20:26,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=1095, skipped=0, lr=[4.779758933333334e-06, 4.779758933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:20:26,485] [INFO] [timer.py:215:stop] epoch=0/micro_step=1095/global_step=1095, RunningAvgSamplesPerSec=1.5875370114033198, CurrSamplesPerSec=1.5796052120557624, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1095/109863281 | consumed samples:         4380 | consumed tokens:      8970240 | elapsed time per iteration (ms): 2668.0 | learning rate: 4.780E-06 | global batch size:     4 | lm loss: 7.068336E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.499 | TFLOPs: 9.13 |
[default0]:[2023-07-25 16:20:40,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=0, lr=[4.801604266666667e-06, 4.801604266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:20:40,191] [INFO] [timer.py:215:stop] epoch=0/micro_step=1100/global_step=1100, RunningAvgSamplesPerSec=1.5875626647271206, CurrSamplesPerSec=1.538850781183501, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1100/109863281 | consumed samples:         4400 | consumed tokens:      9011200 | elapsed time per iteration (ms): 2729.4 | learning rate: 4.802E-06 | global batch size:     4 | lm loss: 6.982446E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.466 | TFLOPs: 8.92 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 1100 | lm loss value: 7.126317E+00 | lm loss PPL: 1.244285E+03 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    1100 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:20:51,572] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1100 is about to be saved!
[default1]:[2023-07-25 16:20:51,659] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1100 is ready now!
[default0]:[2023-07-25 16:20:51,669] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1100 is ready now!
[default0]:[2023-07-25 16:20:51,659] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1100/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:20:51,659] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1100/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 16:20:51,661] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1100 is ready now!
[default0]:[2023-07-25 16:22:02,296] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1100/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:22:02,306] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1100 is ready now!
[default0]:  successfully saved checkpoint at iteration    1100 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.932
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70932.24, 70949.94)
[default0]:[2023-07-25 16:22:15,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=1105, skipped=0, lr=[4.823449600000001e-06, 4.823449600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:22:15,759] [INFO] [timer.py:215:stop] epoch=0/micro_step=1105/global_step=1105, RunningAvgSamplesPerSec=1.5874979648988266, CurrSamplesPerSec=1.5532352324430065, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1105/109863281 | consumed samples:         4420 | consumed tokens:      9052160 | elapsed time per iteration (ms): 19103.7 | learning rate: 4.823E-06 | global batch size:     4 | lm loss: 7.051488E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.209 | TFLOPs: 1.27 |
[default0]:[2023-07-25 16:22:28,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=0, lr=[4.845294933333333e-06, 4.845294933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:22:28,901] [INFO] [timer.py:215:stop] epoch=0/micro_step=1110/global_step=1110, RunningAvgSamplesPerSec=1.5876311581368152, CurrSamplesPerSec=1.8026679399897774, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1110/109863281 | consumed samples:         4440 | consumed tokens:      9093120 | elapsed time per iteration (ms): 2626.2 | learning rate: 4.845E-06 | global batch size:     4 | lm loss: 6.894234E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.523 | TFLOPs: 9.27 |
[default0]:[2023-07-25 16:22:41,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=1115, skipped=0, lr=[4.867140266666666e-06, 4.867140266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:22:41,867] [INFO] [timer.py:215:stop] epoch=0/micro_step=1115/global_step=1115, RunningAvgSamplesPerSec=1.5879458649543843, CurrSamplesPerSec=1.6340789463175405, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1115/109863281 | consumed samples:         4460 | consumed tokens:      9134080 | elapsed time per iteration (ms): 2582.2 | learning rate: 4.867E-06 | global batch size:     4 | lm loss: 7.179523E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.549 | TFLOPs: 9.43 |
[default0]:[2023-07-25 16:22:54,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=0, lr=[4.8889856e-06, 4.8889856e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:22:54,933] [INFO] [timer.py:215:stop] epoch=0/micro_step=1120/global_step=1120, RunningAvgSamplesPerSec=1.588076814944444, CurrSamplesPerSec=1.6701557942520382, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1120/109863281 | consumed samples:         4480 | consumed tokens:      9175040 | elapsed time per iteration (ms): 2604.4 | learning rate: 4.889E-06 | global batch size:     4 | lm loss: 6.981525E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.536 | TFLOPs: 9.35 |
[default0]:[2023-07-25 16:23:07,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=1125, skipped=0, lr=[4.910830933333334e-06, 4.910830933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:23:07,989] [INFO] [timer.py:215:stop] epoch=0/micro_step=1125/global_step=1125, RunningAvgSamplesPerSec=1.5883234340149863, CurrSamplesPerSec=1.579146684432348, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1125/109863281 | consumed samples:         4500 | consumed tokens:      9216000 | elapsed time per iteration (ms): 2603.9 | learning rate: 4.911E-06 | global batch size:     4 | lm loss: 7.147269E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.536 | TFLOPs: 9.35 |
[default0]:[2023-07-25 16:23:21,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=0, lr=[4.932676266666667e-06, 4.932676266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:23:21,128] [INFO] [timer.py:215:stop] epoch=0/micro_step=1130/global_step=1130, RunningAvgSamplesPerSec=1.5883942547246186, CurrSamplesPerSec=1.5655026153270784, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1130/109863281 | consumed samples:         4520 | consumed tokens:      9256960 | elapsed time per iteration (ms): 2627.3 | learning rate: 4.933E-06 | global batch size:     4 | lm loss: 7.040159E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.522 | TFLOPs: 9.27 |
[default0]:[2023-07-25 16:23:34,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=1135, skipped=0, lr=[4.9545216e-06, 4.9545216e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:23:34,535] [INFO] [timer.py:215:stop] epoch=0/micro_step=1135/global_step=1135, RunningAvgSamplesPerSec=1.588487313469099, CurrSamplesPerSec=1.5943319790349517, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1135/109863281 | consumed samples:         4540 | consumed tokens:      9297920 | elapsed time per iteration (ms): 2666.2 | learning rate: 4.955E-06 | global batch size:     4 | lm loss: 6.861906E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.500 | TFLOPs: 9.13 |
[default0]:[2023-07-25 16:23:47,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=0, lr=[4.9763669333333334e-06, 4.9763669333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:23:47,609] [INFO] [timer.py:215:stop] epoch=0/micro_step=1140/global_step=1140, RunningAvgSamplesPerSec=1.5886484826527882, CurrSamplesPerSec=1.619429115391116, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1140/109863281 | consumed samples:         4560 | consumed tokens:      9338880 | elapsed time per iteration (ms): 2613.8 | learning rate: 4.976E-06 | global batch size:     4 | lm loss: 6.937020E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.530 | TFLOPs: 9.32 |
[default0]:[2023-07-25 16:24:01,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=1145, skipped=0, lr=[4.998212266666667e-06, 4.998212266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:24:01,161] [INFO] [timer.py:215:stop] epoch=0/micro_step=1145/global_step=1145, RunningAvgSamplesPerSec=1.5885752600120375, CurrSamplesPerSec=1.606966080013946, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1145/109863281 | consumed samples:         4580 | consumed tokens:      9379840 | elapsed time per iteration (ms): 2701.3 | learning rate: 4.998E-06 | global batch size:     4 | lm loss: 6.784051E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.481 | TFLOPs: 9.02 |
[default0]:[2023-07-25 16:24:13,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=0, lr=[5.0200576e-06, 5.0200576e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:24:13,931] [INFO] [timer.py:215:stop] epoch=0/micro_step=1150/global_step=1150, RunningAvgSamplesPerSec=1.5889002795261582, CurrSamplesPerSec=1.6049352198819995, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1150/109863281 | consumed samples:         4600 | consumed tokens:      9420800 | elapsed time per iteration (ms): 2538.5 | learning rate: 5.020E-06 | global batch size:     4 | lm loss: 7.056839E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.576 | TFLOPs: 9.59 |
[default0]:saving checkpoint at iteration    1150 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:[2023-07-25 16:24:14,170] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1150 is ready now!
[default0]:[2023-07-25 16:24:14,111] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1150 is about to be saved!
[default0]:[2023-07-25 16:24:14,168] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1150/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:24:14,168] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1150/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:24:14,169] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1150 is ready now!
[default1]:[2023-07-25 16:24:14,170] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1150 is ready now!
[default0]:[2023-07-25 16:25:25,319] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1150/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:25:25,340] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1150 is ready now!
[default0]:  successfully saved checkpoint at iteration    1150 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.432
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71431.63, 71442.32)
[default0]:[2023-07-25 16:25:39,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=1155, skipped=0, lr=[5.041902933333333e-06, 5.041902933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:25:39,122] [INFO] [timer.py:215:stop] epoch=0/micro_step=1155/global_step=1155, RunningAvgSamplesPerSec=1.5887614117667772, CurrSamplesPerSec=1.5796902860137585, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1155/109863281 | consumed samples:         4620 | consumed tokens:      9461760 | elapsed time per iteration (ms): 17036.7 | learning rate: 5.042E-06 | global batch size:     4 | lm loss: 6.791684E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.235 | TFLOPs: 1.43 |
[default0]:[2023-07-25 16:25:52,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=0, lr=[5.0637482666666665e-06, 5.0637482666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:25:52,436] [INFO] [timer.py:215:stop] epoch=0/micro_step=1160/global_step=1160, RunningAvgSamplesPerSec=1.5888809500737844, CurrSamplesPerSec=1.7080271252839125, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1160/109863281 | consumed samples:         4640 | consumed tokens:      9502720 | elapsed time per iteration (ms): 2661.5 | learning rate: 5.064E-06 | global batch size:     4 | lm loss: 6.919860E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.503 | TFLOPs: 9.15 |
[default0]:[2023-07-25 16:26:06,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=1165, skipped=0, lr=[5.0855936000000006e-06, 5.0855936000000006e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:26:06,078] [INFO] [timer.py:215:stop] epoch=0/micro_step=1165/global_step=1165, RunningAvgSamplesPerSec=1.5887380822704371, CurrSamplesPerSec=1.424805392195204, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1165/109863281 | consumed samples:         4660 | consumed tokens:      9543680 | elapsed time per iteration (ms): 2720.1 | learning rate: 5.086E-06 | global batch size:     4 | lm loss: 6.814139E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.471 | TFLOPs: 8.95 |
[default0]:[2023-07-25 16:26:19,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=0, lr=[5.107438933333334e-06, 5.107438933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:26:19,145] [INFO] [timer.py:215:stop] epoch=0/micro_step=1170/global_step=1170, RunningAvgSamplesPerSec=1.5889140661639385, CurrSamplesPerSec=1.5871433301484292, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1170/109863281 | consumed samples:         4680 | consumed tokens:      9584640 | elapsed time per iteration (ms): 2606.4 | learning rate: 5.107E-06 | global batch size:     4 | lm loss: 6.959761E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.535 | TFLOPs: 9.34 |
[default0]:[2023-07-25 16:26:32,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=1175, skipped=0, lr=[5.129284266666667e-06, 5.129284266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:26:32,448] [INFO] [timer.py:215:stop] epoch=0/micro_step=1175/global_step=1175, RunningAvgSamplesPerSec=1.5889613167760228, CurrSamplesPerSec=1.714097841574322, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1175/109863281 | consumed samples:         4700 | consumed tokens:      9625600 | elapsed time per iteration (ms): 2653.6 | learning rate: 5.129E-06 | global batch size:     4 | lm loss: 6.840781E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.507 | TFLOPs: 9.18 |
[default0]:[2023-07-25 16:26:45,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=0, lr=[5.1511296e-06, 5.1511296e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:26:45,616] [INFO] [timer.py:215:stop] epoch=0/micro_step=1180/global_step=1180, RunningAvgSamplesPerSec=1.5890523302401638, CurrSamplesPerSec=1.671593371540595, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1180/109863281 | consumed samples:         4720 | consumed tokens:      9666560 | elapsed time per iteration (ms): 2624.2 | learning rate: 5.151E-06 | global batch size:     4 | lm loss: 6.887254E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.524 | TFLOPs: 9.28 |
[default0]:[2023-07-25 16:26:59,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=1185, skipped=0, lr=[5.172974933333334e-06, 5.172974933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:26:59,174] [INFO] [timer.py:215:stop] epoch=0/micro_step=1185/global_step=1185, RunningAvgSamplesPerSec=1.5889516445198066, CurrSamplesPerSec=1.6089520914957087, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1185/109863281 | consumed samples:         4740 | consumed tokens:      9707520 | elapsed time per iteration (ms): 2710.0 | learning rate: 5.173E-06 | global batch size:     4 | lm loss: 6.887624E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.476 | TFLOPs: 8.99 |
[default0]:[2023-07-25 16:27:12,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=0, lr=[5.194820266666667e-06, 5.194820266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:27:12,417] [INFO] [timer.py:215:stop] epoch=0/micro_step=1190/global_step=1190, RunningAvgSamplesPerSec=1.5890075733519071, CurrSamplesPerSec=1.685101671279536, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1190/109863281 | consumed samples:         4760 | consumed tokens:      9748480 | elapsed time per iteration (ms): 2659.4 | learning rate: 5.195E-06 | global batch size:     4 | lm loss: 7.071598E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.504 | TFLOPs: 9.16 |
[default0]:[2023-07-25 16:27:25,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=1195, skipped=0, lr=[5.2166656e-06, 5.2166656e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:27:25,712] [INFO] [timer.py:215:stop] epoch=0/micro_step=1195/global_step=1195, RunningAvgSamplesPerSec=1.5891213243433975, CurrSamplesPerSec=1.5817670251311704, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1195/109863281 | consumed samples:         4780 | consumed tokens:      9789440 | elapsed time per iteration (ms): 2637.6 | learning rate: 5.217E-06 | global batch size:     4 | lm loss: 6.982300E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.517 | TFLOPs: 9.23 |
[default0]:[2023-07-25 16:27:39,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=0, lr=[5.238510933333333e-06, 5.238510933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:27:39,406] [INFO] [timer.py:215:stop] epoch=0/micro_step=1200/global_step=1200, RunningAvgSamplesPerSec=1.58902954759197, CurrSamplesPerSec=1.6344839424761026, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1200/109863281 | consumed samples:         4800 | consumed tokens:      9830400 | elapsed time per iteration (ms): 2731.7 | learning rate: 5.239E-06 | global batch size:     4 | lm loss: 6.961002E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.464 | TFLOPs: 8.92 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 1200 | lm loss value: 7.158478E+00 | lm loss PPL: 1.284953E+03 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    1200 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:27:50,258] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1200 is about to be saved!
[default1]:[2023-07-25 16:27:50,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!
[default0]:[2023-07-25 16:27:50,331] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!
[default1]:[2023-07-25 16:27:50,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!
[default0]:[2023-07-25 16:27:50,330] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1200/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:27:50,330] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1200/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:29:00,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1200/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:29:00,765] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1200 is ready now!
[default0]:  successfully saved checkpoint at iteration    1200 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.696
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70692.57, 70709.56)
[default0]:[2023-07-25 16:29:14,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=1205, skipped=0, lr=[5.260356266666667e-06, 5.260356266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:29:14,278] [INFO] [timer.py:215:stop] epoch=0/micro_step=1205/global_step=1205, RunningAvgSamplesPerSec=1.5889882906273154, CurrSamplesPerSec=1.6008282178002897, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1205/109863281 | consumed samples:         4820 | consumed tokens:      9871360 | elapsed time per iteration (ms): 18975.1 | learning rate: 5.260E-06 | global batch size:     4 | lm loss: 6.830314E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.211 | TFLOPs: 1.28 |
[default0]:[2023-07-25 16:29:27,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=0, lr=[5.282201600000001e-06, 5.282201600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:29:27,960] [INFO] [timer.py:215:stop] epoch=0/micro_step=1210/global_step=1210, RunningAvgSamplesPerSec=1.5888741394596841, CurrSamplesPerSec=1.571165030514035, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1210/109863281 | consumed samples:         4840 | consumed tokens:      9912320 | elapsed time per iteration (ms): 2728.1 | learning rate: 5.282E-06 | global batch size:     4 | lm loss: 6.999271E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.466 | TFLOPs: 8.93 |
[default0]:[2023-07-25 16:29:41,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=1215, skipped=0, lr=[5.304046933333334e-06, 5.304046933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:29:41,152] [INFO] [timer.py:215:stop] epoch=0/micro_step=1215/global_step=1215, RunningAvgSamplesPerSec=1.5889699542349642, CurrSamplesPerSec=1.670212990479998, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1215/109863281 | consumed samples:         4860 | consumed tokens:      9953280 | elapsed time per iteration (ms): 2619.2 | learning rate: 5.304E-06 | global batch size:     4 | lm loss: 6.904175E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.527 | TFLOPs: 9.30 |
[default0]:[2023-07-25 16:29:54,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=0, lr=[5.325892266666667e-06, 5.325892266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:29:54,464] [INFO] [timer.py:215:stop] epoch=0/micro_step=1220/global_step=1220, RunningAvgSamplesPerSec=1.5890326040296836, CurrSamplesPerSec=1.6760787076507608, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1220/109863281 | consumed samples:         4880 | consumed tokens:      9994240 | elapsed time per iteration (ms): 2658.2 | learning rate: 5.326E-06 | global batch size:     4 | lm loss: 6.864913E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.505 | TFLOPs: 9.16 |
[default0]:[2023-07-25 16:30:07,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=1225, skipped=0, lr=[5.3477376000000005e-06, 5.3477376000000005e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:30:07,468] [INFO] [timer.py:215:stop] epoch=0/micro_step=1225/global_step=1225, RunningAvgSamplesPerSec=1.589233184026596, CurrSamplesPerSec=1.6348333810807283, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1225/109863281 | consumed samples:         4900 | consumed tokens:     10035200 | elapsed time per iteration (ms): 2590.0 | learning rate: 5.348E-06 | global batch size:     4 | lm loss: 6.820579E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.544 | TFLOPs: 9.40 |
[default0]:[2023-07-25 16:30:20,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=0, lr=[5.369582933333334e-06, 5.369582933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:30:20,724] [INFO] [timer.py:215:stop] epoch=0/micro_step=1230/global_step=1230, RunningAvgSamplesPerSec=1.5892636422614763, CurrSamplesPerSec=1.6149619188854258, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1230/109863281 | consumed samples:         4920 | consumed tokens:     10076160 | elapsed time per iteration (ms): 2647.8 | learning rate: 5.370E-06 | global batch size:     4 | lm loss: 7.106732E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.511 | TFLOPs: 9.20 |
[default0]:[2023-07-25 16:30:33,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=1235, skipped=0, lr=[5.391428266666667e-06, 5.391428266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:30:33,958] [INFO] [timer.py:215:stop] epoch=0/micro_step=1235/global_step=1235, RunningAvgSamplesPerSec=1.589411666500103, CurrSamplesPerSec=1.6880495938299296, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1235/109863281 | consumed samples:         4940 | consumed tokens:     10117120 | elapsed time per iteration (ms): 2640.2 | learning rate: 5.391E-06 | global batch size:     4 | lm loss: 7.150558E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.515 | TFLOPs: 9.23 |
[default0]:[2023-07-25 16:30:47,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=0, lr=[5.4132736e-06, 5.4132736e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:30:47,603] [INFO] [timer.py:215:stop] epoch=0/micro_step=1240/global_step=1240, RunningAvgSamplesPerSec=1.5893376623143673, CurrSamplesPerSec=1.5303650453325015, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1240/109863281 | consumed samples:         4960 | consumed tokens:     10158080 | elapsed time per iteration (ms): 2719.3 | learning rate: 5.413E-06 | global batch size:     4 | lm loss: 6.970024E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.471 | TFLOPs: 8.96 |
[default0]:[2023-07-25 16:31:01,231] [INFO] [logging.py:96:log_dist] [Rank 0] step=1245, skipped=0, lr=[5.4351189333333335e-06, 5.4351189333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:31:01,251] [INFO] [timer.py:215:stop] epoch=0/micro_step=1245/global_step=1245, RunningAvgSamplesPerSec=1.5891985925160472, CurrSamplesPerSec=1.5846688918681662, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1245/109863281 | consumed samples:         4980 | consumed tokens:     10199040 | elapsed time per iteration (ms): 2723.4 | learning rate: 5.435E-06 | global batch size:     4 | lm loss: 6.826465E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.469 | TFLOPs: 8.94 |
[default0]:[2023-07-25 16:31:14,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=0, lr=[5.456964266666667e-06, 5.456964266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:31:14,664] [INFO] [timer.py:215:stop] epoch=0/micro_step=1250/global_step=1250, RunningAvgSamplesPerSec=1.5891972028666934, CurrSamplesPerSec=1.603816454713632, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1250/109863281 | consumed samples:         5000 | consumed tokens:     10240000 | elapsed time per iteration (ms): 2676.4 | learning rate: 5.457E-06 | global batch size:     4 | lm loss: 6.893445E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.495 | TFLOPs: 9.10 |
[default0]:saving checkpoint at iteration    1250 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:31:14,862] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1250 is about to be saved!
[default1]:[2023-07-25 16:31:14,909] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1250 is ready now!
[default0]:[2023-07-25 16:31:14,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1250 is ready now!
[default1]:[2023-07-25 16:31:14,909] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1250 is ready now!
[default0]:[2023-07-25 16:31:14,906] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1250/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:31:14,907] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1250/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:32:26,197] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1250/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:32:26,218] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1250 is ready now!
[default0]:  successfully saved checkpoint at iteration    1250 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71513.31, 71524.72)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.514
[default0]:[2023-07-25 16:32:39,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=1255, skipped=0, lr=[5.478809600000001e-06, 5.478809600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:32:39,821] [INFO] [timer.py:215:stop] epoch=0/micro_step=1255/global_step=1255, RunningAvgSamplesPerSec=1.5891447916403016, CurrSamplesPerSec=1.5324752787983034, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1255/109863281 | consumed samples:         5020 | consumed tokens:     10280960 | elapsed time per iteration (ms): 17028.2 | learning rate: 5.479E-06 | global batch size:     4 | lm loss: 6.746574E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.235 | TFLOPs: 1.43 |
[default0]:[2023-07-25 16:32:53,693] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=0, lr=[5.500654933333334e-06, 5.500654933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:32:53,718] [INFO] [timer.py:215:stop] epoch=0/micro_step=1260/global_step=1260, RunningAvgSamplesPerSec=1.5888885966931667, CurrSamplesPerSec=1.5382832953772143, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1260/109863281 | consumed samples:         5040 | consumed tokens:     10321920 | elapsed time per iteration (ms): 2766.5 | learning rate: 5.501E-06 | global batch size:     4 | lm loss: 6.957002E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.446 | TFLOPs: 8.80 |
[default0]:[2023-07-25 16:33:07,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=1265, skipped=0, lr=[5.522500266666667e-06, 5.522500266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:33:07,525] [INFO] [timer.py:215:stop] epoch=0/micro_step=1265/global_step=1265, RunningAvgSamplesPerSec=1.5887325361701343, CurrSamplesPerSec=1.6165221720451928, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1265/109863281 | consumed samples:         5060 | consumed tokens:     10362880 | elapsed time per iteration (ms): 2759.8 | learning rate: 5.523E-06 | global batch size:     4 | lm loss: 6.674985E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.449 | TFLOPs: 8.83 |
[default0]:[2023-07-25 16:33:20,850] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=0, lr=[5.544345600000001e-06, 5.544345600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:33:20,871] [INFO] [timer.py:215:stop] epoch=0/micro_step=1270/global_step=1270, RunningAvgSamplesPerSec=1.5887934169247715, CurrSamplesPerSec=1.4735572903126075, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1270/109863281 | consumed samples:         5080 | consumed tokens:     10403840 | elapsed time per iteration (ms): 2658.5 | learning rate: 5.544E-06 | global batch size:     4 | lm loss: 6.635080E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.505 | TFLOPs: 9.16 |
[default0]:[2023-07-25 16:33:33,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=1275, skipped=0, lr=[5.566190933333334e-06, 5.566190933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:33:34,005] [INFO] [timer.py:215:stop] epoch=0/micro_step=1275/global_step=1275, RunningAvgSamplesPerSec=1.588933857902258, CurrSamplesPerSec=1.5406061151458998, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1275/109863281 | consumed samples:         5100 | consumed tokens:     10444800 | elapsed time per iteration (ms): 2614.0 | learning rate: 5.566E-06 | global batch size:     4 | lm loss: 7.074122E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.530 | TFLOPs: 9.32 |
[default0]:[2023-07-25 16:33:47,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=0, lr=[5.588036266666667e-06, 5.588036266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:33:47,423] [INFO] [timer.py:215:stop] epoch=0/micro_step=1280/global_step=1280, RunningAvgSamplesPerSec=1.5890469808116623, CurrSamplesPerSec=1.6135502235445656, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1280/109863281 | consumed samples:         5120 | consumed tokens:     10485760 | elapsed time per iteration (ms): 2678.7 | learning rate: 5.588E-06 | global batch size:     4 | lm loss: 6.761156E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.493 | TFLOPs: 9.09 |
[default0]:[2023-07-25 16:34:00,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=1285, skipped=0, lr=[5.6098816000000004e-06, 5.6098816000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:34:00,285] [INFO] [timer.py:215:stop] epoch=0/micro_step=1285/global_step=1285, RunningAvgSamplesPerSec=1.5893156355835256, CurrSamplesPerSec=1.6266607199992245, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1285/109863281 | consumed samples:         5140 | consumed tokens:     10526720 | elapsed time per iteration (ms): 2565.0 | learning rate: 5.610E-06 | global batch size:     4 | lm loss: 6.737270E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.559 | TFLOPs: 9.50 |
[default0]:[2023-07-25 16:34:13,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=0, lr=[5.631726933333334e-06, 5.631726933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:34:13,332] [INFO] [timer.py:215:stop] epoch=0/micro_step=1290/global_step=1290, RunningAvgSamplesPerSec=1.5895124207382856, CurrSamplesPerSec=1.6266130913472512, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1290/109863281 | consumed samples:         5160 | consumed tokens:     10567680 | elapsed time per iteration (ms): 2601.9 | learning rate: 5.632E-06 | global batch size:     4 | lm loss: 6.898862E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.537 | TFLOPs: 9.36 |
[default0]:[2023-07-25 16:34:26,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=1295, skipped=0, lr=[5.653572266666667e-06, 5.653572266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:34:26,988] [INFO] [timer.py:215:stop] epoch=0/micro_step=1295/global_step=1295, RunningAvgSamplesPerSec=1.589374278841004, CurrSamplesPerSec=1.492852693809417, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1295/109863281 | consumed samples:         5180 | consumed tokens:     10608640 | elapsed time per iteration (ms): 2720.4 | learning rate: 5.654E-06 | global batch size:     4 | lm loss: 6.773486E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.470 | TFLOPs: 8.95 |
[default0]:[2023-07-25 16:34:40,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=0, lr=[5.675417600000001e-06, 5.675417600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:34:40,296] [INFO] [timer.py:215:stop] epoch=0/micro_step=1300/global_step=1300, RunningAvgSamplesPerSec=1.5894873765547135, CurrSamplesPerSec=1.575272197371732, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1300/109863281 | consumed samples:         5200 | consumed tokens:     10649600 | elapsed time per iteration (ms): 2660.5 | learning rate: 5.675E-06 | global batch size:     4 | lm loss: 6.866235E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.503 | TFLOPs: 9.15 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 1300 | lm loss value: 7.004925E+00 | lm loss PPL: 1.102048E+03 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    1300 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:34:51,853] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1300 is ready now!
[default1]:[2023-07-25 16:34:51,855] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1300 is ready now!
[default1]:[2023-07-25 16:34:51,856] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1300 is ready now!
[default0]:[2023-07-25 16:34:51,785] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1300 is about to be saved!
[default0]:[2023-07-25 16:34:51,853] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1300/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:34:51,853] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1300/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:36:04,407] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1300/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:36:04,432] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1300 is ready now!
[default0]:  successfully saved checkpoint at iteration    1300 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.25, Latency(second): 72.773
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (72772.11, 72782.78)
[default0]:[2023-07-25 16:36:18,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=1305, skipped=0, lr=[5.697262933333334e-06, 5.697262933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:36:18,069] [INFO] [timer.py:215:stop] epoch=0/micro_step=1305/global_step=1305, RunningAvgSamplesPerSec=1.5894367806008107, CurrSamplesPerSec=1.566793117330907, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1305/109863281 | consumed samples:         5220 | consumed tokens:     10690560 | elapsed time per iteration (ms): 19541.6 | learning rate: 5.697E-06 | global batch size:     4 | lm loss: 6.869115E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.205 | TFLOPs: 1.25 |
[default0]:[2023-07-25 16:36:31,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=0, lr=[5.719108266666668e-06, 5.719108266666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:36:31,499] [INFO] [timer.py:215:stop] epoch=0/micro_step=1310/global_step=1310, RunningAvgSamplesPerSec=1.5894228334393772, CurrSamplesPerSec=1.5969370272167793, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1310/109863281 | consumed samples:         5240 | consumed tokens:     10731520 | elapsed time per iteration (ms): 2684.7 | learning rate: 5.719E-06 | global batch size:     4 | lm loss: 7.016060E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.490 | TFLOPs: 9.07 |
[default0]:[2023-07-25 16:36:45,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=1315, skipped=0, lr=[5.740953600000001e-06, 5.740953600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:36:45,210] [INFO] [timer.py:215:stop] epoch=0/micro_step=1315/global_step=1315, RunningAvgSamplesPerSec=1.589305313996635, CurrSamplesPerSec=1.636222418055346, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1315/109863281 | consumed samples:         5260 | consumed tokens:     10772480 | elapsed time per iteration (ms): 2741.2 | learning rate: 5.741E-06 | global batch size:     4 | lm loss: 6.805984E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.459 | TFLOPs: 8.89 |
[default0]:[2023-07-25 16:36:58,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=0, lr=[5.762798933333334e-06, 5.762798933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:36:58,412] [INFO] [timer.py:215:stop] epoch=0/micro_step=1320/global_step=1320, RunningAvgSamplesPerSec=1.589348821847602, CurrSamplesPerSec=1.4984885797651537, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1320/109863281 | consumed samples:         5280 | consumed tokens:     10813440 | elapsed time per iteration (ms): 2623.8 | learning rate: 5.763E-06 | global batch size:     4 | lm loss: 6.927718E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.524 | TFLOPs: 9.28 |
[default0]:[2023-07-25 16:37:11,761] [INFO] [logging.py:96:log_dist] [Rank 0] step=1325, skipped=0, lr=[5.784644266666667e-06, 5.784644266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:37:11,777] [INFO] [timer.py:215:stop] epoch=0/micro_step=1325/global_step=1325, RunningAvgSamplesPerSec=1.589334492965272, CurrSamplesPerSec=1.7200596646024233, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1325/109863281 | consumed samples:         5300 | consumed tokens:     10854400 | elapsed time per iteration (ms): 2673.6 | learning rate: 5.785E-06 | global batch size:     4 | lm loss: 6.720021E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.496 | TFLOPs: 9.11 |
[default0]:[2023-07-25 16:37:24,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=0, lr=[5.806489600000001e-06, 5.806489600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:37:24,787] [INFO] [timer.py:215:stop] epoch=0/micro_step=1330/global_step=1330, RunningAvgSamplesPerSec=1.5895152304943891, CurrSamplesPerSec=1.6377664850539748, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1330/109863281 | consumed samples:         5320 | consumed tokens:     10895360 | elapsed time per iteration (ms): 2596.3 | learning rate: 5.806E-06 | global batch size:     4 | lm loss: 6.826298E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.541 | TFLOPs: 9.38 |
[default0]:[2023-07-25 16:37:38,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=1335, skipped=0, lr=[5.828334933333334e-06, 5.828334933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:37:38,074] [INFO] [timer.py:215:stop] epoch=0/micro_step=1335/global_step=1335, RunningAvgSamplesPerSec=1.5896119588219224, CurrSamplesPerSec=1.5191411860430961, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1335/109863281 | consumed samples:         5340 | consumed tokens:     10936320 | elapsed time per iteration (ms): 2658.5 | learning rate: 5.828E-06 | global batch size:     4 | lm loss: 6.822687E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.505 | TFLOPs: 9.16 |
[default0]:[2023-07-25 16:37:51,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=0, lr=[5.850180266666667e-06, 5.850180266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:37:51,468] [INFO] [timer.py:215:stop] epoch=0/micro_step=1340/global_step=1340, RunningAvgSamplesPerSec=1.5896876206491553, CurrSamplesPerSec=1.5515589142279274, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1340/109863281 | consumed samples:         5360 | consumed tokens:     10977280 | elapsed time per iteration (ms): 2665.1 | learning rate: 5.850E-06 | global batch size:     4 | lm loss: 6.936719E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.501 | TFLOPs: 9.14 |
[default0]:[2023-07-25 16:38:04,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=1345, skipped=0, lr=[5.872025600000001e-06, 5.872025600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:38:04,695] [INFO] [timer.py:215:stop] epoch=0/micro_step=1345/global_step=1345, RunningAvgSamplesPerSec=1.5897205938085435, CurrSamplesPerSec=1.4872701174695122, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1345/109863281 | consumed samples:         5380 | consumed tokens:     11018240 | elapsed time per iteration (ms): 2637.1 | learning rate: 5.872E-06 | global batch size:     4 | lm loss: 6.851942E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.517 | TFLOPs: 9.24 |
[default0]:[2023-07-25 16:38:18,208] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=0, lr=[5.8938709333333345e-06, 5.8938709333333345e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:38:18,235] [INFO] [timer.py:215:stop] epoch=0/micro_step=1350/global_step=1350, RunningAvgSamplesPerSec=1.5896143229800603, CurrSamplesPerSec=1.4762521540315587, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1350/109863281 | consumed samples:         5400 | consumed tokens:     11059200 | elapsed time per iteration (ms): 2701.3 | learning rate: 5.894E-06 | global batch size:     4 | lm loss: 6.787914E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.481 | TFLOPs: 9.02 |
[default0]:saving checkpoint at iteration    1350 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:38:18,402] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1350 is about to be saved!
[default0]:[2023-07-25 16:38:18,438] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1350/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:38:18,438] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1350/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:38:18,437] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1350 is ready now!
[default1]:[2023-07-25 16:38:18,439] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1350 is ready now!
[default1]:[2023-07-25 16:38:18,440] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1350 is ready now!
[default0]:[2023-07-25 16:39:29,058] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1350/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:39:29,079] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1350 is ready now!
[default0]:  successfully saved checkpoint at iteration    1350 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.846
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70846.50, 70864.21)
[default0]:[2023-07-25 16:39:42,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=1355, skipped=0, lr=[5.915716266666668e-06, 5.915716266666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:39:42,847] [INFO] [timer.py:215:stop] epoch=0/micro_step=1355/global_step=1355, RunningAvgSamplesPerSec=1.5894760109390558, CurrSamplesPerSec=1.576150964326668, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1355/109863281 | consumed samples:         5420 | consumed tokens:     11100160 | elapsed time per iteration (ms): 16916.2 | learning rate: 5.916E-06 | global batch size:     4 | lm loss: 6.827982E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.236 | TFLOPs: 1.44 |
[default0]:[2023-07-25 16:39:56,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=0, lr=[5.937561599999999e-06, 5.937561599999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:39:56,264] [INFO] [timer.py:215:stop] epoch=0/micro_step=1360/global_step=1360, RunningAvgSamplesPerSec=1.5894831912908698, CurrSamplesPerSec=1.5524912875332861, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1360/109863281 | consumed samples:         5440 | consumed tokens:     11141120 | elapsed time per iteration (ms): 2679.2 | learning rate: 5.938E-06 | global batch size:     4 | lm loss: 7.039102E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.493 | TFLOPs: 9.09 |
[default0]:[2023-07-25 16:40:09,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=1365, skipped=0, lr=[5.959406933333333e-06, 5.959406933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:40:09,664] [INFO] [timer.py:215:stop] epoch=0/micro_step=1365/global_step=1365, RunningAvgSamplesPerSec=1.5895094845029267, CurrSamplesPerSec=1.571572707462584, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1365/109863281 | consumed samples:         5460 | consumed tokens:     11182080 | elapsed time per iteration (ms): 2677.5 | learning rate: 5.959E-06 | global batch size:     4 | lm loss: 6.718090E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.494 | TFLOPs: 9.10 |
[default0]:[2023-07-25 16:40:23,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=0, lr=[5.981252266666667e-06, 5.981252266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:40:23,117] [INFO] [timer.py:215:stop] epoch=0/micro_step=1370/global_step=1370, RunningAvgSamplesPerSec=1.5894732234320006, CurrSamplesPerSec=1.5664971680725153, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1370/109863281 | consumed samples:         5480 | consumed tokens:     11223040 | elapsed time per iteration (ms): 2686.4 | learning rate: 5.981E-06 | global batch size:     4 | lm loss: 6.757596E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.489 | TFLOPs: 9.07 |
[default0]:[2023-07-25 16:40:36,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=1375, skipped=0, lr=[6.0030976e-06, 6.0030976e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:40:36,454] [INFO] [timer.py:215:stop] epoch=0/micro_step=1375/global_step=1375, RunningAvgSamplesPerSec=1.5895667881093567, CurrSamplesPerSec=1.7657654926903281, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1375/109863281 | consumed samples:         5500 | consumed tokens:     11264000 | elapsed time per iteration (ms): 2660.0 | learning rate: 6.003E-06 | global batch size:     4 | lm loss: 6.855605E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.504 | TFLOPs: 9.16 |
[default0]:[2023-07-25 16:40:49,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=0, lr=[6.024942933333333e-06, 6.024942933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:40:49,521] [INFO] [timer.py:215:stop] epoch=0/micro_step=1380/global_step=1380, RunningAvgSamplesPerSec=1.589756433412636, CurrSamplesPerSec=1.6308217750639995, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1380/109863281 | consumed samples:         5520 | consumed tokens:     11304960 | elapsed time per iteration (ms): 2598.9 | learning rate: 6.025E-06 | global batch size:     4 | lm loss: 6.805531E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.539 | TFLOPs: 9.37 |
[default0]:[2023-07-25 16:41:02,888] [INFO] [logging.py:96:log_dist] [Rank 0] step=1385, skipped=0, lr=[6.0467882666666665e-06, 6.0467882666666665e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:41:02,897] [INFO] [timer.py:215:stop] epoch=0/micro_step=1385/global_step=1385, RunningAvgSamplesPerSec=1.5897466181077868, CurrSamplesPerSec=1.5764883467090842, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1385/109863281 | consumed samples:         5540 | consumed tokens:     11345920 | elapsed time per iteration (ms): 2669.0 | learning rate: 6.047E-06 | global batch size:     4 | lm loss: 6.892715E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.499 | TFLOPs: 9.13 |
[default0]:[2023-07-25 16:41:15,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=0, lr=[6.0686336e-06, 6.0686336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:41:15,908] [INFO] [timer.py:215:stop] epoch=0/micro_step=1390/global_step=1390, RunningAvgSamplesPerSec=1.5899446748383799, CurrSamplesPerSec=1.697463095804203, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1390/109863281 | consumed samples:         5560 | consumed tokens:     11386880 | elapsed time per iteration (ms): 2598.2 | learning rate: 6.069E-06 | global batch size:     4 | lm loss: 6.776550E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.540 | TFLOPs: 9.37 |
[default0]:[2023-07-25 16:41:29,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=1395, skipped=0, lr=[6.090478933333333e-06, 6.090478933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:41:29,726] [INFO] [timer.py:215:stop] epoch=0/micro_step=1395/global_step=1395, RunningAvgSamplesPerSec=1.589770498894817, CurrSamplesPerSec=1.5804745281411186, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1395/109863281 | consumed samples:         5580 | consumed tokens:     11427840 | elapsed time per iteration (ms): 2755.5 | learning rate: 6.090E-06 | global batch size:     4 | lm loss: 6.826098E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.452 | TFLOPs: 8.84 |
[default0]:[2023-07-25 16:41:42,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=0, lr=[6.112324266666666e-06, 6.112324266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:41:42,925] [INFO] [timer.py:215:stop] epoch=0/micro_step=1400/global_step=1400, RunningAvgSamplesPerSec=1.5898532670058099, CurrSamplesPerSec=1.55225284714036, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1400/109863281 | consumed samples:         5600 | consumed tokens:     11468800 | elapsed time per iteration (ms): 2634.6 | learning rate: 6.112E-06 | global batch size:     4 | lm loss: 6.711710E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.518 | TFLOPs: 9.24 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 1400 | lm loss value: 7.084085E+00 | lm loss PPL: 1.192831E+03 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    1400 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:41:53,634] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1400 is about to be saved!
[default0]:[2023-07-25 16:41:53,725] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1400 is ready now!
[default1]:[2023-07-25 16:41:53,703] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1400 is ready now!
[default0]:[2023-07-25 16:41:53,702] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1400/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:41:53,702] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1400/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 16:41:53,704] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1400 is ready now!
[default0]:[2023-07-25 16:43:04,865] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1400/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:43:04,886] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1400 is ready now!
[default0]:  successfully saved checkpoint at iteration    1400 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.43
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71416.95, 71429.79)
[default0]:[2023-07-25 16:43:18,224] [INFO] [logging.py:96:log_dist] [Rank 0] step=1405, skipped=0, lr=[6.1341695999999995e-06, 6.1341695999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:43:18,230] [INFO] [timer.py:215:stop] epoch=0/micro_step=1405/global_step=1405, RunningAvgSamplesPerSec=1.5899651960537233, CurrSamplesPerSec=1.6458238872413748, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1405/109863281 | consumed samples:         5620 | consumed tokens:     11509760 | elapsed time per iteration (ms): 19051.2 | learning rate: 6.134E-06 | global batch size:     4 | lm loss: 6.935976E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.210 | TFLOPs: 1.28 |
[default0]:[2023-07-25 16:43:31,693] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=0, lr=[6.156014933333334e-06, 6.156014933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:43:31,714] [INFO] [timer.py:215:stop] epoch=0/micro_step=1410/global_step=1410, RunningAvgSamplesPerSec=1.5899324770084506, CurrSamplesPerSec=1.6930052044834745, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1410/109863281 | consumed samples:         5640 | consumed tokens:     11550720 | elapsed time per iteration (ms): 2699.7 | learning rate: 6.156E-06 | global batch size:     4 | lm loss: 6.615642E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.482 | TFLOPs: 9.02 |
[default0]:[2023-07-25 16:43:44,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=1415, skipped=0, lr=[6.177860266666667e-06, 6.177860266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:43:44,790] [INFO] [timer.py:215:stop] epoch=0/micro_step=1415/global_step=1415, RunningAvgSamplesPerSec=1.5900414190536158, CurrSamplesPerSec=1.6102316392574736, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1415/109863281 | consumed samples:         5660 | consumed tokens:     11591680 | elapsed time per iteration (ms): 2610.5 | learning rate: 6.178E-06 | global batch size:     4 | lm loss: 6.669673E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.532 | TFLOPs: 9.33 |
[default0]:[2023-07-25 16:43:58,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=0, lr=[6.1997056e-06, 6.1997056e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:43:58,336] [INFO] [timer.py:215:stop] epoch=0/micro_step=1420/global_step=1420, RunningAvgSamplesPerSec=1.5899736643792215, CurrSamplesPerSec=1.579557919657584, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1420/109863281 | consumed samples:         5680 | consumed tokens:     11632640 | elapsed time per iteration (ms): 2693.4 | learning rate: 6.200E-06 | global batch size:     4 | lm loss: 6.870206E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.485 | TFLOPs: 9.04 |
[default0]:[2023-07-25 16:44:11,725] [INFO] [logging.py:96:log_dist] [Rank 0] step=1425, skipped=0, lr=[6.221550933333333e-06, 6.221550933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:44:11,741] [INFO] [timer.py:215:stop] epoch=0/micro_step=1425/global_step=1425, RunningAvgSamplesPerSec=1.589929022687742, CurrSamplesPerSec=1.566029699575767, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1425/109863281 | consumed samples:         5700 | consumed tokens:     11673600 | elapsed time per iteration (ms): 2681.2 | learning rate: 6.222E-06 | global batch size:     4 | lm loss: 6.897224E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.492 | TFLOPs: 9.08 |
[default0]:[2023-07-25 16:44:25,345] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=0, lr=[6.243396266666667e-06, 6.243396266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:44:25,354] [INFO] [timer.py:215:stop] epoch=0/micro_step=1430/global_step=1430, RunningAvgSamplesPerSec=1.5898546314682924, CurrSamplesPerSec=1.528123643713228, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1430/109863281 | consumed samples:         5720 | consumed tokens:     11714560 | elapsed time per iteration (ms): 2723.5 | learning rate: 6.243E-06 | global batch size:     4 | lm loss: 6.697147E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.469 | TFLOPs: 8.94 |
[default0]:[2023-07-25 16:44:38,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=1435, skipped=0, lr=[6.2652416e-06, 6.2652416e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:44:38,616] [INFO] [timer.py:215:stop] epoch=0/micro_step=1435/global_step=1435, RunningAvgSamplesPerSec=1.5899839899939567, CurrSamplesPerSec=1.6575616719214228, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1435/109863281 | consumed samples:         5740 | consumed tokens:     11755520 | elapsed time per iteration (ms): 2648.0 | learning rate: 6.265E-06 | global batch size:     4 | lm loss: 6.814649E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.511 | TFLOPs: 9.20 |
[default0]:[2023-07-25 16:44:51,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=0, lr=[6.287086933333333e-06, 6.287086933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:44:51,857] [INFO] [timer.py:215:stop] epoch=0/micro_step=1440/global_step=1440, RunningAvgSamplesPerSec=1.5900453309184248, CurrSamplesPerSec=1.5974716530271875, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1440/109863281 | consumed samples:         5760 | consumed tokens:     11796480 | elapsed time per iteration (ms): 2634.2 | learning rate: 6.287E-06 | global batch size:     4 | lm loss: 6.669575E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.518 | TFLOPs: 9.25 |
[default0]:[2023-07-25 16:45:05,015] [INFO] [logging.py:96:log_dist] [Rank 0] step=1445, skipped=0, lr=[6.308932266666666e-06, 6.308932266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:45:05,047] [INFO] [timer.py:215:stop] epoch=0/micro_step=1445/global_step=1445, RunningAvgSamplesPerSec=1.590180569574551, CurrSamplesPerSec=1.5703300271371474, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1445/109863281 | consumed samples:         5780 | consumed tokens:     11837440 | elapsed time per iteration (ms): 2637.1 | learning rate: 6.309E-06 | global batch size:     4 | lm loss: 6.914538E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.517 | TFLOPs: 9.24 |
[default0]:[2023-07-25 16:45:18,745] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=0, lr=[6.3307776e-06, 6.3307776e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:45:18,762] [INFO] [timer.py:215:stop] epoch=0/micro_step=1450/global_step=1450, RunningAvgSamplesPerSec=1.5900804775331128, CurrSamplesPerSec=1.528660953251841, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1450/109863281 | consumed samples:         5800 | consumed tokens:     11878400 | elapsed time per iteration (ms): 2737.3 | learning rate: 6.331E-06 | global batch size:     4 | lm loss: 6.831234E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.461 | TFLOPs: 8.90 |
[default0]:saving checkpoint at iteration    1450 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:45:19,020] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1450 is about to be saved!
[default1]:[2023-07-25 16:45:19,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1450 is ready now!
[default0]:[2023-07-25 16:45:19,079] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1450/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:45:19,079] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1450/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 16:45:19,078] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1450 is ready now!
[default0]:[2023-07-25 16:45:19,089] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1450 is ready now!
[default0]:[2023-07-25 16:46:30,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1450/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:46:30,070] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1450 is ready now!
[default0]:  successfully saved checkpoint at iteration    1450 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71219.21, 71230.20)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.219
[default0]:[2023-07-25 16:46:44,042] [INFO] [logging.py:96:log_dist] [Rank 0] step=1455, skipped=0, lr=[6.352622933333334e-06, 6.352622933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:46:44,078] [INFO] [timer.py:215:stop] epoch=0/micro_step=1455/global_step=1455, RunningAvgSamplesPerSec=1.5898567332637157, CurrSamplesPerSec=1.5049874212228571, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1455/109863281 | consumed samples:         5820 | consumed tokens:     11919360 | elapsed time per iteration (ms): 17053.9 | learning rate: 6.353E-06 | global batch size:     4 | lm loss: 6.817064E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.235 | TFLOPs: 1.43 |
[default0]:[2023-07-25 16:46:57,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=0, lr=[6.374468266666667e-06, 6.374468266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:46:57,445] [INFO] [timer.py:215:stop] epoch=0/micro_step=1460/global_step=1460, RunningAvgSamplesPerSec=1.589854707229771, CurrSamplesPerSec=1.5567898528710093, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1460/109863281 | consumed samples:         5840 | consumed tokens:     11960320 | elapsed time per iteration (ms): 2663.4 | learning rate: 6.374E-06 | global batch size:     4 | lm loss: 6.600204E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.502 | TFLOPs: 9.14 |
[default0]:[2023-07-25 16:47:11,257] [INFO] [logging.py:96:log_dist] [Rank 0] step=1465, skipped=0, lr=[6.3963136e-06, 6.3963136e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:47:11,286] [INFO] [timer.py:215:stop] epoch=0/micro_step=1465/global_step=1465, RunningAvgSamplesPerSec=1.589701161994122, CurrSamplesPerSec=1.5014826693166834, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1465/109863281 | consumed samples:         5860 | consumed tokens:     12001280 | elapsed time per iteration (ms): 2763.1 | learning rate: 6.396E-06 | global batch size:     4 | lm loss: 6.765288E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.448 | TFLOPs: 8.81 |
[default0]:[2023-07-25 16:47:24,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=0, lr=[6.4181589333333335e-06, 6.4181589333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:47:24,764] [INFO] [timer.py:215:stop] epoch=0/micro_step=1470/global_step=1470, RunningAvgSamplesPerSec=1.5896888175856432, CurrSamplesPerSec=1.5983912066655017, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1470/109863281 | consumed samples:         5880 | consumed tokens:     12042240 | elapsed time per iteration (ms): 2697.9 | learning rate: 6.418E-06 | global batch size:     4 | lm loss: 6.738238E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.483 | TFLOPs: 9.03 |
[default0]:[2023-07-25 16:47:38,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=1475, skipped=0, lr=[6.440004266666667e-06, 6.440004266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:47:38,351] [INFO] [timer.py:215:stop] epoch=0/micro_step=1475/global_step=1475, RunningAvgSamplesPerSec=1.5896005633852246, CurrSamplesPerSec=1.5063216300494053, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1475/109863281 | consumed samples:         5900 | consumed tokens:     12083200 | elapsed time per iteration (ms): 2708.0 | learning rate: 6.440E-06 | global batch size:     4 | lm loss: 6.767900E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.477 | TFLOPs: 8.99 |
[default0]:[2023-07-25 16:47:51,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=0, lr=[6.4618496e-06, 6.4618496e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:47:51,788] [INFO] [timer.py:215:stop] epoch=0/micro_step=1480/global_step=1480, RunningAvgSamplesPerSec=1.5895827270177465, CurrSamplesPerSec=1.6003340846358556, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1480/109863281 | consumed samples:         5920 | consumed tokens:     12124160 | elapsed time per iteration (ms): 2673.7 | learning rate: 6.462E-06 | global batch size:     4 | lm loss: 6.906052E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.496 | TFLOPs: 9.11 |
[default0]:[2023-07-25 16:48:04,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=1485, skipped=0, lr=[6.483694933333333e-06, 6.483694933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:48:04,902] [INFO] [timer.py:215:stop] epoch=0/micro_step=1485/global_step=1485, RunningAvgSamplesPerSec=1.5897169195291472, CurrSamplesPerSec=1.6514752000875683, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1485/109863281 | consumed samples:         5940 | consumed tokens:     12165120 | elapsed time per iteration (ms): 2616.8 | learning rate: 6.484E-06 | global batch size:     4 | lm loss: 6.727425E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.529 | TFLOPs: 9.31 |
[default0]:[2023-07-25 16:48:18,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=0, lr=[6.5055402666666666e-06, 6.5055402666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:48:18,215] [INFO] [timer.py:215:stop] epoch=0/micro_step=1490/global_step=1490, RunningAvgSamplesPerSec=1.5897491498000351, CurrSamplesPerSec=1.487250209317777, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1490/109863281 | consumed samples:         5960 | consumed tokens:     12206080 | elapsed time per iteration (ms): 2655.0 | learning rate: 6.506E-06 | global batch size:     4 | lm loss: 6.743692E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.507 | TFLOPs: 9.17 |
[default0]:[2023-07-25 16:48:31,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=1495, skipped=0, lr=[6.5273856e-06, 6.5273856e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:48:31,534] [INFO] [timer.py:215:stop] epoch=0/micro_step=1495/global_step=1495, RunningAvgSamplesPerSec=1.5898705375217512, CurrSamplesPerSec=1.566504920136542, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1495/109863281 | consumed samples:         5980 | consumed tokens:     12247040 | elapsed time per iteration (ms): 2650.9 | learning rate: 6.527E-06 | global batch size:     4 | lm loss: 6.538477E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.509 | TFLOPs: 9.19 |
[default0]:[2023-07-25 16:48:45,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=0, lr=[6.549230933333334e-06, 6.549230933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:48:45,135] [INFO] [timer.py:215:stop] epoch=0/micro_step=1500/global_step=1500, RunningAvgSamplesPerSec=1.589792232859791, CurrSamplesPerSec=1.4357441520943264, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1500/109863281 | consumed samples:         6000 | consumed tokens:     12288000 | elapsed time per iteration (ms): 2717.9 | learning rate: 6.549E-06 | global batch size:     4 | lm loss: 6.820067E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.472 | TFLOPs: 8.96 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 1500 | lm loss value: 6.949294E+00 | lm loss PPL: 1.042414E+03 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    1500 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:48:56,219] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1500 is about to be saved!
[default0]:[2023-07-25 16:48:56,307] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1500/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:48:56,307] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1500/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:48:56,313] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1500 is ready now!
[default1]:[2023-07-25 16:48:56,305] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1500 is ready now!
[default1]:[2023-07-25 16:48:56,305] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1500 is ready now!
[default0]:[2023-07-25 16:50:07,119] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1500/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:50:07,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1500 is ready now!
[default0]:  successfully saved checkpoint at iteration    1500 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.08
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71073.31, 71080.56)
[default0]:[2023-07-25 16:50:09,727] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default0]:[2023-07-25 16:50:09,727] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[default1]:[2023-07-25 16:50:09,722] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default1]:[2023-07-25 16:50:09,734] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[default0]:[2023-07-25 16:50:09,734] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default0]:[2023-07-25 16:50:09,753] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[default1]:[2023-07-25 16:50:09,709] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default1]:[2023-07-25 16:50:09,709] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8192 to 16384
[default0]:[2023-07-25 16:50:20,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=1505, skipped=0, lr=[6.571076266666667e-06, 6.571076266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:50:20,569] [INFO] [timer.py:215:stop] epoch=0/micro_step=1505/global_step=1505, RunningAvgSamplesPerSec=1.5897832335543343, CurrSamplesPerSec=1.6573324334704729, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1505/109863281 | consumed samples:         6020 | consumed tokens:     12328960 | elapsed time per iteration (ms): 19073.8 | learning rate: 6.571E-06 | global batch size:     4 | lm loss: 6.758487E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.210 | TFLOPs: 1.28 |
[default0]:[2023-07-25 16:50:33,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=0, lr=[6.5929216000000004e-06, 6.5929216000000004e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:50:33,999] [INFO] [timer.py:215:stop] epoch=0/micro_step=1510/global_step=1510, RunningAvgSamplesPerSec=1.5898259651051823, CurrSamplesPerSec=1.5833659716883426, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1510/109863281 | consumed samples:         6040 | consumed tokens:     12369920 | elapsed time per iteration (ms): 2684.5 | learning rate: 6.593E-06 | global batch size:     4 | lm loss: 6.753259E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.490 | TFLOPs: 9.07 |
[default0]:[2023-07-25 16:50:47,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=1515, skipped=0, lr=[6.614766933333334e-06, 6.614766933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:50:47,165] [INFO] [timer.py:215:stop] epoch=0/micro_step=1515/global_step=1515, RunningAvgSamplesPerSec=1.5899370049251416, CurrSamplesPerSec=1.570018341755568, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1515/109863281 | consumed samples:         6060 | consumed tokens:     12410880 | elapsed time per iteration (ms): 2628.4 | learning rate: 6.615E-06 | global batch size:     4 | lm loss: 6.785870E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.522 | TFLOPs: 9.27 |
[default0]:[2023-07-25 16:51:00,740] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=0, lr=[6.636612266666667e-06, 6.636612266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:51:00,772] [INFO] [timer.py:215:stop] epoch=0/micro_step=1520/global_step=1520, RunningAvgSamplesPerSec=1.5898657259737394, CurrSamplesPerSec=1.6267476258197022, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1520/109863281 | consumed samples:         6080 | consumed tokens:     12451840 | elapsed time per iteration (ms): 2711.4 | learning rate: 6.637E-06 | global batch size:     4 | lm loss: 6.606076E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.475 | TFLOPs: 8.98 |
[default0]:[2023-07-25 16:51:13,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=1525, skipped=0, lr=[6.6584576e-06, 6.6584576e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:51:13,938] [INFO] [timer.py:215:stop] epoch=0/micro_step=1525/global_step=1525, RunningAvgSamplesPerSec=1.5900359261560357, CurrSamplesPerSec=1.6319499727249878, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1525/109863281 | consumed samples:         6100 | consumed tokens:     12492800 | elapsed time per iteration (ms): 2629.6 | learning rate: 6.658E-06 | global batch size:     4 | lm loss: 6.654980E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.521 | TFLOPs: 9.26 |
[default0]:[2023-07-25 16:51:27,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=0, lr=[6.6803029333333335e-06, 6.6803029333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:51:27,357] [INFO] [timer.py:215:stop] epoch=0/micro_step=1530/global_step=1530, RunningAvgSamplesPerSec=1.5900558512396143, CurrSamplesPerSec=1.4318973321408548, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1530/109863281 | consumed samples:         6120 | consumed tokens:     12533760 | elapsed time per iteration (ms): 2676.4 | learning rate: 6.680E-06 | global batch size:     4 | lm loss: 6.720689E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.495 | TFLOPs: 9.10 |
[default0]:[2023-07-25 16:51:40,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=1535, skipped=0, lr=[6.702148266666667e-06, 6.702148266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:51:40,720] [INFO] [timer.py:215:stop] epoch=0/micro_step=1535/global_step=1535, RunningAvgSamplesPerSec=1.5901177055818259, CurrSamplesPerSec=1.704126356239387, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1535/109863281 | consumed samples:         6140 | consumed tokens:     12574720 | elapsed time per iteration (ms): 2654.3 | learning rate: 6.702E-06 | global batch size:     4 | lm loss: 6.803365E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.507 | TFLOPs: 9.18 |
[default0]:[2023-07-25 16:51:54,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=0, lr=[6.7239936e-06, 6.7239936e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:51:54,418] [INFO] [timer.py:215:stop] epoch=0/micro_step=1540/global_step=1540, RunningAvgSamplesPerSec=1.5899617017476495, CurrSamplesPerSec=1.665134349908735, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1540/109863281 | consumed samples:         6160 | consumed tokens:     12615680 | elapsed time per iteration (ms): 2732.3 | learning rate: 6.724E-06 | global batch size:     4 | lm loss: 6.861840E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.464 | TFLOPs: 8.91 |
[default0]:[2023-07-25 16:52:07,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=1545, skipped=0, lr=[6.745838933333334e-06, 6.745838933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:52:07,771] [INFO] [timer.py:215:stop] epoch=0/micro_step=1545/global_step=1545, RunningAvgSamplesPerSec=1.5899713919276706, CurrSamplesPerSec=1.6045517919161163, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1545/109863281 | consumed samples:         6180 | consumed tokens:     12656640 | elapsed time per iteration (ms): 2672.8 | learning rate: 6.746E-06 | global batch size:     4 | lm loss: 6.851950E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.497 | TFLOPs: 9.11 |
[default0]:[2023-07-25 16:52:21,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=0, lr=[6.767684266666667e-06, 6.767684266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:52:21,480] [INFO] [timer.py:215:stop] epoch=0/micro_step=1550/global_step=1550, RunningAvgSamplesPerSec=1.589841464756167, CurrSamplesPerSec=1.4159614450233482, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1550/109863281 | consumed samples:         6200 | consumed tokens:     12697600 | elapsed time per iteration (ms): 2728.1 | learning rate: 6.768E-06 | global batch size:     4 | lm loss: 6.737291E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.466 | TFLOPs: 8.93 |
[default0]:saving checkpoint at iteration    1550 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:52:21,678] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1550 is about to be saved!
[default0]:[2023-07-25 16:52:21,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1550 is ready now!
[default0]:[2023-07-25 16:52:21,744] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1550/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:52:21,744] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1550/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 16:52:21,747] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1550 is ready now!
[default1]:[2023-07-25 16:52:21,745] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1550 is ready now!
[default0]:[2023-07-25 16:53:32,911] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1550/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:53:32,937] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1550 is ready now!
[default0]:  successfully saved checkpoint at iteration    1550 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.409
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71408.69, 71421.91)
[default0]:[2023-07-25 16:53:48,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=1555, skipped=0, lr=[6.789529600000001e-06, 6.789529600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:53:48,175] [INFO] [timer.py:215:stop] epoch=0/micro_step=1555/global_step=1555, RunningAvgSamplesPerSec=1.5890958215717406, CurrSamplesPerSec=1.4028261687719379, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1555/109863281 | consumed samples:         6220 | consumed tokens:     12738560 | elapsed time per iteration (ms): 17335.0 | learning rate: 6.790E-06 | global batch size:     4 | lm loss: 6.801079E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.231 | TFLOPs: 1.41 |
[default0]:[2023-07-25 16:54:03,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=0, lr=[6.811374933333334e-06, 6.811374933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:54:03,254] [INFO] [timer.py:215:stop] epoch=0/micro_step=1560/global_step=1560, RunningAvgSamplesPerSec=1.5884788408048898, CurrSamplesPerSec=1.3752605477363455, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1560/109863281 | consumed samples:         6240 | consumed tokens:     12779520 | elapsed time per iteration (ms): 3010.1 | learning rate: 6.811E-06 | global batch size:     4 | lm loss: 6.909138E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.329 | TFLOPs: 8.09 |
[default0]:[2023-07-25 16:54:18,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=1565, skipped=0, lr=[6.833220266666667e-06, 6.833220266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:54:18,955] [INFO] [timer.py:215:stop] epoch=0/micro_step=1565/global_step=1565, RunningAvgSamplesPerSec=1.5877146108073135, CurrSamplesPerSec=1.4219617327114198, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1565/109863281 | consumed samples:         6260 | consumed tokens:     12820480 | elapsed time per iteration (ms): 3124.7 | learning rate: 6.833E-06 | global batch size:     4 | lm loss: 6.772522E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.280 | TFLOPs: 7.79 |
[default0]:[2023-07-25 16:54:34,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=0, lr=[6.8550656e-06, 6.8550656e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:54:34,515] [INFO] [timer.py:215:stop] epoch=0/micro_step=1570/global_step=1570, RunningAvgSamplesPerSec=1.586849594058137, CurrSamplesPerSec=1.2742979754440464, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1570/109863281 | consumed samples:         6280 | consumed tokens:     12861440 | elapsed time per iteration (ms): 3109.0 | learning rate: 6.855E-06 | global batch size:     4 | lm loss: 6.786433E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.287 | TFLOPs: 7.83 |
[default0]:[2023-07-25 16:54:49,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=1575, skipped=0, lr=[6.876910933333334e-06, 6.876910933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:54:49,271] [INFO] [timer.py:215:stop] epoch=0/micro_step=1575/global_step=1575, RunningAvgSamplesPerSec=1.5862895781373123, CurrSamplesPerSec=1.4844631961035843, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1575/109863281 | consumed samples:         6300 | consumed tokens:     12902400 | elapsed time per iteration (ms): 2951.2 | learning rate: 6.877E-06 | global batch size:     4 | lm loss: 6.613037E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.355 | TFLOPs: 8.25 |
[default0]:[2023-07-25 16:55:04,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=0, lr=[6.898756266666667e-06, 6.898756266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:55:04,121] [INFO] [timer.py:215:stop] epoch=0/micro_step=1580/global_step=1580, RunningAvgSamplesPerSec=1.5857230105689675, CurrSamplesPerSec=1.450595482659827, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1580/109863281 | consumed samples:         6320 | consumed tokens:     12943360 | elapsed time per iteration (ms): 2947.6 | learning rate: 6.899E-06 | global batch size:     4 | lm loss: 6.881272E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.357 | TFLOPs: 8.26 |
[default0]:[2023-07-25 16:55:19,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=1585, skipped=0, lr=[6.9206016e-06, 6.9206016e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:55:19,438] [INFO] [timer.py:215:stop] epoch=0/micro_step=1585/global_step=1585, RunningAvgSamplesPerSec=1.5849939766758587, CurrSamplesPerSec=1.3252874980557776, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1585/109863281 | consumed samples:         6340 | consumed tokens:     12984320 | elapsed time per iteration (ms): 3059.3 | learning rate: 6.921E-06 | global batch size:     4 | lm loss: 6.723586E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.307 | TFLOPs: 7.96 |
[default0]:[2023-07-25 16:55:35,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=0, lr=[6.942446933333334e-06, 6.942446933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:55:35,223] [INFO] [timer.py:215:stop] epoch=0/micro_step=1590/global_step=1590, RunningAvgSamplesPerSec=1.5842940273121124, CurrSamplesPerSec=1.3738665562586894, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1590/109863281 | consumed samples:         6360 | consumed tokens:     13025280 | elapsed time per iteration (ms): 3137.9 | learning rate: 6.942E-06 | global batch size:     4 | lm loss: 6.767213E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.275 | TFLOPs: 7.76 |
[default0]:[2023-07-25 16:55:50,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=1595, skipped=0, lr=[6.9642922666666675e-06, 6.9642922666666675e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:55:50,266] [INFO] [timer.py:215:stop] epoch=0/micro_step=1595/global_step=1595, RunningAvgSamplesPerSec=1.5836450331924716, CurrSamplesPerSec=1.462899363463423, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1595/109863281 | consumed samples:         6380 | consumed tokens:     13066240 | elapsed time per iteration (ms): 3004.9 | learning rate: 6.964E-06 | global batch size:     4 | lm loss: 6.557027E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.331 | TFLOPs: 8.11 |
[default0]:[2023-07-25 16:56:05,481] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=0, lr=[6.986137600000001e-06, 6.986137600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:56:05,518] [INFO] [timer.py:215:stop] epoch=0/micro_step=1600/global_step=1600, RunningAvgSamplesPerSec=1.5829436490999913, CurrSamplesPerSec=1.4501028680758739, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1600/109863281 | consumed samples:         6400 | consumed tokens:     13107200 | elapsed time per iteration (ms): 3049.3 | learning rate: 6.986E-06 | global batch size:     4 | lm loss: 6.685688E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.312 | TFLOPs: 7.99 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 1600 | lm loss value: 6.923419E+00 | lm loss PPL: 1.015788E+03 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    1600 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 16:56:18,866] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1600 is about to be saved!
[default0]:[2023-07-25 16:56:18,912] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1600/mp_rank_00_model_states.pt
[default0]:[2023-07-25 16:56:18,912] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1600/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 16:56:18,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1600 is ready now!
[default1]:[2023-07-25 16:56:18,910] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1600 is ready now!
[default1]:[2023-07-25 16:56:18,910] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1600 is ready now!
[default0]:[2023-07-25 16:57:40,363] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1600/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 16:57:40,384] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1600 is ready now!
[default0]:  successfully saved checkpoint at iteration    1600 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.23, Latency(second): 81.742
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (81741.78, 81753.27)
[default0]:[2023-07-25 16:57:54,257] [INFO] [logging.py:96:log_dist] [Rank 0] step=1605, skipped=0, lr=[7.007982933333334e-06, 7.007982933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:57:54,273] [INFO] [timer.py:215:stop] epoch=0/micro_step=1605/global_step=1605, RunningAvgSamplesPerSec=1.5829835808225232, CurrSamplesPerSec=1.524913107339703, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1605/109863281 | consumed samples:         6420 | consumed tokens:     13148160 | elapsed time per iteration (ms): 21733.9 | learning rate: 7.008E-06 | global batch size:     4 | lm loss: 6.760564E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.184 | TFLOPs: 1.12 |
[default0]:[2023-07-25 16:58:07,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=0, lr=[7.029828266666667e-06, 7.029828266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:58:07,183] [INFO] [timer.py:215:stop] epoch=0/micro_step=1610/global_step=1610, RunningAvgSamplesPerSec=1.5832192048976712, CurrSamplesPerSec=1.6974041897632375, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1610/109863281 | consumed samples:         6440 | consumed tokens:     13189120 | elapsed time per iteration (ms): 2574.4 | learning rate: 7.030E-06 | global batch size:     4 | lm loss: 6.764606E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.554 | TFLOPs: 9.46 |
[default0]:[2023-07-25 16:58:22,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=1615, skipped=0, lr=[7.0516736000000006e-06, 7.0516736000000006e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:58:22,845] [INFO] [timer.py:215:stop] epoch=0/micro_step=1615/global_step=1615, RunningAvgSamplesPerSec=1.5823645664283543, CurrSamplesPerSec=1.2898416952470133, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1615/109863281 | consumed samples:         6460 | consumed tokens:     13230080 | elapsed time per iteration (ms): 3125.4 | learning rate: 7.052E-06 | global batch size:     4 | lm loss: 6.747887E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.280 | TFLOPs: 7.79 |
[default0]:[2023-07-25 16:58:38,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=0, lr=[7.073518933333334e-06, 7.073518933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:58:38,435] [INFO] [timer.py:215:stop] epoch=0/micro_step=1620/global_step=1620, RunningAvgSamplesPerSec=1.5816967787843719, CurrSamplesPerSec=1.37461759543584, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1620/109863281 | consumed samples:         6480 | consumed tokens:     13271040 | elapsed time per iteration (ms): 3106.5 | learning rate: 7.074E-06 | global batch size:     4 | lm loss: 6.529253E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.288 | TFLOPs: 7.84 |
[default0]:[2023-07-25 16:58:53,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=1625, skipped=0, lr=[7.095364266666667e-06, 7.095364266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:58:53,449] [INFO] [timer.py:215:stop] epoch=0/micro_step=1625/global_step=1625, RunningAvgSamplesPerSec=1.5811005355561407, CurrSamplesPerSec=1.3701960350661784, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1625/109863281 | consumed samples:         6500 | consumed tokens:     13312000 | elapsed time per iteration (ms): 2997.4 | learning rate: 7.095E-06 | global batch size:     4 | lm loss: 6.569409E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.334 | TFLOPs: 8.13 |
[default0]:[2023-07-25 16:59:08,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=0, lr=[7.1172096e-06, 7.1172096e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:59:08,858] [INFO] [timer.py:215:stop] epoch=0/micro_step=1630/global_step=1630, RunningAvgSamplesPerSec=1.5803654171051145, CurrSamplesPerSec=1.3673929912821974, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1630/109863281 | consumed samples:         6520 | consumed tokens:     13352960 | elapsed time per iteration (ms): 3063.1 | learning rate: 7.117E-06 | global batch size:     4 | lm loss: 6.865715E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.306 | TFLOPs: 7.95 |
[default0]:[2023-07-25 16:59:23,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=1635, skipped=0, lr=[7.1390549333333344e-06, 7.1390549333333344e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:59:23,860] [INFO] [timer.py:215:stop] epoch=0/micro_step=1635/global_step=1635, RunningAvgSamplesPerSec=1.5797790728004169, CurrSamplesPerSec=1.4491793502418493, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1635/109863281 | consumed samples:         6540 | consumed tokens:     13393920 | elapsed time per iteration (ms): 3006.1 | learning rate: 7.139E-06 | global batch size:     4 | lm loss: 6.713470E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.331 | TFLOPs: 8.10 |
[default0]:[2023-07-25 16:59:39,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=0, lr=[7.160900266666668e-06, 7.160900266666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:59:39,242] [INFO] [timer.py:215:stop] epoch=0/micro_step=1640/global_step=1640, RunningAvgSamplesPerSec=1.5791062618228113, CurrSamplesPerSec=1.3926640248144515, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1640/109863281 | consumed samples:         6560 | consumed tokens:     13434880 | elapsed time per iteration (ms): 3060.8 | learning rate: 7.161E-06 | global batch size:     4 | lm loss: 6.805209E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.307 | TFLOPs: 7.96 |
[default0]:[2023-07-25 16:59:54,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=1645, skipped=0, lr=[7.182745600000001e-06, 7.182745600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 16:59:54,315] [INFO] [timer.py:215:stop] epoch=0/micro_step=1645/global_step=1645, RunningAvgSamplesPerSec=1.5786352861004114, CurrSamplesPerSec=1.3107844127652835, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1645/109863281 | consumed samples:         6580 | consumed tokens:     13475840 | elapsed time per iteration (ms): 3007.5 | learning rate: 7.183E-06 | global batch size:     4 | lm loss: 6.724699E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.330 | TFLOPs: 8.10 |
[default0]:[2023-07-25 17:00:09,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=0, lr=[7.204590933333334e-06, 7.204590933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:00:09,458] [INFO] [timer.py:215:stop] epoch=0/micro_step=1650/global_step=1650, RunningAvgSamplesPerSec=1.5779929560166654, CurrSamplesPerSec=1.4555890969217078, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1650/109863281 | consumed samples:         6600 | consumed tokens:     13516800 | elapsed time per iteration (ms): 3020.6 | learning rate: 7.205E-06 | global batch size:     4 | lm loss: 6.775190E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.324 | TFLOPs: 8.06 |
[default0]:saving checkpoint at iteration    1650 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:00:09,610] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1650 is about to be saved!
[default1]:[2023-07-25 17:00:09,648] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1650 is ready now!
[default0]:[2023-07-25 17:00:09,647] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1650 is ready now!
[default1]:[2023-07-25 17:00:09,649] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1650 is ready now!
[default0]:[2023-07-25 17:00:09,646] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1650/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:00:09,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1650/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 17:01:20,469] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1650/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:01:20,490] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1650 is ready now!
[default0]:  successfully saved checkpoint at iteration    1650 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.071
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71070.30, 71081.59)
[default0]:[2023-07-25 17:01:35,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=1655, skipped=0, lr=[7.2264362666666675e-06, 7.2264362666666675e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:01:36,022] [INFO] [timer.py:215:stop] epoch=0/micro_step=1655/global_step=1655, RunningAvgSamplesPerSec=1.5772838439084274, CurrSamplesPerSec=1.3422345128796291, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1655/109863281 | consumed samples:         6620 | consumed tokens:     13557760 | elapsed time per iteration (ms): 17314.9 | learning rate: 7.226E-06 | global batch size:     4 | lm loss: 6.692375E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.231 | TFLOPs: 1.41 |
[default0]:[2023-07-25 17:01:51,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=0, lr=[7.248281600000001e-06, 7.248281600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:01:51,354] [INFO] [timer.py:215:stop] epoch=0/micro_step=1660/global_step=1660, RunningAvgSamplesPerSec=1.5765708310737208, CurrSamplesPerSec=1.3187206161632334, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1660/109863281 | consumed samples:         6640 | consumed tokens:     13598720 | elapsed time per iteration (ms): 3058.0 | learning rate: 7.248E-06 | global batch size:     4 | lm loss: 6.687836E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.308 | TFLOPs: 7.96 |
[default0]:[2023-07-25 17:02:06,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=1665, skipped=0, lr=[7.270126933333334e-06, 7.270126933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:02:06,449] [INFO] [timer.py:215:stop] epoch=0/micro_step=1665/global_step=1665, RunningAvgSamplesPerSec=1.5759923268517202, CurrSamplesPerSec=1.2457660501230046, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1665/109863281 | consumed samples:         6660 | consumed tokens:     13639680 | elapsed time per iteration (ms): 3010.2 | learning rate: 7.270E-06 | global batch size:     4 | lm loss: 6.595516E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.329 | TFLOPs: 8.09 |
[default0]:[2023-07-25 17:02:21,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=0, lr=[7.291972266666667e-06, 7.291972266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:02:21,710] [INFO] [timer.py:215:stop] epoch=0/micro_step=1670/global_step=1670, RunningAvgSamplesPerSec=1.5753271614330289, CurrSamplesPerSec=1.3700623228647448, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1670/109863281 | consumed samples:         6680 | consumed tokens:     13680640 | elapsed time per iteration (ms): 3046.9 | learning rate: 7.292E-06 | global batch size:     4 | lm loss: 6.686124E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.313 | TFLOPs: 7.99 |
[default0]:[2023-07-25 17:02:37,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=1675, skipped=0, lr=[7.313817600000001e-06, 7.313817600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:02:37,480] [INFO] [timer.py:215:stop] epoch=0/micro_step=1675/global_step=1675, RunningAvgSamplesPerSec=1.5745455360463936, CurrSamplesPerSec=1.3783742705029403, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1675/109863281 | consumed samples:         6700 | consumed tokens:     13721600 | elapsed time per iteration (ms): 3093.3 | learning rate: 7.314E-06 | global batch size:     4 | lm loss: 6.964420E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.293 | TFLOPs: 7.87 |
[default0]:[2023-07-25 17:02:52,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=0, lr=[7.335662933333335e-06, 7.335662933333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:02:52,334] [INFO] [timer.py:215:stop] epoch=0/micro_step=1680/global_step=1680, RunningAvgSamplesPerSec=1.5740978807031245, CurrSamplesPerSec=1.3529020047722304, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1680/109863281 | consumed samples:         6720 | consumed tokens:     13762560 | elapsed time per iteration (ms): 2962.1 | learning rate: 7.336E-06 | global batch size:     4 | lm loss: 6.659956E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.350 | TFLOPs: 8.22 |
[default0]:[2023-07-25 17:03:07,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=1685, skipped=0, lr=[7.357508266666668e-06, 7.357508266666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:03:07,469] [INFO] [timer.py:215:stop] epoch=0/micro_step=1685/global_step=1685, RunningAvgSamplesPerSec=1.573512466119578, CurrSamplesPerSec=1.4900949529178866, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1685/109863281 | consumed samples:         6740 | consumed tokens:     13803520 | elapsed time per iteration (ms): 3028.7 | learning rate: 7.358E-06 | global batch size:     4 | lm loss: 6.724580E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.321 | TFLOPs: 8.04 |
[default0]:[2023-07-25 17:03:23,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=0, lr=[7.379353599999999e-06, 7.379353599999999e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:03:23,193] [INFO] [timer.py:215:stop] epoch=0/micro_step=1690/global_step=1690, RunningAvgSamplesPerSec=1.5727879620961425, CurrSamplesPerSec=1.3961919245272154, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1690/109863281 | consumed samples:         6760 | consumed tokens:     13844480 | elapsed time per iteration (ms): 3177.7 | learning rate: 7.379E-06 | global batch size:     4 | lm loss: 6.559007E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.259 | TFLOPs: 7.66 |
[default0]:[2023-07-25 17:03:38,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=1695, skipped=0, lr=[7.4011989333333335e-06, 7.4011989333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:03:38,925] [INFO] [timer.py:215:stop] epoch=0/micro_step=1695/global_step=1695, RunningAvgSamplesPerSec=1.5720811311638148, CurrSamplesPerSec=1.3383239776036835, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1695/109863281 | consumed samples:         6780 | consumed tokens:     13885440 | elapsed time per iteration (ms): 3090.4 | learning rate: 7.401E-06 | global batch size:     4 | lm loss: 6.610910E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.294 | TFLOPs: 7.88 |
[default0]:[2023-07-25 17:03:54,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=0, lr=[7.423044266666667e-06, 7.423044266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:03:54,362] [INFO] [timer.py:215:stop] epoch=0/micro_step=1700/global_step=1700, RunningAvgSamplesPerSec=1.5713850807630458, CurrSamplesPerSec=1.2614837947247843, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1700/109863281 | consumed samples:         6800 | consumed tokens:     13926400 | elapsed time per iteration (ms): 3081.2 | learning rate: 7.423E-06 | global batch size:     4 | lm loss: 7.004518E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.298 | TFLOPs: 7.90 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 1700 | lm loss value: 6.829819E+00 | lm loss PPL: 9.250231E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    1700 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:04:07,204] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1700 is about to be saved!
[default0]:[2023-07-25 17:04:07,239] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1700/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:04:07,239] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1700/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 17:04:07,241] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1700 is ready now!
[default1]:[2023-07-25 17:04:07,239] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1700 is ready now!
[default0]:[2023-07-25 17:04:07,250] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1700 is ready now!
[default0]:[2023-07-25 17:05:18,459] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1700/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:05:18,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1700 is ready now!
[default0]:  successfully saved checkpoint at iteration    1700 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.515
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71514.24, 71515.50)
[default0]:[2023-07-25 17:05:33,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=1705, skipped=0, lr=[7.4448896e-06, 7.4448896e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:05:33,024] [INFO] [timer.py:215:stop] epoch=0/micro_step=1705/global_step=1705, RunningAvgSamplesPerSec=1.5710879060235676, CurrSamplesPerSec=1.5085716464295993, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1705/109863281 | consumed samples:         6820 | consumed tokens:     13967360 | elapsed time per iteration (ms): 19738.4 | learning rate: 7.445E-06 | global batch size:     4 | lm loss: 6.764317E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.203 | TFLOPs: 1.23 |
[default0]:[2023-07-25 17:05:47,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=0, lr=[7.466734933333333e-06, 7.466734933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:05:47,712] [INFO] [timer.py:215:stop] epoch=0/micro_step=1710/global_step=1710, RunningAvgSamplesPerSec=1.570892047222194, CurrSamplesPerSec=1.374679430637512, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1710/109863281 | consumed samples:         6840 | consumed tokens:     14008320 | elapsed time per iteration (ms): 2916.1 | learning rate: 7.467E-06 | global batch size:     4 | lm loss: 7.010477E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.372 | TFLOPs: 8.35 |
[default0]:[2023-07-25 17:06:02,750] [INFO] [logging.py:96:log_dist] [Rank 0] step=1715, skipped=0, lr=[7.4885802666666666e-06, 7.4885802666666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:06:02,782] [INFO] [timer.py:215:stop] epoch=0/micro_step=1715/global_step=1715, RunningAvgSamplesPerSec=1.5703628490909347, CurrSamplesPerSec=1.4606305625588365, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1715/109863281 | consumed samples:         6860 | consumed tokens:     14049280 | elapsed time per iteration (ms): 3002.6 | learning rate: 7.489E-06 | global batch size:     4 | lm loss: 6.875162E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.332 | TFLOPs: 8.11 |
[default0]:[2023-07-25 17:06:18,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=0, lr=[7.5104256e-06, 7.5104256e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:06:18,150] [INFO] [timer.py:215:stop] epoch=0/micro_step=1720/global_step=1720, RunningAvgSamplesPerSec=1.5697126821263079, CurrSamplesPerSec=1.3266728261491858, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1720/109863281 | consumed samples:         6880 | consumed tokens:     14090240 | elapsed time per iteration (ms): 3065.9 | learning rate: 7.510E-06 | global batch size:     4 | lm loss: 6.666882E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.305 | TFLOPs: 7.94 |
[default0]:[2023-07-25 17:06:33,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=1725, skipped=0, lr=[7.532270933333333e-06, 7.532270933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:06:33,177] [INFO] [timer.py:215:stop] epoch=0/micro_step=1725/global_step=1725, RunningAvgSamplesPerSec=1.5691657858297374, CurrSamplesPerSec=1.3462459583852704, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1725/109863281 | consumed samples:         6900 | consumed tokens:     14131200 | elapsed time per iteration (ms): 3000.9 | learning rate: 7.532E-06 | global batch size:     4 | lm loss: 6.747412E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.333 | TFLOPs: 8.12 |
[default0]:[2023-07-25 17:06:49,169] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=0, lr=[7.554116266666666e-06, 7.554116266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:06:49,207] [INFO] [timer.py:215:stop] epoch=0/micro_step=1730/global_step=1730, RunningAvgSamplesPerSec=1.5683925268608014, CurrSamplesPerSec=1.3352754016817552, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1730/109863281 | consumed samples:         6920 | consumed tokens:     14172160 | elapsed time per iteration (ms): 3202.6 | learning rate: 7.554E-06 | global batch size:     4 | lm loss: 6.852177E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.249 | TFLOPs: 7.60 |
[default0]:[2023-07-25 17:07:04,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=1735, skipped=0, lr=[7.5759616e-06, 7.5759616e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:07:04,839] [INFO] [timer.py:215:stop] epoch=0/micro_step=1735/global_step=1735, RunningAvgSamplesPerSec=1.567681769983604, CurrSamplesPerSec=1.2964217843964816, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1735/109863281 | consumed samples:         6940 | consumed tokens:     14213120 | elapsed time per iteration (ms): 3118.7 | learning rate: 7.576E-06 | global batch size:     4 | lm loss: 6.703942E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.283 | TFLOPs: 7.81 |
[default0]:[2023-07-25 17:07:20,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=0, lr=[7.597806933333334e-06, 7.597806933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:07:20,483] [INFO] [timer.py:215:stop] epoch=0/micro_step=1740/global_step=1740, RunningAvgSamplesPerSec=1.567020898385367, CurrSamplesPerSec=1.424665770002956, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1740/109863281 | consumed samples:         6960 | consumed tokens:     14254080 | elapsed time per iteration (ms): 3160.5 | learning rate: 7.598E-06 | global batch size:     4 | lm loss: 6.725842E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.266 | TFLOPs: 7.71 |
[default0]:[2023-07-25 17:07:35,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=1745, skipped=0, lr=[7.619652266666667e-06, 7.619652266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:07:35,673] [INFO] [timer.py:215:stop] epoch=0/micro_step=1745/global_step=1745, RunningAvgSamplesPerSec=1.5665176864248478, CurrSamplesPerSec=1.3977508473083708, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1745/109863281 | consumed samples:         6980 | consumed tokens:     14295040 | elapsed time per iteration (ms): 2990.3 | learning rate: 7.620E-06 | global batch size:     4 | lm loss: 6.787311E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.338 | TFLOPs: 8.14 |
[default0]:[2023-07-25 17:07:50,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=0, lr=[7.6414976e-06, 7.6414976e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:07:50,430] [INFO] [timer.py:215:stop] epoch=0/micro_step=1750/global_step=1750, RunningAvgSamplesPerSec=1.5660900827675222, CurrSamplesPerSec=1.3739109969034375, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1750/109863281 | consumed samples:         7000 | consumed tokens:     14336000 | elapsed time per iteration (ms): 2955.3 | learning rate: 7.641E-06 | global batch size:     4 | lm loss: 6.779932E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.353 | TFLOPs: 8.24 |
[default0]:saving checkpoint at iteration    1750 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:07:50,716] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1750 is about to be saved!
[default1]:[2023-07-25 17:07:50,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1750 is ready now!
[default0]:[2023-07-25 17:07:50,783] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1750 is ready now!
[default1]:[2023-07-25 17:07:50,772] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1750 is ready now!
[default0]:[2023-07-25 17:07:50,774] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1750/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:07:50,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1750/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 17:09:01,674] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1750/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:09:01,698] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1750 is ready now!
[default0]:  successfully saved checkpoint at iteration    1750 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.158
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71155.36, 71169.43)
[default0]:[2023-07-25 17:09:17,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=1755, skipped=0, lr=[7.663342933333333e-06, 7.663342933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:09:17,060] [INFO] [timer.py:215:stop] epoch=0/micro_step=1755/global_step=1755, RunningAvgSamplesPerSec=1.5655135701598801, CurrSamplesPerSec=1.2718420731645839, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1755/109863281 | consumed samples:         7020 | consumed tokens:     14376960 | elapsed time per iteration (ms): 17340.4 | learning rate: 7.663E-06 | global batch size:     4 | lm loss: 6.838655E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.231 | TFLOPs: 1.40 |
[default0]:[2023-07-25 17:09:32,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=0, lr=[7.685188266666666e-06, 7.685188266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:09:32,472] [INFO] [timer.py:215:stop] epoch=0/micro_step=1760/global_step=1760, RunningAvgSamplesPerSec=1.564951716120161, CurrSamplesPerSec=1.4064250254422423, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1760/109863281 | consumed samples:         7040 | consumed tokens:     14417920 | elapsed time per iteration (ms): 3038.4 | learning rate: 7.685E-06 | global batch size:     4 | lm loss: 6.432764E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.316 | TFLOPs: 8.02 |
[default0]:[2023-07-25 17:09:47,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=1765, skipped=0, lr=[7.7070336e-06, 7.7070336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:09:47,452] [INFO] [timer.py:215:stop] epoch=0/micro_step=1765/global_step=1765, RunningAvgSamplesPerSec=1.564455542955657, CurrSamplesPerSec=1.330297721348359, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1765/109863281 | consumed samples:         7060 | consumed tokens:     14458880 | elapsed time per iteration (ms): 2999.9 | learning rate: 7.707E-06 | global batch size:     4 | lm loss: 6.646346E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.333 | TFLOPs: 8.12 |
[default0]:[2023-07-25 17:10:02,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=0, lr=[7.728878933333334e-06, 7.728878933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:10:02,656] [INFO] [timer.py:215:stop] epoch=0/micro_step=1770/global_step=1770, RunningAvgSamplesPerSec=1.5638946595262642, CurrSamplesPerSec=1.3584473495678806, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1770/109863281 | consumed samples:         7080 | consumed tokens:     14499840 | elapsed time per iteration (ms): 3020.3 | learning rate: 7.729E-06 | global batch size:     4 | lm loss: 6.718770E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.324 | TFLOPs: 8.06 |
[default0]:[2023-07-25 17:10:17,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=1775, skipped=0, lr=[7.750724266666667e-06, 7.750724266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:10:17,720] [INFO] [timer.py:215:stop] epoch=0/micro_step=1775/global_step=1775, RunningAvgSamplesPerSec=1.563345330821823, CurrSamplesPerSec=1.4171835923050886, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1775/109863281 | consumed samples:         7100 | consumed tokens:     14540800 | elapsed time per iteration (ms): 3010.9 | learning rate: 7.751E-06 | global batch size:     4 | lm loss: 6.699875E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.328 | TFLOPs: 8.09 |
[default0]:[2023-07-25 17:10:32,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=0, lr=[7.7725696e-06, 7.7725696e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:10:32,611] [INFO] [timer.py:215:stop] epoch=0/micro_step=1780/global_step=1780, RunningAvgSamplesPerSec=1.5629188794498798, CurrSamplesPerSec=1.4542935390046785, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1780/109863281 | consumed samples:         7120 | consumed tokens:     14581760 | elapsed time per iteration (ms): 2969.0 | learning rate: 7.773E-06 | global batch size:     4 | lm loss: 6.664205E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.347 | TFLOPs: 8.20 |
[default0]:[2023-07-25 17:10:48,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=1785, skipped=0, lr=[7.794414933333334e-06, 7.794414933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:10:48,437] [INFO] [timer.py:215:stop] epoch=0/micro_step=1785/global_step=1785, RunningAvgSamplesPerSec=1.5621034479348426, CurrSamplesPerSec=1.4008804466594311, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1785/109863281 | consumed samples:         7140 | consumed tokens:     14622720 | elapsed time per iteration (ms): 3160.7 | learning rate: 7.794E-06 | global batch size:     4 | lm loss: 6.791636E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.266 | TFLOPs: 7.71 |
[default0]:[2023-07-25 17:11:03,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=0, lr=[7.816260266666667e-06, 7.816260266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:11:03,259] [INFO] [timer.py:215:stop] epoch=0/micro_step=1790/global_step=1790, RunningAvgSamplesPerSec=1.5616821247477999, CurrSamplesPerSec=1.4450972195867455, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1790/109863281 | consumed samples:         7160 | consumed tokens:     14663680 | elapsed time per iteration (ms): 2965.2 | learning rate: 7.816E-06 | global batch size:     4 | lm loss: 6.773379E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.349 | TFLOPs: 8.21 |
[default0]:[2023-07-25 17:11:18,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=1795, skipped=0, lr=[7.8381056e-06, 7.8381056e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:11:18,437] [INFO] [timer.py:215:stop] epoch=0/micro_step=1795/global_step=1795, RunningAvgSamplesPerSec=1.561121612722124, CurrSamplesPerSec=1.4601466132989385, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1795/109863281 | consumed samples:         7180 | consumed tokens:     14704640 | elapsed time per iteration (ms): 3030.8 | learning rate: 7.838E-06 | global batch size:     4 | lm loss: 6.597741E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.320 | TFLOPs: 8.04 |
[default0]:[2023-07-25 17:11:33,154] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=0, lr=[7.859950933333334e-06, 7.859950933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:11:33,174] [INFO] [timer.py:215:stop] epoch=0/micro_step=1800/global_step=1800, RunningAvgSamplesPerSec=1.5607670648751932, CurrSamplesPerSec=1.3921306011956884, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1800/109863281 | consumed samples:         7200 | consumed tokens:     14745600 | elapsed time per iteration (ms): 2933.4 | learning rate: 7.860E-06 | global batch size:     4 | lm loss: 6.763604E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.364 | TFLOPs: 8.30 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 1800 | lm loss value: 6.840066E+00 | lm loss PPL: 9.345512E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    1800 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:11:46,738] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1800 is about to be saved!
[default1]:[2023-07-25 17:11:46,827] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1800 is ready now!
[default0]:[2023-07-25 17:11:46,824] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1800/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:11:46,824] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1800/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 17:11:46,824] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1800 is ready now!
[default1]:[2023-07-25 17:11:46,826] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1800 is ready now!
[default0]:[2023-07-25 17:12:57,559] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1800/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:12:57,581] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1800 is ready now!
[default0]:  successfully saved checkpoint at iteration    1800 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.993
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70992.41, 71007.85)
[default0]:[2023-07-25 17:13:12,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=1805, skipped=0, lr=[7.881796266666667e-06, 7.881796266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:13:12,944] [INFO] [timer.py:215:stop] epoch=0/micro_step=1805/global_step=1805, RunningAvgSamplesPerSec=1.5602765820925757, CurrSamplesPerSec=1.390112933518127, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1805/109863281 | consumed samples:         7220 | consumed tokens:     14786560 | elapsed time per iteration (ms): 19949.7 | learning rate: 7.882E-06 | global batch size:     4 | lm loss: 6.678352E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.201 | TFLOPs: 1.22 |
[default0]:[2023-07-25 17:13:27,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=0, lr=[7.9036416e-06, 7.9036416e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:13:28,006] [INFO] [timer.py:215:stop] epoch=0/micro_step=1810/global_step=1810, RunningAvgSamplesPerSec=1.5597846723924624, CurrSamplesPerSec=1.4178069596633747, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1810/109863281 | consumed samples:         7240 | consumed tokens:     14827520 | elapsed time per iteration (ms): 2995.2 | learning rate: 7.904E-06 | global batch size:     4 | lm loss: 6.662283E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.335 | TFLOPs: 8.13 |
[default0]:[2023-07-25 17:13:42,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=1815, skipped=0, lr=[7.925486933333333e-06, 7.925486933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:13:42,728] [INFO] [timer.py:215:stop] epoch=0/micro_step=1815/global_step=1815, RunningAvgSamplesPerSec=1.5593670764722236, CurrSamplesPerSec=1.3719799778713118, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1815/109863281 | consumed samples:         7260 | consumed tokens:     14868480 | elapsed time per iteration (ms): 2942.3 | learning rate: 7.925E-06 | global batch size:     4 | lm loss: 6.699400E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.359 | TFLOPs: 8.28 |
[default0]:[2023-07-25 17:13:57,717] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=0, lr=[7.947332266666667e-06, 7.947332266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:13:57,738] [INFO] [timer.py:215:stop] epoch=0/micro_step=1820/global_step=1820, RunningAvgSamplesPerSec=1.5589110241158954, CurrSamplesPerSec=1.4588520668094689, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1820/109863281 | consumed samples:         7280 | consumed tokens:     14909440 | elapsed time per iteration (ms): 2995.8 | learning rate: 7.947E-06 | global batch size:     4 | lm loss: 6.779196E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.335 | TFLOPs: 8.13 |
[default0]:[2023-07-25 17:14:12,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=1825, skipped=0, lr=[7.9691776e-06, 7.9691776e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:14:12,206] [INFO] [timer.py:215:stop] epoch=0/micro_step=1825/global_step=1825, RunningAvgSamplesPerSec=1.5586206625012977, CurrSamplesPerSec=1.3945967117965885, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1825/109863281 | consumed samples:         7300 | consumed tokens:     14950400 | elapsed time per iteration (ms): 2889.8 | learning rate: 7.969E-06 | global batch size:     4 | lm loss: 6.794673E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.384 | TFLOPs: 8.43 |
[default0]:[2023-07-25 17:14:27,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=0, lr=[7.991022933333333e-06, 7.991022933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:14:27,258] [INFO] [timer.py:215:stop] epoch=0/micro_step=1830/global_step=1830, RunningAvgSamplesPerSec=1.5581496789799512, CurrSamplesPerSec=1.3971789623550186, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1830/109863281 | consumed samples:         7320 | consumed tokens:     14991360 | elapsed time per iteration (ms): 3001.2 | learning rate: 7.991E-06 | global batch size:     4 | lm loss: 6.783585E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.333 | TFLOPs: 8.12 |
[default0]:[2023-07-25 17:14:42,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=1835, skipped=0, lr=[8.012868266666666e-06, 8.012868266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:14:42,612] [INFO] [timer.py:215:stop] epoch=0/micro_step=1835/global_step=1835, RunningAvgSamplesPerSec=1.5575858756320062, CurrSamplesPerSec=1.3855105066045643, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1835/109863281 | consumed samples:         7340 | consumed tokens:     15032320 | elapsed time per iteration (ms): 3062.7 | learning rate: 8.013E-06 | global batch size:     4 | lm loss: 6.774332E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.306 | TFLOPs: 7.95 |
[default0]:[2023-07-25 17:14:58,343] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=0, lr=[8.0347136e-06, 8.0347136e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:14:58,362] [INFO] [timer.py:215:stop] epoch=0/micro_step=1840/global_step=1840, RunningAvgSamplesPerSec=1.5569843350327652, CurrSamplesPerSec=1.5155910914346673, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1840/109863281 | consumed samples:         7360 | consumed tokens:     15073280 | elapsed time per iteration (ms): 3154.9 | learning rate: 8.035E-06 | global batch size:     4 | lm loss: 6.651077E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.268 | TFLOPs: 7.72 |
[default0]:[2023-07-25 17:15:13,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=1845, skipped=0, lr=[8.056558933333333e-06, 8.056558933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:15:13,897] [INFO] [timer.py:215:stop] epoch=0/micro_step=1845/global_step=1845, RunningAvgSamplesPerSec=1.556413314345855, CurrSamplesPerSec=1.3052909076070243, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1845/109863281 | consumed samples:         7380 | consumed tokens:     15114240 | elapsed time per iteration (ms): 3094.0 | learning rate: 8.057E-06 | global batch size:     4 | lm loss: 6.565857E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.293 | TFLOPs: 7.87 |
[default0]:[2023-07-25 17:15:29,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=0, lr=[8.078404266666666e-06, 8.078404266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:15:29,935] [INFO] [timer.py:215:stop] epoch=0/micro_step=1850/global_step=1850, RunningAvgSamplesPerSec=1.5556376834407781, CurrSamplesPerSec=1.1335013193496326, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1850/109863281 | consumed samples:         7400 | consumed tokens:     15155200 | elapsed time per iteration (ms): 3197.1 | learning rate: 8.078E-06 | global batch size:     4 | lm loss: 6.653455E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.251 | TFLOPs: 7.62 |
[default0]:saving checkpoint at iteration    1850 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:15:30,148] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1850 is about to be saved!
[default0]:[2023-07-25 17:15:30,198] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1850/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:15:30,198] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1850/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 17:15:30,201] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1850 is ready now!
[default0]:[2023-07-25 17:15:30,199] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1850 is ready now!
[default1]:[2023-07-25 17:15:30,200] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1850 is ready now!
[default0]:[2023-07-25 17:16:41,939] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1850/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:16:41,949] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1850 is ready now!
[default0]:  successfully saved checkpoint at iteration    1850 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 72.065
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (72065.47, 72091.72)
[default0]:[2023-07-25 17:16:57,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=1855, skipped=0, lr=[8.100249600000001e-06, 8.100249600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:16:57,485] [INFO] [timer.py:215:stop] epoch=0/micro_step=1855/global_step=1855, RunningAvgSamplesPerSec=1.5551924779359414, CurrSamplesPerSec=1.3500282885101333, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1855/109863281 | consumed samples:         7420 | consumed tokens:     15196160 | elapsed time per iteration (ms): 17500.4 | learning rate: 8.100E-06 | global batch size:     4 | lm loss: 6.699744E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.229 | TFLOPs: 1.39 |
[default0]:[2023-07-25 17:17:12,222] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=0, lr=[8.122094933333334e-06, 8.122094933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:17:12,233] [INFO] [timer.py:215:stop] epoch=0/micro_step=1860/global_step=1860, RunningAvgSamplesPerSec=1.5548404048564288, CurrSamplesPerSec=1.4608077221462286, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1860/109863281 | consumed samples:         7440 | consumed tokens:     15237120 | elapsed time per iteration (ms): 2951.3 | learning rate: 8.122E-06 | global batch size:     4 | lm loss: 6.695661E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.355 | TFLOPs: 8.25 |
[default0]:[2023-07-25 17:17:27,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=1865, skipped=0, lr=[8.143940266666668e-06, 8.143940266666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:17:27,426] [INFO] [timer.py:215:stop] epoch=0/micro_step=1865/global_step=1865, RunningAvgSamplesPerSec=1.5543631463451228, CurrSamplesPerSec=1.4392503401200645, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1865/109863281 | consumed samples:         7460 | consumed tokens:     15278080 | elapsed time per iteration (ms): 3024.4 | learning rate: 8.144E-06 | global batch size:     4 | lm loss: 6.606982E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.323 | TFLOPs: 8.05 |
[default0]:[2023-07-25 17:17:42,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=0, lr=[8.165785600000001e-06, 8.165785600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:17:42,490] [INFO] [timer.py:215:stop] epoch=0/micro_step=1870/global_step=1870, RunningAvgSamplesPerSec=1.5539393793974208, CurrSamplesPerSec=1.4845144230616085, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1870/109863281 | consumed samples:         7480 | consumed tokens:     15319040 | elapsed time per iteration (ms): 3004.2 | learning rate: 8.166E-06 | global batch size:     4 | lm loss: 6.500402E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.331 | TFLOPs: 8.11 |
[default0]:[2023-07-25 17:17:58,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=1875, skipped=0, lr=[8.187630933333334e-06, 8.187630933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:17:58,668] [INFO] [timer.py:215:stop] epoch=0/micro_step=1875/global_step=1875, RunningAvgSamplesPerSec=1.553199701035826, CurrSamplesPerSec=1.4176815235557974, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1875/109863281 | consumed samples:         7500 | consumed tokens:     15360000 | elapsed time per iteration (ms): 3225.2 | learning rate: 8.188E-06 | global batch size:     4 | lm loss: 6.648265E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.240 | TFLOPs: 7.55 |
[default0]:[2023-07-25 17:18:13,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=0, lr=[8.209476266666667e-06, 8.209476266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:18:13,893] [INFO] [timer.py:215:stop] epoch=0/micro_step=1880/global_step=1880, RunningAvgSamplesPerSec=1.552746381542688, CurrSamplesPerSec=1.3815445579645467, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1880/109863281 | consumed samples:         7520 | consumed tokens:     15400960 | elapsed time per iteration (ms): 3034.6 | learning rate: 8.209E-06 | global batch size:     4 | lm loss: 6.803379E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.318 | TFLOPs: 8.03 |
[default0]:[2023-07-25 17:18:29,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=1885, skipped=0, lr=[8.2313216e-06, 8.2313216e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:18:29,260] [INFO] [timer.py:215:stop] epoch=0/micro_step=1885/global_step=1885, RunningAvgSamplesPerSec=1.5521912012492327, CurrSamplesPerSec=1.3330620152297812, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1885/109863281 | consumed samples:         7540 | consumed tokens:     15441920 | elapsed time per iteration (ms): 3065.2 | learning rate: 8.231E-06 | global batch size:     4 | lm loss: 6.596278E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.305 | TFLOPs: 7.95 |
[default0]:[2023-07-25 17:18:44,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=0, lr=[8.253166933333334e-06, 8.253166933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:18:44,381] [INFO] [timer.py:215:stop] epoch=0/micro_step=1890/global_step=1890, RunningAvgSamplesPerSec=1.5517243589647192, CurrSamplesPerSec=1.2865152212030806, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1890/109863281 | consumed samples:         7560 | consumed tokens:     15482880 | elapsed time per iteration (ms): 3011.8 | learning rate: 8.253E-06 | global batch size:     4 | lm loss: 6.518841E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.328 | TFLOPs: 8.09 |
[default0]:[2023-07-25 17:19:00,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=1895, skipped=0, lr=[8.275012266666667e-06, 8.275012266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:19:00,108] [INFO] [timer.py:215:stop] epoch=0/micro_step=1895/global_step=1895, RunningAvgSamplesPerSec=1.5510701007541712, CurrSamplesPerSec=1.1165663043302476, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1895/109863281 | consumed samples:         7580 | consumed tokens:     15523840 | elapsed time per iteration (ms): 3143.0 | learning rate: 8.275E-06 | global batch size:     4 | lm loss: 6.658067E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.273 | TFLOPs: 7.75 |
[default0]:[2023-07-25 17:19:15,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=0, lr=[8.2968576e-06, 8.2968576e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:19:15,178] [INFO] [timer.py:215:stop] epoch=0/micro_step=1900/global_step=1900, RunningAvgSamplesPerSec=1.550606571294081, CurrSamplesPerSec=1.4147145622232817, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1900/109863281 | consumed samples:         7600 | consumed tokens:     15564800 | elapsed time per iteration (ms): 3013.5 | learning rate: 8.297E-06 | global batch size:     4 | lm loss: 6.708650E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.327 | TFLOPs: 8.08 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 1900 | lm loss value: 6.873736E+00 | lm loss PPL: 9.665532E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    1900 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:19:28,397] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1900 is about to be saved!
[default1]:[2023-07-25 17:19:28,512] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1900 is ready now!
[default0]:[2023-07-25 17:19:28,510] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1900/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:19:28,510] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1900/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 17:19:28,510] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1900 is ready now!
[default1]:[2023-07-25 17:19:28,513] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1900 is ready now!
[default0]:[2023-07-25 17:20:38,996] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1900/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:20:39,013] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1900 is ready now!
[default0]:  successfully saved checkpoint at iteration    1900 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.783
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70771.13, 70783.10)
[default0]:[2023-07-25 17:20:55,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=1905, skipped=0, lr=[8.318702933333334e-06, 8.318702933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:20:55,339] [INFO] [timer.py:215:stop] epoch=0/micro_step=1905/global_step=1905, RunningAvgSamplesPerSec=1.5498821368615305, CurrSamplesPerSec=1.3905196408681366, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1905/109863281 | consumed samples:         7620 | consumed tokens:     15605760 | elapsed time per iteration (ms): 20020.6 | learning rate: 8.319E-06 | global batch size:     4 | lm loss: 6.641228E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.200 | TFLOPs: 1.22 |
[default0]:[2023-07-25 17:21:09,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=0, lr=[8.340548266666667e-06, 8.340548266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:21:09,991] [INFO] [timer.py:215:stop] epoch=0/micro_step=1910/global_step=1910, RunningAvgSamplesPerSec=1.549589384518013, CurrSamplesPerSec=1.5005206185889726, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1910/109863281 | consumed samples:         7640 | consumed tokens:     15646720 | elapsed time per iteration (ms): 2921.6 | learning rate: 8.341E-06 | global batch size:     4 | lm loss: 6.653546E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.369 | TFLOPs: 8.34 |
[default0]:[2023-07-25 17:21:25,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=1915, skipped=0, lr=[8.3623936e-06, 8.3623936e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:21:25,076] [INFO] [timer.py:215:stop] epoch=0/micro_step=1915/global_step=1915, RunningAvgSamplesPerSec=1.549154362988736, CurrSamplesPerSec=1.3310117947372517, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1915/109863281 | consumed samples:         7660 | consumed tokens:     15687680 | elapsed time per iteration (ms): 3002.8 | learning rate: 8.362E-06 | global batch size:     4 | lm loss: 6.617823E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.332 | TFLOPs: 8.11 |
[default0]:[2023-07-25 17:21:40,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=0, lr=[8.384238933333334e-06, 8.384238933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:21:40,037] [INFO] [timer.py:215:stop] epoch=0/micro_step=1920/global_step=1920, RunningAvgSamplesPerSec=1.5487751536327174, CurrSamplesPerSec=1.374255593833041, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1920/109863281 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 2994.3 | learning rate: 8.384E-06 | global batch size:     4 | lm loss: 6.606136E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.336 | TFLOPs: 8.13 |
[default0]:[2023-07-25 17:21:55,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=1925, skipped=0, lr=[8.406084266666667e-06, 8.406084266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:21:55,789] [INFO] [timer.py:215:stop] epoch=0/micro_step=1925/global_step=1925, RunningAvgSamplesPerSec=1.5482930637116372, CurrSamplesPerSec=1.4154122975363015, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1925/109863281 | consumed samples:         7700 | consumed tokens:     15769600 | elapsed time per iteration (ms): 3138.3 | learning rate: 8.406E-06 | global batch size:     4 | lm loss: 6.589764E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.275 | TFLOPs: 7.76 |
[default0]:[2023-07-25 17:22:10,876] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=0, lr=[8.4279296e-06, 8.4279296e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:22:10,890] [INFO] [timer.py:215:stop] epoch=0/micro_step=1930/global_step=1930, RunningAvgSamplesPerSec=1.5478824544837064, CurrSamplesPerSec=1.3843502363481284, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1930/109863281 | consumed samples:         7720 | consumed tokens:     15810560 | elapsed time per iteration (ms): 3011.4 | learning rate: 8.428E-06 | global batch size:     4 | lm loss: 6.526064E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.328 | TFLOPs: 8.09 |
[default0]:[2023-07-25 17:22:25,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=1935, skipped=0, lr=[8.449774933333333e-06, 8.449774933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:22:25,774] [INFO] [timer.py:215:stop] epoch=0/micro_step=1935/global_step=1935, RunningAvgSamplesPerSec=1.5474967947878286, CurrSamplesPerSec=1.4592457996015198, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1935/109863281 | consumed samples:         7740 | consumed tokens:     15851520 | elapsed time per iteration (ms): 2971.4 | learning rate: 8.450E-06 | global batch size:     4 | lm loss: 6.506927E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.346 | TFLOPs: 8.20 |
[default0]:[2023-07-25 17:22:40,469] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=0, lr=[8.471620266666668e-06, 8.471620266666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:22:40,481] [INFO] [timer.py:215:stop] epoch=0/micro_step=1940/global_step=1940, RunningAvgSamplesPerSec=1.5472170046030362, CurrSamplesPerSec=1.4732261692248534, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1940/109863281 | consumed samples:         7760 | consumed tokens:     15892480 | elapsed time per iteration (ms): 2943.9 | learning rate: 8.472E-06 | global batch size:     4 | lm loss: 6.501615E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.359 | TFLOPs: 8.27 |
[default0]:[2023-07-25 17:22:56,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=1945, skipped=0, lr=[8.493465600000002e-06, 8.493465600000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:22:56,241] [INFO] [timer.py:215:stop] epoch=0/micro_step=1945/global_step=1945, RunningAvgSamplesPerSec=1.5465905542155252, CurrSamplesPerSec=1.3751102914086506, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1945/109863281 | consumed samples:         7780 | consumed tokens:     15933440 | elapsed time per iteration (ms): 3136.0 | learning rate: 8.493E-06 | global batch size:     4 | lm loss: 6.815767E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.275 | TFLOPs: 7.77 |
[default0]:[2023-07-25 17:23:11,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=0, lr=[8.515310933333335e-06, 8.515310933333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:23:11,845] [INFO] [timer.py:215:stop] epoch=0/micro_step=1950/global_step=1950, RunningAvgSamplesPerSec=1.5460962963238984, CurrSamplesPerSec=1.412904688986012, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1950/109863281 | consumed samples:         7800 | consumed tokens:     15974400 | elapsed time per iteration (ms): 3079.3 | learning rate: 8.515E-06 | global batch size:     4 | lm loss: 6.635537E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.299 | TFLOPs: 7.91 |
[default0]:saving checkpoint at iteration    1950 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:23:12,110] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1950 is about to be saved!
[default1]:[2023-07-25 17:23:12,199] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1950 is ready now!
[default0]:[2023-07-25 17:23:12,218] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1950 is ready now!
[default1]:[2023-07-25 17:23:12,196] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1950 is ready now!
[default0]:[2023-07-25 17:23:12,197] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1950/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:23:12,197] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1950/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 17:24:23,560] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step1950/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:24:23,581] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1950 is ready now!
[default0]:  successfully saved checkpoint at iteration    1950 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.855
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71852.19, 71859.70)
[default0]:[2023-07-25 17:24:39,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=1955, skipped=0, lr=[8.537156266666668e-06, 8.537156266666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:24:39,104] [INFO] [timer.py:215:stop] epoch=0/micro_step=1955/global_step=1955, RunningAvgSamplesPerSec=1.5456914152974284, CurrSamplesPerSec=1.3662108970223141, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1955/109863281 | consumed samples:         7820 | consumed tokens:     16015360 | elapsed time per iteration (ms): 17435.4 | learning rate: 8.537E-06 | global batch size:     4 | lm loss: 6.705230E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.229 | TFLOPs: 1.40 |
[default0]:[2023-07-25 17:24:54,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=0, lr=[8.559001600000001e-06, 8.559001600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:24:54,367] [INFO] [timer.py:215:stop] epoch=0/micro_step=1960/global_step=1960, RunningAvgSamplesPerSec=1.545253046942809, CurrSamplesPerSec=1.3854576468481967, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1960/109863281 | consumed samples:         7840 | consumed tokens:     16056320 | elapsed time per iteration (ms): 3035.8 | learning rate: 8.559E-06 | global batch size:     4 | lm loss: 6.686656E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.318 | TFLOPs: 8.02 |
[default0]:[2023-07-25 17:25:09,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=1965, skipped=0, lr=[8.580846933333335e-06, 8.580846933333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:25:09,082] [INFO] [timer.py:215:stop] epoch=0/micro_step=1965/global_step=1965, RunningAvgSamplesPerSec=1.5449242118139614, CurrSamplesPerSec=1.4391835472822796, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1965/109863281 | consumed samples:         7860 | consumed tokens:     16097280 | elapsed time per iteration (ms): 2944.3 | learning rate: 8.581E-06 | global batch size:     4 | lm loss: 6.704868E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.359 | TFLOPs: 8.27 |
[default0]:[2023-07-25 17:25:24,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=0, lr=[8.602692266666668e-06, 8.602692266666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:25:24,189] [INFO] [timer.py:215:stop] epoch=0/micro_step=1970/global_step=1970, RunningAvgSamplesPerSec=1.544518387282826, CurrSamplesPerSec=1.396180189392729, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1970/109863281 | consumed samples:         7880 | consumed tokens:     16138240 | elapsed time per iteration (ms): 3012.8 | learning rate: 8.603E-06 | global batch size:     4 | lm loss: 6.692833E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.328 | TFLOPs: 8.08 |
[default0]:[2023-07-25 17:25:39,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=1975, skipped=0, lr=[8.624537600000001e-06, 8.624537600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:25:39,992] [INFO] [timer.py:215:stop] epoch=0/micro_step=1975/global_step=1975, RunningAvgSamplesPerSec=1.5439823516487217, CurrSamplesPerSec=1.2739743981294527, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1975/109863281 | consumed samples:         7900 | consumed tokens:     16179200 | elapsed time per iteration (ms): 3154.7 | learning rate: 8.625E-06 | global batch size:     4 | lm loss: 6.771832E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.268 | TFLOPs: 7.72 |
[default0]:[2023-07-25 17:25:55,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=0, lr=[8.646382933333334e-06, 8.646382933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:25:55,404] [INFO] [timer.py:215:stop] epoch=0/micro_step=1980/global_step=1980, RunningAvgSamplesPerSec=1.5434609632826986, CurrSamplesPerSec=1.3631627894581269, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1980/109863281 | consumed samples:         7920 | consumed tokens:     16220160 | elapsed time per iteration (ms): 3077.5 | learning rate: 8.646E-06 | global batch size:     4 | lm loss: 6.779622E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.300 | TFLOPs: 7.91 |
[default0]:[2023-07-25 17:26:10,487] [INFO] [fused_optimizer.py:362:_update_scale] 
[default0]:Grad overflow on iteration 1984
[default0]:[2023-07-25 17:26:10,490] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384 to 8192.0
[default1]:[2023-07-25 17:26:10,476] [INFO] [fused_optimizer.py:362:_update_scale] 
[default1]:Grad overflow on iteration 1984
[default1]:[2023-07-25 17:26:10,476] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384 to 8192.0
[default1]:[2023-07-25 17:26:10,476] [INFO] [fused_optimizer.py:362:_update_scale] 
[default1]:Grad overflow on iteration 1984
[default1]:[2023-07-25 17:26:10,490] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384 to 8192.0
[default0]:[2023-07-25 17:26:10,490] [INFO] [fused_optimizer.py:362:_update_scale] 
[default0]:Grad overflow on iteration 1984
[default0]:[2023-07-25 17:26:10,517] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384 to 8192.0
[default0]:[2023-07-25 17:26:10,517] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384, reducing to 8192.0
[default0]:[2023-07-25 17:26:10,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=1985, skipped=1, lr=[8.6638592e-06, 8.6638592e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:26:10,517] [INFO] [timer.py:215:stop] epoch=0/micro_step=1985/global_step=1985, RunningAvgSamplesPerSec=1.5430773984692279, CurrSamplesPerSec=1.4073799548120836, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1985/109863281 | consumed samples:         7940 | consumed tokens:     16261120 | elapsed time per iteration (ms): 3013.0 | learning rate: 8.664E-06 | global batch size:     4 | lm loss: 6.378132E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.328 | TFLOPs: 8.08 |
[default0]:[2023-07-25 17:26:26,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=1, lr=[8.6900736e-06, 8.6900736e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:26:26,860] [INFO] [timer.py:215:stop] epoch=0/micro_step=1990/global_step=1990, RunningAvgSamplesPerSec=1.542379861757144, CurrSamplesPerSec=1.3751631535444933, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1990/109863281 | consumed samples:         7960 | consumed tokens:     16302080 | elapsed time per iteration (ms): 3270.0 | learning rate: 8.690E-06 | global batch size:     4 | lm loss: 6.541274E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.223 | TFLOPs: 7.45 |
[default0]:[2023-07-25 17:26:41,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=1995, skipped=1, lr=[8.711918933333334e-06, 8.711918933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:26:41,626] [INFO] [timer.py:215:stop] epoch=0/micro_step=1995/global_step=1995, RunningAvgSamplesPerSec=1.5420807614121759, CurrSamplesPerSec=1.4724832091066873, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     1995/109863281 | consumed samples:         7980 | consumed tokens:     16343040 | elapsed time per iteration (ms): 2936.2 | learning rate: 8.712E-06 | global batch size:     4 | lm loss: 6.624794E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.362 | TFLOPs: 8.29 |
[default0]:[2023-07-25 17:26:56,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=1, lr=[8.733764266666667e-06, 8.733764266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:26:56,560] [INFO] [timer.py:215:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=1.5417000822062727, CurrSamplesPerSec=1.3953458740685165, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2000/109863281 | consumed samples:         8000 | consumed tokens:     16384000 | elapsed time per iteration (ms): 2990.5 | learning rate: 8.734E-06 | global batch size:     4 | lm loss: 6.544817E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.338 | TFLOPs: 8.14 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 2000 | lm loss value: 6.793738E+00 | lm loss PPL: 8.922429E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    2000 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:27:10,231] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
[default0]:[2023-07-25 17:27:10,278] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2000/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:27:10,278] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2000/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 17:27:10,277] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
[default0]:[2023-07-25 17:27:10,288] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
[default1]:[2023-07-25 17:27:10,280] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
[default0]:[2023-07-25 17:28:21,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2000/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:28:21,123] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
[default0]:  successfully saved checkpoint at iteration    2000 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.156
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71156.05, 71171.62)
[default0]:[2023-07-25 17:28:36,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=2005, skipped=1, lr=[8.7556096e-06, 8.7556096e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:28:36,616] [INFO] [timer.py:215:stop] epoch=0/micro_step=2005/global_step=2005, RunningAvgSamplesPerSec=1.5413201890543389, CurrSamplesPerSec=1.3648349597603804, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2005/109863281 | consumed samples:         8020 | consumed tokens:     16424960 | elapsed time per iteration (ms): 19992.1 | learning rate: 8.756E-06 | global batch size:     4 | lm loss: 6.597767E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.200 | TFLOPs: 1.22 |
[default0]:[2023-07-25 17:28:51,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=1, lr=[8.777454933333334e-06, 8.777454933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:28:51,773] [INFO] [timer.py:215:stop] epoch=0/micro_step=2010/global_step=2010, RunningAvgSamplesPerSec=1.540912096738295, CurrSamplesPerSec=1.3553366227646766, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2010/109863281 | consumed samples:         8040 | consumed tokens:     16465920 | elapsed time per iteration (ms): 3023.1 | learning rate: 8.777E-06 | global batch size:     4 | lm loss: 6.710096E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.323 | TFLOPs: 8.06 |
[default0]:[2023-07-25 17:29:07,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=2015, skipped=1, lr=[8.799300266666667e-06, 8.799300266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:29:07,311] [INFO] [timer.py:215:stop] epoch=0/micro_step=2015/global_step=2015, RunningAvgSamplesPerSec=1.5404452956793433, CurrSamplesPerSec=1.3634078304451867, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2015/109863281 | consumed samples:         8060 | consumed tokens:     16506880 | elapsed time per iteration (ms): 3144.1 | learning rate: 8.799E-06 | global batch size:     4 | lm loss: 6.688258E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.272 | TFLOPs: 7.75 |
[default0]:[2023-07-25 17:29:22,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=1, lr=[8.8211456e-06, 8.8211456e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:29:22,510] [INFO] [timer.py:215:stop] epoch=0/micro_step=2020/global_step=2020, RunningAvgSamplesPerSec=1.5401210821737807, CurrSamplesPerSec=1.4244117616843857, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2020/109863281 | consumed samples:         8080 | consumed tokens:     16547840 | elapsed time per iteration (ms): 2999.2 | learning rate: 8.821E-06 | global batch size:     4 | lm loss: 6.681873E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.334 | TFLOPs: 8.12 |
[default0]:[2023-07-25 17:29:37,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=2025, skipped=1, lr=[8.842990933333334e-06, 8.842990933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:29:37,541] [INFO] [timer.py:215:stop] epoch=0/micro_step=2025/global_step=2025, RunningAvgSamplesPerSec=1.5397746197780782, CurrSamplesPerSec=1.4394937357903625, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2025/109863281 | consumed samples:         8100 | consumed tokens:     16588800 | elapsed time per iteration (ms): 2990.8 | learning rate: 8.843E-06 | global batch size:     4 | lm loss: 6.717658E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.337 | TFLOPs: 8.14 |
[default0]:[2023-07-25 17:29:52,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=1, lr=[8.864836266666667e-06, 8.864836266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:29:52,209] [INFO] [timer.py:215:stop] epoch=0/micro_step=2030/global_step=2030, RunningAvgSamplesPerSec=1.5395450327444669, CurrSamplesPerSec=1.4972752297712706, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2030/109863281 | consumed samples:         8120 | consumed tokens:     16629760 | elapsed time per iteration (ms): 2934.6 | learning rate: 8.865E-06 | global batch size:     4 | lm loss: 6.480659E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.363 | TFLOPs: 8.30 |
[default0]:[2023-07-25 17:30:07,169] [INFO] [logging.py:96:log_dist] [Rank 0] step=2035, skipped=1, lr=[8.8866816e-06, 8.8866816e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:30:07,185] [INFO] [timer.py:215:stop] epoch=0/micro_step=2035/global_step=2035, RunningAvgSamplesPerSec=1.5392074996530325, CurrSamplesPerSec=1.3250790953910399, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2035/109863281 | consumed samples:         8140 | consumed tokens:     16670720 | elapsed time per iteration (ms): 2986.4 | learning rate: 8.887E-06 | global batch size:     4 | lm loss: 6.680716E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.339 | TFLOPs: 8.16 |
[default0]:[2023-07-25 17:30:22,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=1, lr=[8.908526933333333e-06, 8.908526933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:30:22,528] [INFO] [timer.py:215:stop] epoch=0/micro_step=2040/global_step=2040, RunningAvgSamplesPerSec=1.538762723840181, CurrSamplesPerSec=1.3411340922576782, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2040/109863281 | consumed samples:         8160 | consumed tokens:     16711680 | elapsed time per iteration (ms): 3055.9 | learning rate: 8.909E-06 | global batch size:     4 | lm loss: 6.871489E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.309 | TFLOPs: 7.97 |
[default0]:[2023-07-25 17:30:38,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=2045, skipped=1, lr=[8.930372266666667e-06, 8.930372266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:30:38,465] [INFO] [timer.py:215:stop] epoch=0/micro_step=2045/global_step=2045, RunningAvgSamplesPerSec=1.5382549076559122, CurrSamplesPerSec=1.3677242908114355, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2045/109863281 | consumed samples:         8180 | consumed tokens:     16752640 | elapsed time per iteration (ms): 3183.8 | learning rate: 8.930E-06 | global batch size:     4 | lm loss: 6.390385E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.256 | TFLOPs: 7.65 |
[default0]:[2023-07-25 17:30:53,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=1, lr=[8.9522176e-06, 8.9522176e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:30:53,362] [INFO] [timer.py:215:stop] epoch=0/micro_step=2050/global_step=2050, RunningAvgSamplesPerSec=1.5379409894139446, CurrSamplesPerSec=1.449949722990313, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2050/109863281 | consumed samples:         8200 | consumed tokens:     16793600 | elapsed time per iteration (ms): 2967.6 | learning rate: 8.952E-06 | global batch size:     4 | lm loss: 6.785831E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.348 | TFLOPs: 8.21 |
[default0]:saving checkpoint at iteration    2050 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:30:53,529] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2050 is about to be saved!
[default1]:[2023-07-25 17:30:53,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2050 is ready now!
[default1]:[2023-07-25 17:30:53,611] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2050 is ready now!
[default0]:[2023-07-25 17:30:53,610] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2050/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:30:53,610] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2050/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 17:32:04,144] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2050/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:32:04,169] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2050 is ready now!
[default0]:  successfully saved checkpoint at iteration    2050 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.786
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70785.73, 70796.61)
[default0]:[2023-07-25 17:32:20,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=2055, skipped=1, lr=[8.974062933333333e-06, 8.974062933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:32:20,449] [INFO] [timer.py:215:stop] epoch=0/micro_step=2055/global_step=2055, RunningAvgSamplesPerSec=1.5373146853428545, CurrSamplesPerSec=1.3138100195812104, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2055/109863281 | consumed samples:         8220 | consumed tokens:     16834560 | elapsed time per iteration (ms): 17417.6 | learning rate: 8.974E-06 | global batch size:     4 | lm loss: 6.656930E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.230 | TFLOPs: 1.40 |
[default0]:[2023-07-25 17:32:35,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=1, lr=[8.995908266666666e-06, 8.995908266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:32:35,671] [INFO] [timer.py:215:stop] epoch=0/micro_step=2060/global_step=2060, RunningAvgSamplesPerSec=1.536923510620275, CurrSamplesPerSec=1.4269876056022726, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2060/109863281 | consumed samples:         8240 | consumed tokens:     16875520 | elapsed time per iteration (ms): 3030.9 | learning rate: 8.996E-06 | global batch size:     4 | lm loss: 6.527197E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.320 | TFLOPs: 8.04 |
[default0]:[2023-07-25 17:32:50,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=2065, skipped=1, lr=[9.0177536e-06, 9.0177536e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:32:50,672] [INFO] [timer.py:215:stop] epoch=0/micro_step=2065/global_step=2065, RunningAvgSamplesPerSec=1.5365741300476823, CurrSamplesPerSec=1.4333500385906823, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2065/109863281 | consumed samples:         8260 | consumed tokens:     16916480 | elapsed time per iteration (ms): 2989.7 | learning rate: 9.018E-06 | global batch size:     4 | lm loss: 6.782796E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.338 | TFLOPs: 8.15 |
[default0]:[2023-07-25 17:33:05,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=1, lr=[9.039598933333333e-06, 9.039598933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:33:05,708] [INFO] [timer.py:215:stop] epoch=0/micro_step=2070/global_step=2070, RunningAvgSamplesPerSec=1.5362362148977227, CurrSamplesPerSec=1.3810551991856514, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2070/109863281 | consumed samples:         8280 | consumed tokens:     16957440 | elapsed time per iteration (ms): 3003.1 | learning rate: 9.040E-06 | global batch size:     4 | lm loss: 6.377074E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.332 | TFLOPs: 8.11 |
[default0]:[2023-07-25 17:33:21,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=2075, skipped=1, lr=[9.061444266666666e-06, 9.061444266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:33:21,337] [INFO] [timer.py:215:stop] epoch=0/micro_step=2075/global_step=2075, RunningAvgSamplesPerSec=1.5358048120037653, CurrSamplesPerSec=1.4505581079508443, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2075/109863281 | consumed samples:         8300 | consumed tokens:     16998400 | elapsed time per iteration (ms): 3118.8 | learning rate: 9.061E-06 | global batch size:     4 | lm loss: 6.750701E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.283 | TFLOPs: 7.81 |
[default0]:[2023-07-25 17:33:36,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=2080, skipped=1, lr=[9.0832896e-06, 9.0832896e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:33:36,558] [INFO] [timer.py:215:stop] epoch=0/micro_step=2080/global_step=2080, RunningAvgSamplesPerSec=1.5354218710667489, CurrSamplesPerSec=1.3430878935918553, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2080/109863281 | consumed samples:         8320 | consumed tokens:     17039360 | elapsed time per iteration (ms): 3045.2 | learning rate: 9.083E-06 | global batch size:     4 | lm loss: 6.622594E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.314 | TFLOPs: 8.00 |
[default0]:[2023-07-25 17:33:52,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=2085, skipped=1, lr=[9.105134933333333e-06, 9.105134933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:33:52,822] [INFO] [timer.py:215:stop] epoch=0/micro_step=2085/global_step=2085, RunningAvgSamplesPerSec=1.5348845920173533, CurrSamplesPerSec=1.3295922110690357, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2085/109863281 | consumed samples:         8340 | consumed tokens:     17080320 | elapsed time per iteration (ms): 3244.8 | learning rate: 9.105E-06 | global batch size:     4 | lm loss: 6.530390E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.233 | TFLOPs: 7.51 |
[default0]:[2023-07-25 17:34:08,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=2090, skipped=1, lr=[9.126980266666666e-06, 9.126980266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:34:08,762] [INFO] [timer.py:215:stop] epoch=0/micro_step=2090/global_step=2090, RunningAvgSamplesPerSec=1.5343080302988872, CurrSamplesPerSec=1.3716473986903417, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2090/109863281 | consumed samples:         8360 | consumed tokens:     17121280 | elapsed time per iteration (ms): 3179.8 | learning rate: 9.127E-06 | global batch size:     4 | lm loss: 6.705976E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.258 | TFLOPs: 7.66 |
[default0]:[2023-07-25 17:34:25,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=2095, skipped=1, lr=[9.148825600000001e-06, 9.148825600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:34:25,259] [INFO] [timer.py:215:stop] epoch=0/micro_step=2095/global_step=2095, RunningAvgSamplesPerSec=1.5336077720405714, CurrSamplesPerSec=1.1917576355469228, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2095/109863281 | consumed samples:         8380 | consumed tokens:     17162240 | elapsed time per iteration (ms): 3351.4 | learning rate: 9.149E-06 | global batch size:     4 | lm loss: 6.483022E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.194 | TFLOPs: 7.27 |
[default0]:[2023-07-25 17:34:42,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=2100, skipped=1, lr=[9.170670933333334e-06, 9.170670933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:34:42,042] [INFO] [timer.py:215:stop] epoch=0/micro_step=2100/global_step=2100, RunningAvgSamplesPerSec=1.5329723497824825, CurrSamplesPerSec=1.2573211767121666, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2100/109863281 | consumed samples:         8400 | consumed tokens:     17203200 | elapsed time per iteration (ms): 3292.2 | learning rate: 9.171E-06 | global batch size:     4 | lm loss: 6.589726E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.215 | TFLOPs: 7.40 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 2100 | lm loss value: 6.777386E+00 | lm loss PPL: 8.777710E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    2100 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:34:54,902] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2100 is about to be saved!
[default1]:[2023-07-25 17:34:54,960] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2100 is ready now!
[default0]:[2023-07-25 17:34:54,961] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2100 is ready now!
[default1]:[2023-07-25 17:34:54,963] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2100 is ready now!
[default0]:[2023-07-25 17:34:54,963] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2100/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:34:54,963] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2100/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 17:36:05,999] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2100/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:36:06,022] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2100 is ready now!
[default0]:  successfully saved checkpoint at iteration    2100 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.307
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71306.88, 71317.62)
[default0]:[2023-07-25 17:36:22,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=2105, skipped=1, lr=[9.192516266666667e-06, 9.192516266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:36:22,505] [INFO] [timer.py:215:stop] epoch=0/micro_step=2105/global_step=2105, RunningAvgSamplesPerSec=1.532343837546545, CurrSamplesPerSec=1.3273464662752956, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2105/109863281 | consumed samples:         8420 | consumed tokens:     17244160 | elapsed time per iteration (ms): 20171.6 | learning rate: 9.193E-06 | global batch size:     4 | lm loss: 6.665894E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.198 | TFLOPs: 1.21 |
[default0]:[2023-07-25 17:36:38,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=2110, skipped=1, lr=[9.2143616e-06, 9.2143616e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:36:38,417] [INFO] [timer.py:215:stop] epoch=0/micro_step=2110/global_step=2110, RunningAvgSamplesPerSec=1.5318850068626484, CurrSamplesPerSec=1.3486222919885216, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2110/109863281 | consumed samples:         8440 | consumed tokens:     17285120 | elapsed time per iteration (ms): 3101.7 | learning rate: 9.214E-06 | global batch size:     4 | lm loss: 6.518314E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.290 | TFLOPs: 7.85 |
[default0]:[2023-07-25 17:36:53,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=2115, skipped=1, lr=[9.236206933333334e-06, 9.236206933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:36:53,865] [INFO] [timer.py:215:stop] epoch=0/micro_step=2115/global_step=2115, RunningAvgSamplesPerSec=1.5315291515681966, CurrSamplesPerSec=1.3627557642031567, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2115/109863281 | consumed samples:         8460 | consumed tokens:     17326080 | elapsed time per iteration (ms): 3075.3 | learning rate: 9.236E-06 | global batch size:     4 | lm loss: 6.452712E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.301 | TFLOPs: 7.92 |
[default0]:[2023-07-25 17:37:09,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=2120, skipped=1, lr=[9.258052266666667e-06, 9.258052266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:37:09,510] [INFO] [timer.py:215:stop] epoch=0/micro_step=2120/global_step=2120, RunningAvgSamplesPerSec=1.5311062865326532, CurrSamplesPerSec=1.3867498612193454, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2120/109863281 | consumed samples:         8480 | consumed tokens:     17367040 | elapsed time per iteration (ms): 3124.0 | learning rate: 9.258E-06 | global batch size:     4 | lm loss: 6.799780E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.280 | TFLOPs: 7.80 |
[default0]:[2023-07-25 17:37:25,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=2125, skipped=1, lr=[9.2798976e-06, 9.2798976e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:37:25,582] [INFO] [timer.py:215:stop] epoch=0/micro_step=2125/global_step=2125, RunningAvgSamplesPerSec=1.5305505460445055, CurrSamplesPerSec=1.3468245790193811, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2125/109863281 | consumed samples:         8500 | consumed tokens:     17408000 | elapsed time per iteration (ms): 3197.9 | learning rate: 9.280E-06 | global batch size:     4 | lm loss: 6.460743E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.251 | TFLOPs: 7.62 |
[default0]:[2023-07-25 17:37:40,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=2130, skipped=1, lr=[9.301742933333334e-06, 9.301742933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:37:40,984] [INFO] [timer.py:215:stop] epoch=0/micro_step=2130/global_step=2130, RunningAvgSamplesPerSec=1.5301779550120231, CurrSamplesPerSec=1.3258033959317002, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2130/109863281 | consumed samples:         8520 | consumed tokens:     17448960 | elapsed time per iteration (ms): 3082.6 | learning rate: 9.302E-06 | global batch size:     4 | lm loss: 6.649046E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.298 | TFLOPs: 7.90 |
[default0]:[2023-07-25 17:37:56,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=2135, skipped=1, lr=[9.323588266666667e-06, 9.323588266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:37:56,345] [INFO] [timer.py:215:stop] epoch=0/micro_step=2135/global_step=2135, RunningAvgSamplesPerSec=1.5297936206319118, CurrSamplesPerSec=1.3420747454894446, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2135/109863281 | consumed samples:         8540 | consumed tokens:     17489920 | elapsed time per iteration (ms): 3107.3 | learning rate: 9.324E-06 | global batch size:     4 | lm loss: 6.578787E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.287 | TFLOPs: 7.84 |
[default0]:[2023-07-25 17:38:12,169] [INFO] [logging.py:96:log_dist] [Rank 0] step=2140, skipped=1, lr=[9.3454336e-06, 9.3454336e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:38:12,199] [INFO] [timer.py:215:stop] epoch=0/micro_step=2140/global_step=2140, RunningAvgSamplesPerSec=1.5293729834281706, CurrSamplesPerSec=1.3087754459818997, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2140/109863281 | consumed samples:         8560 | consumed tokens:     17530880 | elapsed time per iteration (ms): 3113.4 | learning rate: 9.345E-06 | global batch size:     4 | lm loss: 6.770084E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.285 | TFLOPs: 7.82 |
[default0]:[2023-07-25 17:38:27,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=2145, skipped=1, lr=[9.367278933333334e-06, 9.367278933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:38:27,872] [INFO] [timer.py:215:stop] epoch=0/micro_step=2145/global_step=2145, RunningAvgSamplesPerSec=1.5289467087119395, CurrSamplesPerSec=1.4443096121927534, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2145/109863281 | consumed samples:         8580 | consumed tokens:     17571840 | elapsed time per iteration (ms): 3129.5 | learning rate: 9.367E-06 | global batch size:     4 | lm loss: 6.604885E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.278 | TFLOPs: 7.78 |
[default0]:[2023-07-25 17:38:43,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=2150, skipped=1, lr=[9.389124266666667e-06, 9.389124266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:38:43,556] [INFO] [timer.py:215:stop] epoch=0/micro_step=2150/global_step=2150, RunningAvgSamplesPerSec=1.5285753909971163, CurrSamplesPerSec=1.3941530917774196, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2150/109863281 | consumed samples:         8600 | consumed tokens:     17612800 | elapsed time per iteration (ms): 3127.4 | learning rate: 9.389E-06 | global batch size:     4 | lm loss: 6.503960E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.279 | TFLOPs: 7.79 |
[default0]:saving checkpoint at iteration    2150 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:38:43,816] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2150 is about to be saved!
[default0]:[2023-07-25 17:38:43,900] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2150/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:38:43,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2150/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 17:38:43,898] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2150 is ready now!
[default1]:[2023-07-25 17:38:43,897] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2150 is ready now!
[default0]:[2023-07-25 17:38:43,908] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2150 is ready now!
[default0]:[2023-07-25 17:39:54,684] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2150/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:39:54,697] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2150 is ready now!
[default0]:  successfully saved checkpoint at iteration    2150 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.021
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71020.65, 71032.01)
[default0]:[2023-07-25 17:40:10,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=2155, skipped=1, lr=[9.4109696e-06, 9.4109696e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:40:10,033] [INFO] [timer.py:215:stop] epoch=0/micro_step=2155/global_step=2155, RunningAvgSamplesPerSec=1.5282762492404973, CurrSamplesPerSec=1.3729372245895726, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2155/109863281 | consumed samples:         8620 | consumed tokens:     17653760 | elapsed time per iteration (ms): 17279.0 | learning rate: 9.411E-06 | global batch size:     4 | lm loss: 6.674940E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.231 | TFLOPs: 1.41 |
[default0]:[2023-07-25 17:40:25,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=2160, skipped=1, lr=[9.432814933333333e-06, 9.432814933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:40:25,944] [INFO] [timer.py:215:stop] epoch=0/micro_step=2160/global_step=2160, RunningAvgSamplesPerSec=1.5277456916059418, CurrSamplesPerSec=1.3029701776900082, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2160/109863281 | consumed samples:         8640 | consumed tokens:     17694720 | elapsed time per iteration (ms): 3173.6 | learning rate: 9.433E-06 | global batch size:     4 | lm loss: 6.787788E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.260 | TFLOPs: 7.67 |
[default0]:[2023-07-25 17:40:41,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=2165, skipped=1, lr=[9.454660266666667e-06, 9.454660266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:40:41,753] [INFO] [timer.py:215:stop] epoch=0/micro_step=2165/global_step=2165, RunningAvgSamplesPerSec=1.5272922707406063, CurrSamplesPerSec=1.393482750879107, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2165/109863281 | consumed samples:         8660 | consumed tokens:     17735680 | elapsed time per iteration (ms): 3164.9 | learning rate: 9.455E-06 | global batch size:     4 | lm loss: 6.704831E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.264 | TFLOPs: 7.70 |
[default0]:[2023-07-25 17:40:57,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=2170, skipped=1, lr=[9.4765056e-06, 9.4765056e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:40:57,583] [INFO] [timer.py:215:stop] epoch=0/micro_step=2170/global_step=2170, RunningAvgSamplesPerSec=1.5268411664591766, CurrSamplesPerSec=1.3951103330384933, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2170/109863281 | consumed samples:         8680 | consumed tokens:     17776640 | elapsed time per iteration (ms): 3157.7 | learning rate: 9.477E-06 | global batch size:     4 | lm loss: 6.396197E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.267 | TFLOPs: 7.71 |
[default0]:[2023-07-25 17:41:13,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=2175, skipped=1, lr=[9.498350933333333e-06, 9.498350933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:41:13,306] [INFO] [timer.py:215:stop] epoch=0/micro_step=2175/global_step=2175, RunningAvgSamplesPerSec=1.5264044523817617, CurrSamplesPerSec=1.3310171801109545, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2175/109863281 | consumed samples:         8700 | consumed tokens:     17817600 | elapsed time per iteration (ms): 3142.2 | learning rate: 9.498E-06 | global batch size:     4 | lm loss: 6.539188E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.273 | TFLOPs: 7.75 |
[default0]:[2023-07-25 17:41:29,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=2180, skipped=1, lr=[9.520196266666666e-06, 9.520196266666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:41:29,368] [INFO] [timer.py:215:stop] epoch=0/micro_step=2180/global_step=2180, RunningAvgSamplesPerSec=1.5259161512395516, CurrSamplesPerSec=1.3625740322088635, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2180/109863281 | consumed samples:         8720 | consumed tokens:     17858560 | elapsed time per iteration (ms): 3204.7 | learning rate: 9.520E-06 | global batch size:     4 | lm loss: 6.363313E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.248 | TFLOPs: 7.60 |
[default0]:[2023-07-25 17:41:45,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=2185, skipped=1, lr=[9.542041600000001e-06, 9.542041600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:41:45,215] [INFO] [timer.py:215:stop] epoch=0/micro_step=2185/global_step=2185, RunningAvgSamplesPerSec=1.5254747379481364, CurrSamplesPerSec=1.3016227753821514, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2185/109863281 | consumed samples:         8740 | consumed tokens:     17899520 | elapsed time per iteration (ms): 3163.4 | learning rate: 9.542E-06 | global batch size:     4 | lm loss: 6.554882E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.264 | TFLOPs: 7.70 |
[default0]:[2023-07-25 17:42:01,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=2190, skipped=1, lr=[9.563886933333335e-06, 9.563886933333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:42:01,129] [INFO] [timer.py:215:stop] epoch=0/micro_step=2190/global_step=2190, RunningAvgSamplesPerSec=1.525021473371506, CurrSamplesPerSec=1.264742938295406, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2190/109863281 | consumed samples:         8760 | consumed tokens:     17940480 | elapsed time per iteration (ms): 3172.4 | learning rate: 9.564E-06 | global batch size:     4 | lm loss: 6.836026E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.261 | TFLOPs: 7.68 |
[default0]:[2023-07-25 17:42:16,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=2195, skipped=1, lr=[9.585732266666668e-06, 9.585732266666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:42:16,169] [INFO] [timer.py:215:stop] epoch=0/micro_step=2195/global_step=2195, RunningAvgSamplesPerSec=1.524729583069602, CurrSamplesPerSec=1.3378689139789353, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2195/109863281 | consumed samples:         8780 | consumed tokens:     17981440 | elapsed time per iteration (ms): 3003.4 | learning rate: 9.586E-06 | global batch size:     4 | lm loss: 6.673257E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.332 | TFLOPs: 8.11 |
[default0]:[2023-07-25 17:42:31,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=2200, skipped=1, lr=[9.607577600000001e-06, 9.607577600000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:42:32,015] [INFO] [timer.py:215:stop] epoch=0/micro_step=2200/global_step=2200, RunningAvgSamplesPerSec=1.5243240819619288, CurrSamplesPerSec=1.3351701999769052, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2200/109863281 | consumed samples:         8800 | consumed tokens:     18022400 | elapsed time per iteration (ms): 3160.6 | learning rate: 9.608E-06 | global batch size:     4 | lm loss: 6.652735E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.266 | TFLOPs: 7.71 |
[default0]:saving checkpoint at iteration    2200 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 2200 | lm loss value: 6.734206E+00 | lm loss PPL: 8.406759E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-25 17:42:45,525] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2200 is about to be saved!
[default0]:[2023-07-25 17:42:45,595] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2200/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:42:45,595] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2200/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 17:42:45,599] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2200 is ready now!
[default0]:[2023-07-25 17:42:45,597] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2200 is ready now!
[default1]:[2023-07-25 17:42:45,598] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2200 is ready now!
[default0]:[2023-07-25 17:43:57,191] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2200/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:43:57,258] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2200 is ready now!
[default0]:  successfully saved checkpoint at iteration    2200 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 72.152
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (72152.28, 72167.74)
[default0]:[2023-07-25 17:44:13,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=2205, skipped=1, lr=[9.629422933333334e-06, 9.629422933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:44:13,184] [INFO] [timer.py:215:stop] epoch=0/micro_step=2205/global_step=2205, RunningAvgSamplesPerSec=1.5238806433703431, CurrSamplesPerSec=1.4036694561208554, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2205/109863281 | consumed samples:         8820 | consumed tokens:     18063360 | elapsed time per iteration (ms): 20224.4 | learning rate: 9.629E-06 | global batch size:     4 | lm loss: 6.593013E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.198 | TFLOPs: 1.20 |
[default0]:[2023-07-25 17:44:28,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=2210, skipped=1, lr=[9.651268266666668e-06, 9.651268266666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:44:28,827] [INFO] [timer.py:215:stop] epoch=0/micro_step=2210/global_step=2210, RunningAvgSamplesPerSec=1.5235050952894087, CurrSamplesPerSec=1.4104880039022691, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2210/109863281 | consumed samples:         8840 | consumed tokens:     18104320 | elapsed time per iteration (ms): 3074.6 | learning rate: 9.651E-06 | global batch size:     4 | lm loss: 6.547041E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.301 | TFLOPs: 7.92 |
[default0]:[2023-07-25 17:44:44,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=2215, skipped=1, lr=[9.6731136e-06, 9.6731136e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:44:44,699] [INFO] [timer.py:215:stop] epoch=0/micro_step=2215/global_step=2215, RunningAvgSamplesPerSec=1.5230624457629114, CurrSamplesPerSec=1.3591021243909556, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2215/109863281 | consumed samples:         8860 | consumed tokens:     18145280 | elapsed time per iteration (ms): 3174.2 | learning rate: 9.673E-06 | global batch size:     4 | lm loss: 6.924660E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.260 | TFLOPs: 7.67 |
[default0]:[2023-07-25 17:45:00,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=2220, skipped=1, lr=[9.694958933333334e-06, 9.694958933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:45:00,612] [INFO] [timer.py:215:stop] epoch=0/micro_step=2220/global_step=2220, RunningAvgSamplesPerSec=1.522635234714834, CurrSamplesPerSec=1.3175939287008425, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2220/109863281 | consumed samples:         8880 | consumed tokens:     18186240 | elapsed time per iteration (ms): 3170.2 | learning rate: 9.695E-06 | global batch size:     4 | lm loss: 6.470563E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.262 | TFLOPs: 7.68 |
[default0]:[2023-07-25 17:45:16,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=2225, skipped=1, lr=[9.716804266666667e-06, 9.716804266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:45:16,447] [INFO] [timer.py:215:stop] epoch=0/micro_step=2225/global_step=2225, RunningAvgSamplesPerSec=1.5222112166189294, CurrSamplesPerSec=1.335076276301607, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2225/109863281 | consumed samples:         8900 | consumed tokens:     18227200 | elapsed time per iteration (ms): 3155.8 | learning rate: 9.717E-06 | global batch size:     4 | lm loss: 6.691348E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.268 | TFLOPs: 7.72 |
[default0]:[2023-07-25 17:45:31,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=2230, skipped=1, lr=[9.7386496e-06, 9.7386496e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:45:31,901] [INFO] [timer.py:215:stop] epoch=0/micro_step=2230/global_step=2230, RunningAvgSamplesPerSec=1.52185132767173, CurrSamplesPerSec=1.2901261595952485, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2230/109863281 | consumed samples:         8920 | consumed tokens:     18268160 | elapsed time per iteration (ms): 3089.6 | learning rate: 9.739E-06 | global batch size:     4 | lm loss: 6.780155E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.295 | TFLOPs: 7.88 |
[default0]:[2023-07-25 17:45:47,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=2235, skipped=1, lr=[9.760494933333334e-06, 9.760494933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:45:47,201] [INFO] [timer.py:215:stop] epoch=0/micro_step=2235/global_step=2235, RunningAvgSamplesPerSec=1.521488070794718, CurrSamplesPerSec=1.3556142367639432, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2235/109863281 | consumed samples:         8940 | consumed tokens:     18309120 | elapsed time per iteration (ms): 3089.2 | learning rate: 9.760E-06 | global batch size:     4 | lm loss: 6.631497E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.295 | TFLOPs: 7.88 |
[default0]:[2023-07-25 17:46:03,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=2240, skipped=1, lr=[9.782340266666667e-06, 9.782340266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:46:03,356] [INFO] [timer.py:215:stop] epoch=0/micro_step=2240/global_step=2240, RunningAvgSamplesPerSec=1.5209959353214881, CurrSamplesPerSec=1.3228452199555694, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2240/109863281 | consumed samples:         8960 | consumed tokens:     18350080 | elapsed time per iteration (ms): 3275.7 | learning rate: 9.782E-06 | global batch size:     4 | lm loss: 6.644829E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.221 | TFLOPs: 7.44 |
[default0]:[2023-07-25 17:46:19,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=2245, skipped=1, lr=[9.8041856e-06, 9.8041856e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:46:19,307] [INFO] [timer.py:215:stop] epoch=0/micro_step=2245/global_step=2245, RunningAvgSamplesPerSec=1.520618617385835, CurrSamplesPerSec=1.404615824475934, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2245/109863281 | consumed samples:         8980 | consumed tokens:     18391040 | elapsed time per iteration (ms): 3096.0 | learning rate: 9.804E-06 | global batch size:     4 | lm loss: 6.850279E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.292 | TFLOPs: 7.87 |
[default0]:[2023-07-25 17:46:34,850] [INFO] [logging.py:96:log_dist] [Rank 0] step=2250, skipped=1, lr=[9.826030933333334e-06, 9.826030933333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:46:34,883] [INFO] [timer.py:215:stop] epoch=0/micro_step=2250/global_step=2250, RunningAvgSamplesPerSec=1.5203020433408194, CurrSamplesPerSec=1.4150363706981512, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2250/109863281 | consumed samples:         9000 | consumed tokens:     18432000 | elapsed time per iteration (ms): 3113.8 | learning rate: 9.826E-06 | global batch size:     4 | lm loss: 6.542152E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.285 | TFLOPs: 7.82 |
[default0]:saving checkpoint at iteration    2250 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:46:35,106] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2250 is about to be saved!
[default1]:[2023-07-25 17:46:35,156] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2250 is ready now!
[default0]:[2023-07-25 17:46:35,153] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2250 is ready now!
[default1]:[2023-07-25 17:46:35,156] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2250 is ready now!
[default0]:[2023-07-25 17:46:35,153] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2250/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:46:35,153] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2250/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 17:47:45,809] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2250/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:47:45,837] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2250 is ready now!
[default0]:  successfully saved checkpoint at iteration    2250 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70864.93, 70875.60)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.868
[default0]:[2023-07-25 17:48:00,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=2255, skipped=1, lr=[9.847876266666667e-06, 9.847876266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:48:00,870] [INFO] [timer.py:215:stop] epoch=0/micro_step=2255/global_step=2255, RunningAvgSamplesPerSec=1.5200308577671373, CurrSamplesPerSec=1.3448665828083102, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2255/109863281 | consumed samples:         9020 | consumed tokens:     18472960 | elapsed time per iteration (ms): 17186.1 | learning rate: 9.848E-06 | global batch size:     4 | lm loss: 6.866297E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.233 | TFLOPs: 1.42 |
[default0]:[2023-07-25 17:48:16,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=2260, skipped=1, lr=[9.8697216e-06, 9.8697216e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:48:16,893] [INFO] [timer.py:215:stop] epoch=0/micro_step=2260/global_step=2260, RunningAvgSamplesPerSec=1.5195700785798962, CurrSamplesPerSec=1.3947249367157006, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2260/109863281 | consumed samples:         9040 | consumed tokens:     18513920 | elapsed time per iteration (ms): 3197.8 | learning rate: 9.870E-06 | global batch size:     4 | lm loss: 6.570405E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.251 | TFLOPs: 7.62 |
[default0]:[2023-07-25 17:48:32,760] [INFO] [logging.py:96:log_dist] [Rank 0] step=2265, skipped=1, lr=[9.891566933333333e-06, 9.891566933333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:48:32,779] [INFO] [timer.py:215:stop] epoch=0/micro_step=2265/global_step=2265, RunningAvgSamplesPerSec=1.5191386476318072, CurrSamplesPerSec=1.3232010943221941, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2265/109863281 | consumed samples:         9060 | consumed tokens:     18554880 | elapsed time per iteration (ms): 3168.5 | learning rate: 9.892E-06 | global batch size:     4 | lm loss: 6.615762E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.262 | TFLOPs: 7.69 |
[default0]:[2023-07-25 17:48:48,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=2270, skipped=1, lr=[9.913412266666667e-06, 9.913412266666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:48:48,855] [INFO] [timer.py:215:stop] epoch=0/micro_step=2270/global_step=2270, RunningAvgSamplesPerSec=1.5186829582317503, CurrSamplesPerSec=1.3119432846173364, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2270/109863281 | consumed samples:         9080 | consumed tokens:     18595840 | elapsed time per iteration (ms): 3191.8 | learning rate: 9.913E-06 | global batch size:     4 | lm loss: 6.504713E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.253 | TFLOPs: 7.63 |
[default0]:[2023-07-25 17:49:04,317] [INFO] [logging.py:96:log_dist] [Rank 0] step=2275, skipped=1, lr=[9.935257600000002e-06, 9.935257600000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:49:04,347] [INFO] [timer.py:215:stop] epoch=0/micro_step=2275/global_step=2275, RunningAvgSamplesPerSec=1.5183533710138057, CurrSamplesPerSec=1.389061897281615, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2275/109863281 | consumed samples:         9100 | consumed tokens:     18636800 | elapsed time per iteration (ms): 3094.4 | learning rate: 9.935E-06 | global batch size:     4 | lm loss: 6.466277E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.293 | TFLOPs: 7.87 |
[default0]:[2023-07-25 17:49:19,998] [INFO] [logging.py:96:log_dist] [Rank 0] step=2280, skipped=1, lr=[9.957102933333335e-06, 9.957102933333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:49:20,022] [INFO] [timer.py:215:stop] epoch=0/micro_step=2280/global_step=2280, RunningAvgSamplesPerSec=1.517929638695727, CurrSamplesPerSec=1.1675910353657846, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2280/109863281 | consumed samples:         9120 | consumed tokens:     18677760 | elapsed time per iteration (ms): 3135.7 | learning rate: 9.957E-06 | global batch size:     4 | lm loss: 6.495929E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.276 | TFLOPs: 7.77 |
[default0]:[2023-07-25 17:49:35,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=2285, skipped=1, lr=[9.978948266666668e-06, 9.978948266666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:49:35,396] [INFO] [timer.py:215:stop] epoch=0/micro_step=2285/global_step=2285, RunningAvgSamplesPerSec=1.5175709652991363, CurrSamplesPerSec=1.369612592469293, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2285/109863281 | consumed samples:         9140 | consumed tokens:     18718720 | elapsed time per iteration (ms): 3148.3 | learning rate: 9.979E-06 | global batch size:     4 | lm loss: 6.416605E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.271 | TFLOPs: 7.74 |
[default0]:[2023-07-25 17:49:51,325] [INFO] [logging.py:96:log_dist] [Rank 0] step=2290, skipped=1, lr=[1.0000793600000001e-05, 1.0000793600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:49:51,352] [INFO] [timer.py:215:stop] epoch=0/micro_step=2290/global_step=2290, RunningAvgSamplesPerSec=1.5172150754019633, CurrSamplesPerSec=1.4020843043208326, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2290/109863281 | consumed samples:         9160 | consumed tokens:     18759680 | elapsed time per iteration (ms): 3097.8 | learning rate: 1.000E-05 | global batch size:     4 | lm loss: 6.624976E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.291 | TFLOPs: 7.86 |
[default0]:[2023-07-25 17:50:06,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=2295, skipped=1, lr=[1.0022638933333335e-05, 1.0022638933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:50:07,017] [INFO] [timer.py:215:stop] epoch=0/micro_step=2295/global_step=2295, RunningAvgSamplesPerSec=1.5168487930708978, CurrSamplesPerSec=1.3672591570222772, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2295/109863281 | consumed samples:         9180 | consumed tokens:     18800640 | elapsed time per iteration (ms): 3131.1 | learning rate: 1.002E-05 | global batch size:     4 | lm loss: 6.482357E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.277 | TFLOPs: 7.78 |
[default0]:[2023-07-25 17:50:23,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=2300, skipped=1, lr=[1.0044484266666668e-05, 1.0044484266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:50:23,135] [INFO] [timer.py:215:stop] epoch=0/micro_step=2300/global_step=2300, RunningAvgSamplesPerSec=1.516365276982292, CurrSamplesPerSec=1.3416719526757441, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2300/109863281 | consumed samples:         9200 | consumed tokens:     18841600 | elapsed time per iteration (ms): 3223.5 | learning rate: 1.004E-05 | global batch size:     4 | lm loss: 6.426725E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.241 | TFLOPs: 7.56 |
[default0]:saving checkpoint at iteration    2300 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 2300 | lm loss value: 6.747359E+00 | lm loss PPL: 8.518060E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-25 17:50:37,312] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2300 is about to be saved!
[default0]:[2023-07-25 17:50:37,389] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2300/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:50:37,389] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2300/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 17:50:37,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2300 is ready now!
[default0]:[2023-07-25 17:50:37,390] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2300 is ready now!
[default1]:[2023-07-25 17:50:37,392] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2300 is ready now!
[default0]:[2023-07-25 17:51:48,696] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2300/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:51:48,719] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2300 is ready now!
[default0]:  successfully saved checkpoint at iteration    2300 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.768
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71767.92, 71778.63)
[default0]:[2023-07-25 17:52:04,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=2305, skipped=1, lr=[1.0066329600000001e-05, 1.0066329600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:52:04,835] [INFO] [timer.py:215:stop] epoch=0/micro_step=2305/global_step=2305, RunningAvgSamplesPerSec=1.515847699080483, CurrSamplesPerSec=1.3638967353966278, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2305/109863281 | consumed samples:         9220 | consumed tokens:     18882560 | elapsed time per iteration (ms): 20330.4 | learning rate: 1.007E-05 | global batch size:     4 | lm loss: 6.469025E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.197 | TFLOPs: 1.20 |
[default0]:[2023-07-25 17:52:20,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=2310, skipped=1, lr=[1.0088174933333334e-05, 1.0088174933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:52:20,174] [INFO] [timer.py:215:stop] epoch=0/micro_step=2310/global_step=2310, RunningAvgSamplesPerSec=1.515626099279541, CurrSamplesPerSec=1.465354511307025, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2310/109863281 | consumed samples:         9240 | consumed tokens:     18923520 | elapsed time per iteration (ms): 3012.0 | learning rate: 1.009E-05 | global batch size:     4 | lm loss: 6.653062E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.328 | TFLOPs: 8.09 |
[default0]:[2023-07-25 17:52:36,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=2315, skipped=1, lr=[1.0110020266666668e-05, 1.0110020266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:52:36,296] [INFO] [timer.py:215:stop] epoch=0/micro_step=2315/global_step=2315, RunningAvgSamplesPerSec=1.5152587747363886, CurrSamplesPerSec=1.376348263720913, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2315/109863281 | consumed samples:         9260 | consumed tokens:     18964480 | elapsed time per iteration (ms): 3221.4 | learning rate: 1.011E-05 | global batch size:     4 | lm loss: 6.423517E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.242 | TFLOPs: 7.56 |
[default0]:[2023-07-25 17:52:52,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=2320, skipped=1, lr=[1.0131865600000001e-05, 1.0131865600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:52:52,525] [INFO] [timer.py:215:stop] epoch=0/micro_step=2320/global_step=2320, RunningAvgSamplesPerSec=1.5147366863846343, CurrSamplesPerSec=1.2425824110306303, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2320/109863281 | consumed samples:         9280 | consumed tokens:     19005440 | elapsed time per iteration (ms): 3233.2 | learning rate: 1.013E-05 | global batch size:     4 | lm loss: 6.552666E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.237 | TFLOPs: 7.53 |
[default0]:[2023-07-25 17:53:08,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=2325, skipped=1, lr=[1.0153710933333334e-05, 1.0153710933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:53:08,526] [INFO] [timer.py:215:stop] epoch=0/micro_step=2325/global_step=2325, RunningAvgSamplesPerSec=1.5143172187911669, CurrSamplesPerSec=1.3860618843011365, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2325/109863281 | consumed samples:         9300 | consumed tokens:     19046400 | elapsed time per iteration (ms): 3189.1 | learning rate: 1.015E-05 | global batch size:     4 | lm loss: 6.561632E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.254 | TFLOPs: 7.64 |
[default0]:[2023-07-25 17:53:24,048] [INFO] [logging.py:96:log_dist] [Rank 0] step=2330, skipped=1, lr=[1.0175556266666667e-05, 1.0175556266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:53:24,084] [INFO] [timer.py:215:stop] epoch=0/micro_step=2330/global_step=2330, RunningAvgSamplesPerSec=1.513952603781157, CurrSamplesPerSec=1.4247840962274407, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2330/109863281 | consumed samples:         9320 | consumed tokens:     19087360 | elapsed time per iteration (ms): 3109.7 | learning rate: 1.018E-05 | global batch size:     4 | lm loss: 6.558450E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.286 | TFLOPs: 7.83 |
[default0]:[2023-07-25 17:53:40,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=2335, skipped=1, lr=[1.01974016e-05, 1.01974016e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:53:40,361] [INFO] [timer.py:215:stop] epoch=0/micro_step=2335/global_step=2335, RunningAvgSamplesPerSec=1.5134521188257763, CurrSamplesPerSec=1.3145597468605956, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2335/109863281 | consumed samples:         9340 | consumed tokens:     19128320 | elapsed time per iteration (ms): 3253.0 | learning rate: 1.020E-05 | global batch size:     4 | lm loss: 6.447579E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.230 | TFLOPs: 7.49 |
[default0]:[2023-07-25 17:53:55,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=2340, skipped=1, lr=[1.0219246933333334e-05, 1.0219246933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:53:55,640] [INFO] [timer.py:215:stop] epoch=0/micro_step=2340/global_step=2340, RunningAvgSamplesPerSec=1.5132739072200798, CurrSamplesPerSec=1.446979205666338, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2340/109863281 | consumed samples:         9360 | consumed tokens:     19169280 | elapsed time per iteration (ms): 3044.1 | learning rate: 1.022E-05 | global batch size:     4 | lm loss: 6.585768E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.314 | TFLOPs: 8.00 |
[default0]:[2023-07-25 17:54:10,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=2345, skipped=1, lr=[1.0241092266666667e-05, 1.0241092266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:54:11,006] [INFO] [timer.py:215:stop] epoch=0/micro_step=2345/global_step=2345, RunningAvgSamplesPerSec=1.5129316168780833, CurrSamplesPerSec=1.3337777877063635, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2345/109863281 | consumed samples:         9380 | consumed tokens:     19210240 | elapsed time per iteration (ms): 3109.2 | learning rate: 1.024E-05 | global batch size:     4 | lm loss: 6.589844E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.287 | TFLOPs: 7.83 |
[default0]:[2023-07-25 17:54:26,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=2350, skipped=1, lr=[1.02629376e-05, 1.02629376e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:54:26,603] [INFO] [timer.py:215:stop] epoch=0/micro_step=2350/global_step=2350, RunningAvgSamplesPerSec=1.5126801736585131, CurrSamplesPerSec=1.421534982130001, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2350/109863281 | consumed samples:         9400 | consumed tokens:     19251200 | elapsed time per iteration (ms): 3065.8 | learning rate: 1.026E-05 | global batch size:     4 | lm loss: 6.565446E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.305 | TFLOPs: 7.94 |
[default0]:saving checkpoint at iteration    2350 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:54:26,767] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2350 is about to be saved!
[default0]:[2023-07-25 17:54:26,869] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2350/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:54:26,869] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2350/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 17:54:26,870] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2350 is ready now!
[default1]:[2023-07-25 17:54:26,872] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2350 is ready now!
[default1]:[2023-07-25 17:54:26,871] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2350 is ready now!
[default0]:[2023-07-25 17:55:37,899] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2350/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:55:37,921] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2350 is ready now!
[default0]:  successfully saved checkpoint at iteration    2350 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.321
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71306.97, 71320.95)
[default0]:[2023-07-25 17:55:53,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=2355, skipped=1, lr=[1.0284782933333334e-05, 1.0284782933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:55:53,043] [INFO] [timer.py:215:stop] epoch=0/micro_step=2355/global_step=2355, RunningAvgSamplesPerSec=1.5124302519346726, CurrSamplesPerSec=1.3497816257317827, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2355/109863281 | consumed samples:         9420 | consumed tokens:     19292160 | elapsed time per iteration (ms): 17319.6 | learning rate: 1.028E-05 | global batch size:     4 | lm loss: 6.517593E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.231 | TFLOPs: 1.41 |
[default0]:[2023-07-25 17:56:09,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=2360, skipped=1, lr=[1.0306628266666667e-05, 1.0306628266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:56:09,153] [INFO] [timer.py:215:stop] epoch=0/micro_step=2360/global_step=2360, RunningAvgSamplesPerSec=1.511999361180932, CurrSamplesPerSec=1.2952861594391267, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2360/109863281 | consumed samples:         9440 | consumed tokens:     19333120 | elapsed time per iteration (ms): 3233.1 | learning rate: 1.031E-05 | global batch size:     4 | lm loss: 6.600187E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.237 | TFLOPs: 7.53 |
[default0]:[2023-07-25 17:56:25,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=2365, skipped=1, lr=[1.03284736e-05, 1.03284736e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:56:25,134] [INFO] [timer.py:215:stop] epoch=0/micro_step=2365/global_step=2365, RunningAvgSamplesPerSec=1.5116703508669371, CurrSamplesPerSec=1.2816354854682879, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2365/109863281 | consumed samples:         9460 | consumed tokens:     19374080 | elapsed time per iteration (ms): 3095.1 | learning rate: 1.033E-05 | global batch size:     4 | lm loss: 6.766402E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.292 | TFLOPs: 7.87 |
[default0]:[2023-07-25 17:56:40,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=2370, skipped=1, lr=[1.0350318933333334e-05, 1.0350318933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:56:40,766] [INFO] [timer.py:215:stop] epoch=0/micro_step=2370/global_step=2370, RunningAvgSamplesPerSec=1.5113492698452398, CurrSamplesPerSec=1.4134462952505293, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2370/109863281 | consumed samples:         9480 | consumed tokens:     19415040 | elapsed time per iteration (ms): 3115.3 | learning rate: 1.035E-05 | global batch size:     4 | lm loss: 6.469026E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.284 | TFLOPs: 7.82 |
[default0]:[2023-07-25 17:56:56,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=2375, skipped=1, lr=[1.0372164266666667e-05, 1.0372164266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:56:56,424] [INFO] [timer.py:215:stop] epoch=0/micro_step=2375/global_step=2375, RunningAvgSamplesPerSec=1.511017300301067, CurrSamplesPerSec=1.477842643539906, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2375/109863281 | consumed samples:         9500 | consumed tokens:     19456000 | elapsed time per iteration (ms): 3126.9 | learning rate: 1.037E-05 | global batch size:     4 | lm loss: 6.591314E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.279 | TFLOPs: 7.79 |
[default0]:[2023-07-25 17:57:12,231] [INFO] [logging.py:96:log_dist] [Rank 0] step=2380, skipped=1, lr=[1.03940096e-05, 1.03940096e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:57:12,249] [INFO] [timer.py:215:stop] epoch=0/micro_step=2380/global_step=2380, RunningAvgSamplesPerSec=1.5106429453026613, CurrSamplesPerSec=1.3411275526419961, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2380/109863281 | consumed samples:         9520 | consumed tokens:     19496960 | elapsed time per iteration (ms): 3156.6 | learning rate: 1.039E-05 | global batch size:     4 | lm loss: 6.700256E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.267 | TFLOPs: 7.72 |
[default0]:[2023-07-25 17:57:28,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=2385, skipped=1, lr=[1.0415854933333333e-05, 1.0415854933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:57:28,107] [INFO] [timer.py:215:stop] epoch=0/micro_step=2385/global_step=2385, RunningAvgSamplesPerSec=1.5102786322709538, CurrSamplesPerSec=1.3912159736106056, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2385/109863281 | consumed samples:         9540 | consumed tokens:     19537920 | elapsed time per iteration (ms): 3166.1 | learning rate: 1.042E-05 | global batch size:     4 | lm loss: 6.708961E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.263 | TFLOPs: 7.69 |
[default0]:[2023-07-25 17:57:43,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=2390, skipped=1, lr=[1.0437700266666667e-05, 1.0437700266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:57:43,580] [INFO] [timer.py:215:stop] epoch=0/micro_step=2390/global_step=2390, RunningAvgSamplesPerSec=1.509995456940822, CurrSamplesPerSec=1.3307747758723971, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2390/109863281 | consumed samples:         9560 | consumed tokens:     19578880 | elapsed time per iteration (ms): 3090.8 | learning rate: 1.044E-05 | global batch size:     4 | lm loss: 6.440828E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.294 | TFLOPs: 7.88 |
[default0]:[2023-07-25 17:57:58,928] [INFO] [logging.py:96:log_dist] [Rank 0] step=2395, skipped=1, lr=[1.04595456e-05, 1.04595456e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:57:58,934] [INFO] [timer.py:215:stop] epoch=0/micro_step=2395/global_step=2395, RunningAvgSamplesPerSec=1.5096871801334213, CurrSamplesPerSec=1.3073264664707065, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2395/109863281 | consumed samples:         9580 | consumed tokens:     19619840 | elapsed time per iteration (ms): 3115.2 | learning rate: 1.046E-05 | global batch size:     4 | lm loss: 6.499377E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.284 | TFLOPs: 7.82 |
[default0]:[2023-07-25 17:58:14,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=2400, skipped=1, lr=[1.0481390933333333e-05, 1.0481390933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:58:14,809] [INFO] [timer.py:215:stop] epoch=0/micro_step=2400/global_step=2400, RunningAvgSamplesPerSec=1.509291489808908, CurrSamplesPerSec=1.2872848782228223, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2400/109863281 | consumed samples:         9600 | consumed tokens:     19660800 | elapsed time per iteration (ms): 3124.6 | learning rate: 1.048E-05 | global batch size:     4 | lm loss: 6.658277E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.280 | TFLOPs: 7.79 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 2400 | lm loss value: 6.708756E+00 | lm loss PPL: 8.195509E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    2400 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 17:58:28,814] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2400 is about to be saved!
[default0]:[2023-07-25 17:58:28,893] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2400/mp_rank_00_model_states.pt
[default0]:[2023-07-25 17:58:28,893] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2400/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 17:58:28,891] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2400 is ready now!
[default0]:[2023-07-25 17:58:28,902] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2400 is ready now!
[default1]:[2023-07-25 17:58:28,892] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2400 is ready now!
[default0]:[2023-07-25 17:59:39,632] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2400/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 17:59:39,654] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2400 is ready now!
[default0]:  successfully saved checkpoint at iteration    2400 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.982
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70971.12, 70983.18)
[default0]:[2023-07-25 17:59:56,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=2405, skipped=1, lr=[1.0503236266666666e-05, 1.0503236266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 17:59:56,063] [INFO] [timer.py:215:stop] epoch=0/micro_step=2405/global_step=2405, RunningAvgSamplesPerSec=1.5088178665526786, CurrSamplesPerSec=1.2484980747391021, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2405/109863281 | consumed samples:         9620 | consumed tokens:     19701760 | elapsed time per iteration (ms): 20234.3 | learning rate: 1.050E-05 | global batch size:     4 | lm loss: 6.533481E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.198 | TFLOPs: 1.20 |
[default0]:[2023-07-25 18:00:11,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=2410, skipped=1, lr=[1.05250816e-05, 1.05250816e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:00:12,033] [INFO] [timer.py:215:stop] epoch=0/micro_step=2410/global_step=2410, RunningAvgSamplesPerSec=1.5084366830427929, CurrSamplesPerSec=1.4156837718588777, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2410/109863281 | consumed samples:         9640 | consumed tokens:     19742720 | elapsed time per iteration (ms): 3192.5 | learning rate: 1.053E-05 | global batch size:     4 | lm loss: 6.471130E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.253 | TFLOPs: 7.63 |
[default0]:[2023-07-25 18:00:27,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=2415, skipped=1, lr=[1.0546926933333333e-05, 1.0546926933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:00:27,861] [INFO] [timer.py:215:stop] epoch=0/micro_step=2415/global_step=2415, RunningAvgSamplesPerSec=1.5081346334825954, CurrSamplesPerSec=1.4179241492606893, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2415/109863281 | consumed samples:         9660 | consumed tokens:     19783680 | elapsed time per iteration (ms): 3152.6 | learning rate: 1.055E-05 | global batch size:     4 | lm loss: 6.662696E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.269 | TFLOPs: 7.73 |
[default0]:[2023-07-25 18:00:43,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=2420, skipped=1, lr=[1.0568772266666666e-05, 1.0568772266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:00:43,785] [INFO] [timer.py:215:stop] epoch=0/micro_step=2420/global_step=2420, RunningAvgSamplesPerSec=1.5077904631583194, CurrSamplesPerSec=1.2984808039843196, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2420/109863281 | consumed samples:         9680 | consumed tokens:     19824640 | elapsed time per iteration (ms): 3181.7 | learning rate: 1.057E-05 | global batch size:     4 | lm loss: 6.800775E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.257 | TFLOPs: 7.65 |
[default0]:[2023-07-25 18:00:59,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=2425, skipped=1, lr=[1.0590617600000001e-05, 1.0590617600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:00:59,357] [INFO] [timer.py:215:stop] epoch=0/micro_step=2425/global_step=2425, RunningAvgSamplesPerSec=1.5075676477668079, CurrSamplesPerSec=1.3161491150997866, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2425/109863281 | consumed samples:         9700 | consumed tokens:     19865600 | elapsed time per iteration (ms): 3099.7 | learning rate: 1.059E-05 | global batch size:     4 | lm loss: 6.521037E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.290 | TFLOPs: 7.86 |
[default0]:[2023-07-25 18:01:15,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=2430, skipped=1, lr=[1.0612462933333334e-05, 1.0612462933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:01:15,458] [INFO] [timer.py:215:stop] epoch=0/micro_step=2430/global_step=2430, RunningAvgSamplesPerSec=1.5071286409781874, CurrSamplesPerSec=1.1995575629353854, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2430/109863281 | consumed samples:         9720 | consumed tokens:     19906560 | elapsed time per iteration (ms): 3219.5 | learning rate: 1.061E-05 | global batch size:     4 | lm loss: 6.553256E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.242 | TFLOPs: 7.57 |
[default0]:[2023-07-25 18:01:30,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=2435, skipped=1, lr=[1.0634308266666668e-05, 1.0634308266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:01:30,673] [INFO] [timer.py:215:stop] epoch=0/micro_step=2435/global_step=2435, RunningAvgSamplesPerSec=1.5068698319342717, CurrSamplesPerSec=1.323860870539563, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2435/109863281 | consumed samples:         9740 | consumed tokens:     19947520 | elapsed time per iteration (ms): 3114.5 | learning rate: 1.063E-05 | global batch size:     4 | lm loss: 6.610009E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.284 | TFLOPs: 7.82 |
[default0]:[2023-07-25 18:01:46,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=2440, skipped=1, lr=[1.06561536e-05, 1.06561536e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:01:46,701] [INFO] [timer.py:215:stop] epoch=0/micro_step=2440/global_step=2440, RunningAvgSamplesPerSec=1.506496648684862, CurrSamplesPerSec=1.3860378375113998, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2440/109863281 | consumed samples:         9760 | consumed tokens:     19988480 | elapsed time per iteration (ms): 3116.0 | learning rate: 1.066E-05 | global batch size:     4 | lm loss: 6.354362E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.284 | TFLOPs: 7.82 |
[default0]:[2023-07-25 18:02:02,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=2445, skipped=1, lr=[1.0677998933333334e-05, 1.0677998933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:02:02,870] [INFO] [timer.py:215:stop] epoch=0/micro_step=2445/global_step=2445, RunningAvgSamplesPerSec=1.5060455709993468, CurrSamplesPerSec=1.2348135964791227, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2445/109863281 | consumed samples:         9780 | consumed tokens:     20029440 | elapsed time per iteration (ms): 3312.5 | learning rate: 1.068E-05 | global batch size:     4 | lm loss: 6.217897E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.208 | TFLOPs: 7.35 |
[default0]:[2023-07-25 18:02:18,646] [INFO] [logging.py:96:log_dist] [Rank 0] step=2450, skipped=1, lr=[1.0699844266666667e-05, 1.0699844266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:02:18,674] [INFO] [timer.py:215:stop] epoch=0/micro_step=2450/global_step=2450, RunningAvgSamplesPerSec=1.5057151385231995, CurrSamplesPerSec=1.325583100514519, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2450/109863281 | consumed samples:         9800 | consumed tokens:     20070400 | elapsed time per iteration (ms): 3076.2 | learning rate: 1.070E-05 | global batch size:     4 | lm loss: 6.490958E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.300 | TFLOPs: 7.92 |
[default0]:saving checkpoint at iteration    2450 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:02:18,853] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2450 is about to be saved!
[default0]:[2023-07-25 18:02:19,169] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2450/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:02:19,169] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2450/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 18:02:19,168] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2450 is ready now!
[default1]:[2023-07-25 18:02:19,167] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2450 is ready now!
[default0]:[2023-07-25 18:02:19,171] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2450 is ready now!
[default0]:[2023-07-25 18:03:29,967] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2450/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:03:29,977] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2450 is ready now!
[default0]:  successfully saved checkpoint at iteration    2450 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.298
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71290.63, 71314.74)
[default0]:[2023-07-25 18:03:46,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=2455, skipped=1, lr=[1.07216896e-05, 1.07216896e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:03:46,134] [INFO] [timer.py:215:stop] epoch=0/micro_step=2455/global_step=2455, RunningAvgSamplesPerSec=1.5053339044245815, CurrSamplesPerSec=1.393323858044741, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2455/109863281 | consumed samples:         9820 | consumed tokens:     20111360 | elapsed time per iteration (ms): 17489.6 | learning rate: 1.072E-05 | global batch size:     4 | lm loss: 6.517207E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.229 | TFLOPs: 1.39 |
[default0]:[2023-07-25 18:04:02,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=2460, skipped=1, lr=[1.0743534933333334e-05, 1.0743534933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:04:02,024] [INFO] [timer.py:215:stop] epoch=0/micro_step=2460/global_step=2460, RunningAvgSamplesPerSec=1.504999113638395, CurrSamplesPerSec=1.4256539977110971, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2460/109863281 | consumed samples:         9840 | consumed tokens:     20152320 | elapsed time per iteration (ms): 3166.1 | learning rate: 1.074E-05 | global batch size:     4 | lm loss: 6.680013E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.263 | TFLOPs: 7.69 |
[default0]:[2023-07-25 18:04:17,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=2465, skipped=1, lr=[1.0765380266666667e-05, 1.0765380266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:04:17,698] [INFO] [timer.py:215:stop] epoch=0/micro_step=2465/global_step=2465, RunningAvgSamplesPerSec=1.504663136344927, CurrSamplesPerSec=1.3992668059771451, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2465/109863281 | consumed samples:         9860 | consumed tokens:     20193280 | elapsed time per iteration (ms): 3125.4 | learning rate: 1.077E-05 | global batch size:     4 | lm loss: 6.604271E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.280 | TFLOPs: 7.79 |
[default0]:[2023-07-25 18:04:33,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=2470, skipped=1, lr=[1.07872256e-05, 1.07872256e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:04:33,838] [INFO] [timer.py:215:stop] epoch=0/micro_step=2470/global_step=2470, RunningAvgSamplesPerSec=1.504226000701491, CurrSamplesPerSec=1.2569948611996449, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2470/109863281 | consumed samples:         9880 | consumed tokens:     20234240 | elapsed time per iteration (ms): 3228.9 | learning rate: 1.079E-05 | global batch size:     4 | lm loss: 6.505653E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.239 | TFLOPs: 7.54 |
[default0]:[2023-07-25 18:04:49,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=2475, skipped=1, lr=[1.0809070933333334e-05, 1.0809070933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:04:49,221] [INFO] [timer.py:215:stop] epoch=0/micro_step=2475/global_step=2475, RunningAvgSamplesPerSec=1.5039223831615451, CurrSamplesPerSec=1.3755439034886812, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2475/109863281 | consumed samples:         9900 | consumed tokens:     20275200 | elapsed time per iteration (ms): 3094.3 | learning rate: 1.081E-05 | global batch size:     4 | lm loss: 6.600385E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.293 | TFLOPs: 7.87 |
[default0]:[2023-07-25 18:05:05,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=2480, skipped=1, lr=[1.0830916266666667e-05, 1.0830916266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:05:05,160] [INFO] [timer.py:215:stop] epoch=0/micro_step=2480/global_step=2480, RunningAvgSamplesPerSec=1.5035847846290151, CurrSamplesPerSec=1.3879829377173736, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2480/109863281 | consumed samples:         9920 | consumed tokens:     20316160 | elapsed time per iteration (ms): 3117.6 | learning rate: 1.083E-05 | global batch size:     4 | lm loss: 6.582598E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.283 | TFLOPs: 7.81 |
[default0]:[2023-07-25 18:05:21,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=2485, skipped=1, lr=[1.08527616e-05, 1.08527616e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:05:21,425] [INFO] [timer.py:215:stop] epoch=0/micro_step=2485/global_step=2485, RunningAvgSamplesPerSec=1.5031935277157331, CurrSamplesPerSec=1.3439913457588515, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2485/109863281 | consumed samples:         9940 | consumed tokens:     20357120 | elapsed time per iteration (ms): 3225.9 | learning rate: 1.085E-05 | global batch size:     4 | lm loss: 6.505071E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.240 | TFLOPs: 7.55 |
[default1]:[2023-07-25 18:05:24,641] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default1]:[2023-07-25 18:05:24,659] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[default1]:[2023-07-25 18:05:24,658] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default1]:[2023-07-25 18:05:24,658] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[default0]:[2023-07-25 18:05:24,655] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default0]:[2023-07-25 18:05:24,674] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[default0]:[2023-07-25 18:05:24,659] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default0]:[2023-07-25 18:05:24,711] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[default0]:[2023-07-25 18:05:37,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=2490, skipped=1, lr=[1.0874606933333333e-05, 1.0874606933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:05:37,124] [INFO] [timer.py:215:stop] epoch=0/micro_step=2490/global_step=2490, RunningAvgSamplesPerSec=1.5029096544542957, CurrSamplesPerSec=1.3528909860655716, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2490/109863281 | consumed samples:         9960 | consumed tokens:     20398080 | elapsed time per iteration (ms): 3066.1 | learning rate: 1.087E-05 | global batch size:     4 | lm loss: 6.449823E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.305 | TFLOPs: 7.94 |
[default0]:[2023-07-25 18:05:53,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=2495, skipped=1, lr=[1.0896452266666667e-05, 1.0896452266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:05:53,331] [INFO] [timer.py:215:stop] epoch=0/micro_step=2495/global_step=2495, RunningAvgSamplesPerSec=1.5025295696640861, CurrSamplesPerSec=1.2825596984215553, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2495/109863281 | consumed samples:         9980 | consumed tokens:     20439040 | elapsed time per iteration (ms): 3222.9 | learning rate: 1.090E-05 | global batch size:     4 | lm loss: 6.366077E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.241 | TFLOPs: 7.56 |
[default0]:[2023-07-25 18:06:09,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=2500, skipped=1, lr=[1.09182976e-05, 1.09182976e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:06:09,304] [INFO] [timer.py:215:stop] epoch=0/micro_step=2500/global_step=2500, RunningAvgSamplesPerSec=1.502254665046624, CurrSamplesPerSec=1.3120252601298823, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2500/109863281 | consumed samples:        10000 | consumed tokens:     20480000 | elapsed time per iteration (ms): 3182.7 | learning rate: 1.092E-05 | global batch size:     4 | lm loss: 6.646404E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.257 | TFLOPs: 7.65 |
[default0]:saving checkpoint at iteration    2500 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 2500 | lm loss value: 6.684758E+00 | lm loss PPL: 8.001172E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-25 18:06:23,313] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2500 is about to be saved!
[default1]:[2023-07-25 18:06:23,378] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2500 is ready now!
[default0]:[2023-07-25 18:06:23,383] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2500 is ready now!
[default1]:[2023-07-25 18:06:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2500 is ready now!
[default0]:[2023-07-25 18:06:23,376] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2500/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:06:23,376] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2500/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 18:07:34,259] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2500/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:07:34,277] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2500 is ready now!
[default0]:  successfully saved checkpoint at iteration    2500 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.397
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71397.03, 71407.20)
[default0]:[2023-07-25 18:07:50,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=2505, skipped=1, lr=[1.0940142933333333e-05, 1.0940142933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:07:50,247] [INFO] [timer.py:215:stop] epoch=0/micro_step=2505/global_step=2505, RunningAvgSamplesPerSec=1.5019404760260877, CurrSamplesPerSec=1.3733076372504143, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2505/109863281 | consumed samples:        10020 | consumed tokens:     20520960 | elapsed time per iteration (ms): 20184.0 | learning rate: 1.094E-05 | global batch size:     4 | lm loss: 6.211438E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.198 | TFLOPs: 1.21 |
[default0]:[2023-07-25 18:08:06,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=2510, skipped=1, lr=[1.0961988266666666e-05, 1.0961988266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:08:06,984] [INFO] [timer.py:215:stop] epoch=0/micro_step=2510/global_step=2510, RunningAvgSamplesPerSec=1.5013998592844802, CurrSamplesPerSec=1.3265569131607031, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2510/109863281 | consumed samples:        10040 | consumed tokens:     20561920 | elapsed time per iteration (ms): 3347.8 | learning rate: 1.096E-05 | global batch size:     4 | lm loss: 6.532503E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.195 | TFLOPs: 7.28 |
[default0]:[2023-07-25 18:08:22,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=2515, skipped=1, lr=[1.0983833600000001e-05, 1.0983833600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:08:22,917] [INFO] [timer.py:215:stop] epoch=0/micro_step=2515/global_step=2515, RunningAvgSamplesPerSec=1.501126297047436, CurrSamplesPerSec=1.3501699621350485, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2515/109863281 | consumed samples:        10060 | consumed tokens:     20602880 | elapsed time per iteration (ms): 3172.6 | learning rate: 1.098E-05 | global batch size:     4 | lm loss: 6.471463E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.261 | TFLOPs: 7.68 |
[default0]:[2023-07-25 18:08:38,418] [INFO] [logging.py:96:log_dist] [Rank 0] step=2520, skipped=1, lr=[1.1005678933333335e-05, 1.1005678933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:08:38,437] [INFO] [timer.py:215:stop] epoch=0/micro_step=2520/global_step=2520, RunningAvgSamplesPerSec=1.5008518601934369, CurrSamplesPerSec=1.3369343582208013, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2520/109863281 | consumed samples:        10080 | consumed tokens:     20643840 | elapsed time per iteration (ms): 3087.9 | learning rate: 1.101E-05 | global batch size:     4 | lm loss: 6.399725E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.295 | TFLOPs: 7.89 |
[default0]:[2023-07-25 18:08:54,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=2525, skipped=1, lr=[1.1027524266666668e-05, 1.1027524266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:08:54,134] [INFO] [timer.py:215:stop] epoch=0/micro_step=2525/global_step=2525, RunningAvgSamplesPerSec=1.5005543548672706, CurrSamplesPerSec=1.3257206325289899, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2525/109863281 | consumed samples:        10100 | consumed tokens:     20684800 | elapsed time per iteration (ms): 3132.8 | learning rate: 1.103E-05 | global batch size:     4 | lm loss: 6.679164E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.277 | TFLOPs: 7.77 |
[default0]:[2023-07-25 18:09:10,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=2530, skipped=1, lr=[1.1049369600000001e-05, 1.1049369600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:09:10,067] [INFO] [timer.py:215:stop] epoch=0/micro_step=2530/global_step=2530, RunningAvgSamplesPerSec=1.5001534961688827, CurrSamplesPerSec=1.235511245202118, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2530/109863281 | consumed samples:        10120 | consumed tokens:     20725760 | elapsed time per iteration (ms): 3190.2 | learning rate: 1.105E-05 | global batch size:     4 | lm loss: 6.390533E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.254 | TFLOPs: 7.63 |
[default0]:[2023-07-25 18:09:25,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=2535, skipped=1, lr=[1.1071214933333334e-05, 1.1071214933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:09:25,312] [INFO] [timer.py:215:stop] epoch=0/micro_step=2535/global_step=2535, RunningAvgSamplesPerSec=1.4999099060434276, CurrSamplesPerSec=1.3825996263229998, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2535/109863281 | consumed samples:        10140 | consumed tokens:     20766720 | elapsed time per iteration (ms): 3033.1 | learning rate: 1.107E-05 | global batch size:     4 | lm loss: 6.449286E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.319 | TFLOPs: 8.03 |
[default0]:[2023-07-25 18:09:40,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=2540, skipped=1, lr=[1.1093060266666668e-05, 1.1093060266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:09:40,923] [INFO] [timer.py:215:stop] epoch=0/micro_step=2540/global_step=2540, RunningAvgSamplesPerSec=1.4996564974286315, CurrSamplesPerSec=1.354445741167839, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2540/109863281 | consumed samples:        10160 | consumed tokens:     20807680 | elapsed time per iteration (ms): 3106.2 | learning rate: 1.109E-05 | global batch size:     4 | lm loss: 6.577388E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.288 | TFLOPs: 7.84 |
[default0]:[2023-07-25 18:09:56,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=2545, skipped=1, lr=[1.1114905600000001e-05, 1.1114905600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:09:56,614] [INFO] [timer.py:215:stop] epoch=0/micro_step=2545/global_step=2545, RunningAvgSamplesPerSec=1.499388717651279, CurrSamplesPerSec=1.3482230356654525, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2545/109863281 | consumed samples:        10180 | consumed tokens:     20848640 | elapsed time per iteration (ms): 3143.0 | learning rate: 1.111E-05 | global batch size:     4 | lm loss: 6.654691E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.273 | TFLOPs: 7.75 |
[default0]:[2023-07-25 18:10:11,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=2550, skipped=1, lr=[1.1136750933333334e-05, 1.1136750933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:10:11,652] [INFO] [timer.py:215:stop] epoch=0/micro_step=2550/global_step=2550, RunningAvgSamplesPerSec=1.4992153193639275, CurrSamplesPerSec=1.4576352884815713, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2550/109863281 | consumed samples:        10200 | consumed tokens:     20889600 | elapsed time per iteration (ms): 2988.5 | learning rate: 1.114E-05 | global batch size:     4 | lm loss: 6.385698E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.338 | TFLOPs: 8.15 |
[default0]:saving checkpoint at iteration    2550 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:10:12,256] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2550 is about to be saved!
[default0]:[2023-07-25 18:10:12,337] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2550/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:10:12,337] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2550/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 18:10:12,340] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2550 is ready now!
[default1]:[2023-07-25 18:10:12,338] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2550 is ready now!
[default0]:[2023-07-25 18:10:12,349] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2550 is ready now!
[default0]:[2023-07-25 18:11:23,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2550/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:11:23,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2550 is ready now!
[default0]:  successfully saved checkpoint at iteration    2550 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.636
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71617.16, 71636.50)
[default0]:[2023-07-25 18:11:39,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=2555, skipped=1, lr=[1.1158596266666667e-05, 1.1158596266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:11:39,299] [INFO] [timer.py:215:stop] epoch=0/micro_step=2555/global_step=2555, RunningAvgSamplesPerSec=1.4989130604596883, CurrSamplesPerSec=1.3988171780241871, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2555/109863281 | consumed samples:        10220 | consumed tokens:     20930560 | elapsed time per iteration (ms): 17532.6 | learning rate: 1.116E-05 | global batch size:     4 | lm loss: 6.582230E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.228 | TFLOPs: 1.39 |
[default0]:[2023-07-25 18:11:54,393] [INFO] [logging.py:96:log_dist] [Rank 0] step=2560, skipped=1, lr=[1.11804416e-05, 1.11804416e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:11:54,403] [INFO] [timer.py:215:stop] epoch=0/micro_step=2560/global_step=2560, RunningAvgSamplesPerSec=1.4987315583907301, CurrSamplesPerSec=1.4356344405098187, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2560/109863281 | consumed samples:        10240 | consumed tokens:     20971520 | elapsed time per iteration (ms): 3015.3 | learning rate: 1.118E-05 | global batch size:     4 | lm loss: 6.710561E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.327 | TFLOPs: 8.08 |
[default0]:[2023-07-25 18:12:09,174] [INFO] [logging.py:96:log_dist] [Rank 0] step=2565, skipped=1, lr=[1.1202286933333334e-05, 1.1202286933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:12:09,181] [INFO] [timer.py:215:stop] epoch=0/micro_step=2565/global_step=2565, RunningAvgSamplesPerSec=1.4986247500980503, CurrSamplesPerSec=1.4470560847981027, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2565/109863281 | consumed samples:        10260 | consumed tokens:     21012480 | elapsed time per iteration (ms): 2933.8 | learning rate: 1.120E-05 | global batch size:     4 | lm loss: 6.304803E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.363 | TFLOPs: 8.30 |
[default0]:[2023-07-25 18:12:24,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=2570, skipped=1, lr=[1.1224132266666667e-05, 1.1224132266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:12:24,193] [INFO] [timer.py:215:stop] epoch=0/micro_step=2570/global_step=2570, RunningAvgSamplesPerSec=1.4984682220841403, CurrSamplesPerSec=1.4144841242072559, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2570/109863281 | consumed samples:        10280 | consumed tokens:     21053440 | elapsed time per iteration (ms): 2999.1 | learning rate: 1.122E-05 | global batch size:     4 | lm loss: 6.597832E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.334 | TFLOPs: 8.12 |
[default0]:[2023-07-25 18:12:40,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=2575, skipped=1, lr=[1.12459776e-05, 1.12459776e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:12:40,373] [INFO] [timer.py:215:stop] epoch=0/micro_step=2575/global_step=2575, RunningAvgSamplesPerSec=1.498063257606411, CurrSamplesPerSec=1.4157798220260167, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2575/109863281 | consumed samples:        10300 | consumed tokens:     21094400 | elapsed time per iteration (ms): 3228.8 | learning rate: 1.125E-05 | global batch size:     4 | lm loss: 6.624629E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.239 | TFLOPs: 7.54 |
[default0]:[2023-07-25 18:12:55,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=2580, skipped=1, lr=[1.1267822933333334e-05, 1.1267822933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:12:55,867] [INFO] [timer.py:215:stop] epoch=0/micro_step=2580/global_step=2580, RunningAvgSamplesPerSec=1.4977837681784645, CurrSamplesPerSec=1.3367035325025434, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2580/109863281 | consumed samples:        10320 | consumed tokens:     21135360 | elapsed time per iteration (ms): 3100.4 | learning rate: 1.127E-05 | global batch size:     4 | lm loss: 6.315928E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.290 | TFLOPs: 7.86 |
[default0]:[2023-07-25 18:13:10,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=2585, skipped=1, lr=[1.1289668266666667e-05, 1.1289668266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:13:10,570] [INFO] [timer.py:215:stop] epoch=0/micro_step=2585/global_step=2585, RunningAvgSamplesPerSec=1.4976699649513594, CurrSamplesPerSec=1.418767814706252, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2585/109863281 | consumed samples:        10340 | consumed tokens:     21176320 | elapsed time per iteration (ms): 2929.5 | learning rate: 1.129E-05 | global batch size:     4 | lm loss: 6.384960E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.365 | TFLOPs: 8.31 |
[default0]:[2023-07-25 18:13:26,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=2590, skipped=1, lr=[1.13115136e-05, 1.13115136e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:13:26,075] [INFO] [timer.py:215:stop] epoch=0/micro_step=2590/global_step=2590, RunningAvgSamplesPerSec=1.497389331701342, CurrSamplesPerSec=1.3397076094045193, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2590/109863281 | consumed samples:        10360 | consumed tokens:     21217280 | elapsed time per iteration (ms): 3090.4 | learning rate: 1.131E-05 | global batch size:     4 | lm loss: 6.270040E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.294 | TFLOPs: 7.88 |
[default0]:[2023-07-25 18:13:41,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=2595, skipped=1, lr=[1.1333358933333333e-05, 1.1333358933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:13:41,568] [INFO] [timer.py:215:stop] epoch=0/micro_step=2595/global_step=2595, RunningAvgSamplesPerSec=1.4972215148298853, CurrSamplesPerSec=1.3289891360477781, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2595/109863281 | consumed samples:        10380 | consumed tokens:     21258240 | elapsed time per iteration (ms): 3091.3 | learning rate: 1.133E-05 | global batch size:     4 | lm loss: 6.570183E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.294 | TFLOPs: 7.88 |
[default0]:[2023-07-25 18:13:56,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=2600, skipped=1, lr=[1.1355204266666667e-05, 1.1355204266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:13:56,963] [INFO] [timer.py:215:stop] epoch=0/micro_step=2600/global_step=2600, RunningAvgSamplesPerSec=1.4969632507147417, CurrSamplesPerSec=1.3577518889883589, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2600/109863281 | consumed samples:        10400 | consumed tokens:     21299200 | elapsed time per iteration (ms): 3078.5 | learning rate: 1.136E-05 | global batch size:     4 | lm loss: 6.409483E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.299 | TFLOPs: 7.91 |
[default0]:saving checkpoint at iteration    2600 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:14:10,374] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2600 is about to be saved!
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 2600 | lm loss value: 6.699386E+00 | lm loss PPL: 8.119069E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default1]:[2023-07-25 18:14:10,455] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2600 is ready now!
[default0]:[2023-07-25 18:14:10,453] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2600/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:14:10,453] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2600/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 18:14:10,453] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2600 is ready now!
[default0]:[2023-07-25 18:14:10,475] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2600 is ready now!
[default0]:[2023-07-25 18:15:21,171] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2600/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:15:21,194] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2600 is ready now!
[default0]:  successfully saved checkpoint at iteration    2600 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.981
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70979.26, 70980.51)
[default0]:[2023-07-25 18:15:37,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=2605, skipped=1, lr=[1.1377049600000002e-05, 1.1377049600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:15:37,040] [INFO] [timer.py:215:stop] epoch=0/micro_step=2605/global_step=2605, RunningAvgSamplesPerSec=1.4965996707800295, CurrSamplesPerSec=1.303783971131702, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2605/109863281 | consumed samples:        10420 | consumed tokens:     21340160 | elapsed time per iteration (ms): 20006.4 | learning rate: 1.138E-05 | global batch size:     4 | lm loss: 6.322683E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.200 | TFLOPs: 1.22 |
[default0]:[2023-07-25 18:15:52,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=2610, skipped=1, lr=[1.1398894933333335e-05, 1.1398894933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:15:52,858] [INFO] [timer.py:215:stop] epoch=0/micro_step=2610/global_step=2610, RunningAvgSamplesPerSec=1.4962376270837727, CurrSamplesPerSec=1.3269801722750374, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2610/109863281 | consumed samples:        10440 | consumed tokens:     21381120 | elapsed time per iteration (ms): 3153.9 | learning rate: 1.140E-05 | global batch size:     4 | lm loss: 6.524174E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.268 | TFLOPs: 7.72 |
[default0]:[2023-07-25 18:16:08,477] [INFO] [logging.py:96:log_dist] [Rank 0] step=2615, skipped=1, lr=[1.1420740266666668e-05, 1.1420740266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:16:08,497] [INFO] [timer.py:215:stop] epoch=0/micro_step=2615/global_step=2615, RunningAvgSamplesPerSec=1.4959585718215456, CurrSamplesPerSec=1.3596346627574871, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2615/109863281 | consumed samples:        10460 | consumed tokens:     21422080 | elapsed time per iteration (ms): 3073.4 | learning rate: 1.142E-05 | global batch size:     4 | lm loss: 6.476642E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.302 | TFLOPs: 7.92 |
[default0]:[2023-07-25 18:16:23,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=2620, skipped=1, lr=[1.1442585600000001e-05, 1.1442585600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:16:23,608] [INFO] [timer.py:215:stop] epoch=0/micro_step=2620/global_step=2620, RunningAvgSamplesPerSec=1.4957682293288581, CurrSamplesPerSec=1.4115609195918717, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2620/109863281 | consumed samples:        10480 | consumed tokens:     21463040 | elapsed time per iteration (ms): 3014.7 | learning rate: 1.144E-05 | global batch size:     4 | lm loss: 6.455215E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.327 | TFLOPs: 8.08 |
[default0]:[2023-07-25 18:16:38,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=2625, skipped=1, lr=[1.1464430933333335e-05, 1.1464430933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:16:38,389] [INFO] [timer.py:215:stop] epoch=0/micro_step=2625/global_step=2625, RunningAvgSamplesPerSec=1.4956266431258638, CurrSamplesPerSec=1.4427842410465241, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2625/109863281 | consumed samples:        10500 | consumed tokens:     21504000 | elapsed time per iteration (ms): 2955.7 | learning rate: 1.146E-05 | global batch size:     4 | lm loss: 6.613914E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.353 | TFLOPs: 8.24 |
[default0]:[2023-07-25 18:16:53,076] [INFO] [logging.py:96:log_dist] [Rank 0] step=2630, skipped=1, lr=[1.1486276266666668e-05, 1.1486276266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:16:53,097] [INFO] [timer.py:215:stop] epoch=0/micro_step=2630/global_step=2630, RunningAvgSamplesPerSec=1.495524949746371, CurrSamplesPerSec=1.4431787820612185, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2630/109863281 | consumed samples:        10520 | consumed tokens:     21544960 | elapsed time per iteration (ms): 2929.6 | learning rate: 1.149E-05 | global batch size:     4 | lm loss: 6.666016E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.365 | TFLOPs: 8.31 |
[default0]:[2023-07-25 18:17:08,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=2635, skipped=1, lr=[1.1508121600000001e-05, 1.1508121600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:17:08,080] [INFO] [timer.py:215:stop] epoch=0/micro_step=2635/global_step=2635, RunningAvgSamplesPerSec=1.4953569090018015, CurrSamplesPerSec=1.367563860261015, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2635/109863281 | consumed samples:        10540 | consumed tokens:     21585920 | elapsed time per iteration (ms): 2996.0 | learning rate: 1.151E-05 | global batch size:     4 | lm loss: 6.667219E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.335 | TFLOPs: 8.13 |
[default0]:[2023-07-25 18:17:22,885] [INFO] [logging.py:96:log_dist] [Rank 0] step=2640, skipped=1, lr=[1.1529966933333335e-05, 1.1529966933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:17:22,907] [INFO] [timer.py:215:stop] epoch=0/micro_step=2640/global_step=2640, RunningAvgSamplesPerSec=1.4952442515276043, CurrSamplesPerSec=1.4168367561079978, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2640/109863281 | consumed samples:        10560 | consumed tokens:     21626880 | elapsed time per iteration (ms): 2955.2 | learning rate: 1.153E-05 | global batch size:     4 | lm loss: 6.497673E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.354 | TFLOPs: 8.24 |
[default0]:[2023-07-25 18:17:39,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=2645, skipped=1, lr=[1.1551812266666668e-05, 1.1551812266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:17:39,088] [INFO] [timer.py:215:stop] epoch=0/micro_step=2645/global_step=2645, RunningAvgSamplesPerSec=1.4949191854316946, CurrSamplesPerSec=1.3521002899438865, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2645/109863281 | consumed samples:        10580 | consumed tokens:     21667840 | elapsed time per iteration (ms): 3223.3 | learning rate: 1.155E-05 | global batch size:     4 | lm loss: 6.395239E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.241 | TFLOPs: 7.56 |
[default0]:[2023-07-25 18:17:54,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=2650, skipped=1, lr=[1.1573657600000001e-05, 1.1573657600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:17:54,525] [INFO] [timer.py:215:stop] epoch=0/micro_step=2650/global_step=2650, RunningAvgSamplesPerSec=1.4946984292434757, CurrSamplesPerSec=1.429803411166862, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2650/109863281 | consumed samples:        10600 | consumed tokens:     21708800 | elapsed time per iteration (ms): 3085.3 | learning rate: 1.157E-05 | global batch size:     4 | lm loss: 6.575273E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.296 | TFLOPs: 7.89 |
[default0]:saving checkpoint at iteration    2650 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:17:54,761] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2650 is about to be saved!
[default0]:[2023-07-25 18:17:54,855] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2650/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:17:54,855] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2650/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 18:17:54,858] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2650 is ready now!
[default0]:[2023-07-25 18:19:05,337] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2650/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:19:05,360] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2650 is ready now!
[default0]:  successfully saved checkpoint at iteration    2650 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.766
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70758.08, 70770.07)
[default0]:[2023-07-25 18:19:21,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=2655, skipped=1, lr=[1.1595502933333334e-05, 1.1595502933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:19:21,119] [INFO] [timer.py:215:stop] epoch=0/micro_step=2655/global_step=2655, RunningAvgSamplesPerSec=1.4944720733259262, CurrSamplesPerSec=1.417059822432705, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2655/109863281 | consumed samples:        10620 | consumed tokens:     21749760 | elapsed time per iteration (ms): 17310.5 | learning rate: 1.160E-05 | global batch size:     4 | lm loss: 6.407410E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.231 | TFLOPs: 1.41 |
[default0]:[2023-07-25 18:19:36,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=2660, skipped=1, lr=[1.1617348266666668e-05, 1.1617348266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:19:36,752] [INFO] [timer.py:215:stop] epoch=0/micro_step=2660/global_step=2660, RunningAvgSamplesPerSec=1.4942029287859264, CurrSamplesPerSec=1.2896197058302867, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2660/109863281 | consumed samples:        10640 | consumed tokens:     21790720 | elapsed time per iteration (ms): 3124.2 | learning rate: 1.162E-05 | global batch size:     4 | lm loss: 6.448039E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.280 | TFLOPs: 7.80 |
[default0]:[2023-07-25 18:19:52,364] [INFO] [logging.py:96:log_dist] [Rank 0] step=2665, skipped=1, lr=[1.16391936e-05, 1.16391936e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:19:52,400] [INFO] [timer.py:215:stop] epoch=0/micro_step=2665/global_step=2665, RunningAvgSamplesPerSec=1.493962630857372, CurrSamplesPerSec=1.3984647010781635, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2665/109863281 | consumed samples:        10660 | consumed tokens:     21831680 | elapsed time per iteration (ms): 3121.1 | learning rate: 1.164E-05 | global batch size:     4 | lm loss: 6.368627E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.282 | TFLOPs: 7.80 |
[default0]:[2023-07-25 18:20:07,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=2670, skipped=1, lr=[1.1661038933333334e-05, 1.1661038933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:20:07,577] [INFO] [timer.py:215:stop] epoch=0/micro_step=2670/global_step=2670, RunningAvgSamplesPerSec=1.4937798070439452, CurrSamplesPerSec=1.6465617363882417, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2670/109863281 | consumed samples:        10680 | consumed tokens:     21872640 | elapsed time per iteration (ms): 3021.6 | learning rate: 1.166E-05 | global batch size:     4 | lm loss: 6.589023E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.324 | TFLOPs: 8.06 |
[default0]:[2023-07-25 18:20:21,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=2675, skipped=1, lr=[1.1682884266666667e-05, 1.1682884266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:20:21,162] [INFO] [timer.py:215:stop] epoch=0/micro_step=2675/global_step=2675, RunningAvgSamplesPerSec=1.493930942676535, CurrSamplesPerSec=1.5443542552270983, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2675/109863281 | consumed samples:        10700 | consumed tokens:     21913600 | elapsed time per iteration (ms): 2711.2 | learning rate: 1.168E-05 | global batch size:     4 | lm loss: 6.463824E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.475 | TFLOPs: 8.98 |
[default0]:[2023-07-25 18:20:34,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=2680, skipped=1, lr=[1.17047296e-05, 1.17047296e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:20:34,054] [INFO] [timer.py:215:stop] epoch=0/micro_step=2680/global_step=2680, RunningAvgSamplesPerSec=1.4941956636817748, CurrSamplesPerSec=1.5976351837456784, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2680/109863281 | consumed samples:        10720 | consumed tokens:     21954560 | elapsed time per iteration (ms): 2577.8 | learning rate: 1.170E-05 | global batch size:     4 | lm loss: 6.425808E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.552 | TFLOPs: 9.45 |
[default0]:[2023-07-25 18:20:46,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=2685, skipped=1, lr=[1.1726574933333334e-05, 1.1726574933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:20:46,915] [INFO] [timer.py:215:stop] epoch=0/micro_step=2685/global_step=2685, RunningAvgSamplesPerSec=1.4944739104791378, CurrSamplesPerSec=1.7128194981831197, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2685/109863281 | consumed samples:        10740 | consumed tokens:     21995520 | elapsed time per iteration (ms): 2566.8 | learning rate: 1.173E-05 | global batch size:     4 | lm loss: 6.459398E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.558 | TFLOPs: 9.49 |
[default0]:[2023-07-25 18:21:00,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=2690, skipped=1, lr=[1.1748420266666667e-05, 1.1748420266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:21:00,412] [INFO] [timer.py:215:stop] epoch=0/micro_step=2690/global_step=2690, RunningAvgSamplesPerSec=1.4946395026892252, CurrSamplesPerSec=1.4775302831683752, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2690/109863281 | consumed samples:        10760 | consumed tokens:     22036480 | elapsed time per iteration (ms): 2686.6 | learning rate: 1.175E-05 | global batch size:     4 | lm loss: 6.391565E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.489 | TFLOPs: 9.07 |
[default0]:[2023-07-25 18:21:13,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=2695, skipped=1, lr=[1.1770265600000002e-05, 1.1770265600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:21:13,494] [INFO] [timer.py:215:stop] epoch=0/micro_step=2695/global_step=2695, RunningAvgSamplesPerSec=1.494901538103454, CurrSamplesPerSec=1.5586702132453025, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2695/109863281 | consumed samples:        10780 | consumed tokens:     22077440 | elapsed time per iteration (ms): 2600.9 | learning rate: 1.177E-05 | global batch size:     4 | lm loss: 6.412514E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.538 | TFLOPs: 9.36 |
[default0]:[2023-07-25 18:21:15,878] [INFO] [fused_optimizer.py:362:_update_scale] 
[default0]:Grad overflow on iteration 2695
[default0]:[2023-07-25 18:21:15,879] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[default0]:[2023-07-25 18:21:15,879] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[default1]:[2023-07-25 18:21:15,879] [INFO] [fused_optimizer.py:362:_update_scale] 
[default1]:Grad overflow on iteration 2695
[default1]:[2023-07-25 18:21:15,879] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[default0]:[2023-07-25 18:21:15,875] [INFO] [fused_optimizer.py:362:_update_scale] 
[default0]:Grad overflow on iteration 2695
[default0]:[2023-07-25 18:21:15,882] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[default1]:[2023-07-25 18:21:15,875] [INFO] [fused_optimizer.py:362:_update_scale] 
[default1]:Grad overflow on iteration 2695
[default1]:[2023-07-25 18:21:15,890] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[default0]:[2023-07-25 18:21:26,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=2700, skipped=2, lr=[1.1792110933333335e-05, 1.1792110933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:21:26,708] [INFO] [timer.py:215:stop] epoch=0/micro_step=2700/global_step=2700, RunningAvgSamplesPerSec=1.4950860176955807, CurrSamplesPerSec=1.486295376074753, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2700/109863281 | consumed samples:        10800 | consumed tokens:     22118400 | elapsed time per iteration (ms): 2641.8 | learning rate: 1.179E-05 | global batch size:     4 | lm loss: 6.759912E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.514 | TFLOPs: 9.22 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 2700 | lm loss value: 6.668693E+00 | lm loss PPL: 7.873659E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    2700 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:21:38,372] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2700 is about to be saved!
[default0]:[2023-07-25 18:21:38,450] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2700/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:21:38,450] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2700/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 18:21:38,453] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2700 is ready now!
[default0]:[2023-07-25 18:21:38,473] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2700 is ready now!
[default1]:[2023-07-25 18:21:38,451] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2700 is ready now!
[default0]:[2023-07-25 18:22:49,741] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2700/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:22:49,757] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2700 is ready now!
[default0]:  successfully saved checkpoint at iteration    2700 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.56
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71559.54, 71574.91)
[default0]:[2023-07-25 18:23:03,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=2705, skipped=2, lr=[1.1813956266666669e-05, 1.1813956266666669e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:23:03,028] [INFO] [timer.py:215:stop] epoch=0/micro_step=2705/global_step=2705, RunningAvgSamplesPerSec=1.4952860717115068, CurrSamplesPerSec=1.591433140274436, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2705/109863281 | consumed samples:        10820 | consumed tokens:     22159360 | elapsed time per iteration (ms): 19262.0 | learning rate: 1.181E-05 | global batch size:     4 | lm loss: 6.540964E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.208 | TFLOPs: 1.26 |
[default0]:[2023-07-25 18:23:16,832] [INFO] [logging.py:96:log_dist] [Rank 0] step=2710, skipped=2, lr=[1.1835801600000002e-05, 1.1835801600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:23:16,879] [INFO] [timer.py:215:stop] epoch=0/micro_step=2710/global_step=2710, RunningAvgSamplesPerSec=1.4953585437274601, CurrSamplesPerSec=1.5616958735810573, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2710/109863281 | consumed samples:        10840 | consumed tokens:     22200320 | elapsed time per iteration (ms): 2762.5 | learning rate: 1.184E-05 | global batch size:     4 | lm loss: 6.389446E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.448 | TFLOPs: 8.82 |
[default0]:[2023-07-25 18:23:30,357] [INFO] [logging.py:96:log_dist] [Rank 0] step=2715, skipped=2, lr=[1.1857646933333335e-05, 1.1857646933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:23:30,381] [INFO] [timer.py:215:stop] epoch=0/micro_step=2715/global_step=2715, RunningAvgSamplesPerSec=1.4955255318440484, CurrSamplesPerSec=1.6091310995757058, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2715/109863281 | consumed samples:        10860 | consumed tokens:     22241280 | elapsed time per iteration (ms): 2697.0 | learning rate: 1.186E-05 | global batch size:     4 | lm loss: 6.433781E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.483 | TFLOPs: 9.03 |
[default0]:[2023-07-25 18:23:44,149] [INFO] [logging.py:96:log_dist] [Rank 0] step=2720, skipped=2, lr=[1.1879492266666668e-05, 1.1879492266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:23:44,174] [INFO] [timer.py:215:stop] epoch=0/micro_step=2720/global_step=2720, RunningAvgSamplesPerSec=1.4956302935468604, CurrSamplesPerSec=1.4534923522612964, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2720/109863281 | consumed samples:        10880 | consumed tokens:     22282240 | elapsed time per iteration (ms): 2738.4 | learning rate: 1.188E-05 | global batch size:     4 | lm loss: 6.561063E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.461 | TFLOPs: 8.89 |
[default0]:[2023-07-25 18:23:57,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=2725, skipped=2, lr=[1.1901337600000002e-05, 1.1901337600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:23:57,931] [INFO] [timer.py:215:stop] epoch=0/micro_step=2725/global_step=2725, RunningAvgSamplesPerSec=1.49572933859808, CurrSamplesPerSec=1.4527918017772203, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2725/109863281 | consumed samples:        10900 | consumed tokens:     22323200 | elapsed time per iteration (ms): 2753.9 | learning rate: 1.190E-05 | global batch size:     4 | lm loss: 6.429549E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.452 | TFLOPs: 8.84 |
[default0]:[2023-07-25 18:24:11,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=2730, skipped=2, lr=[1.1923182933333335e-05, 1.1923182933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:24:11,339] [INFO] [timer.py:215:stop] epoch=0/micro_step=2730/global_step=2730, RunningAvgSamplesPerSec=1.495902381050756, CurrSamplesPerSec=1.5374662420674012, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2730/109863281 | consumed samples:        10920 | consumed tokens:     22364160 | elapsed time per iteration (ms): 2676.7 | learning rate: 1.192E-05 | global batch size:     4 | lm loss: 6.478209E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.494 | TFLOPs: 9.10 |
[default0]:[2023-07-25 18:24:24,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=2735, skipped=2, lr=[1.1945028266666668e-05, 1.1945028266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:24:24,433] [INFO] [timer.py:215:stop] epoch=0/micro_step=2735/global_step=2735, RunningAvgSamplesPerSec=1.4961220312821102, CurrSamplesPerSec=1.62452663770114, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2735/109863281 | consumed samples:        10940 | consumed tokens:     22405120 | elapsed time per iteration (ms): 2596.5 | learning rate: 1.195E-05 | global batch size:     4 | lm loss: 6.773050E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.541 | TFLOPs: 9.38 |
[default0]:[2023-07-25 18:24:37,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=2740, skipped=2, lr=[1.1966873600000001e-05, 1.1966873600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:24:38,010] [INFO] [timer.py:215:stop] epoch=0/micro_step=2740/global_step=2740, RunningAvgSamplesPerSec=1.4962210176583075, CurrSamplesPerSec=1.533350515558139, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2740/109863281 | consumed samples:        10960 | consumed tokens:     22446080 | elapsed time per iteration (ms): 2712.3 | learning rate: 1.197E-05 | global batch size:     4 | lm loss: 6.381725E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.475 | TFLOPs: 8.98 |
[default0]:[2023-07-25 18:24:51,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=2745, skipped=2, lr=[1.1988718933333335e-05, 1.1988718933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:24:51,335] [INFO] [timer.py:215:stop] epoch=0/micro_step=2745/global_step=2745, RunningAvgSamplesPerSec=1.4963703596035192, CurrSamplesPerSec=1.6410107134309084, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2745/109863281 | consumed samples:        10980 | consumed tokens:     22487040 | elapsed time per iteration (ms): 2657.9 | learning rate: 1.199E-05 | global batch size:     4 | lm loss: 6.445396E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.505 | TFLOPs: 9.16 |
[default0]:[2023-07-25 18:25:04,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=2750, skipped=2, lr=[1.2010564266666668e-05, 1.2010564266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:25:04,507] [INFO] [timer.py:215:stop] epoch=0/micro_step=2750/global_step=2750, RunningAvgSamplesPerSec=1.496593471803465, CurrSamplesPerSec=1.5417627901163773, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2750/109863281 | consumed samples:        11000 | consumed tokens:     22528000 | elapsed time per iteration (ms): 2633.4 | learning rate: 1.201E-05 | global batch size:     4 | lm loss: 6.536546E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.519 | TFLOPs: 9.25 |
[default0]:saving checkpoint at iteration    2750 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:25:04,736] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2750 is about to be saved!
[default1]:[2023-07-25 18:25:04,846] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2750 is ready now!
[default0]:[2023-07-25 18:25:04,857] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2750 is ready now!
[default1]:[2023-07-25 18:25:04,846] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2750 is ready now!
[default0]:[2023-07-25 18:25:04,844] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2750/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:25:04,844] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2750/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 18:26:15,345] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2750/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:26:15,368] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2750 is ready now!
[default0]:  successfully saved checkpoint at iteration    2750 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70797.14, 70799.51)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.797
[default0]:[2023-07-25 18:26:28,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=2755, skipped=2, lr=[1.2032409600000001e-05, 1.2032409600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:26:28,774] [INFO] [timer.py:215:stop] epoch=0/micro_step=2755/global_step=2755, RunningAvgSamplesPerSec=1.4967646426011143, CurrSamplesPerSec=1.5168890405025044, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2755/109863281 | consumed samples:        11020 | consumed tokens:     22568960 | elapsed time per iteration (ms): 16837.0 | learning rate: 1.203E-05 | global batch size:     4 | lm loss: 6.355439E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.238 | TFLOPs: 1.45 |
[default0]:[2023-07-25 18:26:42,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=2760, skipped=2, lr=[1.2054254933333334e-05, 1.2054254933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:26:42,176] [INFO] [timer.py:215:stop] epoch=0/micro_step=2760/global_step=2760, RunningAvgSamplesPerSec=1.49690679379586, CurrSamplesPerSec=1.5729223455882566, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2760/109863281 | consumed samples:        11040 | consumed tokens:     22609920 | elapsed time per iteration (ms): 2685.1 | learning rate: 1.205E-05 | global batch size:     4 | lm loss: 6.339304E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.490 | TFLOPs: 9.07 |
[default0]:[2023-07-25 18:26:55,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=2765, skipped=2, lr=[1.2076100266666668e-05, 1.2076100266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:26:55,620] [INFO] [timer.py:215:stop] epoch=0/micro_step=2765/global_step=2765, RunningAvgSamplesPerSec=1.497061278755151, CurrSamplesPerSec=1.5372567618222797, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2765/109863281 | consumed samples:        11060 | consumed tokens:     22650880 | elapsed time per iteration (ms): 2678.5 | learning rate: 1.208E-05 | global batch size:     4 | lm loss: 6.461476E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.493 | TFLOPs: 9.09 |
[default0]:[2023-07-25 18:27:08,834] [INFO] [logging.py:96:log_dist] [Rank 0] step=2770, skipped=2, lr=[1.2097945600000001e-05, 1.2097945600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:27:08,845] [INFO] [timer.py:215:stop] epoch=0/micro_step=2770/global_step=2770, RunningAvgSamplesPerSec=1.4972618795440054, CurrSamplesPerSec=1.7109014540792498, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2770/109863281 | consumed samples:        11080 | consumed tokens:     22691840 | elapsed time per iteration (ms): 2626.9 | learning rate: 1.210E-05 | global batch size:     4 | lm loss: 6.407942E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.523 | TFLOPs: 9.27 |
[default0]:[2023-07-25 18:27:22,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=2775, skipped=2, lr=[1.2119790933333334e-05, 1.2119790933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:27:22,115] [INFO] [timer.py:215:stop] epoch=0/micro_step=2775/global_step=2775, RunningAvgSamplesPerSec=1.4974556323231891, CurrSamplesPerSec=1.510096916064823, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2775/109863281 | consumed samples:        11100 | consumed tokens:     22732800 | elapsed time per iteration (ms): 2646.0 | learning rate: 1.212E-05 | global batch size:     4 | lm loss: 6.210991E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.512 | TFLOPs: 9.20 |
[default0]:[2023-07-25 18:27:34,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=2780, skipped=2, lr=[1.2141636266666667e-05, 1.2141636266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:27:34,860] [INFO] [timer.py:215:stop] epoch=0/micro_step=2780/global_step=2780, RunningAvgSamplesPerSec=1.4977333879347792, CurrSamplesPerSec=1.6492794460961822, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2780/109863281 | consumed samples:        11120 | consumed tokens:     22773760 | elapsed time per iteration (ms): 2543.9 | learning rate: 1.214E-05 | global batch size:     4 | lm loss: 6.408896E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.572 | TFLOPs: 9.57 |
[default0]:[2023-07-25 18:27:48,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=2785, skipped=2, lr=[1.2163481600000002e-05, 1.2163481600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:27:48,232] [INFO] [timer.py:215:stop] epoch=0/micro_step=2785/global_step=2785, RunningAvgSamplesPerSec=1.4978993984921127, CurrSamplesPerSec=1.7351209630671274, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2785/109863281 | consumed samples:        11140 | consumed tokens:     22814720 | elapsed time per iteration (ms): 2663.0 | learning rate: 1.216E-05 | global batch size:     4 | lm loss: 6.466789E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.502 | TFLOPs: 9.15 |
[default0]:[2023-07-25 18:28:01,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=2790, skipped=2, lr=[1.2185326933333336e-05, 1.2185326933333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:28:01,304] [INFO] [timer.py:215:stop] epoch=0/micro_step=2790/global_step=2790, RunningAvgSamplesPerSec=1.498138185995445, CurrSamplesPerSec=1.6617202312056285, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2790/109863281 | consumed samples:        11160 | consumed tokens:     22855680 | elapsed time per iteration (ms): 2611.4 | learning rate: 1.219E-05 | global batch size:     4 | lm loss: 6.518997E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.532 | TFLOPs: 9.33 |
[default0]:[2023-07-25 18:28:14,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=2795, skipped=2, lr=[1.2207172266666669e-05, 1.2207172266666669e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:28:14,770] [INFO] [timer.py:215:stop] epoch=0/micro_step=2795/global_step=2795, RunningAvgSamplesPerSec=1.4983008784962997, CurrSamplesPerSec=1.5484891833742194, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2795/109863281 | consumed samples:        11180 | consumed tokens:     22896640 | elapsed time per iteration (ms): 2681.4 | learning rate: 1.221E-05 | global batch size:     4 | lm loss: 6.607443E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.492 | TFLOPs: 9.08 |
[default0]:[2023-07-25 18:28:28,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=2800, skipped=2, lr=[1.2229017600000002e-05, 1.2229017600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:28:28,095] [INFO] [timer.py:215:stop] epoch=0/micro_step=2800/global_step=2800, RunningAvgSamplesPerSec=1.4984608658594833, CurrSamplesPerSec=1.6246951254350752, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2800/109863281 | consumed samples:        11200 | consumed tokens:     22937600 | elapsed time per iteration (ms): 2664.8 | learning rate: 1.223E-05 | global batch size:     4 | lm loss: 6.390686E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.501 | TFLOPs: 9.14 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 2800 | lm loss value: 6.592452E+00 | lm loss PPL: 7.295673E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    2800 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:28:39,396] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2800 is about to be saved!
[default0]:[2023-07-25 18:28:39,476] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2800 is ready now!
[default1]:[2023-07-25 18:28:39,465] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2800 is ready now!
[default1]:[2023-07-25 18:28:39,467] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2800 is ready now!
[default0]:[2023-07-25 18:28:39,464] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2800/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:28:39,464] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2800/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 18:29:50,857] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2800/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:29:50,877] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2800 is ready now!
[default0]:  successfully saved checkpoint at iteration    2800 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.647
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71644.73, 71664.25)
[default0]:[2023-07-25 18:30:04,104] [INFO] [logging.py:96:log_dist] [Rank 0] step=2805, skipped=2, lr=[1.2250862933333335e-05, 1.2250862933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:30:04,124] [INFO] [timer.py:215:stop] epoch=0/micro_step=2805/global_step=2805, RunningAvgSamplesPerSec=1.498689673734323, CurrSamplesPerSec=1.6826465381525186, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2805/109863281 | consumed samples:        11220 | consumed tokens:     22978560 | elapsed time per iteration (ms): 19195.3 | learning rate: 1.225E-05 | global batch size:     4 | lm loss: 6.570553E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.208 | TFLOPs: 1.27 |
[default0]:[2023-07-25 18:30:17,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=2810, skipped=2, lr=[1.2272708266666669e-05, 1.2272708266666669e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:30:17,349] [INFO] [timer.py:215:stop] epoch=0/micro_step=2810/global_step=2810, RunningAvgSamplesPerSec=1.4988786876159246, CurrSamplesPerSec=1.5336137441906341, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2810/109863281 | consumed samples:        11240 | consumed tokens:     23019520 | elapsed time per iteration (ms): 2631.8 | learning rate: 1.227E-05 | global batch size:     4 | lm loss: 6.520892E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.520 | TFLOPs: 9.25 |
[default0]:[2023-07-25 18:30:30,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=2815, skipped=2, lr=[1.2294553600000002e-05, 1.2294553600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:30:30,191] [INFO] [timer.py:215:stop] epoch=0/micro_step=2815/global_step=2815, RunningAvgSamplesPerSec=1.4991602876915904, CurrSamplesPerSec=1.649091394482117, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2815/109863281 | consumed samples:        11260 | consumed tokens:     23060480 | elapsed time per iteration (ms): 2558.8 | learning rate: 1.229E-05 | global batch size:     4 | lm loss: 6.476991E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.563 | TFLOPs: 9.52 |
[default0]:[2023-07-25 18:30:43,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=2820, skipped=2, lr=[1.2316398933333335e-05, 1.2316398933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:30:43,582] [INFO] [timer.py:215:stop] epoch=0/micro_step=2820/global_step=2820, RunningAvgSamplesPerSec=1.4993146821687493, CurrSamplesPerSec=1.6843085940496092, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2820/109863281 | consumed samples:        11280 | consumed tokens:     23101440 | elapsed time per iteration (ms): 2684.2 | learning rate: 1.232E-05 | global batch size:     4 | lm loss: 6.504004E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.490 | TFLOPs: 9.07 |
[default0]:[2023-07-25 18:30:56,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=2825, skipped=2, lr=[1.2338244266666668e-05, 1.2338244266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:30:56,835] [INFO] [timer.py:215:stop] epoch=0/micro_step=2825/global_step=2825, RunningAvgSamplesPerSec=1.4995182102463012, CurrSamplesPerSec=1.5044061846196557, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2825/109863281 | consumed samples:        11300 | consumed tokens:     23142400 | elapsed time per iteration (ms): 2633.5 | learning rate: 1.234E-05 | global batch size:     4 | lm loss: 6.621623E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.519 | TFLOPs: 9.25 |
[default0]:[2023-07-25 18:31:10,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=2830, skipped=2, lr=[1.2360089600000002e-05, 1.2360089600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:31:10,206] [INFO] [timer.py:215:stop] epoch=0/micro_step=2830/global_step=2830, RunningAvgSamplesPerSec=1.4997080568067347, CurrSamplesPerSec=1.5538138057315978, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2830/109863281 | consumed samples:        11320 | consumed tokens:     23183360 | elapsed time per iteration (ms): 2674.9 | learning rate: 1.236E-05 | global batch size:     4 | lm loss: 6.347036E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.495 | TFLOPs: 9.11 |
[default0]:[2023-07-25 18:31:23,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=2835, skipped=2, lr=[1.2381934933333335e-05, 1.2381934933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:31:23,290] [INFO] [timer.py:215:stop] epoch=0/micro_step=2835/global_step=2835, RunningAvgSamplesPerSec=1.4999172998645784, CurrSamplesPerSec=1.710574378338047, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2835/109863281 | consumed samples:        11340 | consumed tokens:     23224320 | elapsed time per iteration (ms): 2598.3 | learning rate: 1.238E-05 | global batch size:     4 | lm loss: 6.543756E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.539 | TFLOPs: 9.37 |
[default0]:[2023-07-25 18:31:36,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=2840, skipped=2, lr=[1.2403780266666668e-05, 1.2403780266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:31:36,954] [INFO] [timer.py:215:stop] epoch=0/micro_step=2840/global_step=2840, RunningAvgSamplesPerSec=1.5000175961307718, CurrSamplesPerSec=1.4759042403220082, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2840/109863281 | consumed samples:        11360 | consumed tokens:     23265280 | elapsed time per iteration (ms): 2729.0 | learning rate: 1.240E-05 | global batch size:     4 | lm loss: 6.434612E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.466 | TFLOPs: 8.92 |
[default0]:[2023-07-25 18:31:50,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=2845, skipped=2, lr=[1.24256256e-05, 1.24256256e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:31:50,236] [INFO] [timer.py:215:stop] epoch=0/micro_step=2845/global_step=2845, RunningAvgSamplesPerSec=1.5002073680119739, CurrSamplesPerSec=1.6474597415003287, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2845/109863281 | consumed samples:        11380 | consumed tokens:     23306240 | elapsed time per iteration (ms): 2642.4 | learning rate: 1.243E-05 | global batch size:     4 | lm loss: 6.621841E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.514 | TFLOPs: 9.22 |
[default0]:[2023-07-25 18:32:03,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=2850, skipped=2, lr=[1.2447470933333333e-05, 1.2447470933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:32:03,162] [INFO] [timer.py:215:stop] epoch=0/micro_step=2850/global_step=2850, RunningAvgSamplesPerSec=1.5004424966284597, CurrSamplesPerSec=1.6398237962277598, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2850/109863281 | consumed samples:        11400 | consumed tokens:     23347200 | elapsed time per iteration (ms): 2583.5 | learning rate: 1.245E-05 | global batch size:     4 | lm loss: 6.443840E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.548 | TFLOPs: 9.43 |
[default0]:saving checkpoint at iteration    2850 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:32:03,320] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2850 is about to be saved!
[default1]:[2023-07-25 18:32:03,399] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2850 is ready now!
[default1]:[2023-07-25 18:32:03,396] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2850 is ready now!
[default0]:[2023-07-25 18:32:03,407] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2850 is ready now!
[default0]:[2023-07-25 18:32:03,396] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2850/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:32:03,396] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2850/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 18:33:14,257] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2850/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:33:14,280] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2850 is ready now!
[default0]:  successfully saved checkpoint at iteration    2850 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.096
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71096.02, 71113.46)
[default0]:[2023-07-25 18:33:28,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=2855, skipped=2, lr=[1.2469316266666666e-05, 1.2469316266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:33:28,089] [INFO] [timer.py:215:stop] epoch=0/micro_step=2855/global_step=2855, RunningAvgSamplesPerSec=1.5005276351677808, CurrSamplesPerSec=1.4970843059006154, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2855/109863281 | consumed samples:        11420 | consumed tokens:     23388160 | elapsed time per iteration (ms): 16985.9 | learning rate: 1.247E-05 | global batch size:     4 | lm loss: 6.456815E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.235 | TFLOPs: 1.43 |
[default0]:[2023-07-25 18:33:41,382] [INFO] [logging.py:96:log_dist] [Rank 0] step=2860, skipped=2, lr=[1.24911616e-05, 1.24911616e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:33:41,420] [INFO] [timer.py:215:stop] epoch=0/micro_step=2860/global_step=2860, RunningAvgSamplesPerSec=1.5006871155453982, CurrSamplesPerSec=1.5871137520306933, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2860/109863281 | consumed samples:        11440 | consumed tokens:     23429120 | elapsed time per iteration (ms): 2657.6 | learning rate: 1.249E-05 | global batch size:     4 | lm loss: 6.515474E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.505 | TFLOPs: 9.16 |
[default0]:[2023-07-25 18:33:55,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=2865, skipped=2, lr=[1.2513006933333333e-05, 1.2513006933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:33:55,181] [INFO] [timer.py:215:stop] epoch=0/micro_step=2865/global_step=2865, RunningAvgSamplesPerSec=1.5007796352424112, CurrSamplesPerSec=1.5265300908263246, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2865/109863281 | consumed samples:        11460 | consumed tokens:     23470080 | elapsed time per iteration (ms): 2736.0 | learning rate: 1.251E-05 | global batch size:     4 | lm loss: 6.561255E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.462 | TFLOPs: 8.90 |
[default0]:[2023-07-25 18:34:08,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=2870, skipped=2, lr=[1.2534852266666666e-05, 1.2534852266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:34:08,423] [INFO] [timer.py:215:stop] epoch=0/micro_step=2870/global_step=2870, RunningAvgSamplesPerSec=1.500971304091179, CurrSamplesPerSec=1.5347502520454306, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2870/109863281 | consumed samples:        11480 | consumed tokens:     23511040 | elapsed time per iteration (ms): 2643.9 | learning rate: 1.253E-05 | global batch size:     4 | lm loss: 6.575195E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.513 | TFLOPs: 9.21 |
[default0]:[2023-07-25 18:34:21,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=2875, skipped=2, lr=[1.25566976e-05, 1.25566976e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:34:21,569] [INFO] [timer.py:215:stop] epoch=0/micro_step=2875/global_step=2875, RunningAvgSamplesPerSec=1.501162060860641, CurrSamplesPerSec=1.6494824602800062, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2875/109863281 | consumed samples:        11500 | consumed tokens:     23552000 | elapsed time per iteration (ms): 2628.4 | learning rate: 1.256E-05 | global batch size:     4 | lm loss: 6.429643E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.522 | TFLOPs: 9.27 |
[default0]:[2023-07-25 18:34:34,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=2880, skipped=2, lr=[1.2578542933333333e-05, 1.2578542933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:34:34,494] [INFO] [timer.py:215:stop] epoch=0/micro_step=2880/global_step=2880, RunningAvgSamplesPerSec=1.5014030356722199, CurrSamplesPerSec=1.5293342718695997, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2880/109863281 | consumed samples:        11520 | consumed tokens:     23592960 | elapsed time per iteration (ms): 2578.4 | learning rate: 1.258E-05 | global batch size:     4 | lm loss: 6.475918E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.551 | TFLOPs: 9.45 |
[default0]:[2023-07-25 18:34:47,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=2885, skipped=2, lr=[1.2600388266666666e-05, 1.2600388266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:34:47,578] [INFO] [timer.py:215:stop] epoch=0/micro_step=2885/global_step=2885, RunningAvgSamplesPerSec=1.5016142107894968, CurrSamplesPerSec=1.6594091898850238, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2885/109863281 | consumed samples:        11540 | consumed tokens:     23633920 | elapsed time per iteration (ms): 2605.4 | learning rate: 1.260E-05 | global batch size:     4 | lm loss: 6.433443E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.535 | TFLOPs: 9.35 |
[default0]:[2023-07-25 18:35:00,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=2890, skipped=2, lr=[1.2622233599999999e-05, 1.2622233599999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:35:00,543] [INFO] [timer.py:215:stop] epoch=0/micro_step=2890/global_step=2890, RunningAvgSamplesPerSec=1.5018423777966334, CurrSamplesPerSec=1.7013722048828561, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2890/109863281 | consumed samples:        11560 | consumed tokens:     23674880 | elapsed time per iteration (ms): 2589.6 | learning rate: 1.262E-05 | global batch size:     4 | lm loss: 6.440980E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.545 | TFLOPs: 9.41 |
[default0]:[2023-07-25 18:35:14,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=2895, skipped=2, lr=[1.2644078933333332e-05, 1.2644078933333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:35:14,297] [INFO] [timer.py:215:stop] epoch=0/micro_step=2895/global_step=2895, RunningAvgSamplesPerSec=1.5019143711899074, CurrSamplesPerSec=1.5096260931871002, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2895/109863281 | consumed samples:        11580 | consumed tokens:     23715840 | elapsed time per iteration (ms): 2740.2 | learning rate: 1.264E-05 | global batch size:     4 | lm loss: 6.555361E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.460 | TFLOPs: 8.89 |
[default0]:[2023-07-25 18:35:27,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=2900, skipped=2, lr=[1.2665924266666666e-05, 1.2665924266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:35:27,493] [INFO] [timer.py:215:stop] epoch=0/micro_step=2900/global_step=2900, RunningAvgSamplesPerSec=1.5020793936318435, CurrSamplesPerSec=1.5314212523518245, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2900/109863281 | consumed samples:        11600 | consumed tokens:     23756800 | elapsed time per iteration (ms): 2641.8 | learning rate: 1.267E-05 | global batch size:     4 | lm loss: 6.280482E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.514 | TFLOPs: 9.22 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 2900 | lm loss value: 6.622675E+00 | lm loss PPL: 7.519538E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    2900 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:35:39,031] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2900 is about to be saved!
[default1]:[2023-07-25 18:35:39,101] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2900 is ready now!
[default1]:[2023-07-25 18:35:39,102] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2900 is ready now!
[default0]:[2023-07-25 18:35:39,123] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2900 is ready now!
[default0]:[2023-07-25 18:35:39,104] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2900/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:35:39,104] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2900/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 18:36:49,813] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2900/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:36:49,834] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2900 is ready now!
[default0]:  successfully saved checkpoint at iteration    2900 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.992
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70981.39, 70992.59)
[default0]:[2023-07-25 18:37:03,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=2905, skipped=2, lr=[1.2687769599999999e-05, 1.2687769599999999e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:37:03,534] [INFO] [timer.py:215:stop] epoch=0/micro_step=2905/global_step=2905, RunningAvgSamplesPerSec=1.502189863393978, CurrSamplesPerSec=1.6059471639943184, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2905/109863281 | consumed samples:        11620 | consumed tokens:     23797760 | elapsed time per iteration (ms): 19197.2 | learning rate: 1.269E-05 | global batch size:     4 | lm loss: 6.192400E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.208 | TFLOPs: 1.27 |
[default0]:[2023-07-25 18:37:17,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=2910, skipped=2, lr=[1.2709614933333332e-05, 1.2709614933333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:37:17,382] [INFO] [timer.py:215:stop] epoch=0/micro_step=2910/global_step=2910, RunningAvgSamplesPerSec=1.5022613632566097, CurrSamplesPerSec=1.5006619482486256, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2910/109863281 | consumed samples:        11640 | consumed tokens:     23838720 | elapsed time per iteration (ms): 2757.0 | learning rate: 1.271E-05 | global batch size:     4 | lm loss: 6.348048E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.451 | TFLOPs: 8.83 |
[default0]:[2023-07-25 18:37:30,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=2915, skipped=2, lr=[1.2731460266666667e-05, 1.2731460266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:37:30,916] [INFO] [timer.py:215:stop] epoch=0/micro_step=2915/global_step=2915, RunningAvgSamplesPerSec=1.5023660977360473, CurrSamplesPerSec=1.4937714175411754, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2915/109863281 | consumed samples:        11660 | consumed tokens:     23879680 | elapsed time per iteration (ms): 2715.7 | learning rate: 1.273E-05 | global batch size:     4 | lm loss: 6.434929E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.473 | TFLOPs: 8.97 |
[default0]:[2023-07-25 18:37:44,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=2920, skipped=2, lr=[1.27533056e-05, 1.27533056e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:37:44,266] [INFO] [timer.py:215:stop] epoch=0/micro_step=2920/global_step=2920, RunningAvgSamplesPerSec=1.502532507541508, CurrSamplesPerSec=1.5515752720724394, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2920/109863281 | consumed samples:        11680 | consumed tokens:     23920640 | elapsed time per iteration (ms): 2655.2 | learning rate: 1.275E-05 | global batch size:     4 | lm loss: 6.409497E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.507 | TFLOPs: 9.17 |
[default0]:[2023-07-25 18:37:57,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=2925, skipped=2, lr=[1.2775150933333334e-05, 1.2775150933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:37:57,479] [INFO] [timer.py:215:stop] epoch=0/micro_step=2925/global_step=2925, RunningAvgSamplesPerSec=1.5027107891896923, CurrSamplesPerSec=1.519770210296639, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2925/109863281 | consumed samples:        11700 | consumed tokens:     23961600 | elapsed time per iteration (ms): 2626.5 | learning rate: 1.278E-05 | global batch size:     4 | lm loss: 6.371614E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.523 | TFLOPs: 9.27 |
[default0]:[2023-07-25 18:38:10,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=2930, skipped=2, lr=[1.2796996266666667e-05, 1.2796996266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:38:10,397] [INFO] [timer.py:215:stop] epoch=0/micro_step=2930/global_step=2930, RunningAvgSamplesPerSec=1.5029464233511483, CurrSamplesPerSec=1.666191719917344, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2930/109863281 | consumed samples:        11720 | consumed tokens:     24002560 | elapsed time per iteration (ms): 2578.6 | learning rate: 1.280E-05 | global batch size:     4 | lm loss: 6.584929E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.551 | TFLOPs: 9.45 |
[default0]:[2023-07-25 18:38:23,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=2935, skipped=2, lr=[1.28188416e-05, 1.28188416e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:38:23,687] [INFO] [timer.py:215:stop] epoch=0/micro_step=2935/global_step=2935, RunningAvgSamplesPerSec=1.5031146353543283, CurrSamplesPerSec=1.5670438034335723, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2935/109863281 | consumed samples:        11740 | consumed tokens:     24043520 | elapsed time per iteration (ms): 2652.9 | learning rate: 1.282E-05 | global batch size:     4 | lm loss: 6.590002E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.508 | TFLOPs: 9.18 |
[default0]:[2023-07-25 18:38:36,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=2940, skipped=2, lr=[1.2840686933333333e-05, 1.2840686933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:38:36,449] [INFO] [timer.py:215:stop] epoch=0/micro_step=2940/global_step=2940, RunningAvgSamplesPerSec=1.5033602367868033, CurrSamplesPerSec=1.6284385361421927, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2940/109863281 | consumed samples:        11760 | consumed tokens:     24084480 | elapsed time per iteration (ms): 2560.6 | learning rate: 1.284E-05 | global batch size:     4 | lm loss: 6.473394E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.562 | TFLOPs: 9.51 |
[default0]:[2023-07-25 18:38:49,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=2945, skipped=2, lr=[1.2862532266666667e-05, 1.2862532266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:38:49,969] [INFO] [timer.py:215:stop] epoch=0/micro_step=2945/global_step=2945, RunningAvgSamplesPerSec=1.5035154447938055, CurrSamplesPerSec=1.5402173131710708, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2945/109863281 | consumed samples:        11780 | consumed tokens:     24125440 | elapsed time per iteration (ms): 2680.7 | learning rate: 1.286E-05 | global batch size:     4 | lm loss: 6.370169E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.492 | TFLOPs: 9.09 |
[default0]:[2023-07-25 18:39:03,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=2950, skipped=2, lr=[1.28843776e-05, 1.28843776e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:39:03,573] [INFO] [timer.py:215:stop] epoch=0/micro_step=2950/global_step=2950, RunningAvgSamplesPerSec=1.5036126088649566, CurrSamplesPerSec=1.5967307841106346, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2950/109863281 | consumed samples:        11800 | consumed tokens:     24166400 | elapsed time per iteration (ms): 2722.4 | learning rate: 1.288E-05 | global batch size:     4 | lm loss: 6.397009E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.469 | TFLOPs: 8.95 |
[default0]:saving checkpoint at iteration    2950 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:39:03,840] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2950 is about to be saved!
[default0]:[2023-07-25 18:39:03,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2950 is ready now!
[default1]:[2023-07-25 18:39:03,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2950 is ready now!
[default1]:[2023-07-25 18:39:03,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2950 is ready now!
[default0]:[2023-07-25 18:39:03,943] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2950/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:39:03,943] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2950/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 18:40:15,507] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step2950/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:40:15,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2950 is ready now!
[default0]:  successfully saved checkpoint at iteration    2950 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.855
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71846.53, 71857.80)
[default0]:[2023-07-25 18:40:29,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=2955, skipped=2, lr=[1.2906222933333333e-05, 1.2906222933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:40:29,546] [INFO] [timer.py:215:stop] epoch=0/micro_step=2955/global_step=2955, RunningAvgSamplesPerSec=1.5036536723354073, CurrSamplesPerSec=1.5716227618106486, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2955/109863281 | consumed samples:        11820 | consumed tokens:     24207360 | elapsed time per iteration (ms): 17177.7 | learning rate: 1.291E-05 | global batch size:     4 | lm loss: 6.500940E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.233 | TFLOPs: 1.42 |
[default0]:[2023-07-25 18:40:43,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=2960, skipped=2, lr=[1.2928068266666666e-05, 1.2928068266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:40:43,330] [INFO] [timer.py:215:stop] epoch=0/micro_step=2960/global_step=2960, RunningAvgSamplesPerSec=1.5037309250868967, CurrSamplesPerSec=1.5407433529493877, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2960/109863281 | consumed samples:        11840 | consumed tokens:     24248320 | elapsed time per iteration (ms): 2747.9 | learning rate: 1.293E-05 | global batch size:     4 | lm loss: 6.660410E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.456 | TFLOPs: 8.86 |
[default0]:[2023-07-25 18:40:56,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=2965, skipped=2, lr=[1.29499136e-05, 1.29499136e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:40:56,976] [INFO] [timer.py:215:stop] epoch=0/micro_step=2965/global_step=2965, RunningAvgSamplesPerSec=1.5038332454047723, CurrSamplesPerSec=1.4817793912638584, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2965/109863281 | consumed samples:        11860 | consumed tokens:     24289280 | elapsed time per iteration (ms): 2727.8 | learning rate: 1.295E-05 | global batch size:     4 | lm loss: 6.285892E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.466 | TFLOPs: 8.93 |
[default0]:[2023-07-25 18:41:10,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=2970, skipped=2, lr=[1.2971758933333333e-05, 1.2971758933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:41:10,952] [INFO] [timer.py:215:stop] epoch=0/micro_step=2970/global_step=2970, RunningAvgSamplesPerSec=1.5038589880248745, CurrSamplesPerSec=1.4896127110087327, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2970/109863281 | consumed samples:        11880 | consumed tokens:     24330240 | elapsed time per iteration (ms): 2787.6 | learning rate: 1.297E-05 | global batch size:     4 | lm loss: 6.355075E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.435 | TFLOPs: 8.74 |
[default0]:[2023-07-25 18:41:24,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=2975, skipped=2, lr=[1.2993604266666666e-05, 1.2993604266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:41:24,173] [INFO] [timer.py:215:stop] epoch=0/micro_step=2975/global_step=2975, RunningAvgSamplesPerSec=1.50402286233049, CurrSamplesPerSec=1.614283487507914, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2975/109863281 | consumed samples:        11900 | consumed tokens:     24371200 | elapsed time per iteration (ms): 2640.0 | learning rate: 1.299E-05 | global batch size:     4 | lm loss: 6.415241E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.515 | TFLOPs: 9.23 |
[default0]:[2023-07-25 18:41:39,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=2980, skipped=2, lr=[1.30154496e-05, 1.30154496e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:41:39,875] [INFO] [timer.py:215:stop] epoch=0/micro_step=2980/global_step=2980, RunningAvgSamplesPerSec=1.5041359194166903, CurrSamplesPerSec=1.6273014544650017, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2980/109863281 | consumed samples:        11920 | consumed tokens:     24412160 | elapsed time per iteration (ms): 3121.8 | learning rate: 1.302E-05 | global batch size:     4 | lm loss: 6.380256E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.281 | TFLOPs: 7.80 |
[default0]:[2023-07-25 18:41:53,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=2985, skipped=2, lr=[1.3037294933333333e-05, 1.3037294933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:41:53,323] [INFO] [timer.py:215:stop] epoch=0/micro_step=2985/global_step=2985, RunningAvgSamplesPerSec=1.5042750784854113, CurrSamplesPerSec=1.6835755605816987, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2985/109863281 | consumed samples:        11940 | consumed tokens:     24453120 | elapsed time per iteration (ms): 2689.6 | learning rate: 1.304E-05 | global batch size:     4 | lm loss: 6.315710E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.487 | TFLOPs: 9.06 |
[default0]:[2023-07-25 18:42:06,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=2990, skipped=2, lr=[1.3059140266666666e-05, 1.3059140266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:42:06,688] [INFO] [timer.py:215:stop] epoch=0/micro_step=2990/global_step=2990, RunningAvgSamplesPerSec=1.5044144860860358, CurrSamplesPerSec=1.6129464759971908, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2990/109863281 | consumed samples:        11960 | consumed tokens:     24494080 | elapsed time per iteration (ms): 2666.5 | learning rate: 1.306E-05 | global batch size:     4 | lm loss: 6.527164E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.500 | TFLOPs: 9.13 |
[default0]:[2023-07-25 18:42:20,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=2995, skipped=2, lr=[1.30809856e-05, 1.30809856e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:42:20,228] [INFO] [timer.py:215:stop] epoch=0/micro_step=2995/global_step=2995, RunningAvgSamplesPerSec=1.504516335034493, CurrSamplesPerSec=1.5897645384070966, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     2995/109863281 | consumed samples:        11980 | consumed tokens:     24535040 | elapsed time per iteration (ms): 2710.6 | learning rate: 1.308E-05 | global batch size:     4 | lm loss: 6.544911E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.476 | TFLOPs: 8.99 |
[default0]:[2023-07-25 18:42:33,701] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=2, lr=[1.3102830933333334e-05, 1.3102830933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:42:33,722] [INFO] [timer.py:215:stop] epoch=0/micro_step=3000/global_step=3000, RunningAvgSamplesPerSec=1.5046907068529685, CurrSamplesPerSec=1.6404096620322455, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3000/109863281 | consumed samples:        12000 | consumed tokens:     24576000 | elapsed time per iteration (ms): 2685.4 | learning rate: 1.310E-05 | global batch size:     4 | lm loss: 6.440965E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.490 | TFLOPs: 9.07 |
[default0]:saving checkpoint at iteration    3000 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:42:47,663] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3000 is about to be saved!
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 3000 | lm loss value: 6.599312E+00 | lm loss PPL: 7.345895E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-25 18:42:47,727] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3000/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:42:47,727] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3000/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 18:42:47,728] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
[default1]:[2023-07-25 18:42:47,743] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
[default1]:[2023-07-25 18:42:47,729] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
[default0]:[2023-07-25 18:43:58,636] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3000/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:43:58,651] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
[default0]:  successfully saved checkpoint at iteration    3000 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.148
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71127.96, 71148.21)
[default0]:[2023-07-25 18:44:11,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=3005, skipped=2, lr=[1.3124676266666667e-05, 1.3124676266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:44:12,021] [INFO] [timer.py:215:stop] epoch=0/micro_step=3005/global_step=3005, RunningAvgSamplesPerSec=1.504833546164753, CurrSamplesPerSec=1.59209672748005, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3005/109863281 | consumed samples:        12020 | consumed tokens:     24616960 | elapsed time per iteration (ms): 19661.4 | learning rate: 1.312E-05 | global batch size:     4 | lm loss: 6.541879E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.203 | TFLOPs: 1.24 |
[default0]:[2023-07-25 18:44:25,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=3010, skipped=2, lr=[1.31465216e-05, 1.31465216e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:44:25,297] [INFO] [timer.py:215:stop] epoch=0/micro_step=3010/global_step=3010, RunningAvgSamplesPerSec=1.5049888838247867, CurrSamplesPerSec=1.4246858526193902, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3010/109863281 | consumed samples:        12040 | consumed tokens:     24657920 | elapsed time per iteration (ms): 2650.5 | learning rate: 1.315E-05 | global batch size:     4 | lm loss: 6.391434E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.509 | TFLOPs: 9.19 |
[default0]:[2023-07-25 18:44:38,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=3015, skipped=2, lr=[1.3168366933333334e-05, 1.3168366933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:44:38,469] [INFO] [timer.py:215:stop] epoch=0/micro_step=3015/global_step=3015, RunningAvgSamplesPerSec=1.505162243474524, CurrSamplesPerSec=1.644895247983615, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3015/109863281 | consumed samples:        12060 | consumed tokens:     24698880 | elapsed time per iteration (ms): 2623.6 | learning rate: 1.317E-05 | global batch size:     4 | lm loss: 6.457059E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.525 | TFLOPs: 9.28 |
[default0]:[2023-07-25 18:44:51,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=3020, skipped=2, lr=[1.3190212266666667e-05, 1.3190212266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:44:51,614] [INFO] [timer.py:215:stop] epoch=0/micro_step=3020/global_step=3020, RunningAvgSamplesPerSec=1.5053873163316243, CurrSamplesPerSec=1.5308934555854536, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3020/109863281 | consumed samples:        12080 | consumed tokens:     24739840 | elapsed time per iteration (ms): 2617.2 | learning rate: 1.319E-05 | global batch size:     4 | lm loss: 6.405553E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.528 | TFLOPs: 9.31 |
[default0]:[2023-07-25 18:45:04,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=3025, skipped=2, lr=[1.32120576e-05, 1.32120576e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:45:04,835] [INFO] [timer.py:215:stop] epoch=0/micro_step=3025/global_step=3025, RunningAvgSamplesPerSec=1.5055426425337182, CurrSamplesPerSec=1.6169780412062422, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3025/109863281 | consumed samples:        12100 | consumed tokens:     24780800 | elapsed time per iteration (ms): 2636.5 | learning rate: 1.321E-05 | global batch size:     4 | lm loss: 6.444302E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.517 | TFLOPs: 9.24 |
[default0]:[2023-07-25 18:45:18,377] [INFO] [logging.py:96:log_dist] [Rank 0] step=3030, skipped=2, lr=[1.3233902933333334e-05, 1.3233902933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:45:18,400] [INFO] [timer.py:215:stop] epoch=0/micro_step=3030/global_step=3030, RunningAvgSamplesPerSec=1.505650665144594, CurrSamplesPerSec=1.609539572271109, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3030/109863281 | consumed samples:        12120 | consumed tokens:     24821760 | elapsed time per iteration (ms): 2701.0 | learning rate: 1.323E-05 | global batch size:     4 | lm loss: 6.543121E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.481 | TFLOPs: 9.02 |
[default0]:[2023-07-25 18:45:31,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=3035, skipped=2, lr=[1.3255748266666667e-05, 1.3255748266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:45:31,425] [INFO] [timer.py:215:stop] epoch=0/micro_step=3035/global_step=3035, RunningAvgSamplesPerSec=1.505840384307039, CurrSamplesPerSec=1.446010056522691, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3035/109863281 | consumed samples:        12140 | consumed tokens:     24862720 | elapsed time per iteration (ms): 2596.8 | learning rate: 1.326E-05 | global batch size:     4 | lm loss: 6.287677E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.540 | TFLOPs: 9.38 |
[default0]:[2023-07-25 18:45:44,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=3040, skipped=2, lr=[1.32775936e-05, 1.32775936e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:45:44,678] [INFO] [timer.py:215:stop] epoch=0/micro_step=3040/global_step=3040, RunningAvgSamplesPerSec=1.5059915329497016, CurrSamplesPerSec=1.5284054091208106, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3040/109863281 | consumed samples:        12160 | consumed tokens:     24903680 | elapsed time per iteration (ms): 2652.5 | learning rate: 1.328E-05 | global batch size:     4 | lm loss: 6.480599E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.508 | TFLOPs: 9.18 |
[default0]:[2023-07-25 18:45:57,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=3045, skipped=2, lr=[1.3299438933333333e-05, 1.3299438933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:45:57,786] [INFO] [timer.py:215:stop] epoch=0/micro_step=3045/global_step=3045, RunningAvgSamplesPerSec=1.5062155035036187, CurrSamplesPerSec=1.6330094739999856, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3045/109863281 | consumed samples:        12180 | consumed tokens:     24944640 | elapsed time per iteration (ms): 2613.2 | learning rate: 1.330E-05 | global batch size:     4 | lm loss: 6.313895E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.531 | TFLOPs: 9.32 |
[default0]:[2023-07-25 18:46:11,730] [INFO] [logging.py:96:log_dist] [Rank 0] step=3050, skipped=2, lr=[1.3321284266666667e-05, 1.3321284266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:46:11,748] [INFO] [timer.py:215:stop] epoch=0/micro_step=3050/global_step=3050, RunningAvgSamplesPerSec=1.5062603605704479, CurrSamplesPerSec=1.5148402228466784, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3050/109863281 | consumed samples:        12200 | consumed tokens:     24985600 | elapsed time per iteration (ms): 2785.2 | learning rate: 1.332E-05 | global batch size:     4 | lm loss: 6.297526E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.436 | TFLOPs: 8.74 |
[default0]:saving checkpoint at iteration    3050 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:46:11,938] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3050 is about to be saved!
[default1]:[2023-07-25 18:46:12,018] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3050 is ready now!
[default0]:[2023-07-25 18:46:12,017] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3050/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:46:12,018] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3050/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 18:46:12,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3050 is ready now!
[default0]:[2023-07-25 18:46:12,028] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3050 is ready now!
[default0]:[2023-07-25 18:47:22,972] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3050/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:47:22,992] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3050 is ready now!
[default0]:  successfully saved checkpoint at iteration    3050 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.199
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71198.56, 71218.66)
[default0]:[2023-07-25 18:47:36,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=3055, skipped=2, lr=[1.33431296e-05, 1.33431296e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:47:36,757] [INFO] [timer.py:215:stop] epoch=0/micro_step=3055/global_step=3055, RunningAvgSamplesPerSec=1.506335834886382, CurrSamplesPerSec=1.542242108210557, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3055/109863281 | consumed samples:        12220 | consumed tokens:     25026560 | elapsed time per iteration (ms): 16998.6 | learning rate: 1.334E-05 | global batch size:     4 | lm loss: 6.515390E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.235 | TFLOPs: 1.43 |
[default0]:[2023-07-25 18:47:49,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=3060, skipped=2, lr=[1.3364974933333333e-05, 1.3364974933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:47:49,717] [INFO] [timer.py:215:stop] epoch=0/micro_step=3060/global_step=3060, RunningAvgSamplesPerSec=1.5065336655492307, CurrSamplesPerSec=1.6922632178308203, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3060/109863281 | consumed samples:        12240 | consumed tokens:     25067520 | elapsed time per iteration (ms): 2591.2 | learning rate: 1.336E-05 | global batch size:     4 | lm loss: 6.254076E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.544 | TFLOPs: 9.40 |
[default0]:[2023-07-25 18:48:03,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=3065, skipped=2, lr=[1.3386820266666667e-05, 1.3386820266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:48:03,113] [INFO] [timer.py:215:stop] epoch=0/micro_step=3065/global_step=3065, RunningAvgSamplesPerSec=1.5066831497515352, CurrSamplesPerSec=1.5202144601097218, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3065/109863281 | consumed samples:        12260 | consumed tokens:     25108480 | elapsed time per iteration (ms): 2668.1 | learning rate: 1.339E-05 | global batch size:     4 | lm loss: 6.587165E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.499 | TFLOPs: 9.13 |
[default0]:[2023-07-25 18:48:16,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=3070, skipped=2, lr=[1.34086656e-05, 1.34086656e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:48:16,523] [INFO] [timer.py:215:stop] epoch=0/micro_step=3070/global_step=3070, RunningAvgSamplesPerSec=1.506816647555994, CurrSamplesPerSec=1.4483847872951259, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3070/109863281 | consumed samples:        12280 | consumed tokens:     25149440 | elapsed time per iteration (ms): 2669.2 | learning rate: 1.341E-05 | global batch size:     4 | lm loss: 6.331323E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.499 | TFLOPs: 9.12 |
[default0]:[2023-07-25 18:48:29,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=3075, skipped=2, lr=[1.3430510933333333e-05, 1.3430510933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:48:29,957] [INFO] [timer.py:215:stop] epoch=0/micro_step=3075/global_step=3075, RunningAvgSamplesPerSec=1.5069387307288835, CurrSamplesPerSec=1.5192314274832373, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3075/109863281 | consumed samples:        12300 | consumed tokens:     25190400 | elapsed time per iteration (ms): 2680.4 | learning rate: 1.343E-05 | global batch size:     4 | lm loss: 6.466176E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.492 | TFLOPs: 9.09 |
[default0]:[2023-07-25 18:48:43,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=3080, skipped=2, lr=[1.3452356266666666e-05, 1.3452356266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:48:43,800] [INFO] [timer.py:215:stop] epoch=0/micro_step=3080/global_step=3080, RunningAvgSamplesPerSec=1.5069686218727296, CurrSamplesPerSec=1.6190873240132544, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3080/109863281 | consumed samples:        12320 | consumed tokens:     25231360 | elapsed time per iteration (ms): 2772.1 | learning rate: 1.345E-05 | global batch size:     4 | lm loss: 6.352057E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.443 | TFLOPs: 8.79 |
[default0]:[2023-07-25 18:48:57,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=3085, skipped=2, lr=[1.34742016e-05, 1.34742016e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:48:57,533] [INFO] [timer.py:215:stop] epoch=0/micro_step=3085/global_step=3085, RunningAvgSamplesPerSec=1.5070313021182793, CurrSamplesPerSec=1.5121134255232664, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3085/109863281 | consumed samples:        12340 | consumed tokens:     25272320 | elapsed time per iteration (ms): 2730.3 | learning rate: 1.347E-05 | global batch size:     4 | lm loss: 6.466347E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.465 | TFLOPs: 8.92 |
[default0]:[2023-07-25 18:49:10,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=3090, skipped=2, lr=[1.3496046933333335e-05, 1.3496046933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:49:10,937] [INFO] [timer.py:215:stop] epoch=0/micro_step=3090/global_step=3090, RunningAvgSamplesPerSec=1.5071526536745583, CurrSamplesPerSec=1.634409741841208, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3090/109863281 | consumed samples:        12360 | consumed tokens:     25313280 | elapsed time per iteration (ms): 2677.6 | learning rate: 1.350E-05 | global batch size:     4 | lm loss: 6.406973E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.494 | TFLOPs: 9.10 |
[default0]:[2023-07-25 18:49:24,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=3095, skipped=2, lr=[1.3517892266666668e-05, 1.3517892266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:49:24,179] [INFO] [timer.py:215:stop] epoch=0/micro_step=3095/global_step=3095, RunningAvgSamplesPerSec=1.5073310247647584, CurrSamplesPerSec=1.6708667022672783, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3095/109863281 | consumed samples:        12380 | consumed tokens:     25354240 | elapsed time per iteration (ms): 2643.1 | learning rate: 1.352E-05 | global batch size:     4 | lm loss: 6.401981E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.513 | TFLOPs: 9.21 |
[default0]:[2023-07-25 18:49:37,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=3100, skipped=2, lr=[1.3539737600000001e-05, 1.3539737600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:49:37,956] [INFO] [timer.py:215:stop] epoch=0/micro_step=3100/global_step=3100, RunningAvgSamplesPerSec=1.5074122677495179, CurrSamplesPerSec=1.5339366660635352, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3100/109863281 | consumed samples:        12400 | consumed tokens:     25395200 | elapsed time per iteration (ms): 2739.1 | learning rate: 1.354E-05 | global batch size:     4 | lm loss: 6.218806E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.460 | TFLOPs: 8.89 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 3100 | lm loss value: 6.551733E+00 | lm loss PPL: 7.004567E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    3100 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:49:49,941] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3100 is about to be saved!
[default1]:[2023-07-25 18:49:50,002] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3100 is ready now!
[default1]:[2023-07-25 18:49:50,004] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3100 is ready now!
[default0]:[2023-07-25 18:49:50,001] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3100/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:49:50,001] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3100/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 18:49:50,066] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3100 is ready now!
[default0]:[2023-07-25 18:51:00,715] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3100/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:51:00,729] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3100 is ready now!
[default0]:  successfully saved checkpoint at iteration    3100 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.963
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70948.14, 70962.88)
[default0]:[2023-07-25 18:51:14,048] [INFO] [logging.py:96:log_dist] [Rank 0] step=3105, skipped=2, lr=[1.3561582933333334e-05, 1.3561582933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:51:14,092] [INFO] [timer.py:215:stop] epoch=0/micro_step=3105/global_step=3105, RunningAvgSamplesPerSec=1.5075502964885676, CurrSamplesPerSec=1.7030958548315498, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3105/109863281 | consumed samples:        12420 | consumed tokens:     25436160 | elapsed time per iteration (ms): 19226.5 | learning rate: 1.356E-05 | global batch size:     4 | lm loss: 6.614117E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.208 | TFLOPs: 1.27 |
[default0]:[2023-07-25 18:51:27,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=3110, skipped=2, lr=[1.3583428266666668e-05, 1.3583428266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:51:27,123] [INFO] [timer.py:215:stop] epoch=0/micro_step=3110/global_step=3110, RunningAvgSamplesPerSec=1.5077449447226428, CurrSamplesPerSec=1.735151290378696, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3110/109863281 | consumed samples:        12440 | consumed tokens:     25477120 | elapsed time per iteration (ms): 2591.3 | learning rate: 1.358E-05 | global batch size:     4 | lm loss: 6.468730E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.544 | TFLOPs: 9.40 |
[default0]:[2023-07-25 18:51:40,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=3115, skipped=2, lr=[1.36052736e-05, 1.36052736e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:51:40,427] [INFO] [timer.py:215:stop] epoch=0/micro_step=3115/global_step=3115, RunningAvgSamplesPerSec=1.5078725758055627, CurrSamplesPerSec=1.5026650308366145, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3115/109863281 | consumed samples:        12460 | consumed tokens:     25518080 | elapsed time per iteration (ms): 2653.2 | learning rate: 1.361E-05 | global batch size:     4 | lm loss: 6.393288E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.508 | TFLOPs: 9.18 |
[default0]:[2023-07-25 18:51:53,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=3120, skipped=2, lr=[1.3627118933333334e-05, 1.3627118933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:51:53,827] [INFO] [timer.py:215:stop] epoch=0/micro_step=3120/global_step=3120, RunningAvgSamplesPerSec=1.5080017826771115, CurrSamplesPerSec=1.6083927936647568, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3120/109863281 | consumed samples:        12480 | consumed tokens:     25559040 | elapsed time per iteration (ms): 2676.7 | learning rate: 1.363E-05 | global batch size:     4 | lm loss: 6.669405E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.494 | TFLOPs: 9.10 |
[default0]:[2023-07-25 18:52:06,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=3125, skipped=2, lr=[1.3648964266666667e-05, 1.3648964266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:52:06,847] [INFO] [timer.py:215:stop] epoch=0/micro_step=3125/global_step=3125, RunningAvgSamplesPerSec=1.5081868704663983, CurrSamplesPerSec=1.6978140407018782, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3125/109863281 | consumed samples:        12500 | consumed tokens:     25600000 | elapsed time per iteration (ms): 2598.2 | learning rate: 1.365E-05 | global batch size:     4 | lm loss: 6.474008E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.540 | TFLOPs: 9.37 |
[default0]:[2023-07-25 18:52:20,069] [INFO] [logging.py:96:log_dist] [Rank 0] step=3130, skipped=2, lr=[1.36708096e-05, 1.36708096e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:52:20,092] [INFO] [timer.py:215:stop] epoch=0/micro_step=3130/global_step=3130, RunningAvgSamplesPerSec=1.5083343973091752, CurrSamplesPerSec=1.5473179736694638, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3130/109863281 | consumed samples:        12520 | consumed tokens:     25640960 | elapsed time per iteration (ms): 2640.1 | learning rate: 1.367E-05 | global batch size:     4 | lm loss: 6.215696E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.515 | TFLOPs: 9.23 |
[default0]:[2023-07-25 18:52:33,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=3135, skipped=2, lr=[1.3692654933333334e-05, 1.3692654933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:52:33,449] [INFO] [timer.py:215:stop] epoch=0/micro_step=3135/global_step=3135, RunningAvgSamplesPerSec=1.508472318733797, CurrSamplesPerSec=1.7163752283295968, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3135/109863281 | consumed samples:        12540 | consumed tokens:     25681920 | elapsed time per iteration (ms): 2670.4 | learning rate: 1.369E-05 | global batch size:     4 | lm loss: 6.342850E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.498 | TFLOPs: 9.12 |
[default0]:[2023-07-25 18:52:46,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=3140, skipped=2, lr=[1.3714500266666667e-05, 1.3714500266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:52:46,567] [INFO] [timer.py:215:stop] epoch=0/micro_step=3140/global_step=3140, RunningAvgSamplesPerSec=1.5086531491661854, CurrSamplesPerSec=1.5412275590391322, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3140/109863281 | consumed samples:        12560 | consumed tokens:     25722880 | elapsed time per iteration (ms): 2603.3 | learning rate: 1.371E-05 | global batch size:     4 | lm loss: 6.536143E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.537 | TFLOPs: 9.36 |
[default0]:[2023-07-25 18:52:59,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=3145, skipped=2, lr=[1.37363456e-05, 1.37363456e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:52:59,776] [INFO] [timer.py:215:stop] epoch=0/micro_step=3145/global_step=3145, RunningAvgSamplesPerSec=1.5087966447924366, CurrSamplesPerSec=1.5556398101804498, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3145/109863281 | consumed samples:        12580 | consumed tokens:     25763840 | elapsed time per iteration (ms): 2633.3 | learning rate: 1.374E-05 | global batch size:     4 | lm loss: 6.494048E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.519 | TFLOPs: 9.25 |
[default0]:[2023-07-25 18:53:13,053] [INFO] [logging.py:96:log_dist] [Rank 0] step=3150, skipped=2, lr=[1.3758190933333334e-05, 1.3758190933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:53:13,067] [INFO] [timer.py:215:stop] epoch=0/micro_step=3150/global_step=3150, RunningAvgSamplesPerSec=1.50893897438545, CurrSamplesPerSec=1.6177942931177562, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3150/109863281 | consumed samples:        12600 | consumed tokens:     25804800 | elapsed time per iteration (ms): 2664.3 | learning rate: 1.376E-05 | global batch size:     4 | lm loss: 6.497555E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.501 | TFLOPs: 9.14 |
[default0]:saving checkpoint at iteration    3150 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:53:13,299] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3150 is about to be saved!
[default1]:[2023-07-25 18:53:13,401] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3150 is ready now!
[default1]:[2023-07-25 18:53:13,400] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3150 is ready now!
[default0]:[2023-07-25 18:53:13,397] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3150/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:53:13,397] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3150/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 18:53:13,410] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3150 is ready now!
[default0]:[2023-07-25 18:54:24,157] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3150/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:54:24,178] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3150 is ready now!
[default0]:  successfully saved checkpoint at iteration    3150 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71075.73, 71088.78)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.076
[default0]:[2023-07-25 18:54:37,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=3155, skipped=2, lr=[1.3780036266666667e-05, 1.3780036266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:54:37,432] [INFO] [timer.py:215:stop] epoch=0/micro_step=3155/global_step=3155, RunningAvgSamplesPerSec=1.5090847888779957, CurrSamplesPerSec=1.5183396635134019, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3155/109863281 | consumed samples:        12620 | consumed tokens:     25845760 | elapsed time per iteration (ms): 16856.8 | learning rate: 1.378E-05 | global batch size:     4 | lm loss: 6.381337E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.237 | TFLOPs: 1.44 |
[default0]:[2023-07-25 18:54:50,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=3160, skipped=2, lr=[1.38018816e-05, 1.38018816e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:54:50,661] [INFO] [timer.py:215:stop] epoch=0/micro_step=3160/global_step=3160, RunningAvgSamplesPerSec=1.509232916914901, CurrSamplesPerSec=1.5763255616732608, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3160/109863281 | consumed samples:        12640 | consumed tokens:     25886720 | elapsed time per iteration (ms): 2641.5 | learning rate: 1.380E-05 | global batch size:     4 | lm loss: 6.459274E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.514 | TFLOPs: 9.22 |
[default0]:[2023-07-25 18:55:03,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=3165, skipped=2, lr=[1.3823726933333333e-05, 1.3823726933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:55:03,841] [INFO] [timer.py:215:stop] epoch=0/micro_step=3165/global_step=3165, RunningAvgSamplesPerSec=1.5093956154383223, CurrSamplesPerSec=1.7156640246033095, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3165/109863281 | consumed samples:        12660 | consumed tokens:     25927680 | elapsed time per iteration (ms): 2629.2 | learning rate: 1.382E-05 | global batch size:     4 | lm loss: 6.543935E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.521 | TFLOPs: 9.26 |
[default0]:[2023-07-25 18:55:17,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=3170, skipped=2, lr=[1.3845572266666667e-05, 1.3845572266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:55:17,464] [INFO] [timer.py:215:stop] epoch=0/micro_step=3170/global_step=3170, RunningAvgSamplesPerSec=1.5095036750737925, CurrSamplesPerSec=1.6117572125614956, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3170/109863281 | consumed samples:        12680 | consumed tokens:     25968640 | elapsed time per iteration (ms): 2727.4 | learning rate: 1.385E-05 | global batch size:     4 | lm loss: 6.388493E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.467 | TFLOPs: 8.93 |
[default0]:[2023-07-25 18:55:30,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=3175, skipped=2, lr=[1.38674176e-05, 1.38674176e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:55:30,674] [INFO] [timer.py:215:stop] epoch=0/micro_step=3175/global_step=3175, RunningAvgSamplesPerSec=1.5096605775009424, CurrSamplesPerSec=1.7302743579364241, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3175/109863281 | consumed samples:        12700 | consumed tokens:     26009600 | elapsed time per iteration (ms): 2623.3 | learning rate: 1.387E-05 | global batch size:     4 | lm loss: 6.255673E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.525 | TFLOPs: 9.28 |
[default0]:[2023-07-25 18:55:44,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=3180, skipped=2, lr=[1.3889262933333335e-05, 1.3889262933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:55:44,083] [INFO] [timer.py:215:stop] epoch=0/micro_step=3180/global_step=3180, RunningAvgSamplesPerSec=1.5097785165631763, CurrSamplesPerSec=1.5734538485114606, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3180/109863281 | consumed samples:        12720 | consumed tokens:     26050560 | elapsed time per iteration (ms): 2689.6 | learning rate: 1.389E-05 | global batch size:     4 | lm loss: 6.238881E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.487 | TFLOPs: 9.06 |
[default0]:[2023-07-25 18:55:57,343] [INFO] [logging.py:96:log_dist] [Rank 0] step=3185, skipped=2, lr=[1.3911108266666668e-05, 1.3911108266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:55:57,349] [INFO] [timer.py:215:stop] epoch=0/micro_step=3185/global_step=3185, RunningAvgSamplesPerSec=1.509951206201738, CurrSamplesPerSec=1.6037868651384317, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3185/109863281 | consumed samples:        12740 | consumed tokens:     26091520 | elapsed time per iteration (ms): 2633.3 | learning rate: 1.391E-05 | global batch size:     4 | lm loss: 6.346619E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.519 | TFLOPs: 9.25 |
[default0]:[2023-07-25 18:56:10,674] [INFO] [logging.py:96:log_dist] [Rank 0] step=3190, skipped=2, lr=[1.3932953600000001e-05, 1.3932953600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:56:10,682] [INFO] [timer.py:215:stop] epoch=0/micro_step=3190/global_step=3190, RunningAvgSamplesPerSec=1.5100751581999896, CurrSamplesPerSec=1.6308393713402698, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3190/109863281 | consumed samples:        12760 | consumed tokens:     26132480 | elapsed time per iteration (ms): 2656.2 | learning rate: 1.393E-05 | global batch size:     4 | lm loss: 6.518571E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.506 | TFLOPs: 9.17 |
[default0]:[2023-07-25 18:56:24,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=3195, skipped=2, lr=[1.3954798933333335e-05, 1.3954798933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:56:24,024] [INFO] [timer.py:215:stop] epoch=0/micro_step=3195/global_step=3195, RunningAvgSamplesPerSec=1.5101728257386882, CurrSamplesPerSec=1.5726099252089585, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3195/109863281 | consumed samples:        12780 | consumed tokens:     26173440 | elapsed time per iteration (ms): 2668.7 | learning rate: 1.395E-05 | global batch size:     4 | lm loss: 6.319999E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.499 | TFLOPs: 9.13 |
[default1]:[2023-07-25 18:56:29,241] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default1]:[2023-07-25 18:56:29,241] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[default0]:[2023-07-25 18:56:29,268] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default0]:[2023-07-25 18:56:29,282] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[default1]:[2023-07-25 18:56:29,264] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default1]:[2023-07-25 18:56:29,269] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[default0]:[2023-07-25 18:56:29,251] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default0]:[2023-07-25 18:56:29,275] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 8192.0 to 16384.0
[default0]:[2023-07-25 18:56:37,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=3200, skipped=2, lr=[1.3976644266666668e-05, 1.3976644266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:56:37,594] [INFO] [timer.py:215:stop] epoch=0/micro_step=3200/global_step=3200, RunningAvgSamplesPerSec=1.510264452687987, CurrSamplesPerSec=1.63381001498914, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3200/109863281 | consumed samples:        12800 | consumed tokens:     26214400 | elapsed time per iteration (ms): 2707.5 | learning rate: 1.398E-05 | global batch size:     4 | lm loss: 6.581560E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.477 | TFLOPs: 9.00 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 3200 | lm loss value: 6.556014E+00 | lm loss PPL: 7.034618E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    3200 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 18:56:49,497] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3200 is about to be saved!
[default0]:[2023-07-25 18:56:49,554] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3200 is ready now!
[default1]:[2023-07-25 18:56:49,543] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3200 is ready now!
[default0]:[2023-07-25 18:56:49,554] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3200/mp_rank_00_model_states.pt
[default0]:[2023-07-25 18:56:49,554] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3200/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 18:56:49,544] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3200 is ready now!
[default0]:[2023-07-25 18:58:00,174] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3200/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 18:58:00,195] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3200 is ready now!
[default0]:  successfully saved checkpoint at iteration    3200 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.842
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70842.19, 70853.71)
[default0]:[2023-07-25 18:58:13,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=3205, skipped=2, lr=[1.3998489600000001e-05, 1.3998489600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:58:13,635] [INFO] [timer.py:215:stop] epoch=0/micro_step=3205/global_step=3205, RunningAvgSamplesPerSec=1.5103863476338675, CurrSamplesPerSec=1.6026756706514764, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3205/109863281 | consumed samples:        12820 | consumed tokens:     26255360 | elapsed time per iteration (ms): 19193.0 | learning rate: 1.400E-05 | global batch size:     4 | lm loss: 6.533780E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.208 | TFLOPs: 1.27 |
[default0]:[2023-07-25 18:58:26,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=3210, skipped=2, lr=[1.4020334933333334e-05, 1.4020334933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:58:26,837] [INFO] [timer.py:215:stop] epoch=0/micro_step=3210/global_step=3210, RunningAvgSamplesPerSec=1.5105600174704508, CurrSamplesPerSec=1.594247896132053, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3210/109863281 | consumed samples:        12840 | consumed tokens:     26296320 | elapsed time per iteration (ms): 2636.8 | learning rate: 1.402E-05 | global batch size:     4 | lm loss: 6.411253E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.517 | TFLOPs: 9.24 |
[default0]:[2023-07-25 18:58:40,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=3215, skipped=2, lr=[1.4042180266666668e-05, 1.4042180266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:58:40,313] [INFO] [timer.py:215:stop] epoch=0/micro_step=3215/global_step=3215, RunningAvgSamplesPerSec=1.5106571205728292, CurrSamplesPerSec=1.6347003729352758, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3215/109863281 | consumed samples:        12860 | consumed tokens:     26337280 | elapsed time per iteration (ms): 2689.3 | learning rate: 1.404E-05 | global batch size:     4 | lm loss: 6.439900E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.487 | TFLOPs: 9.06 |
[default0]:[2023-07-25 18:58:53,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=3220, skipped=2, lr=[1.4064025600000001e-05, 1.4064025600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:58:53,862] [INFO] [timer.py:215:stop] epoch=0/micro_step=3220/global_step=3220, RunningAvgSamplesPerSec=1.510764489512691, CurrSamplesPerSec=1.6022878125313968, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3220/109863281 | consumed samples:        12880 | consumed tokens:     26378240 | elapsed time per iteration (ms): 2708.3 | learning rate: 1.406E-05 | global batch size:     4 | lm loss: 6.389927E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.477 | TFLOPs: 8.99 |
[default0]:[2023-07-25 18:59:07,574] [INFO] [logging.py:96:log_dist] [Rank 0] step=3225, skipped=2, lr=[1.4085870933333334e-05, 1.4085870933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:59:07,596] [INFO] [timer.py:215:stop] epoch=0/micro_step=3225/global_step=3225, RunningAvgSamplesPerSec=1.5108357609472056, CurrSamplesPerSec=1.5741069542037132, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3225/109863281 | consumed samples:        12900 | consumed tokens:     26419200 | elapsed time per iteration (ms): 2739.1 | learning rate: 1.409E-05 | global batch size:     4 | lm loss: 6.617359E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.460 | TFLOPs: 8.89 |
[default0]:[2023-07-25 18:59:21,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=3230, skipped=2, lr=[1.4107716266666667e-05, 1.4107716266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:59:21,158] [INFO] [timer.py:215:stop] epoch=0/micro_step=3230/global_step=3230, RunningAvgSamplesPerSec=1.510915340907948, CurrSamplesPerSec=1.6584192582531911, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3230/109863281 | consumed samples:        12920 | consumed tokens:     26460160 | elapsed time per iteration (ms): 2710.6 | learning rate: 1.411E-05 | global batch size:     4 | lm loss: 6.455943E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.476 | TFLOPs: 8.99 |
[default0]:[2023-07-25 18:59:34,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=3235, skipped=2, lr=[1.41295616e-05, 1.41295616e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:59:34,214] [INFO] [timer.py:215:stop] epoch=0/micro_step=3235/global_step=3235, RunningAvgSamplesPerSec=1.5110835134537208, CurrSamplesPerSec=1.5761990895047273, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3235/109863281 | consumed samples:        12940 | consumed tokens:     26501120 | elapsed time per iteration (ms): 2612.4 | learning rate: 1.413E-05 | global batch size:     4 | lm loss: 6.522543E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.531 | TFLOPs: 9.32 |
[default0]:[2023-07-25 18:59:47,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=3240, skipped=2, lr=[1.4151406933333334e-05, 1.4151406933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 18:59:47,486] [INFO] [timer.py:215:stop] epoch=0/micro_step=3240/global_step=3240, RunningAvgSamplesPerSec=1.5112249865702796, CurrSamplesPerSec=1.5752000216320803, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3240/109863281 | consumed samples:        12960 | consumed tokens:     26542080 | elapsed time per iteration (ms): 2658.8 | learning rate: 1.415E-05 | global batch size:     4 | lm loss: 6.432062E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.504 | TFLOPs: 9.16 |
[default0]:[2023-07-25 19:00:00,473] [INFO] [logging.py:96:log_dist] [Rank 0] step=3245, skipped=2, lr=[1.4173252266666667e-05, 1.4173252266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:00:00,490] [INFO] [timer.py:215:stop] epoch=0/micro_step=3245/global_step=3245, RunningAvgSamplesPerSec=1.511423090530241, CurrSamplesPerSec=1.687084242713362, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3245/109863281 | consumed samples:        12980 | consumed tokens:     26583040 | elapsed time per iteration (ms): 2575.1 | learning rate: 1.417E-05 | global batch size:     4 | lm loss: 6.375734E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.553 | TFLOPs: 9.46 |
[default0]:[2023-07-25 19:00:13,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=3250, skipped=2, lr=[1.41950976e-05, 1.41950976e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:00:14,001] [INFO] [timer.py:215:stop] epoch=0/micro_step=3250/global_step=3250, RunningAvgSamplesPerSec=1.5115133303666108, CurrSamplesPerSec=1.5288445521273055, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3250/109863281 | consumed samples:        13000 | consumed tokens:     26624000 | elapsed time per iteration (ms): 2701.4 | learning rate: 1.420E-05 | global batch size:     4 | lm loss: 6.510095E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.481 | TFLOPs: 9.02 |
[default0]:saving checkpoint at iteration    3250 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:00:14,238] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3250 is about to be saved!
[default0]:[2023-07-25 19:00:14,295] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3250/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:00:14,295] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3250/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 19:00:14,297] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3250 is ready now!
[default1]:[2023-07-25 19:00:14,297] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3250 is ready now!
[default0]:[2023-07-25 19:00:14,295] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3250 is ready now!
[default0]:[2023-07-25 19:01:25,799] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3250/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:01:25,823] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3250 is ready now!
[default0]:  successfully saved checkpoint at iteration    3250 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.774
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71762.60, 71793.92)
[default0]:[2023-07-25 19:01:39,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=3255, skipped=2, lr=[1.4216942933333334e-05, 1.4216942933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:01:39,490] [INFO] [timer.py:215:stop] epoch=0/micro_step=3255/global_step=3255, RunningAvgSamplesPerSec=1.511608987952366, CurrSamplesPerSec=1.5708411002843246, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3255/109863281 | consumed samples:        13020 | consumed tokens:     26664960 | elapsed time per iteration (ms): 17089.0 | learning rate: 1.422E-05 | global batch size:     4 | lm loss: 6.379712E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.234 | TFLOPs: 1.43 |
[default0]:[2023-07-25 19:01:53,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=3260, skipped=2, lr=[1.4238788266666667e-05, 1.4238788266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:01:53,140] [INFO] [timer.py:215:stop] epoch=0/micro_step=3260/global_step=3260, RunningAvgSamplesPerSec=1.5117040287633887, CurrSamplesPerSec=1.6279240543471427, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3260/109863281 | consumed samples:        13040 | consumed tokens:     26705920 | elapsed time per iteration (ms): 2717.4 | learning rate: 1.424E-05 | global batch size:     4 | lm loss: 6.464062E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.472 | TFLOPs: 8.96 |
[default0]:[2023-07-25 19:02:06,726] [INFO] [logging.py:96:log_dist] [Rank 0] step=3265, skipped=2, lr=[1.42606336e-05, 1.42606336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:02:06,737] [INFO] [timer.py:215:stop] epoch=0/micro_step=3265/global_step=3265, RunningAvgSamplesPerSec=1.5117986725115016, CurrSamplesPerSec=1.4867724417598933, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3265/109863281 | consumed samples:        13060 | consumed tokens:     26746880 | elapsed time per iteration (ms): 2711.7 | learning rate: 1.426E-05 | global batch size:     4 | lm loss: 6.456455E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.475 | TFLOPs: 8.98 |
[default0]:[2023-07-25 19:02:20,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=3270, skipped=2, lr=[1.4282478933333335e-05, 1.4282478933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:02:20,303] [INFO] [timer.py:215:stop] epoch=0/micro_step=3270/global_step=3270, RunningAvgSamplesPerSec=1.511911685221414, CurrSamplesPerSec=1.5970454135915506, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3270/109863281 | consumed samples:        13080 | consumed tokens:     26787840 | elapsed time per iteration (ms): 2706.7 | learning rate: 1.428E-05 | global batch size:     4 | lm loss: 6.340615E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.478 | TFLOPs: 9.00 |
[default0]:[2023-07-25 19:02:33,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=3275, skipped=2, lr=[1.4304324266666668e-05, 1.4304324266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:02:33,969] [INFO] [timer.py:215:stop] epoch=0/micro_step=3275/global_step=3275, RunningAvgSamplesPerSec=1.5119832951392362, CurrSamplesPerSec=1.597755076833059, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3275/109863281 | consumed samples:        13100 | consumed tokens:     26828800 | elapsed time per iteration (ms): 2727.1 | learning rate: 1.430E-05 | global batch size:     4 | lm loss: 6.246466E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.467 | TFLOPs: 8.93 |
[default0]:[2023-07-25 19:02:47,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=3280, skipped=2, lr=[1.4326169600000002e-05, 1.4326169600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:02:47,182] [INFO] [timer.py:215:stop] epoch=0/micro_step=3280/global_step=3280, RunningAvgSamplesPerSec=1.5121288406726088, CurrSamplesPerSec=1.7335663600080267, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3280/109863281 | consumed samples:        13120 | consumed tokens:     26869760 | elapsed time per iteration (ms): 2639.5 | learning rate: 1.433E-05 | global batch size:     4 | lm loss: 6.281480E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.515 | TFLOPs: 9.23 |
[default0]:[2023-07-25 19:03:00,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=3285, skipped=2, lr=[1.4348014933333335e-05, 1.4348014933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:03:00,964] [INFO] [timer.py:215:stop] epoch=0/micro_step=3285/global_step=3285, RunningAvgSamplesPerSec=1.5122001809373118, CurrSamplesPerSec=1.6604385928189256, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3285/109863281 | consumed samples:        13140 | consumed tokens:     26910720 | elapsed time per iteration (ms): 2739.8 | learning rate: 1.435E-05 | global batch size:     4 | lm loss: 6.356953E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.460 | TFLOPs: 8.89 |
[default0]:[2023-07-25 19:03:14,446] [INFO] [logging.py:96:log_dist] [Rank 0] step=3290, skipped=2, lr=[1.4369860266666668e-05, 1.4369860266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:03:14,462] [INFO] [timer.py:215:stop] epoch=0/micro_step=3290/global_step=3290, RunningAvgSamplesPerSec=1.5123007617333568, CurrSamplesPerSec=1.5229805335691833, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3290/109863281 | consumed samples:        13160 | consumed tokens:     26951680 | elapsed time per iteration (ms): 2697.9 | learning rate: 1.437E-05 | global batch size:     4 | lm loss: 6.543184E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.483 | TFLOPs: 9.03 |
[default0]:[2023-07-25 19:03:28,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=3295, skipped=2, lr=[1.4391705600000001e-05, 1.4391705600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:03:28,163] [INFO] [timer.py:215:stop] epoch=0/micro_step=3295/global_step=3295, RunningAvgSamplesPerSec=1.5123668512325112, CurrSamplesPerSec=1.4574212948774348, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3295/109863281 | consumed samples:        13180 | consumed tokens:     26992640 | elapsed time per iteration (ms): 2729.8 | learning rate: 1.439E-05 | global batch size:     4 | lm loss: 6.336760E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.465 | TFLOPs: 8.92 |
[default0]:[2023-07-25 19:03:41,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=3300, skipped=2, lr=[1.4413550933333335e-05, 1.4413550933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:03:41,375] [INFO] [timer.py:215:stop] epoch=0/micro_step=3300/global_step=3300, RunningAvgSamplesPerSec=1.5125081066157913, CurrSamplesPerSec=1.631686343679857, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3300/109863281 | consumed samples:        13200 | consumed tokens:     27033600 | elapsed time per iteration (ms): 2642.4 | learning rate: 1.441E-05 | global batch size:     4 | lm loss: 6.238664E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.514 | TFLOPs: 9.22 |
[default0]:saving checkpoint at iteration    3300 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 3300 | lm loss value: 6.547286E+00 | lm loss PPL: 6.973487E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-25 19:03:52,971] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3300 is about to be saved!
[default0]:[2023-07-25 19:03:53,026] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3300/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:03:53,026] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3300/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 19:03:53,034] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3300 is ready now!
[default1]:[2023-07-25 19:03:53,023] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3300 is ready now!
[default1]:[2023-07-25 19:03:53,024] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3300 is ready now!
[default0]:[2023-07-25 19:05:04,103] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3300/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:05:04,129] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3300 is ready now!
[default0]:  successfully saved checkpoint at iteration    3300 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.321
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71318.57, 71341.39)
[default0]:[2023-07-25 19:05:17,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=3305, skipped=2, lr=[1.4435396266666668e-05, 1.4435396266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:05:17,559] [INFO] [timer.py:215:stop] epoch=0/micro_step=3305/global_step=3305, RunningAvgSamplesPerSec=1.5126341263555216, CurrSamplesPerSec=1.7467634743830758, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3305/109863281 | consumed samples:        13220 | consumed tokens:     27074560 | elapsed time per iteration (ms): 19222.0 | learning rate: 1.444E-05 | global batch size:     4 | lm loss: 6.429970E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.208 | TFLOPs: 1.27 |
[default0]:[2023-07-25 19:05:30,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=3310, skipped=2, lr=[1.4457241600000001e-05, 1.4457241600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:05:30,479] [INFO] [timer.py:215:stop] epoch=0/micro_step=3310/global_step=3310, RunningAvgSamplesPerSec=1.5128733801093117, CurrSamplesPerSec=1.7271503991275106, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3310/109863281 | consumed samples:        13240 | consumed tokens:     27115520 | elapsed time per iteration (ms): 2570.7 | learning rate: 1.446E-05 | global batch size:     4 | lm loss: 6.226635E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.556 | TFLOPs: 9.47 |
[default0]:[2023-07-25 19:05:44,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=3315, skipped=2, lr=[1.4479086933333334e-05, 1.4479086933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:05:44,073] [INFO] [timer.py:215:stop] epoch=0/micro_step=3315/global_step=3315, RunningAvgSamplesPerSec=1.512975145323851, CurrSamplesPerSec=1.795561038978274, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3315/109863281 | consumed samples:        13260 | consumed tokens:     27156480 | elapsed time per iteration (ms): 2704.3 | learning rate: 1.448E-05 | global batch size:     4 | lm loss: 6.472816E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.479 | TFLOPs: 9.01 |
[default0]:[2023-07-25 19:05:57,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=3320, skipped=2, lr=[1.4500932266666668e-05, 1.4500932266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:05:57,181] [INFO] [timer.py:215:stop] epoch=0/micro_step=3320/global_step=3320, RunningAvgSamplesPerSec=1.5131387812505674, CurrSamplesPerSec=1.6508073324932302, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3320/109863281 | consumed samples:        13280 | consumed tokens:     27197440 | elapsed time per iteration (ms): 2625.7 | learning rate: 1.450E-05 | global batch size:     4 | lm loss: 6.317939E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.523 | TFLOPs: 9.28 |
[default0]:[2023-07-25 19:06:10,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=3325, skipped=2, lr=[1.4522777600000001e-05, 1.4522777600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:06:10,661] [INFO] [timer.py:215:stop] epoch=0/micro_step=3325/global_step=3325, RunningAvgSamplesPerSec=1.5132340635495545, CurrSamplesPerSec=1.6241193281070865, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3325/109863281 | consumed samples:        13300 | consumed tokens:     27238400 | elapsed time per iteration (ms): 2692.0 | learning rate: 1.452E-05 | global batch size:     4 | lm loss: 6.346919E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.486 | TFLOPs: 9.05 |
[default0]:[2023-07-25 19:06:24,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=3330, skipped=2, lr=[1.4544622933333334e-05, 1.4544622933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:06:24,258] [INFO] [timer.py:215:stop] epoch=0/micro_step=3330/global_step=3330, RunningAvgSamplesPerSec=1.5133171980328732, CurrSamplesPerSec=1.5458331271726913, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3330/109863281 | consumed samples:        13320 | consumed tokens:     27279360 | elapsed time per iteration (ms): 2704.7 | learning rate: 1.454E-05 | global batch size:     4 | lm loss: 6.381002E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.479 | TFLOPs: 9.01 |
[default0]:[2023-07-25 19:06:37,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=3335, skipped=2, lr=[1.4566468266666668e-05, 1.4566468266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:06:37,648] [INFO] [timer.py:215:stop] epoch=0/micro_step=3335/global_step=3335, RunningAvgSamplesPerSec=1.5134404789037337, CurrSamplesPerSec=1.5858478817430397, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3335/109863281 | consumed samples:        13340 | consumed tokens:     27320320 | elapsed time per iteration (ms): 2663.5 | learning rate: 1.457E-05 | global batch size:     4 | lm loss: 6.460770E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.502 | TFLOPs: 9.14 |
[default0]:[2023-07-25 19:06:50,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=3340, skipped=2, lr=[1.45883136e-05, 1.45883136e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:06:50,946] [INFO] [timer.py:215:stop] epoch=0/micro_step=3340/global_step=3340, RunningAvgSamplesPerSec=1.5135557798209058, CurrSamplesPerSec=1.5434485239359317, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3340/109863281 | consumed samples:        13360 | consumed tokens:     27361280 | elapsed time per iteration (ms): 2655.2 | learning rate: 1.459E-05 | global batch size:     4 | lm loss: 6.507135E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.506 | TFLOPs: 9.17 |
[default0]:[2023-07-25 19:07:04,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=3345, skipped=2, lr=[1.4610158933333334e-05, 1.4610158933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:07:04,629] [INFO] [timer.py:215:stop] epoch=0/micro_step=3345/global_step=3345, RunningAvgSamplesPerSec=1.513623421065875, CurrSamplesPerSec=1.6053696756969182, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3345/109863281 | consumed samples:        13380 | consumed tokens:     27402240 | elapsed time per iteration (ms): 2729.7 | learning rate: 1.461E-05 | global batch size:     4 | lm loss: 6.245107E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.465 | TFLOPs: 8.92 |
[default0]:[2023-07-25 19:07:17,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=3350, skipped=2, lr=[1.4632004266666667e-05, 1.4632004266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:07:17,734] [INFO] [timer.py:215:stop] epoch=0/micro_step=3350/global_step=3350, RunningAvgSamplesPerSec=1.5137796113987905, CurrSamplesPerSec=1.6207557626800504, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3350/109863281 | consumed samples:        13400 | consumed tokens:     27443200 | elapsed time per iteration (ms): 2621.9 | learning rate: 1.463E-05 | global batch size:     4 | lm loss: 6.387465E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.526 | TFLOPs: 9.29 |
[default0]:saving checkpoint at iteration    3350 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:07:17,953] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3350 is about to be saved!
[default0]:[2023-07-25 19:07:18,001] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3350/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:07:18,001] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3350/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 19:07:18,003] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3350 is ready now!
[default1]:[2023-07-25 19:07:17,999] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3350 is ready now!
[default0]:[2023-07-25 19:07:18,011] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3350 is ready now!
[default0]:[2023-07-25 19:08:28,786] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3350/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:08:28,807] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3350 is ready now!
[default0]:  successfully saved checkpoint at iteration    3350 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.019
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71014.54, 71029.18)
[default0]:[2023-07-25 19:08:42,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=3355, skipped=2, lr=[1.46538496e-05, 1.46538496e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:08:42,314] [INFO] [timer.py:215:stop] epoch=0/micro_step=3355/global_step=3355, RunningAvgSamplesPerSec=1.5139024286437925, CurrSamplesPerSec=1.578577166749686, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3355/109863281 | consumed samples:        13420 | consumed tokens:     27484160 | elapsed time per iteration (ms): 16897.8 | learning rate: 1.465E-05 | global batch size:     4 | lm loss: 6.522594E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.237 | TFLOPs: 1.44 |
[default0]:[2023-07-25 19:08:55,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=3360, skipped=2, lr=[1.4675694933333336e-05, 1.4675694933333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:08:55,788] [INFO] [timer.py:215:stop] epoch=0/micro_step=3360/global_step=3360, RunningAvgSamplesPerSec=1.5140096896116388, CurrSamplesPerSec=1.6729605216514323, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3360/109863281 | consumed samples:        13440 | consumed tokens:     27525120 | elapsed time per iteration (ms): 2688.4 | learning rate: 1.468E-05 | global batch size:     4 | lm loss: 6.415592E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.488 | TFLOPs: 9.06 |
[default0]:[2023-07-25 19:09:08,945] [INFO] [logging.py:96:log_dist] [Rank 0] step=3365, skipped=2, lr=[1.4697540266666669e-05, 1.4697540266666669e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:09:08,963] [INFO] [timer.py:215:stop] epoch=0/micro_step=3365/global_step=3365, RunningAvgSamplesPerSec=1.5141502260835338, CurrSamplesPerSec=1.6307563075689078, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3365/109863281 | consumed samples:        13460 | consumed tokens:     27566080 | elapsed time per iteration (ms): 2628.3 | learning rate: 1.470E-05 | global batch size:     4 | lm loss: 6.416712E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.522 | TFLOPs: 9.27 |
[default0]:[2023-07-25 19:09:22,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=3370, skipped=2, lr=[1.4719385600000002e-05, 1.4719385600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:09:22,406] [INFO] [timer.py:215:stop] epoch=0/micro_step=3370/global_step=3370, RunningAvgSamplesPerSec=1.5142501077745447, CurrSamplesPerSec=1.591309515058534, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3370/109863281 | consumed samples:        13480 | consumed tokens:     27607040 | elapsed time per iteration (ms): 2680.9 | learning rate: 1.472E-05 | global batch size:     4 | lm loss: 6.528227E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.492 | TFLOPs: 9.08 |
[default0]:[2023-07-25 19:09:36,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=3375, skipped=2, lr=[1.4741230933333335e-05, 1.4741230933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:09:36,166] [INFO] [timer.py:215:stop] epoch=0/micro_step=3375/global_step=3375, RunningAvgSamplesPerSec=1.514316912798965, CurrSamplesPerSec=1.6474429171373492, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3375/109863281 | consumed samples:        13500 | consumed tokens:     27648000 | elapsed time per iteration (ms): 2735.0 | learning rate: 1.474E-05 | global batch size:     4 | lm loss: 6.486404E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.463 | TFLOPs: 8.91 |
[default0]:[2023-07-25 19:09:49,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=3380, skipped=2, lr=[1.4763076266666669e-05, 1.4763076266666669e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:09:49,663] [INFO] [timer.py:215:stop] epoch=0/micro_step=3380/global_step=3380, RunningAvgSamplesPerSec=1.5143888887670203, CurrSamplesPerSec=1.5596994542377225, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3380/109863281 | consumed samples:        13520 | consumed tokens:     27688960 | elapsed time per iteration (ms): 2693.8 | learning rate: 1.476E-05 | global batch size:     4 | lm loss: 6.387753E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.485 | TFLOPs: 9.04 |
[default0]:[2023-07-25 19:10:02,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=3385, skipped=2, lr=[1.4784921600000002e-05, 1.4784921600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:10:02,876] [INFO] [timer.py:215:stop] epoch=0/micro_step=3385/global_step=3385, RunningAvgSamplesPerSec=1.5145177032113573, CurrSamplesPerSec=1.524791147109505, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3385/109863281 | consumed samples:        13540 | consumed tokens:     27729920 | elapsed time per iteration (ms): 2635.5 | learning rate: 1.478E-05 | global batch size:     4 | lm loss: 6.426180E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.518 | TFLOPs: 9.24 |
[default0]:[2023-07-25 19:10:16,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=3390, skipped=2, lr=[1.4806766933333335e-05, 1.4806766933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:10:16,589] [INFO] [timer.py:215:stop] epoch=0/micro_step=3390/global_step=3390, RunningAvgSamplesPerSec=1.5145839166498059, CurrSamplesPerSec=1.6191626401247212, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3390/109863281 | consumed samples:        13560 | consumed tokens:     27770880 | elapsed time per iteration (ms): 2738.8 | learning rate: 1.481E-05 | global batch size:     4 | lm loss: 6.450305E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.460 | TFLOPs: 8.89 |
[default0]:[2023-07-25 19:10:29,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=3395, skipped=2, lr=[1.4828612266666668e-05, 1.4828612266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:10:29,789] [INFO] [timer.py:215:stop] epoch=0/micro_step=3395/global_step=3395, RunningAvgSamplesPerSec=1.5147185022761054, CurrSamplesPerSec=1.624085840306703, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3395/109863281 | consumed samples:        13580 | consumed tokens:     27811840 | elapsed time per iteration (ms): 2619.3 | learning rate: 1.483E-05 | global batch size:     4 | lm loss: 6.443004E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.527 | TFLOPs: 9.30 |
[default0]:[2023-07-25 19:10:43,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=3400, skipped=2, lr=[1.4850457600000002e-05, 1.4850457600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:10:43,363] [INFO] [timer.py:215:stop] epoch=0/micro_step=3400/global_step=3400, RunningAvgSamplesPerSec=1.5147931133760164, CurrSamplesPerSec=1.5180658549970345, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3400/109863281 | consumed samples:        13600 | consumed tokens:     27852800 | elapsed time per iteration (ms): 2712.3 | learning rate: 1.485E-05 | global batch size:     4 | lm loss: 6.423410E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.475 | TFLOPs: 8.98 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 3400 | lm loss value: 6.618137E+00 | lm loss PPL: 7.485492E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default1]:[2023-07-25 19:10:55,363] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3400 is ready now!
[default0]:saving checkpoint at iteration    3400 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:10:55,311] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3400 is about to be saved!
[default0]:[2023-07-25 19:10:55,361] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3400/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:10:55,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3400/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 19:10:55,360] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3400 is ready now!
[default1]:[2023-07-25 19:10:55,362] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3400 is ready now!
[default0]:[2023-07-25 19:12:06,550] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3400/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:12:06,563] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3400 is ready now!
[default0]:  successfully saved checkpoint at iteration    3400 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.458
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71457.96, 71480.34)
[default0]:[2023-07-25 19:12:20,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=3405, skipped=2, lr=[1.4872302933333335e-05, 1.4872302933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:12:20,178] [INFO] [timer.py:215:stop] epoch=0/micro_step=3405/global_step=3405, RunningAvgSamplesPerSec=1.5148752111702373, CurrSamplesPerSec=1.516634811749235, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3405/109863281 | consumed samples:        13620 | consumed tokens:     27893760 | elapsed time per iteration (ms): 19343.3 | learning rate: 1.487E-05 | global batch size:     4 | lm loss: 6.187297E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.207 | TFLOPs: 1.26 |
[default0]:[2023-07-25 19:12:33,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=3410, skipped=2, lr=[1.4894148266666668e-05, 1.4894148266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:12:34,071] [INFO] [timer.py:215:stop] epoch=0/micro_step=3410/global_step=3410, RunningAvgSamplesPerSec=1.5149010128856806, CurrSamplesPerSec=1.6080947940362564, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3410/109863281 | consumed samples:        13640 | consumed tokens:     27934720 | elapsed time per iteration (ms): 2775.9 | learning rate: 1.489E-05 | global batch size:     4 | lm loss: 6.294481E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.441 | TFLOPs: 8.77 |
[default0]:[2023-07-25 19:12:47,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=3415, skipped=2, lr=[1.4915993600000001e-05, 1.4915993600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:12:47,428] [INFO] [timer.py:215:stop] epoch=0/micro_step=3415/global_step=3415, RunningAvgSamplesPerSec=1.5150169247939629, CurrSamplesPerSec=1.5915738458178774, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3415/109863281 | consumed samples:        13660 | consumed tokens:     27975680 | elapsed time per iteration (ms): 2670.2 | learning rate: 1.492E-05 | global batch size:     4 | lm loss: 6.294974E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.498 | TFLOPs: 9.12 |
[default0]:[2023-07-25 19:13:00,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=3420, skipped=2, lr=[1.4937838933333335e-05, 1.4937838933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:13:00,720] [INFO] [timer.py:215:stop] epoch=0/micro_step=3420/global_step=3420, RunningAvgSamplesPerSec=1.5151478546089294, CurrSamplesPerSec=1.5078495333473838, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3420/109863281 | consumed samples:        13680 | consumed tokens:     28016640 | elapsed time per iteration (ms): 2646.2 | learning rate: 1.494E-05 | global batch size:     4 | lm loss: 6.285423E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.512 | TFLOPs: 9.20 |
[default0]:[2023-07-25 19:13:14,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=3425, skipped=2, lr=[1.4959684266666668e-05, 1.4959684266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:13:14,123] [INFO] [timer.py:215:stop] epoch=0/micro_step=3425/global_step=3425, RunningAvgSamplesPerSec=1.515238581792494, CurrSamplesPerSec=1.546252986109063, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3425/109863281 | consumed samples:        13700 | consumed tokens:     28057600 | elapsed time per iteration (ms): 2674.6 | learning rate: 1.496E-05 | global batch size:     4 | lm loss: 6.407050E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.496 | TFLOPs: 9.11 |
[default0]:[2023-07-25 19:13:27,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=3430, skipped=2, lr=[1.4981529600000001e-05, 1.4981529600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:13:27,750] [INFO] [timer.py:215:stop] epoch=0/micro_step=3430/global_step=3430, RunningAvgSamplesPerSec=1.5153091776720375, CurrSamplesPerSec=1.393347348293236, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3430/109863281 | consumed samples:        13720 | consumed tokens:     28098560 | elapsed time per iteration (ms): 2726.6 | learning rate: 1.498E-05 | global batch size:     4 | lm loss: 6.283700E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.467 | TFLOPs: 8.93 |
[default0]:[2023-07-25 19:13:41,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=3435, skipped=2, lr=[1.5003374933333334e-05, 1.5003374933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:13:41,050] [INFO] [timer.py:215:stop] epoch=0/micro_step=3435/global_step=3435, RunningAvgSamplesPerSec=1.5154444370348858, CurrSamplesPerSec=1.552545162223473, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3435/109863281 | consumed samples:        13740 | consumed tokens:     28139520 | elapsed time per iteration (ms): 2650.6 | learning rate: 1.500E-05 | global batch size:     4 | lm loss: 6.309179E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.509 | TFLOPs: 9.19 |
[default0]:[2023-07-25 19:13:54,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=3440, skipped=2, lr=[1.5025220266666668e-05, 1.5025220266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:13:54,557] [INFO] [timer.py:215:stop] epoch=0/micro_step=3440/global_step=3440, RunningAvgSamplesPerSec=1.515525255982306, CurrSamplesPerSec=1.5623516194813436, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3440/109863281 | consumed samples:        13760 | consumed tokens:     28180480 | elapsed time per iteration (ms): 2691.0 | learning rate: 1.503E-05 | global batch size:     4 | lm loss: 6.339669E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.486 | TFLOPs: 9.05 |
[default0]:[2023-07-25 19:14:07,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=3445, skipped=2, lr=[1.5047065600000001e-05, 1.5047065600000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:14:07,525] [INFO] [timer.py:215:stop] epoch=0/micro_step=3445/global_step=3445, RunningAvgSamplesPerSec=1.5157042302510126, CurrSamplesPerSec=1.591583659900871, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3445/109863281 | consumed samples:        13780 | consumed tokens:     28221440 | elapsed time per iteration (ms): 2586.3 | learning rate: 1.505E-05 | global batch size:     4 | lm loss: 6.283902E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.547 | TFLOPs: 9.42 |
[default0]:[2023-07-25 19:14:21,081] [INFO] [logging.py:96:log_dist] [Rank 0] step=3450, skipped=2, lr=[1.5068910933333336e-05, 1.5068910933333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:14:21,101] [INFO] [timer.py:215:stop] epoch=0/micro_step=3450/global_step=3450, RunningAvgSamplesPerSec=1.5157926183957369, CurrSamplesPerSec=1.5966639223218775, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3450/109863281 | consumed samples:        13800 | consumed tokens:     28262400 | elapsed time per iteration (ms): 2714.9 | learning rate: 1.507E-05 | global batch size:     4 | lm loss: 6.442252E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.473 | TFLOPs: 8.97 |
[default0]:saving checkpoint at iteration    3450 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:14:21,347] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3450 is about to be saved!
[default1]:[2023-07-25 19:14:21,403] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3450 is ready now!
[default1]:[2023-07-25 19:14:21,404] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3450 is ready now!
[default0]:[2023-07-25 19:14:21,414] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3450 is ready now!
[default0]:[2023-07-25 19:14:21,405] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3450/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:14:21,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3450/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 19:15:31,965] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3450/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:15:31,988] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3450 is ready now!
[default0]:  successfully saved checkpoint at iteration    3450 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.81
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70809.64, 70830.34)
[default0]:[2023-07-25 19:15:45,686] [INFO] [logging.py:96:log_dist] [Rank 0] step=3455, skipped=2, lr=[1.5090756266666669e-05, 1.5090756266666669e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:15:45,718] [INFO] [timer.py:215:stop] epoch=0/micro_step=3455/global_step=3455, RunningAvgSamplesPerSec=1.5158621126130178, CurrSamplesPerSec=1.495515472417958, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3455/109863281 | consumed samples:        13820 | consumed tokens:     28303360 | elapsed time per iteration (ms): 16914.1 | learning rate: 1.509E-05 | global batch size:     4 | lm loss: 6.280822E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.236 | TFLOPs: 1.44 |
[default0]:[2023-07-25 19:15:59,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=3460, skipped=2, lr=[1.5112601600000002e-05, 1.5112601600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:15:59,186] [INFO] [timer.py:215:stop] epoch=0/micro_step=3460/global_step=3460, RunningAvgSamplesPerSec=1.5159661395636987, CurrSamplesPerSec=1.6015053211636914, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3460/109863281 | consumed samples:        13840 | consumed tokens:     28344320 | elapsed time per iteration (ms): 2688.6 | learning rate: 1.511E-05 | global batch size:     4 | lm loss: 6.207958E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.488 | TFLOPs: 9.06 |
[default0]:[2023-07-25 19:16:13,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=3465, skipped=2, lr=[1.5134446933333336e-05, 1.5134446933333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:16:13,087] [INFO] [timer.py:215:stop] epoch=0/micro_step=3465/global_step=3465, RunningAvgSamplesPerSec=1.5159865455506714, CurrSamplesPerSec=1.4927890683370408, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3465/109863281 | consumed samples:        13860 | consumed tokens:     28385280 | elapsed time per iteration (ms): 2767.5 | learning rate: 1.513E-05 | global batch size:     4 | lm loss: 6.262163E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.445 | TFLOPs: 8.80 |
[default0]:[2023-07-25 19:16:26,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=3470, skipped=2, lr=[1.5156292266666669e-05, 1.5156292266666669e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:16:26,330] [INFO] [timer.py:215:stop] epoch=0/micro_step=3470/global_step=3470, RunningAvgSamplesPerSec=1.516100841929218, CurrSamplesPerSec=1.5088086599483863, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3470/109863281 | consumed samples:        13880 | consumed tokens:     28426240 | elapsed time per iteration (ms): 2641.1 | learning rate: 1.516E-05 | global batch size:     4 | lm loss: 6.395444E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.514 | TFLOPs: 9.22 |
[default0]:[2023-07-25 19:16:39,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=3475, skipped=2, lr=[1.5178137600000002e-05, 1.5178137600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:16:39,626] [INFO] [timer.py:215:stop] epoch=0/micro_step=3475/global_step=3475, RunningAvgSamplesPerSec=1.5162255999374734, CurrSamplesPerSec=1.732154914623011, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3475/109863281 | consumed samples:        13900 | consumed tokens:     28467200 | elapsed time per iteration (ms): 2648.1 | learning rate: 1.518E-05 | global batch size:     4 | lm loss: 6.407941E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.511 | TFLOPs: 9.20 |
[default0]:[2023-07-25 19:16:52,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=3480, skipped=2, lr=[1.5199982933333335e-05, 1.5199982933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:16:52,626] [INFO] [timer.py:215:stop] epoch=0/micro_step=3480/global_step=3480, RunningAvgSamplesPerSec=1.5164028049216725, CurrSamplesPerSec=1.5466417048774221, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3480/109863281 | consumed samples:        13920 | consumed tokens:     28508160 | elapsed time per iteration (ms): 2584.7 | learning rate: 1.520E-05 | global batch size:     4 | lm loss: 6.365126E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.548 | TFLOPs: 9.42 |
[default0]:[2023-07-25 19:17:05,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=3485, skipped=2, lr=[1.5221828266666669e-05, 1.5221828266666669e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:17:05,872] [INFO] [timer.py:215:stop] epoch=0/micro_step=3485/global_step=3485, RunningAvgSamplesPerSec=1.5165290997059004, CurrSamplesPerSec=1.6345801267155953, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3485/109863281 | consumed samples:        13940 | consumed tokens:     28549120 | elapsed time per iteration (ms): 2651.4 | learning rate: 1.522E-05 | global batch size:     4 | lm loss: 6.486646E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.509 | TFLOPs: 9.19 |
[default0]:[2023-07-25 19:17:19,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=3490, skipped=2, lr=[1.5243673600000002e-05, 1.5243673600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:17:19,053] [INFO] [timer.py:215:stop] epoch=0/micro_step=3490/global_step=3490, RunningAvgSamplesPerSec=1.5166711954164493, CurrSamplesPerSec=1.6613230128052023, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3490/109863281 | consumed samples:        13960 | consumed tokens:     28590080 | elapsed time per iteration (ms): 2621.2 | learning rate: 1.524E-05 | global batch size:     4 | lm loss: 6.520982E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.526 | TFLOPs: 9.29 |
[default0]:[2023-07-25 19:17:32,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=3495, skipped=2, lr=[1.5265518933333337e-05, 1.5265518933333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:17:32,400] [INFO] [timer.py:215:stop] epoch=0/micro_step=3495/global_step=3495, RunningAvgSamplesPerSec=1.516778654145779, CurrSamplesPerSec=1.5494440562495349, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3495/109863281 | consumed samples:        13980 | consumed tokens:     28631040 | elapsed time per iteration (ms): 2661.1 | learning rate: 1.527E-05 | global batch size:     4 | lm loss: 6.274358E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.503 | TFLOPs: 9.15 |
[default0]:[2023-07-25 19:17:45,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=3500, skipped=2, lr=[1.528736426666667e-05, 1.528736426666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:17:45,615] [INFO] [timer.py:215:stop] epoch=0/micro_step=3500/global_step=3500, RunningAvgSamplesPerSec=1.5169047104464568, CurrSamplesPerSec=1.6702409249453547, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3500/109863281 | consumed samples:        14000 | consumed tokens:     28672000 | elapsed time per iteration (ms): 2638.0 | learning rate: 1.529E-05 | global batch size:     4 | lm loss: 6.230154E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.516 | TFLOPs: 9.23 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 3500 | lm loss value: 6.520551E+00 | lm loss PPL: 6.789525E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    3500 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:17:57,079] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3500 is about to be saved!
[default0]:[2023-07-25 19:17:57,202] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3500/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:17:57,202] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3500/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 19:17:57,205] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3500 is ready now!
[default1]:[2023-07-25 19:17:57,202] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3500 is ready now!
[default0]:[2023-07-25 19:17:57,224] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3500 is ready now!
[default0]:[2023-07-25 19:19:09,193] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3500/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:19:09,214] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3500 is ready now!
[default0]:  successfully saved checkpoint at iteration    3500 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (72320.95, 72341.56)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.25, Latency(second): 72.338
[default0]:[2023-07-25 19:19:22,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=3505, skipped=2, lr=[1.53092096e-05, 1.53092096e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:19:22,757] [INFO] [timer.py:215:stop] epoch=0/micro_step=3505/global_step=3505, RunningAvgSamplesPerSec=1.5169962013775098, CurrSamplesPerSec=1.4993602531926347, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3505/109863281 | consumed samples:        14020 | consumed tokens:     28712960 | elapsed time per iteration (ms): 19410.7 | learning rate: 1.531E-05 | global batch size:     4 | lm loss: 6.352341E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.206 | TFLOPs: 1.25 |
[default0]:[2023-07-25 19:19:36,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=3510, skipped=2, lr=[1.5331054933333333e-05, 1.5331054933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:19:36,335] [INFO] [timer.py:215:stop] epoch=0/micro_step=3510/global_step=3510, RunningAvgSamplesPerSec=1.5170778494179795, CurrSamplesPerSec=1.626275197271902, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3510/109863281 | consumed samples:        14040 | consumed tokens:     28753920 | elapsed time per iteration (ms): 2705.8 | learning rate: 1.533E-05 | global batch size:     4 | lm loss: 6.510756E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.478 | TFLOPs: 9.00 |
[default0]:[2023-07-25 19:19:49,771] [INFO] [logging.py:96:log_dist] [Rank 0] step=3515, skipped=2, lr=[1.5352900266666667e-05, 1.5352900266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:19:49,793] [INFO] [timer.py:215:stop] epoch=0/micro_step=3515/global_step=3515, RunningAvgSamplesPerSec=1.5171677873942024, CurrSamplesPerSec=1.6363065183889245, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3515/109863281 | consumed samples:        14060 | consumed tokens:     28794880 | elapsed time per iteration (ms): 2685.2 | learning rate: 1.535E-05 | global batch size:     4 | lm loss: 6.621400E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.490 | TFLOPs: 9.07 |
[default0]:[2023-07-25 19:20:03,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=3520, skipped=2, lr=[1.53747456e-05, 1.53747456e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:20:03,332] [INFO] [timer.py:215:stop] epoch=0/micro_step=3520/global_step=3520, RunningAvgSamplesPerSec=1.5172593980617266, CurrSamplesPerSec=1.5261861220825836, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3520/109863281 | consumed samples:        14080 | consumed tokens:     28835840 | elapsed time per iteration (ms): 2697.6 | learning rate: 1.537E-05 | global batch size:     4 | lm loss: 6.321642E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.483 | TFLOPs: 9.03 |
[default0]:[2023-07-25 19:20:16,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=3525, skipped=2, lr=[1.5396590933333333e-05, 1.5396590933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:20:16,854] [INFO] [timer.py:215:stop] epoch=0/micro_step=3525/global_step=3525, RunningAvgSamplesPerSec=1.5173283876554613, CurrSamplesPerSec=1.5447452911057442, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3525/109863281 | consumed samples:        14100 | consumed tokens:     28876800 | elapsed time per iteration (ms): 2699.4 | learning rate: 1.540E-05 | global batch size:     4 | lm loss: 6.417049E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.482 | TFLOPs: 9.02 |
[default0]:[2023-07-25 19:20:30,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=3530, skipped=2, lr=[1.5418436266666666e-05, 1.5418436266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:20:30,225] [INFO] [timer.py:215:stop] epoch=0/micro_step=3530/global_step=3530, RunningAvgSamplesPerSec=1.5174416515716893, CurrSamplesPerSec=1.5832815472831883, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3530/109863281 | consumed samples:        14120 | consumed tokens:     28917760 | elapsed time per iteration (ms): 2673.6 | learning rate: 1.542E-05 | global batch size:     4 | lm loss: 6.327449E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.496 | TFLOPs: 9.11 |
[default0]:[2023-07-25 19:20:43,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=3535, skipped=2, lr=[1.54402816e-05, 1.54402816e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:20:43,295] [INFO] [timer.py:215:stop] epoch=0/micro_step=3535/global_step=3535, RunningAvgSamplesPerSec=1.517585331058648, CurrSamplesPerSec=1.5566601408463772, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3535/109863281 | consumed samples:        14140 | consumed tokens:     28958720 | elapsed time per iteration (ms): 2601.7 | learning rate: 1.544E-05 | global batch size:     4 | lm loss: 6.398569E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.537 | TFLOPs: 9.36 |
[default0]:[2023-07-25 19:20:56,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=3540, skipped=2, lr=[1.5462126933333333e-05, 1.5462126933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:20:56,502] [INFO] [timer.py:215:stop] epoch=0/micro_step=3540/global_step=3540, RunningAvgSamplesPerSec=1.5177091356354315, CurrSamplesPerSec=1.5393209458907715, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3540/109863281 | consumed samples:        14160 | consumed tokens:     28999680 | elapsed time per iteration (ms): 2637.2 | learning rate: 1.546E-05 | global batch size:     4 | lm loss: 6.396411E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.517 | TFLOPs: 9.24 |
[default0]:[2023-07-25 19:21:09,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=3545, skipped=2, lr=[1.5483972266666666e-05, 1.5483972266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:21:09,874] [INFO] [timer.py:215:stop] epoch=0/micro_step=3545/global_step=3545, RunningAvgSamplesPerSec=1.5177914550977145, CurrSamplesPerSec=1.5173180218558993, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3545/109863281 | consumed samples:        14180 | consumed tokens:     29040640 | elapsed time per iteration (ms): 2663.3 | learning rate: 1.548E-05 | global batch size:     4 | lm loss: 6.335497E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.502 | TFLOPs: 9.14 |
[default0]:[2023-07-25 19:21:23,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=3550, skipped=2, lr=[1.55058176e-05, 1.55058176e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:21:23,287] [INFO] [timer.py:215:stop] epoch=0/micro_step=3550/global_step=3550, RunningAvgSamplesPerSec=1.517894488720845, CurrSamplesPerSec=1.5193545638144699, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3550/109863281 | consumed samples:        14200 | consumed tokens:     29081600 | elapsed time per iteration (ms): 2675.0 | learning rate: 1.551E-05 | global batch size:     4 | lm loss: 6.306530E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.495 | TFLOPs: 9.10 |
[default0]:saving checkpoint at iteration    3550 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:21:23,518] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3550 is about to be saved!
[default1]:[2023-07-25 19:21:23,583] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3550 is ready now!
[default0]:[2023-07-25 19:21:23,586] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3550/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:21:23,586] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3550/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 19:21:23,586] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3550 is ready now!
[default0]:[2023-07-25 19:21:23,584] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3550 is ready now!
[default0]:[2023-07-25 19:22:35,083] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3550/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:22:35,107] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3550 is ready now!
[default0]:  successfully saved checkpoint at iteration    3550 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.794
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71794.16, 71808.32)
[default0]:[2023-07-25 19:22:48,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=3555, skipped=2, lr=[1.5527662933333333e-05, 1.5527662933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:22:48,508] [INFO] [timer.py:215:stop] epoch=0/micro_step=3555/global_step=3555, RunningAvgSamplesPerSec=1.5180034868840824, CurrSamplesPerSec=1.6689947601978774, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3555/109863281 | consumed samples:        14220 | consumed tokens:     29122560 | elapsed time per iteration (ms): 17034.4 | learning rate: 1.553E-05 | global batch size:     4 | lm loss: 6.415779E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.235 | TFLOPs: 1.43 |
[default0]:[2023-07-25 19:23:01,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=3560, skipped=2, lr=[1.5549508266666666e-05, 1.5549508266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:23:01,931] [INFO] [timer.py:215:stop] epoch=0/micro_step=3560/global_step=3560, RunningAvgSamplesPerSec=1.5181046659578004, CurrSamplesPerSec=1.5950409003836243, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3560/109863281 | consumed samples:        14240 | consumed tokens:     29163520 | elapsed time per iteration (ms): 2672.5 | learning rate: 1.555E-05 | global batch size:     4 | lm loss: 6.369867E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.497 | TFLOPs: 9.11 |
[default0]:[2023-07-25 19:23:15,701] [INFO] [logging.py:96:log_dist] [Rank 0] step=3565, skipped=2, lr=[1.55713536e-05, 1.55713536e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:23:15,725] [INFO] [timer.py:215:stop] epoch=0/micro_step=3565/global_step=3565, RunningAvgSamplesPerSec=1.5181241273520338, CurrSamplesPerSec=1.504149379825418, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3565/109863281 | consumed samples:        14260 | consumed tokens:     29204480 | elapsed time per iteration (ms): 2766.3 | learning rate: 1.557E-05 | global batch size:     4 | lm loss: 6.274091E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.446 | TFLOPs: 8.80 |
[default0]:[2023-07-25 19:23:29,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=3570, skipped=2, lr=[1.5593198933333332e-05, 1.5593198933333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:23:29,465] [INFO] [timer.py:215:stop] epoch=0/micro_step=3570/global_step=3570, RunningAvgSamplesPerSec=1.518178682103099, CurrSamplesPerSec=1.5617335251661408, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3570/109863281 | consumed samples:        14280 | consumed tokens:     29245440 | elapsed time per iteration (ms): 2728.2 | learning rate: 1.559E-05 | global batch size:     4 | lm loss: 6.364357E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.466 | TFLOPs: 8.93 |
[default0]:[2023-07-25 19:23:42,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=3575, skipped=2, lr=[1.5615044266666666e-05, 1.5615044266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:23:42,991] [INFO] [timer.py:215:stop] epoch=0/micro_step=3575/global_step=3575, RunningAvgSamplesPerSec=1.5182488376280876, CurrSamplesPerSec=1.6750972251773146, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3575/109863281 | consumed samples:        14300 | consumed tokens:     29286400 | elapsed time per iteration (ms): 2700.7 | learning rate: 1.562E-05 | global batch size:     4 | lm loss: 6.494531E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.481 | TFLOPs: 9.02 |
[default0]:[2023-07-25 19:23:56,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=3580, skipped=2, lr=[1.56368896e-05, 1.56368896e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:23:56,245] [INFO] [timer.py:215:stop] epoch=0/micro_step=3580/global_step=3580, RunningAvgSamplesPerSec=1.5183550959371872, CurrSamplesPerSec=1.5893047617712377, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3580/109863281 | consumed samples:        14320 | consumed tokens:     29327360 | elapsed time per iteration (ms): 2646.1 | learning rate: 1.564E-05 | global batch size:     4 | lm loss: 6.570960E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.512 | TFLOPs: 9.20 |
[default0]:[2023-07-25 19:24:10,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=3585, skipped=2, lr=[1.5658734933333332e-05, 1.5658734933333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:24:10,085] [INFO] [timer.py:215:stop] epoch=0/micro_step=3585/global_step=3585, RunningAvgSamplesPerSec=1.518426408123759, CurrSamplesPerSec=1.6235869884779743, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3585/109863281 | consumed samples:        14340 | consumed tokens:     29368320 | elapsed time per iteration (ms): 2761.0 | learning rate: 1.566E-05 | global batch size:     4 | lm loss: 6.338744E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.449 | TFLOPs: 8.82 |
[default0]:[2023-07-25 19:24:23,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=3590, skipped=2, lr=[1.5680580266666665e-05, 1.5680580266666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:24:23,314] [INFO] [timer.py:215:stop] epoch=0/micro_step=3590/global_step=3590, RunningAvgSamplesPerSec=1.5185509659382224, CurrSamplesPerSec=1.5948854809586794, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3590/109863281 | consumed samples:        14360 | consumed tokens:     29409280 | elapsed time per iteration (ms): 2635.4 | learning rate: 1.568E-05 | global batch size:     4 | lm loss: 6.360065E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.518 | TFLOPs: 9.24 |
[default0]:[2023-07-25 19:24:36,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=3595, skipped=2, lr=[1.57024256e-05, 1.57024256e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:24:36,796] [INFO] [timer.py:215:stop] epoch=0/micro_step=3595/global_step=3595, RunningAvgSamplesPerSec=1.518623201727733, CurrSamplesPerSec=1.5043659856442813, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3595/109863281 | consumed samples:        14380 | consumed tokens:     29450240 | elapsed time per iteration (ms): 2685.7 | learning rate: 1.570E-05 | global batch size:     4 | lm loss: 6.440732E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.489 | TFLOPs: 9.07 |
[default0]:[2023-07-25 19:24:49,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=3600, skipped=2, lr=[1.5724270933333332e-05, 1.5724270933333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:24:49,788] [INFO] [timer.py:215:stop] epoch=0/micro_step=3600/global_step=3600, RunningAvgSamplesPerSec=1.5187917541697866, CurrSamplesPerSec=1.6434304378558744, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3600/109863281 | consumed samples:        14400 | consumed tokens:     29491200 | elapsed time per iteration (ms): 2604.4 | learning rate: 1.572E-05 | global batch size:     4 | lm loss: 6.446023E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.536 | TFLOPs: 9.35 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 3600 | lm loss value: 6.532375E+00 | lm loss PPL: 6.870282E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    3600 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:[2023-07-25 19:25:01,527] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3600 is ready now!
[default0]:[2023-07-25 19:25:01,526] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3600 is ready now!
[default0]:[2023-07-25 19:25:01,476] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3600 is about to be saved!
[default0]:[2023-07-25 19:25:01,525] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3600/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:25:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3600/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 19:25:01,528] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3600 is ready now!
[default0]:[2023-07-25 19:26:13,266] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3600/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:26:13,292] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3600 is ready now!
[default0]:  successfully saved checkpoint at iteration    3600 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 72.004
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (72003.55, 72022.88)
[default0]:[2023-07-25 19:26:26,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=3605, skipped=2, lr=[1.5746116266666665e-05, 1.5746116266666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:26:26,873] [INFO] [timer.py:215:stop] epoch=0/micro_step=3605/global_step=3605, RunningAvgSamplesPerSec=1.5188759386300963, CurrSamplesPerSec=1.7175034453021558, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3605/109863281 | consumed samples:        14420 | consumed tokens:     29532160 | elapsed time per iteration (ms): 19403.7 | learning rate: 1.575E-05 | global batch size:     4 | lm loss: 6.262654E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.206 | TFLOPs: 1.26 |
[default0]:[2023-07-25 19:26:40,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=3610, skipped=2, lr=[1.57679616e-05, 1.57679616e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:26:40,434] [INFO] [timer.py:215:stop] epoch=0/micro_step=3610/global_step=3610, RunningAvgSamplesPerSec=1.5189606162084066, CurrSamplesPerSec=1.6675241893250208, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3610/109863281 | consumed samples:        14440 | consumed tokens:     29573120 | elapsed time per iteration (ms): 2706.4 | learning rate: 1.577E-05 | global batch size:     4 | lm loss: 6.420563E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.478 | TFLOPs: 9.00 |
[default0]:[2023-07-25 19:26:53,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=3615, skipped=2, lr=[1.578980693333333e-05, 1.578980693333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:26:53,672] [INFO] [timer.py:215:stop] epoch=0/micro_step=3615/global_step=3615, RunningAvgSamplesPerSec=1.5191117276863522, CurrSamplesPerSec=1.5984582131988245, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3615/109863281 | consumed samples:        14460 | consumed tokens:     29614080 | elapsed time per iteration (ms): 2641.8 | learning rate: 1.579E-05 | global batch size:     4 | lm loss: 6.205391E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.514 | TFLOPs: 9.22 |
[default0]:[2023-07-25 19:27:07,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=3620, skipped=2, lr=[1.581165226666667e-05, 1.581165226666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:27:07,368] [INFO] [timer.py:215:stop] epoch=0/micro_step=3620/global_step=3620, RunningAvgSamplesPerSec=1.5191705144150052, CurrSamplesPerSec=1.5679236685109805, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3620/109863281 | consumed samples:        14480 | consumed tokens:     29655040 | elapsed time per iteration (ms): 2717.5 | learning rate: 1.581E-05 | global batch size:     4 | lm loss: 6.487215E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.472 | TFLOPs: 8.96 |
[default0]:[2023-07-25 19:27:20,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=3625, skipped=2, lr=[1.58334976e-05, 1.58334976e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:27:20,465] [INFO] [timer.py:215:stop] epoch=0/micro_step=3625/global_step=3625, RunningAvgSamplesPerSec=1.5193121478365699, CurrSamplesPerSec=1.6508536270490513, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3625/109863281 | consumed samples:        14500 | consumed tokens:     29696000 | elapsed time per iteration (ms): 2629.7 | learning rate: 1.583E-05 | global batch size:     4 | lm loss: 6.411375E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.521 | TFLOPs: 9.26 |
[default0]:[2023-07-25 19:27:33,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=3630, skipped=2, lr=[1.5855342933333335e-05, 1.5855342933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:27:33,771] [INFO] [timer.py:215:stop] epoch=0/micro_step=3630/global_step=3630, RunningAvgSamplesPerSec=1.5194240033474722, CurrSamplesPerSec=1.5238261321963231, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3630/109863281 | consumed samples:        14520 | consumed tokens:     29736960 | elapsed time per iteration (ms): 2632.6 | learning rate: 1.586E-05 | global batch size:     4 | lm loss: 6.447253E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.519 | TFLOPs: 9.25 |
[default0]:[2023-07-25 19:27:46,717] [INFO] [logging.py:96:log_dist] [Rank 0] step=3635, skipped=2, lr=[1.5877188266666668e-05, 1.5877188266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:27:46,745] [INFO] [timer.py:215:stop] epoch=0/micro_step=3635/global_step=3635, RunningAvgSamplesPerSec=1.51958697995345, CurrSamplesPerSec=1.6037401066367813, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3635/109863281 | consumed samples:        14540 | consumed tokens:     29777920 | elapsed time per iteration (ms): 2593.0 | learning rate: 1.588E-05 | global batch size:     4 | lm loss: 6.247198E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.543 | TFLOPs: 9.39 |
[default0]:[2023-07-25 19:28:00,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=3640, skipped=2, lr=[1.58990336e-05, 1.58990336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:28:00,080] [INFO] [timer.py:215:stop] epoch=0/micro_step=3640/global_step=3640, RunningAvgSamplesPerSec=1.519689801322858, CurrSamplesPerSec=1.603080259424867, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3640/109863281 | consumed samples:        14560 | consumed tokens:     29818880 | elapsed time per iteration (ms): 2663.6 | learning rate: 1.590E-05 | global batch size:     4 | lm loss: 6.318062E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.502 | TFLOPs: 9.14 |
[default0]:[2023-07-25 19:28:12,931] [INFO] [logging.py:96:log_dist] [Rank 0] step=3645, skipped=2, lr=[1.5920878933333335e-05, 1.5920878933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:28:12,938] [INFO] [timer.py:215:stop] epoch=0/micro_step=3645/global_step=3645, RunningAvgSamplesPerSec=1.519872608987054, CurrSamplesPerSec=1.5734032347898732, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3645/109863281 | consumed samples:        14580 | consumed tokens:     29859840 | elapsed time per iteration (ms): 2569.0 | learning rate: 1.592E-05 | global batch size:     4 | lm loss: 6.367783E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.557 | TFLOPs: 9.48 |
[default0]:[2023-07-25 19:28:25,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=3650, skipped=2, lr=[1.5942724266666668e-05, 1.5942724266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:28:25,975] [INFO] [timer.py:215:stop] epoch=0/micro_step=3650/global_step=3650, RunningAvgSamplesPerSec=1.5200299912966044, CurrSamplesPerSec=1.6350690258838902, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3650/109863281 | consumed samples:        14600 | consumed tokens:     29900800 | elapsed time per iteration (ms): 2592.8 | learning rate: 1.594E-05 | global batch size:     4 | lm loss: 6.217916E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.543 | TFLOPs: 9.39 |
[default0]:saving checkpoint at iteration    3650 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:28:26,212] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3650 is about to be saved!
[default1]:[2023-07-25 19:28:26,259] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3650 is ready now!
[default1]:[2023-07-25 19:28:26,260] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3650 is ready now!
[default0]:[2023-07-25 19:28:26,271] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3650 is ready now!
[default0]:[2023-07-25 19:28:26,262] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3650/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:28:26,262] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3650/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 19:29:37,542] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3650/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:29:37,567] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3650 is ready now!
[default0]:  successfully saved checkpoint at iteration    3650 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.516
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71514.28, 71531.78)
[default0]:[2023-07-25 19:29:50,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=3655, skipped=2, lr=[1.59645696e-05, 1.59645696e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:29:50,834] [INFO] [timer.py:215:stop] epoch=0/micro_step=3655/global_step=3655, RunningAvgSamplesPerSec=1.5201556565364582, CurrSamplesPerSec=1.5964879810087271, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3655/109863281 | consumed samples:        14620 | consumed tokens:     29941760 | elapsed time per iteration (ms): 16963.9 | learning rate: 1.596E-05 | global batch size:     4 | lm loss: 6.483292E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.236 | TFLOPs: 1.44 |
[default0]:[2023-07-25 19:30:04,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=3660, skipped=2, lr=[1.5986414933333334e-05, 1.5986414933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:30:04,304] [INFO] [timer.py:215:stop] epoch=0/micro_step=3660/global_step=3660, RunningAvgSamplesPerSec=1.5202654078313824, CurrSamplesPerSec=1.6379131047372246, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3660/109863281 | consumed samples:        14640 | consumed tokens:     29982720 | elapsed time per iteration (ms): 2676.3 | learning rate: 1.599E-05 | global batch size:     4 | lm loss: 6.270749E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.495 | TFLOPs: 9.10 |
[default0]:[2023-07-25 19:30:17,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=3665, skipped=2, lr=[1.6008260266666668e-05, 1.6008260266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:30:17,735] [INFO] [timer.py:215:stop] epoch=0/micro_step=3665/global_step=3665, RunningAvgSamplesPerSec=1.5203636939098852, CurrSamplesPerSec=1.5171330650630737, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3665/109863281 | consumed samples:        14660 | consumed tokens:     30023680 | elapsed time per iteration (ms): 2677.0 | learning rate: 1.601E-05 | global batch size:     4 | lm loss: 6.176010E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.494 | TFLOPs: 9.10 |
[default0]:[2023-07-25 19:30:31,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=3670, skipped=2, lr=[1.60301056e-05, 1.60301056e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:30:31,059] [INFO] [timer.py:215:stop] epoch=0/micro_step=3670/global_step=3670, RunningAvgSamplesPerSec=1.5204582578655461, CurrSamplesPerSec=1.5360389072355043, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3670/109863281 | consumed samples:        14680 | consumed tokens:     30064640 | elapsed time per iteration (ms): 2667.8 | learning rate: 1.603E-05 | global batch size:     4 | lm loss: 6.422316E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.499 | TFLOPs: 9.13 |
[default0]:[2023-07-25 19:30:44,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=3675, skipped=2, lr=[1.6051950933333334e-05, 1.6051950933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:30:44,888] [INFO] [timer.py:215:stop] epoch=0/micro_step=3675/global_step=3675, RunningAvgSamplesPerSec=1.5204889609184444, CurrSamplesPerSec=1.5446730412035792, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3675/109863281 | consumed samples:        14700 | consumed tokens:     30105600 | elapsed time per iteration (ms): 2756.2 | learning rate: 1.605E-05 | global batch size:     4 | lm loss: 6.517068E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.451 | TFLOPs: 8.84 |
[default0]:[2023-07-25 19:30:58,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=3680, skipped=2, lr=[1.6073796266666667e-05, 1.6073796266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:30:58,293] [INFO] [timer.py:215:stop] epoch=0/micro_step=3680/global_step=3680, RunningAvgSamplesPerSec=1.5205861915796834, CurrSamplesPerSec=1.5084720876914777, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3680/109863281 | consumed samples:        14720 | consumed tokens:     30146560 | elapsed time per iteration (ms): 2671.2 | learning rate: 1.607E-05 | global batch size:     4 | lm loss: 6.269757E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.497 | TFLOPs: 9.12 |
[default0]:[2023-07-25 19:31:11,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=3685, skipped=2, lr=[1.60956416e-05, 1.60956416e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:31:11,572] [INFO] [timer.py:215:stop] epoch=0/micro_step=3685/global_step=3685, RunningAvgSamplesPerSec=1.520713283939966, CurrSamplesPerSec=1.6962867802956234, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3685/109863281 | consumed samples:        14740 | consumed tokens:     30187520 | elapsed time per iteration (ms): 2646.5 | learning rate: 1.610E-05 | global batch size:     4 | lm loss: 6.378991E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.511 | TFLOPs: 9.20 |
[default0]:[2023-07-25 19:31:24,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=3690, skipped=2, lr=[1.6117486933333334e-05, 1.6117486933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:31:24,935] [INFO] [timer.py:215:stop] epoch=0/micro_step=3690/global_step=3690, RunningAvgSamplesPerSec=1.5208048623456183, CurrSamplesPerSec=1.624815023142445, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3690/109863281 | consumed samples:        14760 | consumed tokens:     30228480 | elapsed time per iteration (ms): 2664.5 | learning rate: 1.612E-05 | global batch size:     4 | lm loss: 6.619186E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.501 | TFLOPs: 9.14 |
[default0]:[2023-07-25 19:31:38,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=3695, skipped=2, lr=[1.6139332266666667e-05, 1.6139332266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:31:38,754] [INFO] [timer.py:215:stop] epoch=0/micro_step=3695/global_step=3695, RunningAvgSamplesPerSec=1.5208349675358854, CurrSamplesPerSec=1.5451457748032214, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3695/109863281 | consumed samples:        14780 | consumed tokens:     30269440 | elapsed time per iteration (ms): 2751.8 | learning rate: 1.614E-05 | global batch size:     4 | lm loss: 6.428290E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.454 | TFLOPs: 8.85 |
[default0]:[2023-07-25 19:31:44,090] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default0]:[2023-07-25 19:31:44,112] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[default1]:[2023-07-25 19:31:44,094] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default1]:[2023-07-25 19:31:44,094] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[default0]:[2023-07-25 19:31:44,107] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default0]:[2023-07-25 19:31:44,109] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[default1]:[2023-07-25 19:31:44,103] [INFO] [fused_optimizer.py:370:_update_scale] No Grad overflow for 500 iterations
[default1]:[2023-07-25 19:31:44,119] [INFO] [fused_optimizer.py:371:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
[default0]:[2023-07-25 19:31:52,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=3700, skipped=2, lr=[1.61611776e-05, 1.61611776e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:31:52,203] [INFO] [timer.py:215:stop] epoch=0/micro_step=3700/global_step=3700, RunningAvgSamplesPerSec=1.5209203769316242, CurrSamplesPerSec=1.590503328050052, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3700/109863281 | consumed samples:        14800 | consumed tokens:     30310400 | elapsed time per iteration (ms): 2688.1 | learning rate: 1.616E-05 | global batch size:     4 | lm loss: 6.213052E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.488 | TFLOPs: 9.06 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 3700 | lm loss value: 6.617492E+00 | lm loss PPL: 7.480664E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    3700 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:32:03,961] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3700 is about to be saved!
[default1]:[2023-07-25 19:32:04,039] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3700 is ready now!
[default1]:[2023-07-25 19:32:04,039] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3700 is ready now!
[default0]:[2023-07-25 19:32:04,049] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3700 is ready now!
[default0]:[2023-07-25 19:32:04,037] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3700/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:32:04,037] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3700/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 19:33:14,461] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3700/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:33:14,471] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3700 is ready now!
[default0]:  successfully saved checkpoint at iteration    3700 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (70677.24, 70689.51)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 70.689
[default0]:[2023-07-25 19:33:28,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=3705, skipped=2, lr=[1.6183022933333334e-05, 1.6183022933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:33:28,120] [INFO] [timer.py:215:stop] epoch=0/micro_step=3705/global_step=3705, RunningAvgSamplesPerSec=1.520974914205426, CurrSamplesPerSec=1.4795760372971474, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3705/109863281 | consumed samples:        14820 | consumed tokens:     30351360 | elapsed time per iteration (ms): 19167.5 | learning rate: 1.618E-05 | global batch size:     4 | lm loss: 6.481197E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.209 | TFLOPs: 1.27 |
[default0]:[2023-07-25 19:33:41,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=3710, skipped=2, lr=[1.6204868266666667e-05, 1.6204868266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:33:41,511] [INFO] [timer.py:215:stop] epoch=0/micro_step=3710/global_step=3710, RunningAvgSamplesPerSec=1.5210652564527483, CurrSamplesPerSec=1.5870100121192767, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3710/109863281 | consumed samples:        14840 | consumed tokens:     30392320 | elapsed time per iteration (ms): 2671.2 | learning rate: 1.620E-05 | global batch size:     4 | lm loss: 6.404918E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.497 | TFLOPs: 9.12 |
[default0]:[2023-07-25 19:33:54,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=3715, skipped=2, lr=[1.62267136e-05, 1.62267136e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:33:54,694] [INFO] [timer.py:215:stop] epoch=0/micro_step=3715/global_step=3715, RunningAvgSamplesPerSec=1.5211883745812633, CurrSamplesPerSec=1.497515924681194, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3715/109863281 | consumed samples:        14860 | consumed tokens:     30433280 | elapsed time per iteration (ms): 2635.1 | learning rate: 1.623E-05 | global batch size:     4 | lm loss: 6.280374E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.518 | TFLOPs: 9.24 |
[default0]:[2023-07-25 19:34:08,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=3720, skipped=2, lr=[1.6248558933333333e-05, 1.6248558933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:34:08,242] [INFO] [timer.py:215:stop] epoch=0/micro_step=3720/global_step=3720, RunningAvgSamplesPerSec=1.5212625688659351, CurrSamplesPerSec=1.544652704359562, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3720/109863281 | consumed samples:        14880 | consumed tokens:     30474240 | elapsed time per iteration (ms): 2694.8 | learning rate: 1.625E-05 | global batch size:     4 | lm loss: 6.456050E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.484 | TFLOPs: 9.04 |
[default0]:[2023-07-25 19:34:21,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=3725, skipped=2, lr=[1.6270404266666667e-05, 1.6270404266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:34:21,549] [INFO] [timer.py:215:stop] epoch=0/micro_step=3725/global_step=3725, RunningAvgSamplesPerSec=1.5213608054378573, CurrSamplesPerSec=1.582871357907259, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3725/109863281 | consumed samples:        14900 | consumed tokens:     30515200 | elapsed time per iteration (ms): 2657.3 | learning rate: 1.627E-05 | global batch size:     4 | lm loss: 6.387590E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.505 | TFLOPs: 9.17 |
[default0]:[2023-07-25 19:34:34,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=3730, skipped=2, lr=[1.62922496e-05, 1.62922496e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:34:34,898] [INFO] [timer.py:215:stop] epoch=0/micro_step=3730/global_step=3730, RunningAvgSamplesPerSec=1.5214618062006897, CurrSamplesPerSec=1.5572243578052116, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3730/109863281 | consumed samples:        14920 | consumed tokens:     30556160 | elapsed time per iteration (ms): 2675.1 | learning rate: 1.629E-05 | global batch size:     4 | lm loss: 6.308397E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.495 | TFLOPs: 9.10 |
[default0]:[2023-07-25 19:34:47,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=3735, skipped=2, lr=[1.6314094933333333e-05, 1.6314094933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:34:47,960] [INFO] [timer.py:215:stop] epoch=0/micro_step=3735/global_step=3735, RunningAvgSamplesPerSec=1.5216035051289516, CurrSamplesPerSec=1.6313418973223661, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3735/109863281 | consumed samples:        14940 | consumed tokens:     30597120 | elapsed time per iteration (ms): 2599.5 | learning rate: 1.631E-05 | global batch size:     4 | lm loss: 6.309495E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.539 | TFLOPs: 9.37 |
[default0]:[2023-07-25 19:35:01,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=3740, skipped=2, lr=[1.6335940266666666e-05, 1.6335940266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:35:01,138] [INFO] [timer.py:215:stop] epoch=0/micro_step=3740/global_step=3740, RunningAvgSamplesPerSec=1.5217231542510568, CurrSamplesPerSec=1.6702813318074636, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3740/109863281 | consumed samples:        14960 | consumed tokens:     30638080 | elapsed time per iteration (ms): 2615.2 | learning rate: 1.634E-05 | global batch size:     4 | lm loss: 6.221402E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.530 | TFLOPs: 9.31 |
[default0]:[2023-07-25 19:35:14,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=3745, skipped=2, lr=[1.63577856e-05, 1.63577856e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:35:14,211] [INFO] [timer.py:215:stop] epoch=0/micro_step=3745/global_step=3745, RunningAvgSamplesPerSec=1.5218652403647561, CurrSamplesPerSec=1.6326830582289202, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3745/109863281 | consumed samples:        14980 | consumed tokens:     30679040 | elapsed time per iteration (ms): 2615.7 | learning rate: 1.636E-05 | global batch size:     4 | lm loss: 6.431484E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.529 | TFLOPs: 9.31 |
[default0]:[2023-07-25 19:35:27,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=3750, skipped=2, lr=[1.6379630933333333e-05, 1.6379630933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:35:27,557] [INFO] [timer.py:215:stop] epoch=0/micro_step=3750/global_step=3750, RunningAvgSamplesPerSec=1.5219840854371336, CurrSamplesPerSec=1.6797677250659029, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3750/109863281 | consumed samples:        15000 | consumed tokens:     30720000 | elapsed time per iteration (ms): 2653.2 | learning rate: 1.638E-05 | global batch size:     4 | lm loss: 6.355114E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.508 | TFLOPs: 9.18 |
[default0]:saving checkpoint at iteration    3750 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:35:27,700] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3750 is about to be saved!
[default0]:[2023-07-25 19:35:27,742] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3750/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:35:27,742] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3750/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 19:35:27,740] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3750 is ready now!
[default1]:[2023-07-25 19:35:27,742] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3750 is ready now!
[default0]:[2023-07-25 19:35:27,740] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3750 is ready now!
[default0]:[2023-07-25 19:36:38,876] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3750/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:36:38,899] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3750 is ready now!
[default0]:  successfully saved checkpoint at iteration    3750 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71359.62, 71374.07)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.37
[default0]:[2023-07-25 19:36:52,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=3755, skipped=2, lr=[1.6401476266666666e-05, 1.6401476266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:36:52,387] [INFO] [timer.py:215:stop] epoch=0/micro_step=3755/global_step=3755, RunningAvgSamplesPerSec=1.5220519025877601, CurrSamplesPerSec=1.5642819094902494, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3755/109863281 | consumed samples:        15020 | consumed tokens:     30760960 | elapsed time per iteration (ms): 16968.2 | learning rate: 1.640E-05 | global batch size:     4 | lm loss: 6.284302E+00 | loss scale: 32768.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.236 | TFLOPs: 1.44 |
[default0]:[2023-07-25 19:36:55,170] [INFO] [fused_optimizer.py:362:_update_scale] 
[default0]:Grad overflow on iteration 3755
[default0]:[2023-07-25 19:36:55,184] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[default1]:[2023-07-25 19:36:55,173] [INFO] [fused_optimizer.py:362:_update_scale] 
[default1]:Grad overflow on iteration 3755
[default1]:[2023-07-25 19:36:55,174] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[default0]:[2023-07-25 19:36:55,186] [INFO] [fused_optimizer.py:362:_update_scale] 
[default0]:Grad overflow on iteration 3755
[default0]:[2023-07-25 19:36:55,215] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[default0]:[2023-07-25 19:36:55,215] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
[default1]:[2023-07-25 19:36:55,180] [INFO] [fused_optimizer.py:362:_update_scale] 
[default1]:Grad overflow on iteration 3755
[default1]:[2023-07-25 19:36:55,187] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
[default0]:[2023-07-25 19:37:06,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=3760, skipped=3, lr=[1.64233216e-05, 1.64233216e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:37:06,131] [INFO] [timer.py:215:stop] epoch=0/micro_step=3760/global_step=3760, RunningAvgSamplesPerSec=1.5220881962334987, CurrSamplesPerSec=1.4846360684136128, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3760/109863281 | consumed samples:        15040 | consumed tokens:     30801920 | elapsed time per iteration (ms): 2747.9 | learning rate: 1.642E-05 | global batch size:     4 | lm loss: 6.286376E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.456 | TFLOPs: 8.86 |
[default0]:[2023-07-25 19:37:19,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=3765, skipped=3, lr=[1.6445166933333333e-05, 1.6445166933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:37:19,341] [INFO] [timer.py:215:stop] epoch=0/micro_step=3765/global_step=3765, RunningAvgSamplesPerSec=1.5222280559935204, CurrSamplesPerSec=1.7017779328588933, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3765/109863281 | consumed samples:        15060 | consumed tokens:     30842880 | elapsed time per iteration (ms): 2620.8 | learning rate: 1.645E-05 | global batch size:     4 | lm loss: 6.376733E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.526 | TFLOPs: 9.29 |
[default0]:[2023-07-25 19:37:32,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=3770, skipped=3, lr=[1.6467012266666666e-05, 1.6467012266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:37:32,644] [INFO] [timer.py:215:stop] epoch=0/micro_step=3770/global_step=3770, RunningAvgSamplesPerSec=1.5223234803727148, CurrSamplesPerSec=1.6509603580757708, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3770/109863281 | consumed samples:        15080 | consumed tokens:     30883840 | elapsed time per iteration (ms): 2664.0 | learning rate: 1.647E-05 | global batch size:     4 | lm loss: 6.342640E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.502 | TFLOPs: 9.14 |
[default0]:[2023-07-25 19:37:45,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=3775, skipped=3, lr=[1.64888576e-05, 1.64888576e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:37:45,933] [INFO] [timer.py:215:stop] epoch=0/micro_step=3775/global_step=3775, RunningAvgSamplesPerSec=1.5224594623912213, CurrSamplesPerSec=1.5210409926013282, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3775/109863281 | consumed samples:        15100 | consumed tokens:     30924800 | elapsed time per iteration (ms): 2640.7 | learning rate: 1.649E-05 | global batch size:     4 | lm loss: 6.198670E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.515 | TFLOPs: 9.22 |
[default0]:[2023-07-25 19:37:59,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=3780, skipped=3, lr=[1.6510702933333333e-05, 1.6510702933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:37:59,057] [INFO] [timer.py:215:stop] epoch=0/micro_step=3780/global_step=3780, RunningAvgSamplesPerSec=1.5225698530255514, CurrSamplesPerSec=1.5500162279814746, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3780/109863281 | consumed samples:        15120 | consumed tokens:     30965760 | elapsed time per iteration (ms): 2610.2 | learning rate: 1.651E-05 | global batch size:     4 | lm loss: 6.172113E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.532 | TFLOPs: 9.33 |
[default0]:[2023-07-25 19:38:12,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=3785, skipped=3, lr=[1.6532548266666666e-05, 1.6532548266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:38:12,191] [INFO] [timer.py:215:stop] epoch=0/micro_step=3785/global_step=3785, RunningAvgSamplesPerSec=1.5226830222312973, CurrSamplesPerSec=1.5174177910129178, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3785/109863281 | consumed samples:        15140 | consumed tokens:     31006720 | elapsed time per iteration (ms): 2618.1 | learning rate: 1.653E-05 | global batch size:     4 | lm loss: 6.333805E+00 | loss scale: 16384.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.528 | TFLOPs: 9.30 |
[default1]:[2023-07-25 19:38:22,639] [INFO] [fused_optimizer.py:362:_update_scale] 
[default1]:Grad overflow on iteration 3788
[default1]:[2023-07-25 19:38:22,639] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[default1]:[2023-07-25 19:38:22,639] [INFO] [fused_optimizer.py:362:_update_scale] 
[default1]:Grad overflow on iteration 3788
[default1]:[2023-07-25 19:38:22,657] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[default0]:[2023-07-25 19:38:22,648] [INFO] [fused_optimizer.py:362:_update_scale] 
[default0]:Grad overflow on iteration 3788
[default0]:[2023-07-25 19:38:22,648] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[default0]:[2023-07-25 19:38:22,702] [INFO] [fused_optimizer.py:362:_update_scale] 
[default0]:Grad overflow on iteration 3788
[default0]:[2023-07-25 19:38:22,723] [INFO] [fused_optimizer.py:363:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
[default0]:[2023-07-25 19:38:22,723] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
[default0]:[2023-07-25 19:38:25,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=3790, skipped=4, lr=[1.65543936e-05, 1.65543936e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:38:25,441] [INFO] [timer.py:215:stop] epoch=0/micro_step=3790/global_step=3790, RunningAvgSamplesPerSec=1.5227875774913453, CurrSamplesPerSec=1.5316484413164904, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3790/109863281 | consumed samples:        15160 | consumed tokens:     31047680 | elapsed time per iteration (ms): 2646.7 | learning rate: 1.655E-05 | global batch size:     4 | lm loss: 6.311412E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.511 | TFLOPs: 9.20 |
[default0]:[2023-07-25 19:38:39,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=3795, skipped=4, lr=[1.6576238933333332e-05, 1.6576238933333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:38:39,382] [INFO] [timer.py:215:stop] epoch=0/micro_step=3795/global_step=3795, RunningAvgSamplesPerSec=1.5227930052670156, CurrSamplesPerSec=1.6614694382489366, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3795/109863281 | consumed samples:        15180 | consumed tokens:     31088640 | elapsed time per iteration (ms): 2773.8 | learning rate: 1.658E-05 | global batch size:     4 | lm loss: 6.376079E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.442 | TFLOPs: 8.78 |
[default0]:[2023-07-25 19:38:53,364] [INFO] [logging.py:96:log_dist] [Rank 0] step=3800, skipped=4, lr=[1.659808426666667e-05, 1.659808426666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:38:53,396] [INFO] [timer.py:215:stop] epoch=0/micro_step=3800/global_step=3800, RunningAvgSamplesPerSec=1.5227874435581943, CurrSamplesPerSec=1.4823947457615736, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3800/109863281 | consumed samples:        15200 | consumed tokens:     31129600 | elapsed time per iteration (ms): 2793.9 | learning rate: 1.660E-05 | global batch size:     4 | lm loss: 6.355011E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.432 | TFLOPs: 8.72 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 3800 | lm loss value: 6.500584E+00 | lm loss PPL: 6.655303E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    3800 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:39:05,132] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3800 is about to be saved!
[default0]:[2023-07-25 19:39:05,201] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3800/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:39:05,201] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3800/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 19:39:05,192] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3800 is ready now!
[default1]:[2023-07-25 19:39:05,189] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3800 is ready now!
[default0]:[2023-07-25 19:39:05,190] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3800 is ready now!
[default0]:[2023-07-25 19:40:17,400] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3800/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:40:17,424] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3800 is ready now!
[default0]:  successfully saved checkpoint at iteration    3800 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.25, Latency(second): 72.5
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (72477.03, 72500.13)
[default0]:[2023-07-25 19:40:30,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=3805, skipped=4, lr=[1.6619929600000002e-05, 1.6619929600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:40:30,947] [INFO] [timer.py:215:stop] epoch=0/micro_step=3805/global_step=3805, RunningAvgSamplesPerSec=1.5228896263655927, CurrSamplesPerSec=1.700159109820028, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3805/109863281 | consumed samples:        15220 | consumed tokens:     31170560 | elapsed time per iteration (ms): 19502.1 | learning rate: 1.662E-05 | global batch size:     4 | lm loss: 6.200518E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.205 | TFLOPs: 1.25 |
[default0]:[2023-07-25 19:40:43,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=3810, skipped=4, lr=[1.6641774933333335e-05, 1.6641774933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:40:43,957] [INFO] [timer.py:215:stop] epoch=0/micro_step=3810/global_step=3810, RunningAvgSamplesPerSec=1.5230420112768468, CurrSamplesPerSec=1.5717174321332588, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3810/109863281 | consumed samples:        15240 | consumed tokens:     31211520 | elapsed time per iteration (ms): 2601.7 | learning rate: 1.664E-05 | global batch size:     4 | lm loss: 6.297503E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.537 | TFLOPs: 9.36 |
[default0]:[2023-07-25 19:40:57,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=3815, skipped=4, lr=[1.666362026666667e-05, 1.666362026666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:40:57,236] [INFO] [timer.py:215:stop] epoch=0/micro_step=3815/global_step=3815, RunningAvgSamplesPerSec=1.5231460437132038, CurrSamplesPerSec=1.5899094691029234, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3815/109863281 | consumed samples:        15260 | consumed tokens:     31252480 | elapsed time per iteration (ms): 2640.8 | learning rate: 1.666E-05 | global batch size:     4 | lm loss: 6.398270E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.515 | TFLOPs: 9.22 |
[default0]:[2023-07-25 19:41:10,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=3820, skipped=4, lr=[1.6685465600000002e-05, 1.6685465600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:41:10,897] [INFO] [timer.py:215:stop] epoch=0/micro_step=3820/global_step=3820, RunningAvgSamplesPerSec=1.5231822398380395, CurrSamplesPerSec=1.6449028277610767, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3820/109863281 | consumed samples:        15280 | consumed tokens:     31293440 | elapsed time per iteration (ms): 2726.1 | learning rate: 1.669E-05 | global batch size:     4 | lm loss: 6.328408E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.467 | TFLOPs: 8.93 |
[default0]:[2023-07-25 19:41:24,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=3825, skipped=4, lr=[1.6707310933333335e-05, 1.6707310933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:41:24,149] [INFO] [timer.py:215:stop] epoch=0/micro_step=3825/global_step=3825, RunningAvgSamplesPerSec=1.523276493955725, CurrSamplesPerSec=1.6550441845828459, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3825/109863281 | consumed samples:        15300 | consumed tokens:     31334400 | elapsed time per iteration (ms): 2644.6 | learning rate: 1.671E-05 | global batch size:     4 | lm loss: 6.209354E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.513 | TFLOPs: 9.21 |
[default0]:[2023-07-25 19:41:37,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=3830, skipped=4, lr=[1.672915626666667e-05, 1.672915626666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:41:37,606] [INFO] [timer.py:215:stop] epoch=0/micro_step=3830/global_step=3830, RunningAvgSamplesPerSec=1.5233481132996527, CurrSamplesPerSec=1.7264798068162195, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3830/109863281 | consumed samples:        15320 | consumed tokens:     31375360 | elapsed time per iteration (ms): 2699.1 | learning rate: 1.673E-05 | global batch size:     4 | lm loss: 6.192849E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.482 | TFLOPs: 9.02 |
[default0]:[2023-07-25 19:41:51,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=3835, skipped=4, lr=[1.6751001600000002e-05, 1.6751001600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:41:51,920] [INFO] [timer.py:215:stop] epoch=0/micro_step=3835/global_step=3835, RunningAvgSamplesPerSec=1.5233141120408, CurrSamplesPerSec=1.5243926949302171, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3835/109863281 | consumed samples:        15340 | consumed tokens:     31416320 | elapsed time per iteration (ms): 2852.2 | learning rate: 1.675E-05 | global batch size:     4 | lm loss: 6.313129E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.402 | TFLOPs: 8.54 |
[default0]:[2023-07-25 19:42:05,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=3840, skipped=4, lr=[1.6772846933333335e-05, 1.6772846933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:42:05,821] [INFO] [timer.py:215:stop] epoch=0/micro_step=3840/global_step=3840, RunningAvgSamplesPerSec=1.5233274156855499, CurrSamplesPerSec=1.5429318454476189, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3840/109863281 | consumed samples:        15360 | consumed tokens:     31457280 | elapsed time per iteration (ms): 2763.5 | learning rate: 1.677E-05 | global batch size:     4 | lm loss: 6.187727E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.447 | TFLOPs: 8.81 |
[default0]:[2023-07-25 19:42:18,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=3845, skipped=4, lr=[1.6794692266666668e-05, 1.6794692266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:42:18,779] [INFO] [timer.py:215:stop] epoch=0/micro_step=3845/global_step=3845, RunningAvgSamplesPerSec=1.52347449023824, CurrSamplesPerSec=1.5797317852029331, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3845/109863281 | consumed samples:        15380 | consumed tokens:     31498240 | elapsed time per iteration (ms): 2579.6 | learning rate: 1.679E-05 | global batch size:     4 | lm loss: 6.202455E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.551 | TFLOPs: 9.44 |
[default0]:[2023-07-25 19:42:32,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=3850, skipped=4, lr=[1.68165376e-05, 1.68165376e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:42:32,363] [INFO] [timer.py:215:stop] epoch=0/micro_step=3850/global_step=3850, RunningAvgSamplesPerSec=1.523537796479681, CurrSamplesPerSec=1.6481383408508967, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3850/109863281 | consumed samples:        15400 | consumed tokens:     31539200 | elapsed time per iteration (ms): 2714.1 | learning rate: 1.682E-05 | global batch size:     4 | lm loss: 6.384643E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.474 | TFLOPs: 8.97 |
[default0]:saving checkpoint at iteration    3850 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:[2023-07-25 19:42:32,601] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3850 is ready now!
[default0]:[2023-07-25 19:42:32,540] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3850 is about to be saved!
[default0]:[2023-07-25 19:42:32,600] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3850/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:42:32,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3850/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 19:42:32,602] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3850 is ready now!
[default0]:[2023-07-25 19:42:32,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3850 is ready now!
[default0]:[2023-07-25 19:43:43,971] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3850/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:43:43,985] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3850 is ready now!
[default0]:  successfully saved checkpoint at iteration    3850 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71627.03, 71642.54)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.627
[default0]:[2023-07-25 19:43:57,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=3855, skipped=4, lr=[1.6838382933333335e-05, 1.6838382933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:43:57,487] [INFO] [timer.py:215:stop] epoch=0/micro_step=3855/global_step=3855, RunningAvgSamplesPerSec=1.5235959630077323, CurrSamplesPerSec=1.517329411618938, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3855/109863281 | consumed samples:        15420 | consumed tokens:     31580160 | elapsed time per iteration (ms): 17023.1 | learning rate: 1.684E-05 | global batch size:     4 | lm loss: 6.225510E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.235 | TFLOPs: 1.43 |
[default0]:[2023-07-25 19:44:11,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=3860, skipped=4, lr=[1.6860228266666668e-05, 1.6860228266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:44:11,038] [INFO] [timer.py:215:stop] epoch=0/micro_step=3860/global_step=3860, RunningAvgSamplesPerSec=1.5236566791116066, CurrSamplesPerSec=1.5350044113539274, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3860/109863281 | consumed samples:        15440 | consumed tokens:     31621120 | elapsed time per iteration (ms): 2700.3 | learning rate: 1.686E-05 | global batch size:     4 | lm loss: 6.471024E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.481 | TFLOPs: 9.02 |
[default0]:[2023-07-25 19:44:24,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=3865, skipped=4, lr=[1.68820736e-05, 1.68820736e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:44:24,564] [INFO] [timer.py:215:stop] epoch=0/micro_step=3865/global_step=3865, RunningAvgSamplesPerSec=1.5237196058349856, CurrSamplesPerSec=1.5715588694701197, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3865/109863281 | consumed samples:        15460 | consumed tokens:     31662080 | elapsed time per iteration (ms): 2703.3 | learning rate: 1.688E-05 | global batch size:     4 | lm loss: 6.188404E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.480 | TFLOPs: 9.01 |
[default0]:[2023-07-25 19:44:37,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=3870, skipped=4, lr=[1.6903918933333335e-05, 1.6903918933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:44:37,476] [INFO] [timer.py:215:stop] epoch=0/micro_step=3870/global_step=3870, RunningAvgSamplesPerSec=1.5238713804369457, CurrSamplesPerSec=1.7284160425273265, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3870/109863281 | consumed samples:        15480 | consumed tokens:     31703040 | elapsed time per iteration (ms): 2593.8 | learning rate: 1.690E-05 | global batch size:     4 | lm loss: 6.321745E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.542 | TFLOPs: 9.39 |
[default0]:[2023-07-25 19:44:50,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=3875, skipped=4, lr=[1.6925764266666668e-05, 1.6925764266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:44:50,853] [INFO] [timer.py:215:stop] epoch=0/micro_step=3875/global_step=3875, RunningAvgSamplesPerSec=1.5239646647690712, CurrSamplesPerSec=1.479037989726121, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3875/109863281 | consumed samples:        15500 | consumed tokens:     31744000 | elapsed time per iteration (ms): 2642.8 | learning rate: 1.693E-05 | global batch size:     4 | lm loss: 6.283244E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.514 | TFLOPs: 9.22 |
[default0]:[2023-07-25 19:45:04,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=3880, skipped=4, lr=[1.69476096e-05, 1.69476096e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:45:04,249] [INFO] [timer.py:215:stop] epoch=0/micro_step=3880/global_step=3880, RunningAvgSamplesPerSec=1.5240533755413177, CurrSamplesPerSec=1.5977356005897174, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3880/109863281 | consumed samples:        15520 | consumed tokens:     31784960 | elapsed time per iteration (ms): 2678.1 | learning rate: 1.695E-05 | global batch size:     4 | lm loss: 5.929027E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.494 | TFLOPs: 9.09 |
[default0]:[2023-07-25 19:45:17,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=3885, skipped=4, lr=[1.6969454933333334e-05, 1.6969454933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:45:17,408] [INFO] [timer.py:215:stop] epoch=0/micro_step=3885/global_step=3885, RunningAvgSamplesPerSec=1.524165854340948, CurrSamplesPerSec=1.73031647253069, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3885/109863281 | consumed samples:        15540 | consumed tokens:     31825920 | elapsed time per iteration (ms): 2622.7 | learning rate: 1.697E-05 | global batch size:     4 | lm loss: 6.216287E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.525 | TFLOPs: 9.29 |
[default0]:[2023-07-25 19:45:30,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=3890, skipped=4, lr=[1.6991300266666668e-05, 1.6991300266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:45:30,534] [INFO] [timer.py:215:stop] epoch=0/micro_step=3890/global_step=3890, RunningAvgSamplesPerSec=1.524280755716244, CurrSamplesPerSec=1.6069731603401793, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3890/109863281 | consumed samples:        15560 | consumed tokens:     31866880 | elapsed time per iteration (ms): 2623.0 | learning rate: 1.699E-05 | global batch size:     4 | lm loss: 6.294983E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.525 | TFLOPs: 9.29 |
[default0]:[2023-07-25 19:45:43,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=3895, skipped=4, lr=[1.70131456e-05, 1.70131456e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:45:43,886] [INFO] [timer.py:215:stop] epoch=0/micro_step=3895/global_step=3895, RunningAvgSamplesPerSec=1.5243693902770683, CurrSamplesPerSec=1.574748186204794, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3895/109863281 | consumed samples:        15580 | consumed tokens:     31907840 | elapsed time per iteration (ms): 2658.5 | learning rate: 1.701E-05 | global batch size:     4 | lm loss: 6.304181E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.505 | TFLOPs: 9.16 |
[default0]:[2023-07-25 19:45:57,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=3900, skipped=4, lr=[1.7034990933333334e-05, 1.7034990933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:45:57,259] [INFO] [timer.py:215:stop] epoch=0/micro_step=3900/global_step=3900, RunningAvgSamplesPerSec=1.5244535737562486, CurrSamplesPerSec=1.5395284462231384, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3900/109863281 | consumed samples:        15600 | consumed tokens:     31948800 | elapsed time per iteration (ms): 2662.0 | learning rate: 1.703E-05 | global batch size:     4 | lm loss: 6.325022E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.503 | TFLOPs: 9.15 |
[default0]:saving checkpoint at iteration    3900 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:46:09,150] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3900 is about to be saved!
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 3900 | lm loss value: 6.500800E+00 | lm loss PPL: 6.656740E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-25 19:46:09,227] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3900/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:46:09,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3900/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 19:46:09,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3900 is ready now!
[default0]:[2023-07-25 19:46:09,248] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3900 is ready now!
[default1]:[2023-07-25 19:46:09,226] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3900 is ready now!
[default0]:[2023-07-25 19:47:21,143] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3900/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:47:21,160] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3900 is ready now!
[default0]:  successfully saved checkpoint at iteration    3900 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (72160.89, 72191.54)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 72.17
[default0]:[2023-07-25 19:47:34,180] [INFO] [logging.py:96:log_dist] [Rank 0] step=3905, skipped=4, lr=[1.7056836266666667e-05, 1.7056836266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:47:34,196] [INFO] [timer.py:215:stop] epoch=0/micro_step=3905/global_step=3905, RunningAvgSamplesPerSec=1.5245909156308903, CurrSamplesPerSec=1.6330970595384071, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3905/109863281 | consumed samples:        15620 | consumed tokens:     31989760 | elapsed time per iteration (ms): 19373.5 | learning rate: 1.706E-05 | global batch size:     4 | lm loss: 6.422296E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.206 | TFLOPs: 1.26 |
[default0]:[2023-07-25 19:47:47,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=3910, skipped=4, lr=[1.70786816e-05, 1.70786816e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:47:47,900] [INFO] [timer.py:215:stop] epoch=0/micro_step=3910/global_step=3910, RunningAvgSamplesPerSec=1.5246178107523918, CurrSamplesPerSec=1.5338326095178678, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3910/109863281 | consumed samples:        15640 | consumed tokens:     32030720 | elapsed time per iteration (ms): 2741.0 | learning rate: 1.708E-05 | global batch size:     4 | lm loss: 6.269041E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.459 | TFLOPs: 8.89 |
[default0]:[2023-07-25 19:48:01,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=3915, skipped=4, lr=[1.7100526933333334e-05, 1.7100526933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:48:01,381] [INFO] [timer.py:215:stop] epoch=0/micro_step=3915/global_step=3915, RunningAvgSamplesPerSec=1.5246859692108323, CurrSamplesPerSec=1.5823515325467232, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3915/109863281 | consumed samples:        15660 | consumed tokens:     32071680 | elapsed time per iteration (ms): 2683.8 | learning rate: 1.710E-05 | global batch size:     4 | lm loss: 6.424801E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.490 | TFLOPs: 9.07 |
[default1]: iteration     3920/109863281 | consumed samples:        15680 | consumed tokens:     32112640 | elapsed time per iteration (ms): 2654.7 | learning rate: 1.712E-05 | global batch size:     4 | lm loss: 6.287421E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.507 | TFLOPs: 9.17 |
[default0]:[2023-07-25 19:48:27,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=3925, skipped=4, lr=[1.71442176e-05, 1.71442176e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:48:27,909] [INFO] [timer.py:215:stop] epoch=0/micro_step=3925/global_step=3925, RunningAvgSamplesPerSec=1.5249197302456716, CurrSamplesPerSec=1.7111002027653643, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3925/109863281 | consumed samples:        15700 | consumed tokens:     32153600 | elapsed time per iteration (ms): 2637.3 | learning rate: 1.714E-05 | global batch size:     4 | lm loss: 6.402815E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.517 | TFLOPs: 9.24 |
[default0]:[2023-07-25 19:48:41,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=3930, skipped=4, lr=[1.7166062933333334e-05, 1.7166062933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:48:41,123] [INFO] [timer.py:215:stop] epoch=0/micro_step=3930/global_step=3930, RunningAvgSamplesPerSec=1.5250371457346252, CurrSamplesPerSec=1.686546454322241, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3930/109863281 | consumed samples:        15720 | consumed tokens:     32194560 | elapsed time per iteration (ms): 2623.5 | learning rate: 1.717E-05 | global batch size:     4 | lm loss: 6.366886E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.525 | TFLOPs: 9.28 |
[default0]:[2023-07-25 19:48:55,033] [INFO] [logging.py:96:log_dist] [Rank 0] step=3935, skipped=4, lr=[1.7187908266666667e-05, 1.7187908266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:48:55,053] [INFO] [timer.py:215:stop] epoch=0/micro_step=3935/global_step=3935, RunningAvgSamplesPerSec=1.5251196384320151, CurrSamplesPerSec=1.6715497369064205, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3935/109863281 | consumed samples:        15740 | consumed tokens:     32235520 | elapsed time per iteration (ms): 2779.0 | learning rate: 1.719E-05 | global batch size:     4 | lm loss: 6.382826E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.439 | TFLOPs: 8.76 |
[default0]:[2023-07-25 19:49:08,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=3940, skipped=4, lr=[1.72097536e-05, 1.72097536e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:49:08,297] [INFO] [timer.py:215:stop] epoch=0/micro_step=3940/global_step=3940, RunningAvgSamplesPerSec=1.5252307531444558, CurrSamplesPerSec=1.6654145197817172, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3940/109863281 | consumed samples:        15760 | consumed tokens:     32276480 | elapsed time per iteration (ms): 2644.8 | learning rate: 1.721E-05 | global batch size:     4 | lm loss: 6.341672E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.512 | TFLOPs: 9.21 |
[default0]:[2023-07-25 19:49:22,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=3945, skipped=4, lr=[1.7231598933333333e-05, 1.7231598933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:49:22,028] [INFO] [timer.py:215:stop] epoch=0/micro_step=3945/global_step=3945, RunningAvgSamplesPerSec=1.5253082180322541, CurrSamplesPerSec=1.656663910697515, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3945/109863281 | consumed samples:        15780 | consumed tokens:     32317440 | elapsed time per iteration (ms): 2743.5 | learning rate: 1.723E-05 | global batch size:     4 | lm loss: 6.357847E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.458 | TFLOPs: 8.88 |
[default0]:[2023-07-25 19:49:35,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=3950, skipped=4, lr=[1.7253444266666667e-05, 1.7253444266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:49:35,148] [INFO] [timer.py:215:stop] epoch=0/micro_step=3950/global_step=3950, RunningAvgSamplesPerSec=1.5254104464195026, CurrSamplesPerSec=1.6677159712588354, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3950/109863281 | consumed samples:        15800 | consumed tokens:     32358400 | elapsed time per iteration (ms): 2615.3 | learning rate: 1.725E-05 | global batch size:     4 | lm loss: 6.261757E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.529 | TFLOPs: 9.31 |
[default0]:saving checkpoint at iteration    3950 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:49:35,352] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3950 is about to be saved!
[default1]:[2023-07-25 19:49:35,440] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3950 is ready now!
[default0]:[2023-07-25 19:49:35,437] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3950/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:49:35,438] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3950/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 19:49:35,441] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3950 is ready now!
[default0]:[2023-07-25 19:49:35,439] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3950 is ready now!
[default0]:[2023-07-25 19:50:46,720] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step3950/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:50:46,731] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3950 is ready now!
[default0]:  successfully saved checkpoint at iteration    3950 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.573
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71560.35, 71573.06)
[default0]:[2023-07-25 19:51:00,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=3955, skipped=4, lr=[1.72752896e-05, 1.72752896e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:51:00,051] [INFO] [timer.py:215:stop] epoch=0/micro_step=3955/global_step=3955, RunningAvgSamplesPerSec=1.5255189983608817, CurrSamplesPerSec=1.6022856701903063, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3955/109863281 | consumed samples:        15820 | consumed tokens:     32399360 | elapsed time per iteration (ms): 16974.3 | learning rate: 1.728E-05 | global batch size:     4 | lm loss: 6.290373E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.236 | TFLOPs: 1.43 |
[default0]:[2023-07-25 19:51:13,473] [INFO] [logging.py:96:log_dist] [Rank 0] step=3960, skipped=4, lr=[1.7297134933333333e-05, 1.7297134933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:51:13,502] [INFO] [timer.py:215:stop] epoch=0/micro_step=3960/global_step=3960, RunningAvgSamplesPerSec=1.5255890212592242, CurrSamplesPerSec=1.6162498015478497, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3960/109863281 | consumed samples:        15840 | consumed tokens:     32440320 | elapsed time per iteration (ms): 2678.0 | learning rate: 1.730E-05 | global batch size:     4 | lm loss: 6.272051E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.494 | TFLOPs: 9.09 |
[default0]:[2023-07-25 19:51:26,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=3965, skipped=4, lr=[1.7318980266666666e-05, 1.7318980266666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:51:26,375] [INFO] [timer.py:215:stop] epoch=0/micro_step=3965/global_step=3965, RunningAvgSamplesPerSec=1.5257389413683249, CurrSamplesPerSec=1.6069534587175744, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3965/109863281 | consumed samples:        15860 | consumed tokens:     32481280 | elapsed time per iteration (ms): 2572.1 | learning rate: 1.732E-05 | global batch size:     4 | lm loss: 6.465549E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.555 | TFLOPs: 9.47 |
[default0]:[2023-07-25 19:51:39,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=3970, skipped=4, lr=[1.73408256e-05, 1.73408256e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:51:39,741] [INFO] [timer.py:215:stop] epoch=0/micro_step=3970/global_step=3970, RunningAvgSamplesPerSec=1.5258133576832928, CurrSamplesPerSec=1.540624930841735, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3970/109863281 | consumed samples:        15880 | consumed tokens:     32522240 | elapsed time per iteration (ms): 2669.9 | learning rate: 1.734E-05 | global batch size:     4 | lm loss: 6.322957E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.498 | TFLOPs: 9.12 |
[default0]:[2023-07-25 19:51:53,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=3975, skipped=4, lr=[1.7362670933333333e-05, 1.7362670933333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:51:53,111] [INFO] [timer.py:215:stop] epoch=0/micro_step=3975/global_step=3975, RunningAvgSamplesPerSec=1.5258859961991913, CurrSamplesPerSec=1.5331809645655798, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3975/109863281 | consumed samples:        15900 | consumed tokens:     32563200 | elapsed time per iteration (ms): 2666.9 | learning rate: 1.736E-05 | global batch size:     4 | lm loss: 6.471230E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.500 | TFLOPs: 9.13 |
[default0]:[2023-07-25 19:52:06,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=3980, skipped=4, lr=[1.738451626666667e-05, 1.738451626666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:52:06,423] [INFO] [timer.py:215:stop] epoch=0/micro_step=3980/global_step=3980, RunningAvgSamplesPerSec=1.525982663581242, CurrSamplesPerSec=1.6757666506253803, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3980/109863281 | consumed samples:        15920 | consumed tokens:     32604160 | elapsed time per iteration (ms): 2646.6 | learning rate: 1.738E-05 | global batch size:     4 | lm loss: 6.245269E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.511 | TFLOPs: 9.20 |
[default0]:[2023-07-25 19:52:19,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=3985, skipped=4, lr=[1.7406361600000003e-05, 1.7406361600000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:52:19,668] [INFO] [timer.py:215:stop] epoch=0/micro_step=3985/global_step=3985, RunningAvgSamplesPerSec=1.5260767942633373, CurrSamplesPerSec=1.6009744089773268, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3985/109863281 | consumed samples:        15940 | consumed tokens:     32645120 | elapsed time per iteration (ms): 2639.0 | learning rate: 1.741E-05 | global batch size:     4 | lm loss: 6.474651E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.516 | TFLOPs: 9.23 |
[default0]:[2023-07-25 19:52:32,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=3990, skipped=4, lr=[1.7428206933333336e-05, 1.7428206933333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:52:32,820] [INFO] [timer.py:215:stop] epoch=0/micro_step=3990/global_step=3990, RunningAvgSamplesPerSec=1.5261803568477235, CurrSamplesPerSec=1.5637268508119213, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3990/109863281 | consumed samples:        15960 | consumed tokens:     32686080 | elapsed time per iteration (ms): 2629.6 | learning rate: 1.743E-05 | global batch size:     4 | lm loss: 6.407883E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.521 | TFLOPs: 9.26 |
[default0]:[2023-07-25 19:52:45,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=3995, skipped=4, lr=[1.745005226666667e-05, 1.745005226666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:52:45,863] [INFO] [timer.py:215:stop] epoch=0/micro_step=3995/global_step=3995, RunningAvgSamplesPerSec=1.5263070019041707, CurrSamplesPerSec=1.7776824625517473, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     3995/109863281 | consumed samples:        15980 | consumed tokens:     32727040 | elapsed time per iteration (ms): 2592.0 | learning rate: 1.745E-05 | global batch size:     4 | lm loss: 6.433249E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.543 | TFLOPs: 9.40 |
[default0]:[2023-07-25 19:52:59,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=4, lr=[1.7471897600000003e-05, 1.7471897600000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:52:59,184] [INFO] [timer.py:215:stop] epoch=0/micro_step=4000/global_step=4000, RunningAvgSamplesPerSec=1.5263914168407688, CurrSamplesPerSec=1.5677484368505565, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4000/109863281 | consumed samples:        16000 | consumed tokens:     32768000 | elapsed time per iteration (ms): 2661.1 | learning rate: 1.747E-05 | global batch size:     4 | lm loss: 6.140281E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.503 | TFLOPs: 9.15 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 4000 | lm loss value: 6.438452E+00 | lm loss PPL: 6.254377E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:[2023-07-25 19:53:10,597] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
[default0]:saving checkpoint at iteration    4000 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:53:10,556] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4000 is about to be saved!
[default0]:[2023-07-25 19:53:10,596] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step4000/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:53:10,596] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step4000/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 19:53:10,599] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
[default1]:[2023-07-25 19:53:10,599] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
[default0]:[2023-07-25 19:54:21,506] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step4000/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:54:21,529] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
[default0]:  successfully saved checkpoint at iteration    4000 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.136
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71135.67, 71137.37)
[default0]:[2023-07-25 19:54:34,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=4005, skipped=4, lr=[1.7493742933333336e-05, 1.7493742933333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:54:34,794] [INFO] [timer.py:215:stop] epoch=0/micro_step=4005/global_step=4005, RunningAvgSamplesPerSec=1.5264992313036834, CurrSamplesPerSec=1.5821054735976523, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4005/109863281 | consumed samples:        16020 | consumed tokens:     32808960 | elapsed time per iteration (ms): 19103.8 | learning rate: 1.749E-05 | global batch size:     4 | lm loss: 6.207725E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.209 | TFLOPs: 1.27 |
[default0]:[2023-07-25 19:54:47,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=4010, skipped=4, lr=[1.751558826666667e-05, 1.751558826666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:54:47,964] [INFO] [timer.py:215:stop] epoch=0/micro_step=4010/global_step=4010, RunningAvgSamplesPerSec=1.5265962761099605, CurrSamplesPerSec=1.5608788890057004, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4010/109863281 | consumed samples:        16040 | consumed tokens:     32849920 | elapsed time per iteration (ms): 2629.6 | learning rate: 1.752E-05 | global batch size:     4 | lm loss: 6.269504E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.521 | TFLOPs: 9.26 |
[default0]:[2023-07-25 19:55:01,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=4015, skipped=4, lr=[1.7537433600000002e-05, 1.7537433600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:55:01,297] [INFO] [timer.py:215:stop] epoch=0/micro_step=4015/global_step=4015, RunningAvgSamplesPerSec=1.526674775767775, CurrSamplesPerSec=1.5648493307433484, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4015/109863281 | consumed samples:        16060 | consumed tokens:     32890880 | elapsed time per iteration (ms): 2662.1 | learning rate: 1.754E-05 | global batch size:     4 | lm loss: 6.249354E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.503 | TFLOPs: 9.15 |
[default0]:[2023-07-25 19:55:14,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=4020, skipped=4, lr=[1.7559278933333336e-05, 1.7559278933333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:55:14,139] [INFO] [timer.py:215:stop] epoch=0/micro_step=4020/global_step=4020, RunningAvgSamplesPerSec=1.5268476139106992, CurrSamplesPerSec=1.6352872052534375, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4020/109863281 | consumed samples:        16080 | consumed tokens:     32931840 | elapsed time per iteration (ms): 2558.3 | learning rate: 1.756E-05 | global batch size:     4 | lm loss: 6.199706E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.564 | TFLOPs: 9.52 |
[default0]:[2023-07-25 19:55:27,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=4025, skipped=4, lr=[1.758112426666667e-05, 1.758112426666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:55:27,374] [INFO] [timer.py:215:stop] epoch=0/micro_step=4025/global_step=4025, RunningAvgSamplesPerSec=1.5269454126366693, CurrSamplesPerSec=1.4626430160414399, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4025/109863281 | consumed samples:        16100 | consumed tokens:     32972800 | elapsed time per iteration (ms): 2645.7 | learning rate: 1.758E-05 | global batch size:     4 | lm loss: 6.316130E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.512 | TFLOPs: 9.21 |
[default0]:[2023-07-25 19:55:40,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=4030, skipped=4, lr=[1.7602969600000002e-05, 1.7602969600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:55:40,710] [INFO] [timer.py:215:stop] epoch=0/micro_step=4030/global_step=4030, RunningAvgSamplesPerSec=1.527041064731735, CurrSamplesPerSec=1.602251087476798, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4030/109863281 | consumed samples:        16120 | consumed tokens:     33013760 | elapsed time per iteration (ms): 2650.6 | learning rate: 1.760E-05 | global batch size:     4 | lm loss: 6.179762E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.509 | TFLOPs: 9.19 |
[default0]:[2023-07-25 19:55:54,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=4035, skipped=4, lr=[1.7624814933333335e-05, 1.7624814933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:55:54,191] [INFO] [timer.py:215:stop] epoch=0/micro_step=4035/global_step=4035, RunningAvgSamplesPerSec=1.5270993134555242, CurrSamplesPerSec=1.6345369698040793, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4035/109863281 | consumed samples:        16140 | consumed tokens:     33054720 | elapsed time per iteration (ms): 2700.3 | learning rate: 1.762E-05 | global batch size:     4 | lm loss: 6.269852E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.481 | TFLOPs: 9.02 |
[default0]:[2023-07-25 19:56:07,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=4040, skipped=4, lr=[1.764666026666667e-05, 1.764666026666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:56:07,982] [INFO] [timer.py:215:stop] epoch=0/micro_step=4040/global_step=4040, RunningAvgSamplesPerSec=1.5271293597145215, CurrSamplesPerSec=1.568680720688105, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4040/109863281 | consumed samples:        16160 | consumed tokens:     33095680 | elapsed time per iteration (ms): 2749.3 | learning rate: 1.765E-05 | global batch size:     4 | lm loss: 6.267279E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.455 | TFLOPs: 8.86 |
[default0]:[2023-07-25 19:56:21,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=4045, skipped=4, lr=[1.7668505600000002e-05, 1.7668505600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:56:21,523] [INFO] [timer.py:215:stop] epoch=0/micro_step=4045/global_step=4045, RunningAvgSamplesPerSec=1.527169913895316, CurrSamplesPerSec=1.5266071821953142, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4045/109863281 | consumed samples:        16180 | consumed tokens:     33136640 | elapsed time per iteration (ms): 2699.5 | learning rate: 1.767E-05 | global batch size:     4 | lm loss: 6.450665E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.482 | TFLOPs: 9.02 |
[default0]:[2023-07-25 19:56:34,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=4050, skipped=4, lr=[1.7690350933333335e-05, 1.7690350933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:56:34,666] [INFO] [timer.py:215:stop] epoch=0/micro_step=4050/global_step=4050, RunningAvgSamplesPerSec=1.5272609280065992, CurrSamplesPerSec=1.5228733964513885, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4050/109863281 | consumed samples:        16200 | consumed tokens:     33177600 | elapsed time per iteration (ms): 2624.2 | learning rate: 1.769E-05 | global batch size:     4 | lm loss: 6.090639E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.524 | TFLOPs: 9.28 |
[default0]:saving checkpoint at iteration    4050 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 19:56:34,840] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4050 is about to be saved!
[default0]:[2023-07-25 19:56:34,896] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step4050/mp_rank_00_model_states.pt
[default0]:[2023-07-25 19:56:34,896] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step4050/mp_rank_00_model_states.pt...
[default1]:[2023-07-25 19:56:34,897] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4050 is ready now!
[default0]:[2023-07-25 19:56:34,908] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4050 is ready now!
[default1]:[2023-07-25 19:56:34,898] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4050 is ready now!
[default0]:[2023-07-25 19:57:46,150] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step4050/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 19:57:46,168] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4050 is ready now!
[default0]:  successfully saved checkpoint at iteration    4050 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.504
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71502.31, 71507.47)
[default0]:[2023-07-25 19:57:59,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=4055, skipped=4, lr=[1.771219626666667e-05, 1.771219626666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:57:59,627] [INFO] [timer.py:215:stop] epoch=0/micro_step=4055/global_step=4055, RunningAvgSamplesPerSec=1.527338701183047, CurrSamplesPerSec=1.4954882777227796, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4055/109863281 | consumed samples:        16220 | consumed tokens:     33218560 | elapsed time per iteration (ms): 16993.4 | learning rate: 1.771E-05 | global batch size:     4 | lm loss: 6.254271E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.235 | TFLOPs: 1.43 |
[default0]:[2023-07-25 19:58:12,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=4060, skipped=4, lr=[1.7734041600000002e-05, 1.7734041600000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:58:12,734] [INFO] [timer.py:215:stop] epoch=0/micro_step=4060/global_step=4060, RunningAvgSamplesPerSec=1.5274462526298938, CurrSamplesPerSec=1.615624895576045, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4060/109863281 | consumed samples:        16240 | consumed tokens:     33259520 | elapsed time per iteration (ms): 2600.7 | learning rate: 1.773E-05 | global batch size:     4 | lm loss: 6.308561E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.538 | TFLOPs: 9.37 |
[default0]:[2023-07-25 19:58:25,798] [INFO] [logging.py:96:log_dist] [Rank 0] step=4065, skipped=4, lr=[1.7755886933333335e-05, 1.7755886933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:58:25,822] [INFO] [timer.py:215:stop] epoch=0/micro_step=4065/global_step=4065, RunningAvgSamplesPerSec=1.527565518297958, CurrSamplesPerSec=1.614069012996566, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4065/109863281 | consumed samples:        16260 | consumed tokens:     33300480 | elapsed time per iteration (ms): 2615.1 | learning rate: 1.776E-05 | global batch size:     4 | lm loss: 6.238409E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.530 | TFLOPs: 9.31 |
[default0]:[2023-07-25 19:58:39,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=4070, skipped=4, lr=[1.7777732266666668e-05, 1.7777732266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:58:39,026] [INFO] [timer.py:215:stop] epoch=0/micro_step=4070/global_step=4070, RunningAvgSamplesPerSec=1.52766989078457, CurrSamplesPerSec=1.6421942063210302, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4070/109863281 | consumed samples:        16280 | consumed tokens:     33341440 | elapsed time per iteration (ms): 2631.1 | learning rate: 1.778E-05 | global batch size:     4 | lm loss: 6.212513E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.520 | TFLOPs: 9.26 |
[default0]:[2023-07-25 19:58:52,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=4075, skipped=4, lr=[1.77995776e-05, 1.77995776e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:58:52,156] [INFO] [timer.py:215:stop] epoch=0/micro_step=4075/global_step=4075, RunningAvgSamplesPerSec=1.5277628979396094, CurrSamplesPerSec=1.7079662666498352, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4075/109863281 | consumed samples:        16300 | consumed tokens:     33382400 | elapsed time per iteration (ms): 2623.3 | learning rate: 1.780E-05 | global batch size:     4 | lm loss: 6.259368E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.525 | TFLOPs: 9.28 |
[default0]:[2023-07-25 19:59:05,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=4080, skipped=4, lr=[1.7821422933333335e-05, 1.7821422933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:59:05,551] [INFO] [timer.py:215:stop] epoch=0/micro_step=4080/global_step=4080, RunningAvgSamplesPerSec=1.5278331774382723, CurrSamplesPerSec=1.6128934447497616, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4080/109863281 | consumed samples:        16320 | consumed tokens:     33423360 | elapsed time per iteration (ms): 2675.6 | learning rate: 1.782E-05 | global batch size:     4 | lm loss: 6.377079E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.495 | TFLOPs: 9.10 |
[default0]:[2023-07-25 19:59:19,070] [INFO] [logging.py:96:log_dist] [Rank 0] step=4085, skipped=4, lr=[1.7843268266666668e-05, 1.7843268266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:59:19,103] [INFO] [timer.py:215:stop] epoch=0/micro_step=4085/global_step=4085, RunningAvgSamplesPerSec=1.5278899935021886, CurrSamplesPerSec=1.5209221327265556, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4085/109863281 | consumed samples:        16340 | consumed tokens:     33464320 | elapsed time per iteration (ms): 2695.5 | learning rate: 1.784E-05 | global batch size:     4 | lm loss: 6.336736E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.484 | TFLOPs: 9.04 |
[default0]:[2023-07-25 19:59:32,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=4090, skipped=4, lr=[1.78651136e-05, 1.78651136e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:59:32,140] [INFO] [timer.py:215:stop] epoch=0/micro_step=4090/global_step=4090, RunningAvgSamplesPerSec=1.5280216228753527, CurrSamplesPerSec=1.557299087612888, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4090/109863281 | consumed samples:        16360 | consumed tokens:     33505280 | elapsed time per iteration (ms): 2598.7 | learning rate: 1.787E-05 | global batch size:     4 | lm loss: 6.207560E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.539 | TFLOPs: 9.37 |
[default0]:[2023-07-25 19:59:45,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=4095, skipped=4, lr=[1.7886958933333335e-05, 1.7886958933333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:59:45,460] [INFO] [timer.py:215:stop] epoch=0/micro_step=4095/global_step=4095, RunningAvgSamplesPerSec=1.5281113354483624, CurrSamplesPerSec=1.541703569240869, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4095/109863281 | consumed samples:        16380 | consumed tokens:     33546240 | elapsed time per iteration (ms): 2651.5 | learning rate: 1.789E-05 | global batch size:     4 | lm loss: 6.208384E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.509 | TFLOPs: 9.19 |
[default0]:[2023-07-25 19:59:59,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=4100, skipped=4, lr=[1.7908804266666668e-05, 1.7908804266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 19:59:59,119] [INFO] [timer.py:215:stop] epoch=0/micro_step=4100/global_step=4100, RunningAvgSamplesPerSec=1.5281477330023052, CurrSamplesPerSec=1.5337650223982393, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4100/109863281 | consumed samples:        16400 | consumed tokens:     33587200 | elapsed time per iteration (ms): 2723.5 | learning rate: 1.791E-05 | global batch size:     4 | lm loss: 6.379895E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.469 | TFLOPs: 8.94 |
[default1]:------------------------------------------------------------------------------------------------
[default1]: validation loss at iteration 4100 | lm loss value: 6.460151E+00 | lm loss PPL: 6.391577E+02 | 
[default1]:------------------------------------------------------------------------------------------------
[default0]:saving checkpoint at iteration    4100 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 20:00:10,936] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4100 is about to be saved!
[default1]:[2023-07-25 20:00:11,011] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4100 is ready now!
[default0]:[2023-07-25 20:00:11,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4100 is ready now!
[default1]:[2023-07-25 20:00:11,011] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4100 is ready now!
[default0]:[2023-07-25 20:00:11,008] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step4100/mp_rank_00_model_states.pt
[default0]:[2023-07-25 20:00:11,008] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step4100/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 20:01:22,545] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step4100/mp_rank_00_model_states.pt.
[default0]:[2023-07-25 20:01:22,567] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4100 is ready now!
[default0]:  successfully saved checkpoint at iteration    4100 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default1]:(min, max) time across ranks (ms):
[default1]:    save-checkpoint ................................: (71795.41, 71811.05)
[default0]:Checkpoint Save GB: 18.421, GB/Sec: 0.26, Latency(second): 71.796
[default0]:[2023-07-25 20:01:36,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=4105, skipped=4, lr=[1.79306496e-05, 1.79306496e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 20:01:36,255] [INFO] [timer.py:215:stop] epoch=0/micro_step=4105/global_step=4105, RunningAvgSamplesPerSec=1.5281806470678585, CurrSamplesPerSec=1.5403860197881554, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4105/109863281 | consumed samples:        16420 | consumed tokens:     33628160 | elapsed time per iteration (ms): 19413.9 | learning rate: 1.793E-05 | global batch size:     4 | lm loss: 6.285315E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 0.206 | TFLOPs: 1.25 |
[default0]:[2023-07-25 20:01:49,418] [INFO] [logging.py:96:log_dist] [Rank 0] step=4110, skipped=4, lr=[1.7952494933333334e-05, 1.7952494933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 20:01:49,443] [INFO] [timer.py:215:stop] epoch=0/micro_step=4110/global_step=4110, RunningAvgSamplesPerSec=1.5282785068873759, CurrSamplesPerSec=1.6229427383413133, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4110/109863281 | consumed samples:        16440 | consumed tokens:     33669120 | elapsed time per iteration (ms): 2625.5 | learning rate: 1.795E-05 | global batch size:     4 | lm loss: 6.137924E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.524 | TFLOPs: 9.28 |
[default0]:[2023-07-25 20:02:02,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=4115, skipped=4, lr=[1.7974340266666668e-05, 1.7974340266666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 20:02:02,942] [INFO] [timer.py:215:stop] epoch=0/micro_step=4115/global_step=4115, RunningAvgSamplesPerSec=1.5283291655389022, CurrSamplesPerSec=1.509504121189208, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4115/109863281 | consumed samples:        16460 | consumed tokens:     33710080 | elapsed time per iteration (ms): 2702.1 | learning rate: 1.797E-05 | global batch size:     4 | lm loss: 6.229207E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.480 | TFLOPs: 9.01 |
[default0]:[2023-07-25 20:02:16,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=4120, skipped=4, lr=[1.79961856e-05, 1.79961856e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 20:02:16,106] [INFO] [timer.py:215:stop] epoch=0/micro_step=4120/global_step=4120, RunningAvgSamplesPerSec=1.5284460691694068, CurrSamplesPerSec=1.545500621711236, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4120/109863281 | consumed samples:        16480 | consumed tokens:     33751040 | elapsed time per iteration (ms): 2621.3 | learning rate: 1.800E-05 | global batch size:     4 | lm loss: 6.298927E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.526 | TFLOPs: 9.29 |
[default0]:[2023-07-25 20:02:29,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=4125, skipped=4, lr=[1.8018030933333334e-05, 1.8018030933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 20:02:29,203] [INFO] [timer.py:215:stop] epoch=0/micro_step=4125/global_step=4125, RunningAvgSamplesPerSec=1.5285440147601617, CurrSamplesPerSec=1.6335628035810368, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4125/109863281 | consumed samples:        16500 | consumed tokens:     33792000 | elapsed time per iteration (ms): 2617.6 | learning rate: 1.802E-05 | global batch size:     4 | lm loss: 6.258023E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.528 | TFLOPs: 9.30 |
[default0]:[2023-07-25 20:02:42,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=4130, skipped=4, lr=[1.8039876266666667e-05, 1.8039876266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 20:02:42,480] [INFO] [timer.py:215:stop] epoch=0/micro_step=4130/global_step=4130, RunningAvgSamplesPerSec=1.528649840408738, CurrSamplesPerSec=1.6186978858979817, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4130/109863281 | consumed samples:        16520 | consumed tokens:     33832960 | elapsed time per iteration (ms): 2649.0 | learning rate: 1.804E-05 | global batch size:     4 | lm loss: 6.059389E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.510 | TFLOPs: 9.19 |
[default0]:[2023-07-25 20:02:55,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=4135, skipped=4, lr=[1.80617216e-05, 1.80617216e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 20:02:55,466] [INFO] [timer.py:215:stop] epoch=0/micro_step=4135/global_step=4135, RunningAvgSamplesPerSec=1.5287789687622062, CurrSamplesPerSec=1.6202672474585686, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4135/109863281 | consumed samples:        16540 | consumed tokens:     33873920 | elapsed time per iteration (ms): 2583.0 | learning rate: 1.806E-05 | global batch size:     4 | lm loss: 6.227947E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.549 | TFLOPs: 9.43 |
[default0]:[2023-07-25 20:03:08,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=4140, skipped=4, lr=[1.8083566933333334e-05, 1.8083566933333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 20:03:08,314] [INFO] [timer.py:215:stop] epoch=0/micro_step=4140/global_step=4140, RunningAvgSamplesPerSec=1.5289254014265337, CurrSamplesPerSec=1.6615227501001735, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4140/109863281 | consumed samples:        16560 | consumed tokens:     33914880 | elapsed time per iteration (ms): 2562.0 | learning rate: 1.808E-05 | global batch size:     4 | lm loss: 6.142134E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.561 | TFLOPs: 9.51 |
[default0]:[2023-07-25 20:03:21,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=4145, skipped=4, lr=[1.8105412266666667e-05, 1.8105412266666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 20:03:21,769] [INFO] [timer.py:215:stop] epoch=0/micro_step=4145/global_step=4145, RunningAvgSamplesPerSec=1.528993933952189, CurrSamplesPerSec=1.6161428406429694, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4145/109863281 | consumed samples:        16580 | consumed tokens:     33955840 | elapsed time per iteration (ms): 2694.8 | learning rate: 1.811E-05 | global batch size:     4 | lm loss: 6.090470E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.484 | TFLOPs: 9.04 |
[default0]:[2023-07-25 20:03:35,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=4150, skipped=4, lr=[1.81272576e-05, 1.81272576e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
[default0]:[2023-07-25 20:03:35,342] [INFO] [timer.py:215:stop] epoch=0/micro_step=4150/global_step=4150, RunningAvgSamplesPerSec=1.529048193529335, CurrSamplesPerSec=1.662602227860284, MemAllocated=17.28GB, MaxMemAllocated=29.54GB
[default1]: iteration     4150/109863281 | consumed samples:        16600 | consumed tokens:     33996800 | elapsed time per iteration (ms): 2704.0 | learning rate: 1.813E-05 | global batch size:     4 | lm loss: 6.177362E+00 | loss scale: 8192.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.479 | TFLOPs: 9.01 |
[default0]:saving checkpoint at iteration    4150 to /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true
[default0]:[2023-07-25 20:03:35,568] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4150 is about to be saved!
[default1]:[2023-07-25 20:03:35,611] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4150 is ready now!
[default1]:[2023-07-25 20:03:35,614] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4150 is ready now!
[default0]:[2023-07-25 20:03:35,612] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step4150/mp_rank_00_model_states.pt
[default0]:[2023-07-25 20:03:35,612] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /scratch/hpc-prf-lola/nikit/repos/Megatron-DeepSpeed-Microsoft/lola_ws/output/checkpoint/gpt-1.3B-lr-2.0e-4-minlr-2e-06-bs-4-gpus-4-mp-1-pp-1-ep-128-mlc-0.01-cap-1.0-drop-true/global_step4150/mp_rank_00_model_states.pt...
[default0]:[2023-07-25 20:03:35,631] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4150 is ready now!
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 144522 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 144523 closing signal SIGTERM
slurmstepd: error: *** JOB 4470044 ON n2gpu1211 CANCELLED AT 2023-07-25T20:03:49 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 4470044.0 ON n2gpu1211 CANCELLED AT 2023-07-25T20:03:49 DUE TO TIME LIMIT ***
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3555029 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3555030 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 144522 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 144523 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3555029 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3555030 closing signal SIGTERM
